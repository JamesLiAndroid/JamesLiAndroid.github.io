<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于k8s中10250端口报CVE-2016-2183漏洞的问题解决</title>
      <link href="/2022/01/02/guan-yu-k8s-zhong-10250-duan-kou-bao-cve-2016-2183-lou-dong-de-wen-ti-jie-jue/"/>
      <url>/2022/01/02/guan-yu-k8s-zhong-10250-duan-kou-bao-cve-2016-2183-lou-dong-de-wen-ti-jie-jue/</url>
      
        <content type="html"><![CDATA[<h1 id="关于k8s中10250端口报CVE-2016-2183漏洞的问题解决"><a href="#关于k8s中10250端口报CVE-2016-2183漏洞的问题解决" class="headerlink" title="关于k8s中10250端口报CVE-2016-2183漏洞的问题解决"></a>关于k8s中10250端口报CVE-2016-2183漏洞的问题解决</h1><h2 id="问题以及现象"><a href="#问题以及现象" class="headerlink" title="问题以及现象"></a>问题以及现象</h2><p>在之前使用kubeadm安装k8s集群后，甲方内部爆出了CVE安全漏洞的问题，报告是这么描述的：</p><p><img src="CVE%E6%BC%8F%E6%B4%9E%E8%AF%A6%E6%83%85.png" alt></p><p><img src="CVE%E6%BC%8F%E6%B4%9E%E6%B6%89%E5%8F%8A%E7%AB%AF%E5%8F%A3%E5%8F%B7.png" alt></p><p>这里看到10250端口检测到了该问题。之前以为是k8s版本的问题，在甲方环境中实践了两次，使用kubeadm的方式，分别部署1.11.0和1.20.14两个版本的k8s集群，结果发现这个问题还是存在。因而纠正了我的一个误区，以为该证书生成和kubeadm内部的生成机制有关，实际是和操作系统中的openssl组件有关。</p><h2 id="漏洞复现"><a href="#漏洞复现" class="headerlink" title="漏洞复现"></a>漏洞复现</h2><p>使用nmap工具进行漏洞复现，安装nmap 7.92.1版本，注意不要使用系统中自带的6.x版本的nmap，展示信息不全且有编码，不方便查看。这里使用rpm包进行安装：</p><pre><code>$ sudo yum localinstall nmap-7.92-1.x86_64.rpm</code></pre><p>安装完成后，随便找一台k8s节点，测试10250端口号。如下：</p><pre><code>$ nmap -sV --script ssl-enum-ciphers -p 10250 10.14.200.135 -oG myscanStarting Nmap 7.92 ( https://nmap.org ) at 2022-01-02 10:28 CSTNmap scan report for k8s-node-04 (10.14.200.135)Host is up (0.00041s latency).PORT      STATE SERVICE  VERSION10250/tcp open  ssl/http Golang net/http server (Go-IPFS json-rpc or InfluxDB API)| ssl-enum-ciphers: |   TLSv1.2: |     ciphers: |       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA (secp256r1) - A|       TLS_RSA_WITH_AES_128_GCM_SHA256 (rsa 2048) - A|       TLS_RSA_WITH_AES_256_GCM_SHA384 (rsa 2048) - A|       TLS_RSA_WITH_AES_128_CBC_SHA (rsa 2048) - A|       TLS_RSA_WITH_AES_256_CBC_SHA (rsa 2048) - A|       TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA (secp256r1) - C|       TLS_RSA_WITH_3DES_EDE_CBC_SHA (rsa 2048) - C|     compressors: |       NULL|     cipher preference: server|     warnings: |       64-bit block cipher 3DES vulnerable to SWEET32 attack|   TLSv1.3: |     ciphers: |       TLS_AKE_WITH_AES_128_GCM_SHA256 (ecdh_x25519) - A|       TLS_AKE_WITH_CHACHA20_POLY1305_SHA256 (ecdh_x25519) - A|       TLS_AKE_WITH_AES_256_GCM_SHA384 (ecdh_x25519) - A|     cipher preference: server|_  least strength: CService detection performed. Please report any incorrect results at https://nmap.org/submit/ .Nmap done: 1 IP address (1 host up) scanned in 17.64 seconds</code></pre><p>结果显示，3DES加密是C级别的,并且有个warning跟CVE-2016-2183的描述大概一致。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol><li>拆掉k8s集群，并进行初始化操作</li></ol><p>这里因为当前集群并没有业务服务，因此可以肆无忌惮的拆掉，执行也比较粗暴。所有机器同时执行以下命令：</p><pre><code>$ sudo kubeadm reset -f$ sudo ipvsadm --clear</code></pre><p>另外在kubectl执行的机器上，删掉~/.kube/目录下的所有文件。</p><ol start="2"><li>升级openssl组件至1.11版本，编译安装</li></ol><p>下载openssl源码包，完成后创建~/openssl-install/目录并把该源码包放到该目录下。</p><pre><code>$ wget https://www.openssl.org/source/openssl-1.1.1l.tar.gz</code></pre><p>安装脚本如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># !/bin/bash</span><span class="token comment" spellcheck="true"># 升级openssl组件脚本，升级到1.1.1</span><span class="token comment" spellcheck="true"># 在~/openssl-install目录下操作</span><span class="token comment" spellcheck="true"># 以管理员身份运行</span><span class="token function">tar</span> -zxvf openssl-1.1.1l.tar.gz <span class="token operator">&amp;&amp;</span> <span class="token function">cd</span> openssl-1.1.1l/./config --prefix<span class="token operator">=</span>/usr/local/openssl<span class="token function">make</span> <span class="token operator">&amp;&amp;</span> <span class="token function">make</span> <span class="token function">install</span><span class="token function">mv</span> /usr/bin/openssl /usr/bin/openssl.bak<span class="token function">ln</span> -sf /usr/local/openssl/bin/openssl /usr/bin/openssl<span class="token keyword">echo</span> <span class="token string">"/usr/local/openssl/lib"</span> <span class="token operator">>></span> /etc/ld.so.confldconfig -v <span class="token operator">|</span> <span class="token function">grep</span> openssl<span class="token keyword">echo</span> <span class="token string">"----已完成openssl的安装，版本信息如下----"</span>openssl version<span class="token keyword">echo</span> <span class="token string">"----------------------------------------"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li>重建k8s集群</li></ol><p>注意重建k8s集群之前，务必重启所有服务器上的kubelet服务。</p><pre><code>$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart kubelet</code></pre><p>之后按照k8s集群创建步骤执行，这里不再赘述。</p><ol start="4"><li>重新测试</li></ol><p>重新使用nmap进行测试，测试结果如下：</p><pre><code>$ nmap -sV --script ssl-enum-ciphers -p 10250 10.14.200.135 -oG myscan</code></pre><p>结果还是不行。于是我找了个大佬咨询了一下：</p><p><img src="%E5%A4%A7%E4%BD%AC%E7%9A%84%E5%9B%9E%E5%A4%8D.png" alt></p><p>大佬告诉我，使用<a href="https://www.cisecurity.org/benchmark/kubernetes/" target="_blank" rel="noopener">CIS Benchmark</a>来进行测试，进而找到了<a href="https://github.com/aquasecurity/kube-bench" target="_blank" rel="noopener">kube-bench</a>这个工具！</p><ol start="5"><li>使用kube-bench进行k8s集群的扫描</li></ol><pre><code>$ docker pull aquasec/kube-bench:latest// 将扫描后的结果放到文件中$ docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config  -t aquasec/kube-bench:latest --version 1.20 &gt; k8s_check.txt</code></pre><p>扫描完成后，查看文件中的内容，发现两处和TLS加密方式相关的修改建议，如下：</p><pre><code>// 针对apiserver的修改建议1.2.34 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yamlon the master node and set the below parameter.--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384// 针对kubelet的修改建议4.2.13 If using a Kubelet config file, edit the file to set TLSCipherSuites: toTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256or to a subset of these values.If using executable arguments, edit the kubelet service file/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node andset the --tls-cipher-suites parameter as follows, or to a subset of these values.--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256Based on your system, restart the kubelet service. For example:systemctl daemon-reloadsystemctl restart kubelet.service</code></pre><p>根据上述两条修改建议，首先修改kubelet的配置文件，修改如下：</p><pre><code>$ sudo vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf// 上述文件找不到，于是切换下路径$ sudo vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf# Note: This dropin only works with kubeadm and kubelet v1.11+[Service]Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;# This is a file that &quot;kubeadm init&quot; and &quot;kubeadm join&quot; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamicallyEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.EnvironmentFile=-/etc/sysconfig/kubeletExecStart=ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS// 上面的文件不适合，重新找一下/var/lib/kubelet/config.yaml$ sudo vim /var/lib/kubelet/config.yaml// 在文件最后追加tlsCipherSuites:- TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256- TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256- TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305- TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384- TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305- TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384- TLS_RSA_WITH_AES_256_GCM_SHA384- TLS_RSA_WITH_AES_128_GCM_SHA256// :wq保存退出// 重启kubelet$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart kubelet</code></pre><p>然后修改apiserver的配置，修改如下：</p><pre><code>$ sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml......spec:  containers:  - command:    - kube-apiserver    - --advertise-address=10.14.200.130    - --allow-privileged=true    - --authorization-mode=Node,RBAC    - --client-ca-file=/etc/kubernetes/pki/ca.crt    - --enable-admission-plugins=NodeRestriction    - --enable-bootstrap-token-auth=true    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key    - --etcd-servers=https://127.0.0.1:2379    - --insecure-port=0    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key    - --requestheader-allowed-names=front-proxy-client    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt    - --requestheader-extra-headers-prefix=X-Remote-Extra-    - --requestheader-group-headers=X-Remote-Group    - --requestheader-username-headers=X-Remote-User    - --secure-port=6443    - --service-account-issuer=https://kubernetes.default.svc.cluster.local    - --service-account-key-file=/etc/kubernetes/pki/sa.pub    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key    - --service-cluster-ip-range=10.96.0.0/12    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key    # 追加这一句配置信息    - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384......// :wq保存退出</code></pre><p>修改并保存后，apisever的容器会自动重启。查看apiserver的镜像状态以及当前的集群状态：</p><pre><code>$ kubectl get pod -ANAMESPACE     NAME                                    READY   STATUS    RESTARTS   AGEkube-system   coredns-54d67798b7-8gppz                1/1     Running   0          3h49mkube-system   coredns-54d67798b7-s7lqz                1/1     Running   0          3h49mkube-system   etcd-k8s-master-01                      1/1     Running   0          3h49mkube-system   etcd-k8s-master-02                      1/1     Running   0          3h48mkube-system   etcd-k8s-master-03                      1/1     Running   0          3h47mkube-system   kube-apiserver-k8s-master-01            1/1     Running   0          17mkube-system   kube-apiserver-k8s-master-02            1/1     Running   0          17mkube-system   kube-apiserver-k8s-master-03            1/1     Running   0          16mkube-system   kube-controller-manager-k8s-master-01   1/1     Running   1          3h49mkube-system   kube-controller-manager-k8s-master-02   1/1     Running   1          3h48mkube-system   kube-controller-manager-k8s-master-03   0/1     Running   0          3h47mkube-system   kube-proxy-4rlbq                        1/1     Running   0          3h45mkube-system   kube-proxy-6gdb2                        1/1     Running   0          3h45mkube-system   kube-proxy-fdzdc                        1/1     Running   0          3h48mkube-system   kube-proxy-m9t4z                        1/1     Running   0          3h46mkube-system   kube-proxy-qmmqz                        1/1     Running   0          3h47mkube-system   kube-proxy-r6lp9                        1/1     Running   0          3h49mkube-system   kube-proxy-tk7zw                        1/1     Running   0          3h46mkube-system   kube-proxy-w97pr                        1/1     Running   0          3h45mkube-system   kube-scheduler-k8s-master-01            0/1     Running   1          3h49mkube-system   kube-scheduler-k8s-master-02            1/1     Running   1          3h48mkube-system   kube-scheduler-k8s-master-03            1/1     Running   0          3h47m$ kubectl get nodesNAME            STATUS   ROLES                  AGE     VERSIONk8s-master-01   Ready    control-plane,master   3h50m   v1.20.14k8s-master-02   Ready    control-plane,master   3h48m   v1.20.14k8s-master-03   Ready    control-plane,master   3h47m   v1.20.14k8s-node-01     Ready    &lt;none&gt;                 3h46m   v1.20.14k8s-node-02     Ready    &lt;none&gt;                 3h47m   v1.20.14k8s-node-03     Ready    &lt;none&gt;                 3h46m   v1.20.14k8s-node-04     Ready    &lt;none&gt;                 3h46m   v1.20.14k8s-node-05     Ready    &lt;none&gt;                 3h46m   v1.20.14</code></pre><p>如果有处于0/1状态的pod，请等待下，等待其恢复正常即可！</p><ol start="5"><li>再次重新测试</li></ol><p>重新使用nmap进行测试，测试结果如下：</p><pre><code>$ nmap -sV --script ssl-enum-ciphers -p 10250 10.14.200.135 -oG myscanStarting Nmap 7.92 ( https://nmap.org ) at 2022-01-02 12:39 CSTNmap scan report for k8s-node-04 (10.14.200.135)Host is up (0.00040s latency).PORT      STATE SERVICE  VERSION10250/tcp open  ssl/http Golang net/http server (Go-IPFS json-rpc or InfluxDB API)| ssl-enum-ciphers: |   TLSv1.2: |     ciphers: |       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 (secp256r1) - A|       TLS_RSA_WITH_AES_256_GCM_SHA384 (rsa 2048) - A|       TLS_RSA_WITH_AES_128_GCM_SHA256 (rsa 2048) - A|     compressors: |       NULL|     cipher preference: server|   TLSv1.3: |     ciphers: |       TLS_AKE_WITH_AES_128_GCM_SHA256 (ecdh_x25519) - A|       TLS_AKE_WITH_CHACHA20_POLY1305_SHA256 (ecdh_x25519) - A|       TLS_AKE_WITH_AES_256_GCM_SHA384 (ecdh_x25519) - A|     cipher preference: server|_  least strength: AService detection performed. Please report any incorrect results at https://nmap.org/submit/ .Nmap done: 1 IP address (1 host up) scanned in 17.53 seconds</code></pre><p>这样状态就正常了！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>探索过程略为艰难，还是得多关注k8s安全方面的内容！</li><li>日常还得多积累，多去关注业内比较先进的发展趋势！</li><li>多做业务，给予更多实践机会，多去做，少去说！</li></ol><h2 id="参考连接"><a href="#参考连接" class="headerlink" title="参考连接"></a>参考连接</h2><ul><li><a href="https://www.openssl.org/news/secadv/20160922.txt" target="_blank" rel="noopener">CVE-2016-2183说明</a></li><li><a href="https://segmentfault.com/a/1190000038486901" target="_blank" rel="noopener">nmap漏洞复现</a></li><li><a href="https://www.cnblogs.com/itbsl/p/11275728.html" target="_blank" rel="noopener">openssl安装</a></li><li><a href="https://blog.fleeto.us/post/k8s-and-cvs-2016-2183/" target="_blank" rel="noopener">大佬的指南</a></li><li><a href="https://github.com/aquasecurity/kube-bench/blob/main/docs/running.md" target="_blank" rel="noopener">kube-bench使用</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>xxl-job在k8s中出现调度机器ip变化的问题总结</title>
      <link href="/2021/12/23/xxl-job-zai-k8s-zhong-chu-xian-diao-du-ji-qi-ip-bian-hua-de-wen-ti-zong-jie/"/>
      <url>/2021/12/23/xxl-job-zai-k8s-zhong-chu-xian-diao-du-ji-qi-ip-bian-hua-de-wen-ti-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="xxl-job-在k8s中ip地址更换导致定时任务无法调用的问题"><a href="#xxl-job-在k8s中ip地址更换导致定时任务无法调用的问题" class="headerlink" title="xxl-job 在k8s中ip地址更换导致定时任务无法调用的问题"></a>xxl-job 在k8s中ip地址更换导致定时任务无法调用的问题</h1><h2 id="xxl-job在k8s中的部署方式"><a href="#xxl-job在k8s中的部署方式" class="headerlink" title="xxl-job在k8s中的部署方式"></a>xxl-job在k8s中的部署方式</h2><h3 id="xxl-job服务端的部署方式"><a href="#xxl-job服务端的部署方式" class="headerlink" title="xxl-job服务端的部署方式"></a>xxl-job服务端的部署方式</h3><p>直接使用官方镜像进行部署，部署的yaml文件如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>les  <span class="token key atrule">name</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">xxljob</span><span class="token punctuation">:</span> <span class="token string">"on"</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin        <span class="token key atrule">image</span><span class="token punctuation">:</span> xuxueli/xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span class="token punctuation">:</span>2.3.0        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always     <span class="token comment" spellcheck="true"># 优先使用本地镜像</span>        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>        <span class="token key atrule">env</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> PARAMS   <span class="token comment" spellcheck="true"># 定义变量，用来接收sql的用户/密码 mysql为k8s集群内的service名称，在k8s集群内部可以直接使用service名称，因为集群默认做了coredns解析</span>          <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"--spring.datasource.url=jdbc:mysql://192.168.166.115:3306/xxl_job?Unicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false --spring.datasource.username=root --spring.datasource.password=123456"</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>les  <span class="token key atrule">name</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>    <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">30180</span>    <span class="token comment" spellcheck="true">#protocol: TCP</span>    <span class="token comment" spellcheck="true">#name: http</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>让该配置文件生效：</p><pre class="line-numbers language-bash"><code class="language-bash">$ kubectl apply -f xxl-job-admin.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这样xxl-job的server端就已经部署完毕，查看pod、deploy和svc资源情况：</p><pre><code>$ kubectl get pod -n test-les | grep xxlxxl-job-admin-6f9984796f-z5c8b                1/1     Running   0          28h$ kubectl get deploy -n test-les | grep xxlxxl-job-admin                1/1     1            1           44h$ kubectl get svc -n test-les | grep xxlxxl-job-admin                NodePort    10.99.160.251    &lt;none&gt;        8080:30180/TCP,5005:31005/TCP   27h</code></pre><p>访问地址<em>192.168.166.111:30180/xxl-job-admin/</em>即可打开前端页面。</p><h3 id="xxl-job客户端配置以及单个任务编写"><a href="#xxl-job客户端配置以及单个任务编写" class="headerlink" title="xxl-job客户端配置以及单个任务编写"></a>xxl-job客户端配置以及单个任务编写</h3><p>在微服务侧，添加xxl-job的配置，这里我们做了封装，因此展示如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># 添加以下配置文件</span><span class="token key atrule">main</span><span class="token punctuation">:</span>  <span class="token key atrule">job</span><span class="token punctuation">:</span>     <span class="token key atrule">admin</span><span class="token punctuation">:</span>       <span class="token key atrule">addresses</span><span class="token punctuation">:</span> http<span class="token punctuation">:</span>//192.168.166.111<span class="token punctuation">:</span>30180/xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin      <span class="token comment" spellcheck="true"># 如果在k8s中使用先得地址也可以</span>      <span class="token comment" spellcheck="true"># addresses: http://xxl-job-admin.test-les:8080/xxl-job-admin</span>    <span class="token key atrule">executor</span><span class="token punctuation">:</span>       <span class="token key atrule">appName</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>tttt      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">9988</span>      <span class="token key atrule">logRetentionDays</span><span class="token punctuation">:</span> <span class="token number">30</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="xxl-job服务端设置"><a href="#xxl-job服务端设置" class="headerlink" title="xxl-job服务端设置"></a>xxl-job服务端设置</h3><p>参考地址：<a href="https://www.xuxueli.com/xxl-job/#%E6%AD%A5%E9%AA%A4%E4%B8%80%EF%BC%9A%E6%89%A7%E8%A1%8C%E5%99%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%EF%BC%8C%E5%BC%80%E5%8F%91Job%E7%B1%BB%EF%BC%9A" target="_blank" rel="noopener">https://www.xuxueli.com/xxl-job/#%E6%AD%A5%E9%AA%A4%E4%B8%80%EF%BC%9A%E6%89%A7%E8%A1%8C%E5%99%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%EF%BC%8C%E5%BC%80%E5%8F%91Job%E7%B1%BB%EF%BC%9A</a></p><h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>在服务调用时出现了间歇性调用失败的情况，我们用一个10秒执行一次的空定时任务进行测试，发现大概每隔20分钟就会出现一波失败的任务。</p><p>成功任务的截图如下：</p><p><img src="%E6%88%90%E5%8A%9F%E4%BB%BB%E5%8A%A1%E6%88%AA%E5%9B%BE%E7%A4%BA%E4%BE%8B.png" alt></p><p>失败任务的截图如下：</p><p><img src="%E5%A4%B1%E8%B4%A5%E4%BB%BB%E5%8A%A1%E6%88%AA%E5%9B%BE%E7%A4%BA%E4%BE%8B.png" alt></p><p>二者对比，在成功的任务中，显示的调度机器ip为正常xxl-job-admin pod所在的IP地址，也是k8s中的ip地址，但是在失败的任务重，显示的调度机器ip地址为<em>172.17.0.2</em>。调度机器ip发生了变化。</p><p>上述问题是自动执行时发生，在手动执行时，没有错误信息发生！</p><!-- 间歇性调用失败，而且调用成功时，调度机器ip为正常的k8s中分配给xxl-job-admin的pod ip地址。但是调用失败时，调度机器ip为*172.17.0.2*。 --><p>查询该地址来源：</p><pre class="line-numbers language-shell"><code class="language-shell">$ ip a1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000    link/ether 00:0c:29:b5:58:9d brd ff:ff:ff:ff:ff:ff    inet 192.168.166.111/24 brd 192.168.166.255 scope global noprefixroute ens192       valid_lft forever preferred_lft forever    inet6 fe80::4261:3bb2:1f7a:3723/64 scope link noprefixroute       valid_lft forever preferred_lft forever3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default    link/ether 02:42:91:70:f8:c3 brd ff:ff:ff:ff:ff:ff    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0       valid_lft forever preferred_lft forever    inet6 fe80::42:91ff:fe70:f8c3/64 scope link       valid_lft forever preferred_lft forever4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default    link/ether 0a:a4:5f:15:12:7d brd ff:ff:ff:ff:ff:ff    inet 10.244.0.0/32 brd 10.244.0.0 scope global flannel.1       valid_lft forever preferred_lft forever    inet6 fe80::8a4:5fff:fe15:127d/64 scope link       valid_lft forever preferred_lft forever5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000    link/ether f6:0b:54:a1:fb:75 brd ff:ff:ff:ff:ff:ff    inet 10.244.0.1/24 brd 10.244.0.255 scope global cni0       valid_lft forever preferred_lft forever    inet6 fe80::f40b:54ff:fea1:fb75/64 scope link       valid_lft forever preferred_lft forever6: veth71da54bc@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default    link/ether 62:57:00:51:81:bf brd ff:ff:ff:ff:ff:ff link-netnsid 0    inet6 fe80::6057:ff:fe51:81bf/64 scope link       valid_lft forever preferred_lft forever7: veth33804756@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default    link/ether fa:90:1f:d4:c1:06 brd ff:ff:ff:ff:ff:ff link-netnsid 1    inet6 fe80::f890:1fff:fed4:c106/64 scope link       valid_lft forever preferred_lft forever$ route -n Kernel IP routing tableDestination     Gateway         Genmask         Flags Metric Ref    Use Iface0.0.0.0         192.168.166.254     0.0.0.0         UG    100    0        0 ens192192.168.166.0       0.0.0.0         255.255.255.0   U     100    0        0 ens19210.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni010.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.110.244.2.0      10.244.2.0      255.255.255.0   UG    0      0        0 flannel.110.244.3.0      10.244.3.0      255.255.255.0   UG    0      0        0 flannel.1172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为了得到172.17.0.2这个地址的来源我重新查看了集群外机器docker容器的详细信息，获得以下内容：</p><pre class="line-numbers language-shell"><code class="language-shell">$ docker inspect test-cloud-code......        "NetworkSettings": {            "Bridge": "",            "SandboxID": "4c834b802d2ccb427b84d3ad7958bd1cd49ab6025d6dd8ac7c4b6bf5086490fd",            "HairpinMode": false,            "LinkLocalIPv6Address": "",            "LinkLocalIPv6PrefixLen": 0,            "Ports": {                "5007/tcp": [                    {                        "HostIp": "0.0.0.0",                        "HostPort": "5007"                    }                ],                "55518/tcp": [                    {                        "HostIp": "0.0.0.0",                        "HostPort": "55518"                    }                ]            },            "SandboxKey": "/var/run/docker/netns/4c834b802d2c",            "SecondaryIPAddresses": null,            "SecondaryIPv6Addresses": null,            "EndpointID": "346d6f59337ab1f1c9e329b7d11fe002ee0c024eaff086376328f7e1c732fed7",            "Gateway": "172.14.0.1",            "GlobalIPv6Address": "",            "GlobalIPv6PrefixLen": 0,            "IPAddress": "172.14.0.2",            "IPPrefixLen": 24,            "IPv6Gateway": "",            "MacAddress": "02:42:ac:0e:00:02",            "Networks": {                "bridge": {                    "IPAMConfig": null,                    "Links": null,                    "Aliases": null,                    "NetworkID": "ef89708713244c8aee273309d23dbbc1e405eb18773f59413ba78582370e453d",                    "EndpointID": "346d6f59337ab1f1c9e329b7d11fe002ee0c024eaff086376328f7e1c732fed7",                    "Gateway": "172.14.0.1",                    "IPAddress": "172.14.0.2",                    "IPPrefixLen": 24,                    "IPv6Gateway": "",                    "GlobalIPv6Address": "",                    "GlobalIPv6PrefixLen": 0,                    "MacAddress": "02:42:ac:0e:00:02"                }            }        }    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查看网卡得知，172.17.0.2来自于docker0网卡，应该是docker容器对外访问的统一ip地址了。</p><h2 id="尝试过的解决办法"><a href="#尝试过的解决办法" class="headerlink" title="尝试过的解决办法"></a>尝试过的解决办法</h2><ol><li>在微服务侧，修改获取ip地址的方法，以及屏蔽其余网卡，无效。</li></ol><p>例如在pom.xml中引入common包：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.springframework.cloud<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spring-cloud-commons<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>3.1.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在配置文件中添加：</p><pre class="line-numbers language-properties"><code class="language-properties"><span class="token comment" spellcheck="true"># 设置ip地址段</span><span class="token attr-name">spring.cloud.inetutils.preferred-networks</span><span class="token punctuation">=</span><span class="token attr-value">10.244</span><span class="token comment" spellcheck="true"># 忽略eth0, 支持正则表达式</span><span class="token attr-name">spring.cloud.inetutils.ignored-interfaces</span><span class="token punctuation">=</span><span class="token attr-value">eth0 </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参考地址：<a href="https://blog.csdn.net/qq_29824445/article/details/108622838" target="_blank" rel="noopener">https://blog.csdn.net/qq_29824445/article/details/108622838</a></p><ol start="2"><li><p>查看版本对应，客户端是2.3.0的，服务端也是2.3.0的。未发现异常。</p></li><li><p>进一步分析的错误信息，查看客户端日志信息</p></li></ol><p>在客户端执行的过程中出现了以下异常内容：</p><pre class="line-numbers language-log"><code class="language-log">[test-tttt:10.244.3.83:20447] 2021-12-21 17:00:08.073 INFO 7 [] [xxl-job, executor ExecutorRegistryThread] c.x.j.c.thread.ExecutorRegistryThread    : >>>>>>>>>>> xxl-job registry fail, registryParam:RegistryParam{registryGroup='EXECUTOR', registryKey='test-tttt', registryValue='http://10.244.3.83:9988/'}, registryResult:ReturnT [code=500, msg=xxl-rpc remoting error(Connection refused (Connection refused)), for url : http://xxl-job-admin.test-les:8080/xxl-job-admin/api/registry, content=null][test-tttt:10.244.3.83:20447] 2021-12-21 17:06:13.040 INFO 7 [] [Thread-42] com.xxl.job.core.thread.JobThread        : >>>>>>>>>>> xxl-job JobThread stoped, hashCode:Thread[Thread-42,10,main]......ERROR 7 [] [xxl-job, executor TriggerCallbackThread] c.xxl.job.core.util.XxlJobRemotingUtil   : xxl-job-admin.test-lesjava.net.UnknownHostException: xxl-job-admin.test-les        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)        at java.net.Socket.connect(Socket.java:589)        at sun.net.NetworkClient.doConnect(NetworkClient.java:175)        at sun.net.www.http.HttpClient.openServer(HttpClient.java:463)        at sun.net.www.http.HttpClient.openServer(HttpClient.java:558)        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:840)        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678)        at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587)        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492)        at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)        at com.xxl.job.core.util.XxlJobRemotingUtil.postBody(XxlJobRemotingUtil.java:115)        at com.xxl.job.core.biz.client.AdminBizClient.callback(AdminBizClient.java:37)        at com.xxl.job.core.thread.TriggerCallbackThread.doCallback(TriggerCallbackThread.java:168)        at com.xxl.job.core.thread.TriggerCallbackThread.access$200(TriggerCallbackThread.java:26)        at com.xxl.job.core.thread.TriggerCallbackThread$1.run(TriggerCallbackThread.java:76)        at java.lang.Thread.run(Thread.java:748)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查看配置信息：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">main</span><span class="token punctuation">:</span>  <span class="token key atrule">job</span><span class="token punctuation">:</span>     <span class="token key atrule">admin</span><span class="token punctuation">:</span>       <span class="token comment" spellcheck="true"># addresses: http://192.168.166.111:30180/xxl-job-admin</span>      <span class="token key atrule">addresses</span><span class="token punctuation">:</span> http<span class="token punctuation">:</span>//xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin.test<span class="token punctuation">-</span>les<span class="token punctuation">:</span>8080/xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin    <span class="token key atrule">executor</span><span class="token punctuation">:</span>       <span class="token key atrule">appName</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>tttt      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">9988</span>      <span class="token key atrule">logRetentionDays</span><span class="token punctuation">:</span> <span class="token number">30</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改使用ip地址测试，看是否还有调度不到的情况，如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">main</span><span class="token punctuation">:</span>  <span class="token key atrule">job</span><span class="token punctuation">:</span>     <span class="token key atrule">admin</span><span class="token punctuation">:</span>       <span class="token key atrule">addresses</span><span class="token punctuation">:</span> http<span class="token punctuation">:</span>//192.168.166.111<span class="token punctuation">:</span>30180/xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin      <span class="token comment" spellcheck="true">#addresses: http://xxl-job-admin.test-les:8080/xxl-job-admin</span>    <span class="token key atrule">executor</span><span class="token punctuation">:</span>       <span class="token key atrule">appName</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>tttt      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">9988</span>      <span class="token key atrule">logRetentionDays</span><span class="token punctuation">:</span> <span class="token number">30</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>问题依旧。</p><ol start="4"><li>通过arthas监控方法调用，结合xxl-job的源码进行排查</li></ol><p>将arthas工具拷贝到xxl-job-admin的容器中，使用stack和watch命令进行调试：</p><pre class="line-numbers language-bash"><code class="language-bash">$ stack com.xxl.job.admin.core.trigger.XxlJobTrigger triggerPress Q or Ctrl+C to abort.Affect<span class="token punctuation">(</span>class count: 1 , method count: 1<span class="token punctuation">)</span> cost <span class="token keyword">in</span> 169 ms, listenerId: 23ts<span class="token operator">=</span>2021-12-21 22:59:50<span class="token punctuation">;</span>thread_name<span class="token operator">=</span>xxl-job, admin JobTriggerPoolHelper-fastTriggerPool-1091937476<span class="token punctuation">;</span>id<span class="token operator">=</span>2e<span class="token punctuation">;</span>is_daemon<span class="token operator">=</span>true<span class="token punctuation">;</span>priority<span class="token operator">=</span>5<span class="token punctuation">;</span>TCCL<span class="token operator">=</span>org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@316bcf94    @com.xxl.job.admin.core.trigger.XxlJobTrigger.trigger<span class="token punctuation">(</span><span class="token punctuation">)</span>        at com.xxl.job.admin.core.thread.JobTriggerPoolHelper<span class="token variable">$3</span>.run<span class="token punctuation">(</span>JobTriggerPoolHelper.java:95<span class="token punctuation">)</span>        at java.util.concurrent.ThreadPoolExecutor.runWorker<span class="token punctuation">(</span>ThreadPoolExecutor.java:1149<span class="token punctuation">)</span>        at java.util.concurrent.ThreadPoolExecutor<span class="token variable">$Worker</span>.run<span class="token punctuation">(</span>ThreadPoolExecutor.java:624<span class="token punctuation">)</span>        at java.lang.Thread.run<span class="token punctuation">(</span>Thread.java:748<span class="token punctuation">)</span>$ <span class="token function">watch</span> com.xxl.job.core.util.IpUtil getIpPress Q or Ctrl+C to abort.Affect<span class="token punctuation">(</span>class count: 1 , method count: 1<span class="token punctuation">)</span> cost <span class="token keyword">in</span> 157 ms, listenerId: 24method<span class="token operator">=</span>com.xxl.job.core.util.IpUtil.getIp location<span class="token operator">=</span>AtExitts<span class="token operator">=</span>2021-12-21 23:03:00<span class="token punctuation">;</span> <span class="token punctuation">[</span>cost<span class="token operator">=</span>0.292135ms<span class="token punctuation">]</span> result<span class="token operator">=</span>@ArrayList<span class="token punctuation">[</span>    @Object<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">[</span>isEmpty<span class="token operator">=</span>true<span class="token punctuation">;</span>size<span class="token operator">=</span>0<span class="token punctuation">]</span>,    null,    @String<span class="token punctuation">[</span>10.244.1.185<span class="token punctuation">]</span>,<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面是调用成功的日志，但是这里观察到失败时并未调用到com.xxl.job.core.util.IpUtil类中的getIp方法，没有任何调用日志打印。</p><ol start="5"><li>尝试远程调试xxl-job的代码，同样无效</li></ol><p>在Dockerfile中修改jvm启动命令如下：</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile">FROM 192.168.193.105/test-cloud/base_backend:0.0.5ENV PARAMS=""COPY ./xxl-job-admin-2.3.1-SNAPSHOT.jar /appEXPOSE 5005CMD ["sh", "-c", "java -jar $JAVA_OPTS -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 xxl-job-admin-2.3.1-SNAPSHOT.jar $PARAMS"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>添加<strong>-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005</strong>到Dockerfile中，把jar包拷贝到同目录，重新打镜像：</p><pre class="line-numbers language-bash"><code class="language-bash">$ docker build -t 192.168.193.105/test-cloud/xxl-job-admin-test:0.0.2 <span class="token keyword">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>修改xxl-job-admin的部署文件，添加调试端口暴露，如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>les  <span class="token key atrule">name</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">xxljob</span><span class="token punctuation">:</span> <span class="token string">"on"</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin        <span class="token comment" spellcheck="true">#image: xuxueli/xxl-job-admin:2.3.0</span>        <span class="token key atrule">image</span><span class="token punctuation">:</span> 192.168.193.105/test<span class="token punctuation">-</span>cloud/xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span class="token punctuation">-</span>test<span class="token punctuation">:</span>0.0.2        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always     <span class="token comment" spellcheck="true"># 优先使用本地镜像</span>        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>        <span class="token key atrule">env</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> PARAMS   <span class="token comment" spellcheck="true"># 定义变量，用来接收sql的用户/密码 mysql为k8s集群内的service名称，在k8s集群内部可以直接使用service名称，因为集群默认做了coredns解析</span>          <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"--spring.datasource.url=jdbc:mysql://192.168.166.115:3306/xxl_job?Unicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false --spring.datasource.username=root --spring.datasource.password=123456 --spring.cloud.inetutils.preferred-networks=10.244"</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>les  <span class="token key atrule">name</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>    <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">30180</span>    <span class="token comment" spellcheck="true">#protocol: TCP</span>    <span class="token comment" spellcheck="true">#name: http</span>  <span class="token comment" spellcheck="true"># 添加端口号</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> debug    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">5005</span>    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">5005</span>    <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">31005</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> xxl<span class="token punctuation">-</span>job<span class="token punctuation">-</span>admin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果不使用30000到32767之间对的端口，可以考虑使用k8s里面的port-forward机制。通过端口映射进行调试。</p><p>参考地址：<a href="https://kuboard.cn/learning/k8s-practice/access/port-forward.html" target="_blank" rel="noopener">https://kuboard.cn/learning/k8s-practice/access/port-forward.html</a></p><ol start="6"><li>将xxl-job的执行端口以nodePort形式暴露出来，重新进行调用测试</li></ol><p>修改客户端服务部署的内容，修改les后端服务的k8s服务部署文件，如下：</p><pre class="line-numbers language-shell"><code class="language-shell">$ vim test-les-app.yaml// 添加以下内容---apiVersion: v1kind: Servicemetadata:  name: test-les-xxl-job  namespace: test-les  labels:    app: test-les-appspec:  type: NodePort  ports:    - port: 9988      name: xxl-job      targetPort: 9988      nodePort: 30988  selector:    app: test-les-app---...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在xxl-job-admin中，修改执行器地址，由</p><pre><code>http://test-les-app.test-les.svc.cluster.local:9988</code></pre><p>变更为：</p><pre><code>http://192.168.166.111:30988,http://192.168.166.112:30988,http://192.168.166.113:30988,http://192.168.166.114:30988</code></pre><p>变更后保存，开启定时任务执行。</p><p>经过测试发现，除了服务重启更新之外，服务会存在定时任务执行失败的情况，其余情况下会正常执行。执行失败的情况也不再是ip地址有变更的情况了，ip地址也不发生变更了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>善用k8s自己的特性，来辅助解决问题，实现业务逻辑！</p><h1 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h1><ul><li><a href="https://github.com/xuxueli/xxl-job/issues/2539" target="_blank" rel="noopener">https://github.com/xuxueli/xxl-job/issues/2539</a></li><li><a href="https://github.com/xuxueli/xxl-job/issues/191" target="_blank" rel="noopener">https://github.com/xuxueli/xxl-job/issues/191</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch集群修改密码</title>
      <link href="/2021/12/11/elasticsearch-ji-qun-xiu-gai-mi-ma/"/>
      <url>/2021/12/11/elasticsearch-ji-qun-xiu-gai-mi-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="ElasticSearch集群修改密码"><a href="#ElasticSearch集群修改密码" class="headerlink" title="ElasticSearch集群修改密码"></a>ElasticSearch集群修改密码</h1><h2 id="1-停止写入es的相关程序"><a href="#1-停止写入es的相关程序" class="headerlink" title="1. 停止写入es的相关程序"></a>1. 停止写入es的相关程序</h2><p>这里基本上是Logstash，需要将Logstash停机。如果logstash有多个，都需要停机。</p><pre class="line-numbers language-shell"><code class="language-shell">$ sudo systemctl stop logstash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2-修改es密码"><a href="#2-修改es密码" class="headerlink" title="2. 修改es密码"></a>2. 修改es密码</h2><h3 id="2-1-创建新的管理员用户"><a href="#2-1-创建新的管理员用户" class="headerlink" title="2.1 创建新的管理员用户"></a>2.1 创建新的管理员用户</h3><pre class="line-numbers language-shell"><code class="language-shell">$ sudo /opt/es/bin/elasticsearch-users useradd changepasswd -r superuser// 设置密码为：#EDCxsw2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="2-2-curl请求，利用新管理员用户设置es密码："><a href="#2-2-curl请求，利用新管理员用户设置es密码：" class="headerlink" title="2.2 curl请求，利用新管理员用户设置es密码："></a>2.2 curl请求，利用新管理员用户设置es密码：</h3><pre><code>curl -XPUT -u changepasswd:#EDCxsw2 http://10.3.61.242:9200/_xpack/security/user/elastic/_password -H &quot;Content-Type: application/json&quot; -d &#39;{&quot;password&quot;:&quot;!QAZxsw2#EDc&quot;}&#39;</code></pre><p>开始请求验证：</p><pre><code>$ curl -XGET -u &#39;elastic:!QAZxsw2#EDc&#39; http://10.3.61.242:9200/{  &quot;name&quot; : &quot;10.3.61.242&quot;,  &quot;cluster_name&quot; : &quot;bigops&quot;,  &quot;cluster_uuid&quot; : &quot;Jo2866WaRY-_gKmbNEGHpw&quot;,  &quot;version&quot; : {    &quot;number&quot; : &quot;7.6.2&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;tar&quot;,    &quot;build_hash&quot; : &quot;ef48eb35cf30adf4db14086e8aabd07ef6fb113f&quot;,    &quot;build_date&quot; : &quot;2020-03-26T06:34:37.794943Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;8.4.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;  },  &quot;tagline&quot; : &quot;You Know, for Search&quot;}$ curl -u elastic &#39;http://10.3.61.242:9200/_xpack/security/_authenticate?pretty&#39;Enter host password for user &#39;elastic&#39;:{  &quot;username&quot; : &quot;elastic&quot;,  &quot;roles&quot; : [    &quot;superuser&quot;  ],  &quot;full_name&quot; : null,  &quot;email&quot; : null,  &quot;metadata&quot; : {    &quot;_reserved&quot; : true  },  &quot;enabled&quot; : true,  &quot;authentication_realm&quot; : {    &quot;name&quot; : &quot;reserved&quot;,    &quot;type&quot; : &quot;reserved&quot;  },  &quot;lookup_realm&quot; : {    &quot;name&quot; : &quot;reserved&quot;,    &quot;type&quot; : &quot;reserved&quot;  }}</code></pre><h3 id="2-3-异常信息"><a href="#2-3-异常信息" class="headerlink" title="2.3 异常信息"></a>2.3 异常信息</h3><p>这时重启es，会出现以下错误信息：</p><pre class="line-numbers language-log"><code class="language-log">[2021-01-13T17:05:27,359][INFO ][o.e.x.s.a.AuthenticationService] [elekpelk01] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic][2021-01-13T17:05:29,290][ERROR][o.e.x.s.a.e.ReservedRealm] [elekpelk01] failed to retrieve password hash for reserved user [elastic]org.elasticsearch.action.UnavailableShardsException: at least one primary shard for the index [.security-7] is unavailable        at org.elasticsearch.xpack.security.support.SecurityIndexManager.getUnavailableReason(SecurityIndexManager.java:181) ~[x-pack-security-7.8.0.jar:7.8.0]        at org.elasticsearch.xpack.security.authc.esnative.NativeUsersStore.getReservedUserInfo(NativeUsersStore.java:525) [x-pack-security-7.8.0.jar:7.8.0]        at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.getUserInfo(ReservedRealm.java:224) [x-pack-security-7.8.0.jar:7.8.0]        at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.doAuthenticate(ReservedRealm.java:99) [x-pack-security-7.8.0.jar:7.8.0]        at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:167) [x-pack-security-7.8.0.jar:7.8.0]        at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:104) [x-pack-security-7.8.0.jar:7.8.0]        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$15(AuthenticationService.java:449) [x-pack-security-7.8.0.jar:7.8.0]        at org.elasticsearch.xpack.core.common.IteratingActionListener.run(IteratingActionListener.java:102) [x-pack-core-7.8.0.jar:7.8.0]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解决方式如下：</p><pre class="line-numbers language-shell"><code class="language-shell">$ curl -XDELETE -u 'changepasswd:#EDCxsw2' http://10.3.61.242:9200/.security-*// 删除之后重启es，如果是集群，则需要重启所有节点的es$ sudo systemctl daemon-reload && sudo systemctl restart es<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>删掉所有的认证信息，现在需要重新设置密码，操作如下：</p><pre class="line-numbers language-shell"><code class="language-shell">$ sudo /opt/es/bin/elasticsearch-setup-passwords interactive Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.You will be prompted to enter passwords as the process progresses.Please confirm that you would like to continue [y/N]yEnter password for [elastic]: Reenter password for [elastic]: Enter password for [apm_system]: Reenter password for [apm_system]: Enter password for [kibana]: Reenter password for [kibana]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Enter password for [beats_system]: Reenter password for [beats_system]: Enter password for [remote_monitoring_user]: Reenter password for [remote_monitoring_user]: Changed password for user [apm_system]Changed password for user [kibana]Changed password for user [logstash_system]Changed password for user [beats_system]Changed password for user [remote_monitoring_user]Changed password for user [elastic]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改密码后，重启es服务即可。</p><p>es集群恢复后，删除此前添加的用户：</p><pre><code>$ sudo /opt/es/bin/elasticsearch-users userdel changepasswd</code></pre><h3 id="2-4-修改kibana连接密码"><a href="#2-4-修改kibana连接密码" class="headerlink" title="2.4 修改kibana连接密码"></a>2.4 修改kibana连接密码</h3><pre><code>$ sudo vim /opt/kibana/config/kibana.ymlserver.port: 55561server.host: &quot;0.0.0.0&quot;elasticsearch.hosts: [&quot;http://127.0.0.1:9200&quot;]#elasticsearch.hosts: [&quot;http://127.0.0.1:55520&quot;]elasticsearch.username: &quot;elastic&quot;# 修改下面的密码信息为最新elasticsearch.password: &quot;!QAZxsw2#EDc&quot;i18n.locale: zh-CN// :wq 保存退出// 重启kibana$ sudo systemctl daemon-reload$ sudo systemctl restart kibana</code></pre><p>通过浏览器进行访问验证，这时候kibana访问可以读取之前的日志信息，但是最近几分钟或者10多分钟因为logstash停机，是没有日志的。</p><h3 id="2-5-修改logstash中的密码信息并重启"><a href="#2-5-修改logstash中的密码信息并重启" class="headerlink" title="2.5 修改logstash中的密码信息并重启"></a>2.5 修改logstash中的密码信息并重启</h3><pre class="line-numbers language-bash"><code class="language-bash">$ <span class="token function">sudo</span> vim /opt/logstash-conf/leslog.conf<span class="token comment" spellcheck="true"># Sample Logstash configuration for creating a simple</span><span class="token comment" spellcheck="true"># Beats -> Logstash -> Elasticsearch pipeline.</span>input <span class="token punctuation">{</span>  kafka <span class="token punctuation">{</span>    bootstrap_servers <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token string">"10.3.61.246:9092"</span><span class="token punctuation">]</span>    group_id <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"test-consumer-group"</span>    topics <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token string">"leslog"</span><span class="token punctuation">]</span>    auto_offset_reset <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"earliest"</span>    consumer_threads <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"1"</span>    client_id <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"leslog"</span>    enable_auto_commit <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"true"</span>    decorate_events <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"true"</span>    auto_commit_interval_ms <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"100"</span>    <span class="token comment" spellcheck="true">#codec => "json"</span>    codec <span class="token operator">=</span><span class="token operator">></span> json <span class="token punctuation">{</span>        charset <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"UTF-8"</span>    <span class="token punctuation">}</span>    <span class="token function">type</span> <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"kafka-to-elas"</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span>output <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">#if [type] == "kafka-to-elas" {</span>    elasticsearch <span class="token punctuation">{</span>      hosts <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token string">"10.3.61.242:9200"</span><span class="token punctuation">]</span>      index <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"leslog-%{+YYYY.MM.dd}"</span>      user <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"elastic"</span>      <span class="token comment" spellcheck="true"># 只修改这个位置的密码信息！</span>      password <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"!QAZxsw2#EDc"</span>      action <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"update"</span>      document_id <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"%{id}"</span>      doc_as_upsert <span class="token operator">=</span><span class="token operator">></span> <span class="token boolean">true</span>      script <span class="token operator">=</span><span class="token operator">></span> <span class="token string">'ctx._source.content = ctx._source.content + ";" + params.event.get("content");ctx._source.type = params.event.get("type")'</span>      script_type <span class="token operator">=</span><span class="token operator">></span> <span class="token string">"inline"</span>    <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">#}</span><span class="token punctuation">}</span>// :wq 保存退出$ <span class="token function">sudo</span> systemctl daemon-reload <span class="token operator">&amp;&amp;</span> <span class="token function">sudo</span> systemctl restart logstash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>logstash启动后，会逐步从kafka中读取挤压的日志信息，大概10分钟左右就可以看到之前时间段之内缺失的日志信息了。</p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>如果忘记密码，需要使用通过添加管理员，删除安全索引的方式进行！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>某甲方项目漏洞修复说明</title>
      <link href="/2021/11/09/mou-jia-fang-xiang-mu-lou-dong-xiu-fu-shuo-ming/"/>
      <url>/2021/11/09/mou-jia-fang-xiang-mu-lou-dong-xiu-fu-shuo-ming/</url>
      
        <content type="html"><![CDATA[<h1 id="某甲方项目漏洞修复说明"><a href="#某甲方项目漏洞修复说明" class="headerlink" title="某甲方项目漏洞修复说明"></a>某甲方项目漏洞修复说明</h1><h2 id="解决内容说明"><a href="#解决内容说明" class="headerlink" title="解决内容说明"></a>解决内容说明</h2><p>优先解决高风险漏洞，包含非常危险和比较危险这两个等级的漏洞，对于其余的漏洞，给出解决时间。</p><p>对于目前不容易解决的漏洞，尽可能给出解决方案，并给出解决时间。</p><p>正式环境的高危漏洞，开发、测试环境的高危漏洞必须解决！</p><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><p>主要问题集中在MySQL数据库、Nginx、nacos三个组件上，系统层面问题不大。</p><p>对于三者的版本选择要注意！</p><h3 id="与开发相关的漏洞信息"><a href="#与开发相关的漏洞信息" class="headerlink" title="与开发相关的漏洞信息"></a>与开发相关的漏洞信息</h3><ul><li>nacos权限绕过漏洞(CVE-2021-29441)</li></ul><p>通过重新设置配置信息实现，临时可以不选择升级版本。</p><p>解决方案：<a href="https://www.iitter.com/other/55257.html" target="_blank" rel="noopener">https://www.iitter.com/other/55257.html</a></p><p>难易程度：容易</p><ul><li>nacos弱身份认证漏洞(CVE-2021-29442)</li></ul><p>官方建议升级到1.4.1版本的nacos，并且是hotfix版本，如果不升级该版本，需要修改代码以规避风险。</p><p>修改代码解决方案：<a href="https://www.sohu.com/a/448007719_466874" target="_blank" rel="noopener">https://www.sohu.com/a/448007719_466874</a></p><p>难易程度：难</p><p>2021-11-09补充内容：</p><p>上述两个漏洞均需要通过升级nacos解决，目前将nacos升级到1.4.1版本。然后配置授权验证信息，在nacos安装目录下conf/application.properties我呢见中修改，如下：</p><pre class="line-numbers language-properties"><code class="language-properties"><span class="token comment" spellcheck="true">### 开启鉴权</span><span class="token attr-name">nacos.core.auth.enabled</span><span class="token punctuation">=</span><span class="token attr-value">true</span><span class="token comment" spellcheck="true">### 关闭使用user-agent判断服务端请求并放行鉴权的功能</span><span class="token attr-name">nacos.core.auth.enable.userAgentAuthWhite</span><span class="token punctuation">=</span><span class="token attr-value">false</span><span class="token comment" spellcheck="true">### 配置自定义身份识别的key（不可为空）和value（不可为空）</span><span class="token attr-name">nacos.core.auth.server.identity.key</span><span class="token punctuation">=</span><span class="token attr-value">example</span><span class="token attr-name">nacos.core.auth.server.identity.value</span><span class="token punctuation">=</span><span class="token attr-value">example</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后将所有服务添加授权验证信息，在每个微服务的bootstrap.yml文件中进行修改，如下：</p><pre class="line-numbers language-yml"><code class="language-yml">spring:  application:    name: test  main:    allow-bean-definition-overriding: true  cloud:    nacos:      config:        namespace: ${nacos_namespace:abc4976e-798f-4772-bfd7-1d2f85a55c5d}        server-addr: ${nacos_ip:192.168.229.51:38848}        file-extension: yml          # 添加用户名密码配置        username: ${nacos_username:nacos1}        password: ${nacos_passwd:nacos1}      discovery:        namespace: ${nacos_namespace:abc4976e-798f-4772-bfd7-1d2f85a55c5d}        server-addr: ${nacos_ip:192.168.229.51:38848}          # 添加用户名密码配置        username: ${nacos_username:nacos1}        password: ${nacos_passwd:nacos1}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>目前存在的隐患是，微服务中的nacos-client依赖并未升级，目前尚未发现有注册和调用的问题，这个需要进一步验证。</p><ul><li>远端WWW服务支持TRACE请求</li></ul><p>目前这个错误应该和目前所有微服务中使用的内置服务器有关，需要确认内置服务器的版本。</p><p>解决方案如下：<a href="https://www.cnblogs.com/zcg-cpdd/p/14485370.html" target="_blank" rel="noopener">https://www.cnblogs.com/zcg-cpdd/p/14485370.html</a></p><p>2021-11-08补充，找到解决方法，使用在Springboot项目中添加配置类实现，配置类编写如下：</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>example<span class="token punctuation">.</span>demo<span class="token punctuation">.</span>autoconfigure<span class="token punctuation">;</span><span class="token keyword">import</span> io<span class="token punctuation">.</span>undertow<span class="token punctuation">.</span>server<span class="token punctuation">.</span>HandlerWrapper<span class="token punctuation">;</span><span class="token keyword">import</span> io<span class="token punctuation">.</span>undertow<span class="token punctuation">.</span>server<span class="token punctuation">.</span>HttpHandler<span class="token punctuation">;</span><span class="token keyword">import</span> io<span class="token punctuation">.</span>undertow<span class="token punctuation">.</span>server<span class="token punctuation">.</span>handlers<span class="token punctuation">.</span>DisallowedMethodsHandler<span class="token punctuation">;</span><span class="token keyword">import</span> io<span class="token punctuation">.</span>undertow<span class="token punctuation">.</span>util<span class="token punctuation">.</span>HttpString<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>springframework<span class="token punctuation">.</span>boot<span class="token punctuation">.</span>web<span class="token punctuation">.</span>embedded<span class="token punctuation">.</span>undertow<span class="token punctuation">.</span>UndertowServletWebServerFactory<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>springframework<span class="token punctuation">.</span>boot<span class="token punctuation">.</span>web<span class="token punctuation">.</span>server<span class="token punctuation">.</span>WebServerFactoryCustomizer<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>springframework<span class="token punctuation">.</span>context<span class="token punctuation">.</span>annotation<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span><span class="token annotation punctuation">@Configuration</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">UndertowWebServerCustomizerConfig</span> <span class="token keyword">implements</span> <span class="token class-name">WebServerFactoryCustomizer</span><span class="token operator">&lt;</span>UndertowServletWebServerFactory<span class="token operator">></span> <span class="token punctuation">{</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">customize</span><span class="token punctuation">(</span>UndertowServletWebServerFactory factory<span class="token punctuation">)</span> <span class="token punctuation">{</span>        factory<span class="token punctuation">.</span><span class="token function">addDeploymentInfoCustomizers</span><span class="token punctuation">(</span>deploymentInfo <span class="token operator">-</span><span class="token operator">></span> <span class="token punctuation">{</span>            deploymentInfo<span class="token punctuation">.</span><span class="token function">addInitialHandlerChainWrapper</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">HandlerWrapper</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token annotation punctuation">@Override</span>                <span class="token keyword">public</span> HttpHandler <span class="token function">wrap</span><span class="token punctuation">(</span>HttpHandler handler<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    HttpString<span class="token punctuation">[</span><span class="token punctuation">]</span> disallowedHttpMethods <span class="token operator">=</span> <span class="token punctuation">{</span>HttpString<span class="token punctuation">.</span><span class="token function">tryFromString</span><span class="token punctuation">(</span><span class="token string">"TRACE"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                            HttpString<span class="token punctuation">.</span><span class="token function">tryFromString</span><span class="token punctuation">(</span><span class="token string">"TRACK"</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">;</span>                    <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">DisallowedMethodsHandler</span><span class="token punctuation">(</span>handler<span class="token punctuation">,</span> disallowedHttpMethods<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参考地址：<a href="https://blog.csdn.net/nklinsirui/article/details/108540403" target="_blank" rel="noopener">https://blog.csdn.net/nklinsirui/article/details/108540403</a></p><h3 id="与构建相关的漏洞"><a href="#与构建相关的漏洞" class="headerlink" title="与构建相关的漏洞"></a>与构建相关的漏洞</h3><ul><li>Docker Remote API 未授权访问漏洞 —（已完成，对2375端口做了限制，使用tls证书进行认证）</li></ul><p>这个漏洞是因为我们jenkins主机上安装的docker开放了2375远程调用端口导致的，我们jenkins构建时需要开放2375端口进行docker构建打包。</p><p>解决方式：对2375端口做网络访问控制，如设置iptables策略仅允许指定的IP来访问Docker接口。目前可以考虑用防火墙的方式进行限制，但甲方方面的机器防火墙都已经关闭，需要进行测试。</p><p>难易程度：容易</p><p><strong>注意：</strong>在甲方方面无法使用端口限制的方式解决该漏洞，只能使用TLS证书的方式解决！</p><h3 id="与中间件相关的漏洞"><a href="#与中间件相关的漏洞" class="headerlink" title="与中间件相关的漏洞"></a>与中间件相关的漏洞</h3><ul><li>Ffay Lanproxy 路径遍历漏洞(CVE-2021-3019) — （已完成，通过关闭lanproxy服务进行，后续如果需要继续使用，则升级版本）</li><li>uWSGI 路径穿越漏洞(CVE-2018-7490) — （已完成，通过关闭lanproxy服务进行，后续如果需要继续使用，则升级版本）</li><li>SSL/TLS 服务器瞬时 Diffie-Hellman 公共密钥过弱 — （已完成，通过关闭lanproxy服务进行，后续如果需要继续使用，则升级版本）</li></ul><p>这个漏洞是因为在甲方开发环境中部署了内网穿透工具Lanproxy导致的。</p><p>解决方式：如果后续不再使用内网穿透，可以考虑直接关闭该工具解决漏洞。如果需要继续使用，升级最新版本的lanproxy解决。</p><p>难易程度：容易</p><ul><li>nginx 安全漏洞(CVE-2021-23017) — （<em>开发环境</em>已完成，通过修改前端Docker基础镜像、对编译的nginx打补丁）</li></ul><p>这个漏洞是因为在前端部署和在fastdfs文件存储服务中引入nginx导致的。目前针对前端部署的nginx修改，需要变更基础镜像使用的nginx，打补丁和漏洞；针对fastdfs文件存储，直接修复即可。如果粗暴点解决，升级最新版本的nginx，但是需要进行功能层面验证！</p><p>官方给出的解决方式是，更换新版本的nginx。例如：升级到NGINX Open Source 1.20.1 (stable)。但是对于前端docker基础镜像中使用的nginx来说，这个相对来说不好实现</p><p>解决方式：<a href="https://www.nginx.com/blog/updating-nginx-dns-resolver-vulnerability-cve-2021-23017/" target="_blank" rel="noopener">https://www.nginx.com/blog/updating-nginx-dns-resolver-vulnerability-cve-2021-23017/</a><br>         <a href="https://www.secrss.com/articles/31531（编译安装推荐这样解决）" target="_blank" rel="noopener">https://www.secrss.com/articles/31531（编译安装推荐这样解决）</a><br>         <a href="http://nginx.org/download/patch.2021.resolver.txt" target="_blank" rel="noopener">http://nginx.org/download/patch.2021.resolver.txt</a></p><p>难以程度：较难</p><h3 id="与数据库相关的漏洞"><a href="#与数据库相关的漏洞" class="headerlink" title="与数据库相关的漏洞"></a>与数据库相关的漏洞</h3><p>— （<em>开发环境</em>已完成，升级MySQL 5.7.35）</p><ul><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-3822)</li><li>Oracle MySQL Server组件安全漏洞(CVE-2019-5482)</li><li>Oracle MySQL Server安全漏洞(CVE-2019-17543)</li><li>Oracle MySQL Server安全漏洞(CVE-2021-22901)</li><li>Oracle MySQL Server 安全漏洞（CVE-2019-5443）</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-1967)</li><li>Oracle MySQL Server安全漏洞（CVE-2019-2632）</li><li>Oracle MySQL/MariaDB Server 输入验证错误漏洞（CVE-2021-2144）</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14539)</li><li>Oracle MySQL Server 安全漏洞(CVE-2019-2914)</li><li>Oracle MySQL Server InnoDB 安全漏洞(CVE-2020-14775)</li><li>Oracle MySQL Server 安全漏洞(CVE-2019-2946)</li><li>Oracle MySQL Server Server: Optimizer 安全漏洞(CVE-2020-14769)</li><li>Oracle MySQL Server组件安全漏洞(CVE-2020-2790)</li><li>Oracle MySQL Server/MariaDB 组件访问控制错误漏洞(CVE-2019-2805)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14576)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-2579)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2178)</li><li>Oracle MySQL Server/MariaDB 安全漏洞(CVE-2019-2974)</li><li>Oracle MySQL Server/MariaDB 组件安全漏洞(CVE-2020-2780)</li><li>Oracle MySQL Server/MariaDB 组件访问控制错误漏洞(CVE-2019-2740)</li><li>Oracle MySQL Server Server: Security: LDAP Auth 安全漏洞(CVE-2020-14827)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2202)</li><li>Oracle MySQL Server/MariaDB Server: FTS组件拒绝服务漏洞(CVE-2020-14765)</li><li>Oracle MySQL Server 信息泄露漏洞(CVE-2019-2969)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2307)</li><li>Oracle MySQL Server/MariaDB 安全漏洞(CVE-2020-2574)</li><li>Oracle MySQL Server 输入验证错误漏洞（CVE-2021-23890）</li><li>Oracle MySQL/MariaDB 拒绝服务漏洞(CVE-2021-2011)</li><li>Oracle MySQL Server安全漏洞（CVE-2019-1559）</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2356)</li><li>Oracle Java SE 输入验证错误漏洞 (CVE-2021-23841)</li><li>Oracle MySQL Server组件安全漏洞(CVE-2020-2804)</li><li>Oracle MySQL Server 远程安全漏洞(CVE-2021-3449)</li><li>Oracle MySQL 安全漏洞(CVE-2020-2570)</li><li>Oracle MySQL Client 安全漏洞(CVE-2020-2573)</li><li>Oracle MySQL Server 代码问题漏洞(CVE-2020-1971)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2390)</li><li>Oracle MySQL Server/MariaDB 组件访问控制错误漏洞(CVE-2019-2758)</li><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-2819)</li><li>Oracle MySQL Server Server Optimizer 安全漏洞(CVE-2020-14760)</li><li>Oracle MySQL Server/MariaDB InnoDB组件安全漏洞(CVE-2020-2760)</li><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-2778)</li><li>Oracle MySQL Server 安全漏洞(CVE-2019-2993)</li><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-2741)</li><li>Oracle MySQL Server 信息泄露漏洞(CVE-2019-2924)</li><li>Oracle MySQL Server组件安全漏洞(CVE-2020-2806)</li><li>Oracle MySQL Server 信息泄露漏洞(CVE-2019-2922)</li><li>Oracle MySQL Server 信息泄露漏洞(CVE-2019-2923)</li><li>Oracle MySQL Server/MariaDB组件安全漏洞(CVE-2020-2752)</li><li>Oracle MySQL Server/MariaDB 安全漏洞(CVE-2020-14550)</li><li>Oracle MySQL Server/MariaDB 组件访问控制错误漏洞(CVE-2019-2739)</li><li>Oracle MySQL Server 输入验证错误漏洞（CVE-2021-23853）</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-2763)</li><li>Oracle MySQL Server/MariaDB Server: Stored Procedure组件安全漏洞(CVE-2020-2812)</li><li>Oracle MySQL Server/MariaDB 安全漏洞（CVE-2019-2628）</li><li>Oracle MySQL Server/MariaDB 组件安全漏洞(CVE-2020-2814)</li><li>Oracle MySQL/MariaDB Server 输入验证错误漏洞(CVE-2021-2166)</li><li>Oracle MySQL Server组件安全漏洞(CVE-2020-2765)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2179)</li><li>Oracle MySQL 安全漏洞(CVE-2021-2060)</li><li>Oracle MySQL Server Server: Stored Procedure 安全漏洞(CVE-2020-14672)</li><li>Oracle MySQL Server Server：Optimizer 安全漏洞(CVE-2020-14793)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2226)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2169)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14540)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14547)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14567)</li><li>Oracle MySQL Server/MariaDB Server: Locking 安全漏洞(CVE-2020-14812)</li><li>Oracle MySQL 输入验证错误漏洞(CVE-2021-2160)</li><li>Oracle MySQL Server 输入验证错误漏洞（CVE-2021-2146）</li><li>Oracle MySQL/MariaDB Server 输入验证错误漏洞(CVE-2021-2154)</li><li>Oracle MySQL Server/MariaDB InnoDB 安全漏洞(CVE-2020-14776)</li><li>Oracle MySQL Server/MariaDB 拒绝服务漏洞(CVE-2020-14789)</li><li>Oracle MySQL 安全漏洞(CVE-2021-2014)</li><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-2774)</li><li>Oracle MySQL/MariaDB Server 输入验证错误漏洞(CVE-2021-2194)</li><li>Oracle MySQL/MariaDB Server 输入验证错误漏洞(CVE-2021-2180)</li><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-2757)</li><li>Oracle MySQL Server/MariaDB 组件访问控制错误漏洞(CVE-2019-2737)</li><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-2755)</li><li>Oracle MySQL Server/MariaDB 安全漏洞（CVE-2019-2627）</li><li>Oracle MySQL Server 安全漏洞(CVE-2019-2948)</li><li>Oracle MySQL Server 安全漏洞(CVE-2019-2960)</li><li>Oracle MySQL 输入验证错误漏洞(CVE-2021-2342)</li><li>Oracle MySQL Server安全漏洞（CVE-2019-2683）</li><li>Oracle MySQL Server安全漏洞（CVE-2019-2592）</li><li>Oracle MySQL Server安全漏洞（CVE-2019-2581）</li><li>Oracle MySQL Server安全漏洞（CVE-2019-2566）</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-2577)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-2589)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-2660)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14869)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14790)</li><li>Oracle MySQL 安全漏洞(CVE-2021-2001)</li><li>Oracle MySQL Server组件安全漏洞(CVE-2019-1547)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2372)</li><li>Oracle MySQL Server/MariaDB 安全漏洞（CVE-2019-2614）</li><li>Oracle MySQL Server/MariaDB 输入验证错误漏洞(CVE-2019-2938)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-2584)</li><li>Oracle MySQL Server 授权问题漏洞(CVE-2020-14867)</li><li>Oracle MySQL/MariaDB 安全漏洞(CVE-2021-2022)</li><li>Oracle MySQL/MariaDB Server 输入验证错误漏洞(CVE-2021-2174)</li><li>Oracle MySQL Server 输入验证错误漏洞(CVE-2021-2171)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14559)</li><li>Oracle MySQL Server 安全漏洞(CVE-2020-14553)</li><li>Oracle MySQL/MariaDB 访问控制错误漏洞(CVE-2021-2032)</li><li>Oracle MySQL Server 输入验证错误漏洞（CVE-2021-21622）</li><li>Oracle MySQL Server组件访问控制错误漏洞(CVE-2019-2797)</li><li>Oracle MySQL 安全漏洞(CVE-2021-2010)</li></ul><p>数据库层面存在比较严重的问题，目前尚不知道如何去修复！</p><p>解决方案：直接升级MySQL 5.7.35版本解决所有漏洞问题！</p><p>难易程度：较难</p><p>番外：需要同正式环境升级5.7.33 的数据中的CVE数量进行对比，看看升级版本能解决多少问题。</p><p>正式环境服务问题信息：</p><ul><li>Oracle MySQL Server安全漏洞(CVE-2021-22901)</li><li>OracleOracle MySQL Server安全漏洞(CVE-2019-17543)</li><li>OracleOracle MySQL Server 输入验证错误漏洞(CVE-2021-2356)</li><li>OracleOracle MySQL Server 输入验证错误漏洞(CVE-2021-2390)</li><li>OracleOracle MySQL Server 输入验证错误漏洞（CVE-2021-23890)</li><li>OracleOracle MySQL Server 输入验证错误漏洞（CVE-2021-23853）</li><li>OracleOracle MySQL 输入验证错误漏洞(CVE-2021-2342)</li><li>OracleOracle MySQL Server 输入验证错误漏洞(CVE-2021-2372)</li></ul><p>咨询下甲方方面的DBA同志解决下该问题，重新搭建数据库。</p><h3 id="与k8s相关的漏洞"><a href="#与k8s相关的漏洞" class="headerlink" title="与k8s相关的漏洞"></a>与k8s相关的漏洞</h3><ul><li>SSL/TLS协议信息泄露漏洞(CVE-2016-2183)</li></ul><h3 id="与监控相关的漏洞"><a href="#与监控相关的漏洞" class="headerlink" title="与监控相关的漏洞"></a>与监控相关的漏洞</h3><ul><li>Eclipse Jetty HTTP请求走私漏洞（CVE-2017-7658）</li><li>Eclipse Jetty 授权问题漏洞(CVE-2018-12538)</li><li>Eclipse Jetty HTTP请求走私漏洞（CVE-2017-7656）</li><li>Jetty 信息泄露漏洞(CVE-2017-9735)</li><li>Eclipse Jetty 输入验证错误漏洞(CVE-2018-12545)</li><li>Eclipse Jetty跨站脚本执行漏洞（CVE-2019-10241）</li><li>Eclipse Jetty 信息泄露漏洞(CVE-2019-10246)</li><li>Eclipse Jetty信息泄露漏洞（CVE-2018-12536）</li><li>Eclipse Jetty 信息泄露漏洞(CVE-2019-10247)</li></ul><p>这个部分的漏洞和Skywalking相关，目前尚未有合适的解决方案。只能先考虑下升级Skywalking版本看看。</p><p>解决方式：升级Skywalking版本，需要验证对之前的agent有没有影响。</p><p>难易程度：较难</p><p>目前采取的策略是，直接关闭Skywalking服务，因为目前没有用到，也不怎么看调用链路。</p><h2 id="维护策略"><a href="#维护策略" class="headerlink" title="维护策略"></a>维护策略</h2><p>优先解决容易修复的漏洞，以及数据库层面的漏洞。与监控相关的漏洞可以暂时往后放，另外与k8s相关的漏洞暂时想不到处理方式，这个也往后放。</p><p>数据库相关的需要甲方方面的DBA同志支持下。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>某甲方项目二期总结</title>
      <link href="/2021/11/09/mou-jia-fang-xiang-mu-er-qi-zong-jie/"/>
      <url>/2021/11/09/mou-jia-fang-xiang-mu-er-qi-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="某甲方二期项目总结"><a href="#某甲方二期项目总结" class="headerlink" title="某甲方二期项目总结"></a>某甲方二期项目总结</h1><h2 id="起始"><a href="#起始" class="headerlink" title="起始"></a>起始</h2><p>从10月份开始正式进行驻场，一如既往的日子不好过。老问题解决不了，新问题层出不穷，疲于奔命。</p><p>除了为无能的实施买单，也没啥好的挽回余地，更没能找到继续前进的动力。</p><p>二期项目是稀里糊涂开始的，又是稀里糊涂中道崩殂，到现在都没有上线。结构混乱，控制混乱，没有任何计划性，总是做到哪儿算哪儿。一期的问题始终没有得到解决，二期又暴露了很多新的问题：</p><ol><li>职责不明</li></ol><p>到底是项目经理负责制还是项目经理的领导负责制？究竟是谁在把握项目的方向和进度，究竟有没有合适的衡量标准和方案？</p><ol start="2"><li>甲方质疑和不信任</li></ol><p>一期项目乱七八糟，引发了信任危机，但是由于合同方面的问题，导致对面不会轻易换掉公司。</p><ol start="3"><li>甲方开发人员的佛系精神</li></ol><p>甲方开发人员未参与本期开发，版本发布时，多不配合、不参与、不掺和，且甩锅和指责变多。而他们身上还挂着一期的问题，这样就导致必须包含他们修改的东西，才能进行测试。</p><ol start="4"><li>一期项目的问题混合着二期的新需求开发</li></ol><p>一期项目问题尚未解决的很好，导致了二期开发时，还得进行一期错误的修改。然而大部分修改都没有太大意义，因为二期上线推翻了好多之前需求确定的流程，导致代码混乱，产物混乱。</p><ol start="5"><li>开发人员心态问题</li></ol><p>同一期项目不同，二期很明显大家的干劲少了很多，总觉着被坑或者被骗。一鼓作气，再而衰，三而竭。面临“再而衰”的情况，管理层似乎能做的不多。实施层面就更不用说了，根本不管你开发的死活，一副“我就是要改，你奈我和”的嚣张形象。这样的情形下，这种项目是看不到未来的，并且对个人成长更是不利。</p><ol start="6"><li>二期需求依旧不明确</li></ol><p>在二期开发过程中，依旧存在需求不明确的问题，这个问题始终无法更正。我们确实要服务用户，那么当用户自己都不知道自己要啥的时候，我们如何去引导和设计？这一点值得思考。</p><h2 id="离开"><a href="#离开" class="headerlink" title="离开"></a>离开</h2><p>当两位同志辞职的时候，我实际上预料到了。别说是女同志出差了，就是男同志出差到这样的项目上，掉层皮已经算轻的了。好不容易磨合出来的团队，又得重新构建。行业如此，又奈何啊？</p><p>人生本来就是来了穿红的，走了挂绿的。从相聚到离开，这是一个轮回。我比较惋惜的是，二位同志没能看到更好的结局，也没能坚持到更好的时候。匆匆过客，又怎么能留得住，毕竟同事多年，又怎么忘得了？人走茶凉，随他吧。</p><p>引发走人的导火索就是日常的出差，一个公司内部的平台部门，现在沦为干外包的，心疼啊！任老说，拿着手术刀去杀猪。我们团队可能也是这样，但从我个人角度看，我们既没有身怀手术刀一般的精确技术打击能力，潍柴的项目也并不是猪，这头猪没有那么肥，反而这是头野猪，难以驯服更难以下手。从一开始的混乱就注定了这头猪猪皮厚，四肢灵活，冲起来就拦不住。我们的实施同志，如同盲人摸象般盲人抓猪。一个扯住后腿，被拖在地上，一个抓住肚皮，把猪拽的哼哼叫。大家都以为抓住了这头猪，却被动的按在猪身上，动弹不得。没有一个人去将刀子插到猪心上，因为所有人都想让猪停下来，而不是想着杀掉这头猪！</p><p>目的错了，做的一切都是徒劳，上线了又如何？还不是被问题淹没，被自己人搞残，最终被整个项目拖累，自然也就有人离开。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>二期项目比一期少了很多的怨气，知道这是没办法的事情了，反而有些从容。既然事已至此，还是硬着头皮干了。至于结局好不好，我说了不算，至于人家如何评价，我也不在乎了。针对本期项目有以下感受：</p><ol><li><p>人应该持续进步，搞增长，去肥肉。做有利于当下和未来的事情，不做让自己止步不前的事情。</p></li><li><p>做事应该更加平静，即使对面是个傻逼，有可能不需要你收拾他，只需要告诉他，你干的都对。</p></li><li><p>不用事事催的那么急，能做就做，不能做就停下来等一等。</p></li><li><p>不要催促甲方干什么，而是让甲方催促我们，毕竟鞭子在人家手里，你非要拿着人家的鞭子打自己，这不是找不痛快吗？我也不知道对不对，负责任和这一点相悖吗？</p></li><li><p>做技术层面的人员，最重要的是去魅，清楚的认知自己，比云里雾里一通吹要好。</p></li><li><p>减少怨天尤人，多学习，多看书，人生还长，分辨傻逼的能力要越来越高才好。</p></li></ol><p>生活还是要继续，毕竟身上还160个w的贷款，慢慢还吧。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>jenkins打包问题排查</title>
      <link href="/2021/08/25/jenkins-da-bao-wen-ti-pai-cha/"/>
      <url>/2021/08/25/jenkins-da-bao-wen-ti-pai-cha/</url>
      
        <content type="html"><![CDATA[<h1 id="jenkins构建部署问题的排查"><a href="#jenkins构建部署问题的排查" class="headerlink" title="jenkins构建部署问题的排查"></a>jenkins构建部署问题的排查</h1><h2 id="问题出现"><a href="#问题出现" class="headerlink" title="问题出现"></a>问题出现</h2><p>在前端jenkins构建时，出现了异常情况：</p><pre><code>09:21:37 [SSH] Exception:channel is not opened.09:21:37 com.jcraft.jsch.JSchException: channel is not opened.09:21:37     at com.jcraft.jsch.Channel.sendChannelOpen(Channel.java:765)09:21:37     at com.jcraft.jsch.Channel.connect(Channel.java:151)09:21:37     at com.jcraft.jsch.Channel.connect(Channel.java:145)09:21:37     at org.jvnet.hudson.plugins.CredentialsSSHSite.doExecCommand(CredentialsSSHSite.java:250)09:21:37     at org.jvnet.hudson.plugins.CredentialsSSHSite.executeCommand(CredentialsSSHSite.java:224)09:21:37     at org.jvnet.hudson.plugins.SSHBuilder.perform(SSHBuilder.java:104)09:21:37     at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20)09:21:37     at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:741)09:21:37     at hudson.model.Build$BuildExecution.build(Build.java:206)09:21:37     at hudson.model.Build$BuildExecution.doRun(Build.java:163)09:21:37     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:504)09:21:37     at hudson.model.Run.execute(Run.java:1853)09:21:37     at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)09:21:37     at hudson.model.ResourceController.execute(ResourceController.java:97)09:21:37     at hudson.model.Executor.run(Executor.java:427)09:21:37 Build step &#39;Execute shell script on remote host using ssh&#39; marked build as failure</code></pre><h2 id="排查问题"><a href="#排查问题" class="headerlink" title="排查问题"></a>排查问题</h2><p>目前考虑由于ssh连接的问题，所以从jenkins和服务器两方面排查。</p><h3 id="1-升级jenkins和ssh相关插件"><a href="#1-升级jenkins和ssh相关插件" class="headerlink" title="1. 升级jenkins和ssh相关插件"></a>1. 升级jenkins和ssh相关插件</h3><p>升级jenkins.war，然后升级ssh相关插件，包括jsch相关的依赖全部升级，升级完成后，发现问题依旧。</p><p>后来构建其他的job，均可以正常执行。</p><h3 id="2-排查服务器问题"><a href="#2-排查服务器问题" class="headerlink" title="2. 排查服务器问题"></a>2. 排查服务器问题</h3><p>首先查看sshd服务的运行情况：</p><pre><code>$ sudo systemctl status sshd● sshd.service - OpenSSH server daemon   Loaded: loaded (/usr/lib/systemd/system/sshd.service; enabled; vendor preset: enabled)   Active: active (running) since Wed 2021-08-25 09:02:28 CST; 4h 31min ago     Docs: man:sshd(8)           man:sshd_config(5) Main PID: 1811 (sshd)    Tasks: 9   Memory: 10.7M   CGroup: /system.slice/sshd.service           ├─ 1811 /usr/sbin/sshd -D           ├─ 9366 sshd: centos [priv]           ├─ 9375 sshd: centos [priv]           ├─ 9382 sshd: centos@notty           ├─ 9383 sshd: centos@pts/0           ├─ 9384 /usr/libexec/openssh/sftp-server           ├─ 9396 -bash           ├─ 9998 sudo systemctl status sshd           └─10008 systemctl status sshdAug 25 13:31:30 k8s-node-01 sudo[9904]: pam_systemd(sudo:session): Failed to create session: Connection timed outAug 25 13:31:30 k8s-node-01 sudo[9904]: pam_unix(sudo:session): session opened for user root by centos(uid=0)Aug 25 13:31:30 k8s-node-01 sudo[9904]: pam_unix(sudo:session): session closed for user rootAug 25 13:32:47 k8s-node-01 sshd[9982]: Accepted password for centos from 10.0.93.91 port 49194 ssh2Aug 25 13:33:12 k8s-node-01 sshd[9982]: pam_systemd(sshd:session): Failed to create session: Connection timed outAug 25 13:33:12 k8s-node-01 sshd[9982]: pam_unix(sshd:session): session opened for user centos by (uid=0)Aug 25 13:33:12 k8s-node-01 sshd[9982]: pam_unix(sshd:session): session closed for user centosAug 25 13:33:17 k8s-node-01 sudo[9998]:   centos : TTY=pts/0 ; PWD=/home/centos ; USER=root ; COMMAND=/bin/systemctl status sshdAug 25 13:33:42 k8s-node-01 sudo[9998]: pam_systemd(sudo:session): Failed to create session: Failed to activate service &#39;org.freedesktop.login1&#39;: timed outAug 25 13:33:42 k8s-node-01 sudo[9998]: pam_unix(sudo:session): session opened for user root by centos(uid=0)</code></pre><p>根据报错信息进行搜索，重点搜索这个错误信息：</p><pre><code>pam_systemd(sudo:session): Failed to create session: Failed to activate service &#39;org.freedesktop.login1&#39;: timed out</code></pre><p>解决方式是，重启systemd-logind服务，结果出现了报错的情况。</p><pre><code>$ sudo systemctl restart systemd-logindAuthorization not available. Check if polkit service is running or see debug message for more information.Failed to restart systemd-logind.service: Connection timed outSee system logs and &#39;systemctl status systemd-logind.service&#39; for details.</code></pre><p>查看systemd-logind报错信息：</p><pre><code>$ journalctl -u systemd-logind-- Logs begin at Wed 2021-08-25 08:50:28 CST, end at Wed 2021-08-25 13:42:33 CST. --Aug 25 08:54:00 k8s-node-01 systemd[1]: Starting Login Service...Aug 25 08:54:27 k8s-node-01 systemd[1]: systemd-logind.service: main process exited, code=exited, status=1/FAILUREAug 25 08:54:27 k8s-node-01 systemd[1]: Failed to start Login Service.Aug 25 08:54:27 k8s-node-01 systemd[1]: Unit systemd-logind.service entered failed state.Aug 25 08:54:27 k8s-node-01 systemd[1]: systemd-logind.service failed.Aug 25 08:54:27 k8s-node-01 systemd[1]: systemd-logind.service has no holdoff time, scheduling restart.Aug 25 08:54:27 k8s-node-01 systemd[1]: Stopped Login Service.Aug 25 08:54:27 k8s-node-01 systemd[1]: Starting Login Service...Aug 25 08:54:52 k8s-node-01 systemd[1]: systemd-logind.service: main process exited, code=exited, status=1/FAILUREAug 25 08:54:52 k8s-node-01 systemd[1]: Failed to start Login Service.Aug 25 08:54:52 k8s-node-01 systemd[1]: Unit systemd-logind.service entered failed state.Aug 25 08:54:52 k8s-node-01 systemd[1]: systemd-logind.service failed.Aug 25 08:54:52 k8s-node-01 systemd[1]: systemd-logind.service has no holdoff time, scheduling restart.Aug 25 08:54:52 k8s-node-01 systemd[1]: Stopped Login Service.Aug 25 08:54:52 k8s-node-01 systemd[1]: Starting Login Service...Aug 25 16:56:26 k8s-node-01 systemd-logind[1392]: Failed to enable subscription: Failed to activate service &#39;org.freedesktop.systemd1&#39;: timed outAug 25 16:56:26 k8s-node-01 systemd-logind[1392]: Failed to fully start up daemon: Connection timed out</code></pre><p>查看和dbus服务相关的依赖信息：</p><pre><code>$ rpm -qa|grep dbuspython-slip-dbus-0.4.0-4.el7.noarchdbus-glib-0.100-7.el7.x86_64dbus-1.10.24-13.el7_6.x86_64dbus-python-1.1.1-9.el7.x86_64dbus-libs-1.10.24-13.el7_6.x86_64abrt-dbus-2.1.11-55.el7.centos.x86_64</code></pre><p>尝试使用yum进行升级，结果发现没啥作用：</p><pre><code>$ sudo yum update --exclude=kernel-*$ rpm -qa|grep dbuspython-slip-dbus-0.4.0-4.el7.noarchdbus-glib-0.100-7.el7.x86_64dbus-1.10.24-13.el7_6.x86_64dbus-python-1.1.1-9.el7.x86_64dbus-libs-1.10.24-13.el7_6.x86_64abrt-dbus-2.1.11-55.el7.centos.x86_64</code></pre><p>最后进行重启，重启服务器后，发现问题解决！</p><h3 id="其它解决方式（未测试）"><a href="#其它解决方式（未测试）" class="headerlink" title="其它解决方式（未测试）"></a>其它解决方式（未测试）</h3><ol><li>执行：</li></ol><pre><code>systemctl daemon-reexec</code></pre><ol start="2"><li>安装新版本的openssh相关库以及内核信息</li></ol><pre><code>yum update kernel openssh-server openssh-clients</code></pre><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://access.redhat.com/discussions/3536621" target="_blank" rel="noopener">https://access.redhat.com/discussions/3536621</a></li><li><a href="https://github.com/systemd/systemd/issues/2863" target="_blank" rel="noopener">https://github.com/systemd/systemd/issues/2863</a></li><li><a href="https://dba010.com/2019/04/16/pam_systemdsshdsession-failed-to-create-session-failed-to-activate-service-org-freedesktop-login1-timed-out/" target="_blank" rel="noopener">https://dba010.com/2019/04/16/pam_systemdsshdsession-failed-to-create-session-failed-to-activate-service-org-freedesktop-login1-timed-out/</a></li><li><a href="https://blog.csdn.net/weixin_30487701/article/details/97108583" target="_blank" rel="noopener">https://blog.csdn.net/weixin_30487701/article/details/97108583</a></li><li><a href="https://www.cnblogs.com/woki/p/13718878.html" target="_blank" rel="noopener">https://www.cnblogs.com/woki/p/13718878.html</a></li><li><a href="https://blog.csdn.net/baidu_39459954/article/details/90512452" target="_blank" rel="noopener">https://blog.csdn.net/baidu_39459954/article/details/90512452</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>某甲方项目的微服务构建实施总结与心得</title>
      <link href="/2021/07/24/mou-jia-fang-xiang-mu-de-wei-fu-wu-gou-jian-shi-shi-zong-jie-yu-xin-de/"/>
      <url>/2021/07/24/mou-jia-fang-xiang-mu-de-wei-fu-wu-gou-jian-shi-shi-zong-jie-yu-xin-de/</url>
      
        <content type="html"><![CDATA[<h1 id="某甲方项目的微服务构建实施总结与心得"><a href="#某甲方项目的微服务构建实施总结与心得" class="headerlink" title="某甲方项目的微服务构建实施总结与心得"></a>某甲方项目的微服务构建实施总结与心得</h1><p>今年时运不济，我经历了一个极其垃圾的甲方项目，忙来忙去三四个月，终于要上线了，有时间来写一写这个人生中遇到的最大的坑爹项目，历数该项目中个人、团队、公司犯下的罪行，暂且定为十宗罪，从项目开始到项目结束，先历数下各个环节犯下的错误，再从我一个开发运维（DevOps）人员的角度谈一下微服务在外包项目中的实施情况，我们遇到的一些问题，最后总结下如何避免这样的大坑再次来临。</p><p>说一下甲方背景以及项目背景：</p><ul><li>本省内著名国有企业，属于重工制造</li><li>自有技术部门，抽调人手加入</li><li>合作方有做自动化机器人的和AGV小车的</li><li>对接系统包括ERP、WMS、MES等等，且通过ETL获取数据</li></ul><p>中间比较大的精力在各个项目间联调，以及接口对接，需求上的内容反而被忽视，导致下面越来越难的项目问题。</p><h2 id="十宗罪"><a href="#十宗罪" class="headerlink" title="十宗罪"></a>十宗罪</h2><h3 id="1-需求不明确"><a href="#1-需求不明确" class="headerlink" title="1. 需求不明确"></a>1. 需求不明确</h3><p>产品经理的嘴，骗人的鬼。放在我司的角度，就变成了实施的嘴，骗人的鬼；销售的嘴，阎王的鬼。</p><p>招投标肯定不是我这等小兵能进行参与的，但是他们在招投标的时候根本没有问过我们的建议。于是从招投标开始，埋下了祸根，整了一圈微服务的事儿，夸大我们各类设施齐全，完美实施微服务整套架构体系云云。</p><p>从道理上讲，我们确实有这套体系，但是从成熟度讲，夸大了。</p><p>这也就算了，但是重头戏来了，实施同志要开始调研整个项目的需求。一轮又一轮，告诉我们需求很简单，就是几个简单的增删改查，我装逼的托腮一想，并不是！</p><p>3月份中标，4月份开始驻场调研，我们任何一个开发都没有参与这个完整的调研过程，仅凭实施的需求文档就要开始编写整体开发方案以及技术点梳理。输入错误，输出当然就没个正形了。</p><p>没完没了的改文档，改着文档还得改需求，最后还得按照甲方的要求再整一版符合甲方要求的文档，要求写道事无巨细，每一个数据库字段、状态变化引起的业务变化，关联的内容，又是一波消磨耐心，最终草草了事，交付。</p><h3 id="2-临阵换框架"><a href="#2-临阵换框架" class="headerlink" title="2. 临阵换框架"></a>2. 临阵换框架</h3><p>这个就纯属开发之间互相自残了，我们在实施这个项目时，后端换了框架，前端升级了语言。理论上来说前端从vue2升级vue3不应该是什么大的问题，但他们竟然从头开始改。也就是说，之前vue2的积累全部抛弃了，重新进行组件的封装和框架的优化，直接把两个前端累到吐血！</p><p>但是后端也他妈的换了框架，原来使用的东西并不是不行，而是被嫌弃封装程度不足且功能缺失，主要还是用户权限那个部分的功能不足。于是更换了整个的微服务组件，重新使用了一个新的框架。这个新框架我简单扫了下，github上star不足300，gitee上star不足1000。用一个并不够流行且无法完全掌握的框架，去重新支撑这么高金额的一个项目，直接后果就是每个人很疲劳却又看上去没有做什么事情一样，进度巨慢、效率巨低。</p><p>最后也是最操蛋的问题来了，各种外链层出不穷，根本未对整个框架进行代码审查就贸然进行了使用，带来的后果是，应该专注业务线的开发人员都得同框架的问题做斗争。而且由于该框架的封装问题，造成了一个类似上帝类角色的项目，姑且叫它上帝项目吧，只要该项目更新，所有微服务都得进行更新升级，否则由于依赖更新不及时，就导致了服务调用或者内容加载的错误！这样造成的问题就比较验证了，毕竟在自己公司这些东西可控，但是现在在甲方的环境并不如公司这样通常，而更加便秘的网络环境，成为了更新缓慢的直接原因。</p><p>有一个哥们儿说的好，你们当时招投标吹的牛逼，自己去实现这个牛逼的时候会及其痛苦且劝退。他还有一句话，你们之前需求放的水，都是你们未来加班时间的蓄水池！</p><h3 id="3-沙雕一般的开发环境决策"><a href="#3-沙雕一般的开发环境决策" class="headerlink" title="3. 沙雕一般的开发环境决策"></a>3. 沙雕一般的开发环境决策</h3><p>任何一个项目，一般来说有三套环境足够，开发环境、测试环境、生产环境。其他的严格一点还会有预生产环境等等。进一步说，实际开发过程中用到的也就是开发环境和测试环境，测试环境通过后可以推生产环境进行使用。</p><p>一开始想的是，在公司一套开发环境，在甲方建设一套开发环境，两套环境之间同步。本来想的是能同时在公司本部与甲方方面开展开发任务，而且有甲方的开发人员进驻并领取任务，现在却成了第一阶段的拖累，各种问题频发。最大的问题是代码和产物同步，同步镜像、同步代码的脚本漫天飞，两边的gitlab和harbor全部跨起，编写的部署工具在甲方环境中每台机器一个部署，生怕这台机器挂了部署就没了。第二大的问题是，就像甲方的无理取闹一样，甲方的服务器经常报错，而且就是那种，明明在我机器跑的好好的，到他们环境就不行了。而甲方开发人员并不关心，直接告诉我们让我们去排查。</p><p>往后的一个月更是衍生出了三套环境，公司的开发环境、甲方的开发环境、甲方的测试环境，而且甲方的环境并不互通，这里指通过jenkins可以访问到gitlab但不能访问具体的服务器，给服务部署带来极大的阻碍。在公司都是CI/CD一条线，虽然慢点，但至少上线的时候不需要人为干预。到了甲方环境中这些并不是一个完整的东西，只有CI没有CD就像拄着右拐走路且左胳膊摔折的人，找个拐架着也从没有舒服的时候。</p><p>更可气的是自己人捅刀子，驻场以后非得用甲方的测试环境代替甲方的开发环境进行开发，这操作真是三个臭皮匠的水平。建完了就弃用，岂不是沙雕行为？这样搞得甲方三个人都懵逼了，把测试环境当开发环境用我也是头一回看见。</p><p>其它的问题就是保障系统未能很好的使用，在公司的时候，出现问题直接去服务器排查，再调用一次看看结果。而在甲方这边，由于网络权限以及堡垒机的关系，登录一次服务器十分复杂，需要堡垒机、vpn、各类连接工具等，所以才开始重视这一部分工具的使用。而排查时最重要的是通过kibana查看ES中的日志信息，而且对于各个位置的报错信息，并没有很好的配置告警方式。这块的缺陷在早期也十分影响开发进度。</p><p>最后的最后，就是各路内容的同步了，无论是代码同步、镜像同步、数据库同步、配置信息同步，一同步就是腥风血雨，忘了执行sql语句的、配置信息改了不说的、镜像不打tag直接提交的、提交信息不规范修改的代码被插件吞了的。。。死的姿势各种各样，应有尽有，一出事我就得去擦屁股，按照固定套路走一遍，这个环节哪个也不能少，往往到最后才能把问题揪出来，我司开发人员确实牛逼！</p><h3 id="4-复杂且难以开通的网络权限"><a href="#4-复杂且难以开通的网络权限" class="headerlink" title="4. 复杂且难以开通的网络权限"></a>4. 复杂且难以开通的网络权限</h3><p>要问这个项目什么印象最深，那就是开网络权限了。vpn要开权限就算了，wifi不给外网也就算了，为毛wifi连内网都访问不了？！每次开权限都要焚香沐浴等一整天，我连一分钟都等不了。非得急了眼，骂他两句，才磨磨唧唧、骂骂咧咧的给开权限，充分发扬了互相牵制、不愿配合的精神。</p><p>在涉及端口多、服务器多的情况下，不是每一次开权限都能顺利列举出所有端口的。而且vpn和wifi的权限管理不在一个部门你能信，每次给甲方对接的妹子提申请端口的权限的时候，总感觉对不起人家，因为人家老因为我提申请而挨骂，被拉黑。</p><p>因为网络权限耽误的时间最多，后来直接要求他们出人常驻我们这里，遇到开权限的就搞，这样这个问题才堪堪解决。但是HTTP协议和TCP协议不分又导致了数据库、缓存等组件连不上了。你好赖211、985出来的，别丢人好不好，表现成这样，是不是砸自己母校的牌子啊！</p><h3 id="5-便秘的联调体验"><a href="#5-便秘的联调体验" class="headerlink" title="5. 便秘的联调体验"></a>5. 便秘的联调体验</h3><p>由于初期的过于相信甲方的承诺，导致中期开始为自己的选择买单。前期在公司进行开发，部署的是以部分人员到达现场，其余人员留在济南开发，通过轮岗的方式定期更换人员到甲方。这样带来了网络环境联调、代码质量下降、长期出差带来的情绪不稳且易怒等问题。先说网络环境，如果所有人都在公司开发，就必须要使用甲方的vpn连接到甲方服务器环境中，最大的问题在于联调时服务ip地址或者域名变更带来的服务无法调用的问题。</p><p>针对这个问题，我的解决方式是利用端口映射的技术，通过服务器ip端口号。好死不死的是，端口号有多个人争，因为一个服务有多个人开发，一个人登录对应的端口映射工具，另一个人就登不上，其实可以再加台机器解决的。但是，我还是没干，这个是我这边的问题了。在vpn下联调是一个沙雕的体验，网络速度慢的要死，搞不好还得开个人的热点来进行连接，那就更慢了。换个角度说，在甲方现场的同事要通过vpn连接公司的环境进行联调，也是相同的体验。</p><p>代码质量下降，老问题了。面对这样的外包项目是没有代码审查的。第一是根本没时间进行代码审查，第二是甲方的CI/CD流水线残缺不全，没有CD也就罢了，连代码审查的工具都没有。第三，频繁的需求变动，导致上帝类一般的依赖越来越不受控，往往该依赖项目一更新，所有的服务都得重新构建并部署。如果这部分代码同步到甲方了，那还好说，无非是部署上需要人工介入；如果这部分代码没同步到甲方，压根就没给人家，这时候就得从公司拷贝镜像到甲方环境，那么部署的速度就如同乌龟一般缓慢。网络原因经常断联就不提，拷贝过来以后还存在md5校验不通过的情况，只能重新进行拷贝。</p><p>下一个是各个开发联调的时候，我在甲方现场还好一些，有些问题能立马解决。如果我不在现场，那就好多事情解决不了。前面提到了对VPN、甲方现场wifi开放端口访问权限的问题，其实都是这么一点一点聊出来的。按照之前使用云服务的经验在防火墙直接开放TCP协议的端口，自然HTTP协议就可用了。但是甲方这边只给开HTTP端口权限，并不给开TCP端口权限。一到本地联调的时候，数据库就连不上，缓存连不上，就各种报错。</p><p>最后是各个开发同志的情绪，大部分同志来公司工作都是奔着收入稳定以及和谐的同事关系，现在一出差各种情绪上的抵触就来了，毕竟不是一天两天，而是一周两周，后来是整一个月，每周周日能回去一次。所以每个同志并不想做这个事情，而且我们属于平台部门，并不是一线外包工作人员，难免每个人心气很高，并没有想到是这样一个混乱的局面，上下落差并不低，导致士气低落。最严重的问题是长期加班，在甲方现场至少是9116，强度倒是不大，但是时常太长，疲劳感重！</p><p>总结一下的话，整个服务开发过程中的联调及其痛苦，效率低下，且工作时间长，情绪不稳定，难以精细化投入！</p><h3 id="6-糟心而逃避的甲方开发员工"><a href="#6-糟心而逃避的甲方开发员工" class="headerlink" title="6. 糟心而逃避的甲方开发员工"></a>6. 糟心而逃避的甲方开发员工</h3><p>说起这个咱可就不困了，甲方是国企，是不是国企人员都有点大病似的，总会有人不正常，具体表现在：说的不认做的也不认、懈怠懒惰、不愿学习也不愿进步。先说第一条，说的不认。所有需求和对接都在微信群内进行，既然是微信群，难免信息漫天飞，所以强调事情的时候总要拉上对应的人员。这样骚操作就开始了，我用excel排了上线的环境准备的计划，具体到每一台服务器装什么工具，什么时间完成，并且发出来的时候@了甲方对接的三人组及其领导。第二天骚操作来了，三人组中二号人物说他不知道上线前需要进行的环境部署的内容。本人很生气，后果当然也没那么严重，我只是说了一遍，他们并不想听，并摆出王八念经的姿态。这时候一看，就不留面子了，把聊天记录全发出来以后，轮到他不说话了，老老实实带着其它二人去执行。第二次就是联调了，三人组中三号人物报给我错误，说在他本地可以运行正常的流程放到测试环境后挂掉，并质疑我们这边的工作。这个就不好说了，于是就排查了一下他的代码提交记录，结果发现，人家功能代码都不给你提交到位，你给我搁这儿糊弄鬼啊。一发到微信群里，他自己也老实了，把代码提交，还得板着个脸来找我给他上线。我真的又气又笑，真没有人品。</p><p>懒惰懈怠就很明显了，功能节点交付的日期前写了一大堆没用的代码，死活不去搞业务代码，哄着惯着好不容易写完了，结果到线上一跑，不是少字段就是报错，害的人全陪他们查问题，更呲毛的是，连功能都开发不全就往上推，还说我业务理解的就是那样，丢了数据还不承认。有些事情没做完善，认了就是，咱再找补回来，完善上去就行，非得纠结背锅如何如何。净搞那虚的，有那时间扯皮，把技术练练，不要那么拉跨还来掺和开发，全是擦屁股的活儿，还捞不着好。</p><p>不愿学习也不愿进步，具体体现在教过的东西不记，Linux命令重复了教了五遍，给出教程也一概不学，出现问题就找我，自己一概不查也不去思考问题所在。再有强调过的东西不做，告诉三人组一定要更新完配置文件再进行重启，结果一个更新的都没有；告诉三人组要按照excel中写的版本进行更新，结果就是自己执行半天也更新不了，就怨言我们不教。还有就是执行各类命令不仔细、不细心，明明多个点非得少一个点，明明脚本写明白了怎么用还得问人家怎么搞。最要命的是明明是最佳实践（Best Practice），非得去用自己的思路去做，而且还认为自己做的很对。后来我直接不说了，根本就没有做人的原则，没有开拓的精神，没有那股破釜沉舟的勇气，这样能发展啥，当狗够用，当人远远不足。</p><h3 id="7-几乎等同于没有的CI-CD流水线"><a href="#7-几乎等同于没有的CI-CD流水线" class="headerlink" title="7. 几乎等同于没有的CI/CD流水线"></a>7. 几乎等同于没有的CI/CD流水线</h3><p>这个位置想谈的其实是代码同步时的便秘操作，结果后来经过自己的优化，感觉还行，只是网络稍微慢一些，故也不能称之为罪孽。</p><p>可造孽的事情是，CI/CD体系本来是由我们搭建的，而且按照我们的要求进行建设。甲方三人组加一个运维，四个人单方面决定用他们自己的流水线去搞，提供了jenkins和gitlab，其它的就没有了，没有sonarqube，dockerhub还是用harbor新搭建的，nexus权限都没给开全，后端能构建，前端就不能构建了。就这些事情扯皮就得扯了三天，这之外最大的黑天鹅来了，部署到对应机器的时候，ssh连不上了。这时候得为这四个人鼓掌了，当然是夸他们干的好，别的咱也不敢说。</p><p>当服务器连不上的时候，我的错误就是求爷爷告奶奶开权限，但四人组并没有人同意，而且也不给协调。那只能构建出来再进行手动部署了，于是我就写了个简易的部署工具，修改下版本的参数直接就能部署完成，对各个微服务的部署使用postman进行调用。这样勉强是有了CD功能。但这个CD功能并不能自动化，本来以为通过jenkins进行调用就可以访问的，结果人家根本不给你开网络权限，直接胎死腹中。手动部署就意味着人工介入，一介入就容易出问题，毕竟人不是机器，不能时时刻刻都处于最佳的状态，也不能完全保证不会出错。</p><p>所以既然你们选择用微服务架构，那请你们调研好我们提供的内容和要求你们自己能否满足，能否给与我们正常的支撑，能否在你们的使用场景下流畅运转。这些你都不考虑，然后你还要加入进来搞开发，那你玩个球还是玩个蛋？你是来捣蛋还是来添乱？做人要有起码的原则，做开发要有起码的技术素养，这些你都没有，你在这个临时团队里到底是干啥？除了扯淡也没见你做啥贡献。自己都想不清楚自己要做什么用什么，怎么可能完整并正确的传达给作为执行者的乙方？！</p><h3 id="8-背刺队友的奸佞实施"><a href="#8-背刺队友的奸佞实施" class="headerlink" title="8. 背刺队友的奸佞实施"></a>8. 背刺队友的奸佞实施</h3><p>我其实不想一棍子打死所有的实施同志，因为各位在一线的实施同志是做出了自己的贡献，努力的为项目完善做出了成绩，不能一棍子打死。要认真看待各位同志的贡献，并给予肯定。但是必须说，群众当中有坏人，实施内部也有坏人。至于坏人，必须进行专政！</p><p>我想不明白的是，为何现场六个实施，只有不多的几个人在干活，不明白其他人在干啥，不能妄加揣测。但是，坏人们干了什么，我们要说，要批判。</p><p>首先就是某些实施领导人前背后两套戏码，公司大领导一来开始装bility，各种努力工作，努力协调，讨论方案，决策部署，有事没事都要找一下存在感。但仔细一看，说的话是废话，做的事儿根本无关紧要，方案是别人制定的却立马就说是自己搞得。你这邀功加舔狗的路子有点野。大领导一走直接隐身，不见话语更不见人，开始甩锅和埋怨，埋怨完下属埋怨开发。舌头挺长只会说，舔不出味怨厕所。</p><p>到了加班的时候，那就更加一言难尽啊，周五下午回到公司就休息，到周日中午12点来加班，还得拉个人一起。周日原本就是确定的休息，你把人家拉来干啥？走的时候还得拍个照片告知大群，我们多么努力多么感动，鬼知道你们在这里玩儿还是在这里扯犊子？平时加班本来就时间长，有几次开发这边打算提前走，实施人员的领导真是属烂板凳子腿的——死活不走，一到我们收拾东西要走，就来需求了，直接就无语。这个需求没整完，这个内容不正确，这个数据不完善。早不提晚不提，非得到了快走了再提。虽然说今日事今日毕，但开发任务已经完成，实施方面的问题没完成，我觉着没有技术层面的关系。</p><p>我司的实施实际上被赋予了一定的产品经理的内容，要做需求调研，要理清流程，还得做一部分设计。既然有产品经理的职责就请认真搞好需求、挡好需求，最起码的边界要有，所在的职责要明确。本身作为开发人员来说，接收需求并开发出来，在这个角度来说就是个执行者。输入和输出应该是确定的，有稳定且确定的输入，才能有完整且正常的输出。需求变动难免，是不是再变动之前，应该同甲方确定好要变更的点以及最终的解决方案再交付给开发？一上来就是让人嚷嚷要什么，必须得给我做。这就值180个大嘴巴子361度扇。讲不清，辨不明，焉能做？</p><p>可气的是我司实施又被赋予了测试的职责，一测试就鸡飞狗跳，每个人只管自己的流程，流程中的输入输出由手工去导入数据或者输入表格，人工去做就易出错。最关键的是，各个部分就我知道的情况而言至今未能把所有流程穿起来。不能说你自己通过了这个流程就通过了。是否有考虑过测试边界，是否有写过测试的手册或者文档，是否模拟了整个流程中的数据流转，到每一个节点的数据是否正确？啥都不考虑，什么都靠人手工去导入，一旦导入出错就得拉着开发去排查，而监控、日志这几个保障的部分又不能很好的被利用，造成问题排查的困难。</p><p>看着六个人不少为何进度并不往前走？是谁在内斗，又是谁在拖后腿？我没法想这个破事儿！我也不能去抹杀真正认真干活儿的同事。</p><h3 id="9-匹夫无罪，怀璧其罪"><a href="#9-匹夫无罪，怀璧其罪" class="headerlink" title="9. 匹夫无罪，怀璧其罪"></a>9. 匹夫无罪，怀璧其罪</h3><p>事儿摊上了，就认真解决呗，不管多困难，也算是第一阶段马上要上线了。回顾整个走来的路，各个环节上都是各种问题。如何指挥一个大兵团作战，如何同友军进行协同？越想越觉得我们现在是国军，结果甲方还以为我们是共军，就两边的认知出现了严重的偏差。要上线了，早就说明开始逐步封版本、打tag、存产物了。结果事儿非得在黎明之前给你咔嚓一个大雷，就糊弄着添加需求点，啥也不设计就开始搞，还得今天之内上线。这是为了讨好甲方不择手段了？他提什么我们都要做吗？必须要做吗？不能放到二期或者三期吗？你舔甲方我理解，但是不能没有原则，不能没有底线，无论做人还是做事。</p><p>是否有人真正反思，在做了这么多项目以后，为什么还不能形成一套完整的机制？为什么不能真正做好一套方法论的东西？为什么还找不到做什么与不做什么的边界？为什么让我一个开发人员问出来这么多为什么？</p><p>说好听点，以前大意了没有闪；说难听点，就是公司根本不重视这个，只要能干完活拿钱就行，丝毫不在乎可持续发展，也不管各类人员的心理建设，更不在乎技术的演进和公司业务的结合！人文不到位，流程也不到位，该严格的不严格，该做的事情做不好！</p><h2 id="反思和总结"><a href="#反思和总结" class="headerlink" title="反思和总结"></a>反思和总结</h2><ul><li>技术上</li></ul><ol><li>微服务有其合理存在的特性，如果有一点不满足，造成的结果是用百倍精力去维护，人工兜底总是不完全可靠。</li><li>利用一项新的技术点或者确定一条新的技术路线时，要用其长、避其短，充分与业务结合，来决策。不要谁火用谁，不要看流量，多看现实。</li><li>避免多地联合开发的情况，避免因网络问题导致的开发难、调试难。</li><li>发动机只有有效减少损耗才能真正达到高效能，而团队必须磨合到位才能真正开展好工作。</li><li>架构也好，项目也好，最重要的是妥协，明确场景对技术选型的影响，知道在什么场景下采用什么样的技术，实事求是！</li></ol><ul><li>业务上</li></ul><ol><li>知道什么时候卡住需求漫出，知道什么时候开始提速，有合理的节奏感。</li><li>禁止无限跪舔甲方的行为，禁止无原则加需求的行为，尤其是约定的上线时间前，严控需求输出。</li></ol><ul><li>决策上</li></ul><ol><li>尽可能避免后续项目的甲方开发人员对团队的影响，并剥离给他们非核心的需求。</li><li>尽可能和优秀的人共事，减少同甲方的交集，尽可能不和甲方人员发生任何关系，因为不怀好意的太多，你把握不住。</li><li>下一代产品上怎么走，探索的时候考虑以点带面，可以考虑不断通过项目去演化产品</li></ol><h2 id="悬而未决的问题"><a href="#悬而未决的问题" class="headerlink" title="悬而未决的问题"></a>悬而未决的问题</h2><ol><li>究竟什么样的方式才是微服务联调的最佳实践？</li><li>对上述问题的持续性改革与学习？对于无法控制的内容，也就无法想到好的方式去解决，进而延申到如何去搞政治、搞人事。</li><li>如何避免为无能人的无能决策导致的问题买单？</li></ol><h2 id="番外：所谓的外包项目微服务实施陷阱"><a href="#番外：所谓的外包项目微服务实施陷阱" class="headerlink" title="番外：所谓的外包项目微服务实施陷阱"></a>番外：所谓的外包项目微服务实施陷阱</h2><p>推荐阅读：<a href="https://juejin.cn/post/6977913839366963237" target="_blank" rel="noopener">坑蒙拐骗微服务，掌灯填坑架构人 ！</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>cadvisor+node-exporter+prometheus+grafana搭建主机和docker容器监控</title>
      <link href="/2021/06/22/cadvisor-node-exporter-prometheus-grafana-da-jian-zhu-ji-he-docker-rong-qi-jian-kong/"/>
      <url>/2021/06/22/cadvisor-node-exporter-prometheus-grafana-da-jian-zhu-ji-he-docker-rong-qi-jian-kong/</url>
      
        <content type="html"><![CDATA[<h1 id="cadvisor-node-exporter-prometheus-grafana搭建主机和docker容器监控"><a href="#cadvisor-node-exporter-prometheus-grafana搭建主机和docker容器监控" class="headerlink" title="cadvisor+node-exporter+prometheus+grafana搭建主机和docker容器监控"></a>cadvisor+node-exporter+prometheus+grafana搭建主机和docker容器监控</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前缺乏针对主机和各个docker容器的监控，且缺乏全面的覆盖，需要构建全面覆盖从主机到docker容器的监控，甚至针对单个进程以及jvm的监控。</p><p>这样在选择监控平台的时候需要较广的覆盖面，监控指标到位，且得有良好的展示方式。基于以上需求，选择以prometheus为核心的监控体系，各个部分功能如下图：</p><p><img src="%E5%8A%9F%E8%83%BD%E7%BB%93%E6%9E%84%E4%BD%93%E7%B3%BB%E7%A4%BA%E6%84%8F.png" alt></p><p>利用各种exporter，收集对应的监控数据信息，全面覆盖从数据库到服务到主机层面的监控能力，甚至可以对单个特殊进程进行监控！</p><h2 id="版本以及端口号信息"><a href="#版本以及端口号信息" class="headerlink" title="版本以及端口号信息"></a>版本以及端口号信息</h2><table><thead><tr><th>程序信息</th><th>版本号</th><th>占用端口</th></tr></thead><tbody><tr><td>pormetheus</td><td>2.20.1</td><td>9090</td></tr><tr><td>grafana</td><td>7.1.5</td><td>8888</td></tr><tr><td>node-export</td><td>1.0.1</td><td>9100</td></tr><tr><td>cadivisor</td><td>0.32.0</td><td>9999</td></tr></tbody></table><h2 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h2><ol start="0"><li>主机信息</li></ol><table><thead><tr><th>ip地址</th><th>部署程序信息</th></tr></thead><tbody><tr><td>192.168.166.203</td><td>prometheus+grafana部署</td></tr><tr><td>192.168.166.204</td><td>redis-exporter、mongodb-exporter</td></tr><tr><td>192.168.166.205</td><td>mysqld-exporter</td></tr><tr><td>192.168.166.202</td><td>cadvisor</td></tr><tr><td>192.168.166.199</td><td>cadvisor</td></tr><tr><td>192.168.166.206</td><td>process-exporter</td></tr><tr><td>192.168.166.242</td><td>process-exporter</td></tr><tr><td>其它开发环境机器，包括上述提到的机器</td><td>node-exporter</td></tr></tbody></table><ol><li>prometheus</li></ol><p>通过ssh连接到192.168.166.203机器上，以docker方式安装prometheus，如下：</p><pre><code>$ docker pull prom/prometheus:v2.20.1$ mkdir -p docker/prometheus$ cd ~/docker/prometheus// 这里先创建配置文件，临时不进行修改，等最后配置完成后完善该文件$ touch prometheus.yml$ docker run -d --restart=always -p 9090:9090 -v /home/centos/docker/prometheus/:/etc/prometheus/ --name=prometheus prom/prometheus:v2.20.1$ docker ps -a | grep prometheus63d5766795a9        prom/prometheus                                 &quot;/bin/prometheus --c…&quot;   27 hours ago        Up 5 hours          0.0.0.0:9090-&gt;9090/tcp           prometheus// 部署完成后，在防火墙侧开启对应端口信息$ sudo firewall-cmd --add-port=9090/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="2"><li>grafana</li></ol><pre><code>$ docker pull grafana/grafana:7.1.5$ mkdir -p ~/docker/grafana$ docker run -d -p 8888:3000 --name=grafana -v /home/centos/docker/grafana:/var/lib/grafana --restart=always grafana/grafana$ docker ps -a | grep grafanadeabcb596a4f        grafana/grafana                                 &quot;/run.sh&quot;                26 hours ago        Up 26 hours         0.0.0.0:8888-&gt;3000/tcp           grafana// 部署完成后，在防火墙侧开启对应端口信息$ sudo firewall-cmd --add-port=8888/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="3"><li>node-exporter</li></ol><p>在所有主机上进行部署，如下：</p><pre><code>$ docker pull prom/node-exporter:v1.0.1 $ docker run -d -p 9100:9100 -v &quot;/proc:/host/proc:ro&quot; -v &quot;/sys:/host/sys:ro&quot; -v &quot;/:/rootfs:ro&quot; --net=&quot;host&quot; --restart=always --name=node-exporter prom/node-exporter:v1.0.1$ sudo firewall-cmd --add-port=9100/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="4"><li>cadvisor</li></ol><p>在docker镜像集中的主机上进行部署，目前主要是放在192.168.166.202和192.168.166.199这两台机器上，如下：</p><pre><code>$ docker pull google/cadvisor:0.32.0  $ docker run --restart=always --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker:/var/lib/docker:ro --publish=9999:8080 --detach=true --name=cadvisor google/cadvisor:0.32.0$ sudo firewall-cmd --add-port=9999/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="5"><li>mysqld-exporter</li></ol><p>对于MySQL的监控，首先需要设定合适的用户，创建exporter上报的用户信息，如下：</p><pre><code>$ mysql -u root -pEnter password:&gt; GRANT REPLICATION CLIENT, PROCESS ON  *.*  to &#39;exporter&#39;@&#39;%&#39; identified by &#39;123456&#39;;&gt; GRANT SELECT ON performance_schema.* TO &#39;exporter&#39;@&#39;%&#39;;&gt; flush privileges;</code></pre><p>如果执行MySQL命令时提示密码过于简单，可以使用以下的命令执行：</p><pre><code>&gt; GRANT REPLICATION CLIENT, PROCESS ON  *.*  to &#39;exporter&#39;@&#39;%&#39; identified by &#39;ht@MySQL111&#39;;&gt; GRANT SELECT ON performance_schema.* TO &#39;exporter&#39;@&#39;%&#39;;&gt; flush privileges;</code></pre><p>然后在MySQL所在的机器上安装exporter，如下：</p><pre><code>$ docker pull prom/mysqld-exporter:v0.12.1// 注意指定MySQL的连接信息$ docker run -d  --restart=always  --name mysqld-exporter -p 9104:9104   -e DATA_SOURCE_NAME=&quot;exporter:ht@MySQL111@(10.0.93.100:3306)/&quot;   prom/mysqld-exporter:v0.12.1$ sudo firewall-cmd --add-port=9104/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="6"><li>redis-exporter</li></ol><p>在Redis所在的机器上安装exporter，如下：</p><pre><code>$ docker pull oliver006/redis_exporter:v1.11.0-amd64// 注意：后面指定redis连接参数时，如果无密码，最后面--redis.password可以不添加$ docker run -d --restart=always --name=redis_exporter -p 9121:9121 oliver006/redis_exporter:v1.11.0-amd64 --redis.addr redis://192.168.166.204:6379 --redis.password &#39;ht@redis160&#39;$ sudo firewall-cmd --add-port=9121/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="7"><li>mongod-exporter</li></ol><p>首先在mongodb所在的机器，登录mongodb，创建监控用户信息，操作如下：</p><pre><code>$ mongo -u mongo-admin -p --authenticationDatabase adminMongoDB shell version v4.2.2Enter password:// 使用admin&gt; use adminswitched to db admin// 创建监控用户&gt; db.getSiblingDB(&quot;admin&quot;).createUser({ user: &quot;mongodb_exporter&quot;, pwd: &quot;123456&quot;, roles:[{ role: &quot;clusterMonitor&quot;, &quot;db&quot;: &quot;admin&quot;}, { role: &quot;read&quot;, db: &quot;local&quot;}]})Successfully added user: {        &quot;user&quot; : &quot;mongodb_exporter&quot;,        &quot;roles&quot; : [                {                        &quot;role&quot; : &quot;clusterMonitor&quot;,                        &quot;db&quot; : &quot;admin&quot;                },                {                        &quot;role&quot; : &quot;read&quot;,                        &quot;db&quot; : &quot;local&quot;                }        ]}// 退出&gt; exit</code></pre><p>然后启动监控的docker镜像，如下：</p><pre><code>$ docker pull noenv/mongo-exporter:0.11.1// 部署前注意ip地址的变更$ docker run -d --restart=always --name mongo_exporter -p 9123:9123 noenv/mongo-exporter:0.11.1 --web.listen-address=:9123 --mongodb.uri=mongodb://mongodb_exporter:123456@192.168.166.204:27017$ sudo firewall-cmd --add-port=9123/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="8"><li>process-exporter :9256  - 0.7.2，目前需要重新进行设置，尚未启用</li></ol><pre><code>docker pull ncabatoff/process-exporter:0.7.2mkdir -p ./process-exporter/configvim process-exporter.yml// 监控nacosprocess_names:  - name: &quot;{{.Matches}}&quot;    cmdline:    - &#39;.nacos*&#39;:wqdocker run -d -p 9256:9256 --restart=always --name process-exporter --privileged -v /proc:/host/proc -v /home/centos/process-exporter/config:/config ncabatoff/process-exporter:0.7.2 --procfs /host/proc --config.path /config/process-exporter.yml$ sudo firewall-cmd --add-port=9256/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><p>以监控nacos进程为例子，进行配置，通过匹配nacos的进程信息，实现对nacos进程的监控。这里一共有四种进程监控方式：</p><ul><li>“&#123;&#123; .Comm &#125;&#125;” 包含原始可执行文件的basename，/proc/<pid>/stat 中的换句话说，2nd 字段</pid></li><li>“&#123;&#123; .ExeBase &#125;&#125;” 包含可执行文件的basename</li><li>“&#123;&#123; .ExeFull &#125;&#125;” 包含可执行文件的完全限定路径</li><li>“&#123;&#123; .Username &#125;&#125;” 根据用户信息进行匹配</li><li>“&#123;&#123; .Matches &#125;&#125;” 映射包含应用命令行所产生的所有匹配项</li></ul><ol start="9"><li>jmx-exporter的部署</li></ol><ol start="10"><li>kafka-exporter的部署</li></ol><pre><code>$ docker pull danielqsj/kafka-exporter:latest$ docker run -d --rm -p 9308:9308 danielqsj/kafka-exporter:latest --kafka.server=192.168.166.206:9092$ sudo firewall-cmd --add-port=9308/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><ol start="11"><li>fastdfs-exporter部署</li></ol><ol start="12"><li>RocketMQ-exporter部署</li></ol><h2 id="prometheus的配置"><a href="#prometheus的配置" class="headerlink" title="prometheus的配置"></a>prometheus的配置</h2><p>最后，编辑前面指定的prometheus的配置文件，并重启prometheus的docker镜像：</p><pre><code>$ vim ~/docker/prometheus/prometheus.yml// 输入以下信息global:  scrape_interval: 60s  evaluation_interval: 60sscrape_configs:  - job_name: prometheus    static_configs:      - targets: [&#39;localhost:9090&#39;]        labels:          instance: prometheus  - job_name: server199-node-exporter    static_configs:      - targets: [&#39;192.168.166.199:9100&#39;]        labels:          instance: server199-node-exporter  - job_name: server199-cadivisor    static_configs:      - targets: [&#39;192.168.166.199:9999&#39;]        labels:          instance: server199-cadivisor  - job_name: server203-node-exporter    static_configs:      - targets: [&#39;192.168.166.203:9100&#39;]        labels:          instance: server203-node-exporter  - job_name: server201-node-exporter    static_configs:      - targets: [&#39;192.168.166.201:9100&#39;]        labels:          instance: server201-node-exporter  - job_name: server202-node-exporter    static_configs:      - targets: [&#39;192.168.166.202:9100&#39;]        labels:          instance: server202-node-exporter  - job_name: server202-cadivisor    static_configs:      - targets: [&#39;192.168.166.203:9999&#39;]        labels:          instance: server202-cadivisor  - job_name: server236-node-exporter    static_configs:      - targets: [&#39;192.168.166.236:9100&#39;]        labels:          instance: server236-node-exporter  - job_name: server200-node-exporter    static_configs:      - targets: [&#39;192.168.166.200:9100&#39;]        labels:          instance: server200-node-exporter  - job_name: server204-node-exporter    static_configs:      - targets: [&#39;192.168.166.204:9100&#39;]        labels:          instance: server204-node-exporter  - job_name: server204-redis-exporter    static_configs:      - targets: [&#39;192.168.166.204:9121&#39;]        labels:          instance: server204-redis-exporter  - job_name: server204-mongo-exporter    static_configs:      - targets: [&#39;192.168.166.204:9123&#39;]        labels:          instance: server204-mongo-exporter  - job_name: server205-node-exporter    static_configs:      - targets: [&#39;192.168.166.205:9100&#39;]        labels:          instance: server205-node-exporter  - job_name: server205-cadivisor    static_configs:      - targets: [&#39;192.168.166.205:9999&#39;]        labels:          instance: server205-cadivisor  - job_name: server205-mysql-exporter    static_configs:      - targets: [&#39;192.168.166.205:9104&#39;]        labels:          instance: server205-mysql-exporter  - job_name: server206-node-exporter    static_configs:      - targets: [&#39;192.168.166.206:9100&#39;]        labels:          instance: server206-node-exporter  - job_name: server206-cadivisor    static_configs:      - targets: [&#39;192.168.166.206:9999&#39;]        labels:          instance: server206-cadivisor  - job_name: server206-process-exporter    static_configs:      - targets: [&#39;192.168.166.206:9256&#39;]        labels:          instance: server206-process-exporter  - job_name: server207-node-exporter    static_configs:      - targets: [&#39;192.168.166.207:9100&#39;]        labels:          instance: server207-node-exporter  - job_name: server208-node-exporter    static_configs:      - targets: [&#39;192.168.166.208:9100&#39;]        labels:          instance: server208-node-exporter// :wq保存退出</code></pre><p>需要重新启动prometheus的docker容器</p><pre><code>$ docker restart prometheus</code></pre><h2 id="数据展示设置"><a href="#数据展示设置" class="headerlink" title="数据展示设置"></a>数据展示设置</h2><p>配置数据源信息</p><p>针对grafana的图表配置，添加三个模板</p><ul><li>Node Exporter for Prometheus Dashboard CN v20200628</li><li>Docker and OS metrics ( cadvisor, node_exporter )</li><li>Docker monitoring</li></ul><p>导入dashboard之后，就可以查看结果了</p><p><strong>注意：</strong>需要添加模板列表。</p><h2 id="告警操作设置"><a href="#告警操作设置" class="headerlink" title="告警操作设置"></a>告警操作设置</h2><p>使用Prometheus+Alertmanager进行告警的操作，分为两个部分：</p><ul><li>Prometheus负责中配置警报规则，将警报发送到Alertmanager。</li><li>Alertmanager 负责管理这些警报，包括沉默，抑制，合并和发送通知。</li></ul><p>Alertmanager 发送通知有多种方式，支持邮件和Webhook等报警方式，这里采用webhook的方式，通过对接企业微信机器人的形式，达到报警的目的！</p><p>主要分为以下四个方面：</p><ul><li>prometheus报警规则配置</li><li>alertmanager部署及配置</li><li>关联prometheus和alertmanager</li><li>配置报警通知方式</li></ul><h3 id="prometheus报警规则配置"><a href="#prometheus报警规则配置" class="headerlink" title="prometheus报警规则配置"></a>prometheus报警规则配置</h3><p>对prometheus配置文件进行调整，添加报警部分的配置，如下：</p><pre><code>global:  scrape_interval: 60s  evaluation_interval: 30s# 告警功能配置alerting:  alertmanagers:    - static_configs:      - targets:        # - 192.168.166.203:9093# 告警规则文件设置rule_files:  - &quot;rules/*_rules.yml&quot;scrape_configs:  - job_name: prometheus    static_configs:      - targets: [&#39;localhost:9090&#39;]        labels:          instance: prometheus......</code></pre><p>这里临时注释了alertmanager的连接信息，待安装配置alertmanager时，放开即可。</p><p>添加prometheus报警规则，如下：</p><pre><code>$ mkdir /home/centos/docker/prometheus/rules$ vim host_rules.ymlgroups:- name: host-lost-connection-rule  rules:  - alert: ServerStatus    expr: up == 0    for: 2m    labels:      severity: page      status: warning    annotations:      summary: &quot;{{$labels.instance}}:服务器关闭&quot;      description: &quot;{{$labels.instance}}:服务器关闭&quot;# 报警组组名称- name: hostStatsAlert  #报警组规则  rules:   #告警名称，需唯一  - alert: hostCpuUsageAlert    #promQL表达式    expr: sum(avg without (cpu)(irate(node_cpu_seconds_total{mode!=&#39;idle&#39;}[5m]))) by (instance) &gt; 0.85    #满足此表达式持续时间超过for规定的时间才会触发此报警    for: 1m    labels:      #严重级别      severity: page    annotations:      #发出的告警标题      summary: &quot;实例 {{ $labels.instance }} CPU 使用率过高&quot;      #发出的告警内容      description: &quot;实例{{ $labels.instance }} CPU 使用率超过 85% (当前值为: {{ $value }})&quot;  - alert: hostMemUsageAlert    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)/node_memory_MemTotal_bytes &gt; 0.85    for: 1m    labels:      severity: page    annotations:      summary: &quot;实例 {{ $labels.instance }} 内存使用率过高&quot;      description: &quot;实例 {{ $labels.instance }} 内存使用率 85% (当前值为: {{ $value }})&quot;</code></pre><p>配置添加完成后，重启prometheus服务，如下：</p><pre><code>$ docker restart prometheus</code></pre><h3 id="alertmanager部署及配置"><a href="#alertmanager部署及配置" class="headerlink" title="alertmanager部署及配置"></a>alertmanager部署及配置</h3><p>使用docker安装alertmanager，如下：</p><pre><code>$ docker pull prom/alertmanager:v0.21.0$ mkdir -p ~/docker/alertmanager$ docker run -d -p 9093:9093 --restart=always --name alertmanager  -v /home/centos/docker/alertmanager/:/etc/alertmanager/ prom/alertmanager:v0.21.0$ sudo firewall-cmd --add-port=9093/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><p>启动完毕后，配置两种告警方式，一种是邮件告警，另一种是Webhook方式告警。</p><h4 id="1-邮件告警配置"><a href="#1-邮件告警配置" class="headerlink" title="1. 邮件告警配置"></a>1. 邮件告警配置</h4><p>首先确定邮件发送信息的配置，然后编写邮件发送模板，最后测试邮件是否正常发送。</p><p>编辑alertmanager.yml，添加邮件报警配置信息：</p><pre><code>$ vim alertmanager.yml// 输入以下信息global:  resolve_timeout: 2m  smtp_smarthost: &#39;smtp.qq.com:465&#39;  smtp_from: &#39;hoteam-develop@qq.com&#39;  smtp_auth_username: &#39;hoteam-develop@qq.com&#39;  smtp_auth_password: &#39;bysvpwwkrdpvddaj&#39;  smtp_require_tls: false  smtp_hello: &#39;Alert&#39;templates:     ##消息模板  - &#39;/etc/alertmanager/template/*.tmpl&#39;route:  group_by: [&#39;alertname&#39;]  group_wait: 30s  group_interval: 60s  receiver: &#39;email&#39;    # 优先使用wechat发送  repeat_interval: 1hreceivers:  - name: &#39;email&#39;    email_configs:      - to: &#39;lisongyang@hoteamsoft.com&#39;        html: &#39;{{ template "emai.html" . }}&#39; # 模板        headers: { Subject: &quot; {{ .CommonLabels.instance }} {{ .CommonAnnotations.summary }}&quot; } #标题        send_resolved: true// :wq保存退出</code></pre><p>主要是配置smtp信息，添加消息模板的路径，配置receiver中的中邮件发送信息。这些完成后，开始编写邮件模板。</p><pre><code>$ vim template/email.tmpl// 输入以下信息{{ define "emai.html" }}{{ range .Alerts }} &lt;pre&gt;实例: {{ .Labels.instance }}信息: {{ .Annotations.summary }}详情: {{ .Annotations.description }}时间: {{ .StartsAt.Format "2020-08-28 15:04:05" }} &lt;/pre&gt;{{ end }}{{ end }}// :wq保存退出</code></pre><p>完成后注意要重启alertmanager。</p><pre><code>$ docker restart alertmanager</code></pre><p>记得在上述操作完成后，修改prometheus.yml中对告警功能的配置，最终形成的配置文件如下：</p><pre><code>global:  scrape_interval: 60s  evaluation_interval: 30s# 告警功能配置alerting:  alertmanagers:    - static_configs:      - targets:        - 192.168.166.203:9093# 告警规则文件设置rule_files:  - &quot;rules/*_rules.yml&quot;# 采集job列表，各个exporter的集合scrape_configs:  - job_name: prometheus    static_configs:      - targets: [&#39;localhost:9090&#39;]        labels:          instance: prometheus  - job_name: server199-node-exporter    static_configs:      - targets: [&#39;192.168.166.199:9100&#39;]        labels:          instance: server199-node-exporter  - job_name: server199-cadivisor    static_configs:      - targets: [&#39;192.168.166.199:9999&#39;]        labels:          instance: server199-cadivisor  - job_name: server203-node-exporter    static_configs:      - targets: [&#39;192.168.166.203:9100&#39;]        labels:          instance: server203-node-exporter  - job_name: server201-node-exporter    static_configs:      - targets: [&#39;192.168.166.201:9100&#39;]        labels:          instance: server201-node-exporter  - job_name: server202-node-exporter    static_configs:      - targets: [&#39;192.168.166.202:9100&#39;]        labels:          instance: server202-node-exporter  - job_name: server202-cadivisor    static_configs:      - targets: [&#39;192.168.166.203:9999&#39;]        labels:          instance: server202-cadivisor  - job_name: server236-node-exporter    static_configs:      - targets: [&#39;192.168.166.236:9100&#39;]        labels:          instance: server236-node-exporter  - job_name: server200-node-exporter    static_configs:      - targets: [&#39;192.168.166.200:9100&#39;]        labels:          instance: server200-node-exporter  - job_name: server204-node-exporter    static_configs:      - targets: [&#39;192.168.166.204:9100&#39;]        labels:          instance: server204-node-exporter  - job_name: server204-redis-exporter    static_configs:      - targets: [&#39;192.168.166.204:9121&#39;]        labels:          instance: server204-redis-exporter  - job_name: server204-mongo-exporter    static_configs:      - targets: [&#39;192.168.166.204:9123&#39;]        labels:          instance: server204-mongo-exporter  - job_name: server205-node-exporter    static_configs:      - targets: [&#39;192.168.166.205:9100&#39;]        labels:          instance: server205-node-exporter  - job_name: server205-cadivisor    static_configs:      - targets: [&#39;192.168.166.205:9999&#39;]        labels:          instance: server205-cadivisor  - job_name: server205-mysql-exporter    static_configs:      - targets: [&#39;192.168.166.205:9104&#39;]        labels:          instance: server205-mysql-exporter  - job_name: server206-node-exporter    static_configs:      - targets: [&#39;192.168.166.206:9100&#39;]        labels:          instance: server206-node-exporter  - job_name: server206-cadivisor    static_configs:      - targets: [&#39;192.168.166.206:9999&#39;]        labels:          instance: server206-cadivisor  - job_name: server206-process-exporter    static_configs:      - targets: [&#39;192.168.166.206:9256&#39;]        labels:          instance: server206-process-exporter  - job_name: server207-node-exporter    static_configs:      - targets: [&#39;192.168.166.207:9100&#39;]        labels:          instance: server207-node-exporter  - job_name: server208-node-exporter    static_configs:      - targets: [&#39;192.168.166.208:9100&#39;]        labels:          instance: server208-node-exporter  - job_name: server242-node-exporter    static_configs:      - targets: [&#39;192.168.166.242:9100&#39;]        labels:          instance: server242-node-exporter  - job_name: server242-process-exporter    static_configs:      - targets: [&#39;192.168.166.242:9256&#39;]        labels:          instance: server242-process-exporter  - job_name: server237-node-exporter    static_configs:      - targets: [&#39;192.168.166.237:9100&#39;]        labels:          instance: server237-node-exporter  - job_name: server244-node-exporter    static_configs:      - targets: [&#39;192.168.166.244:9100&#39;]        labels:          instance: server244-node-exporter  - job_name: server244-mysql-exporter    static_configs:      - targets: [&#39;192.168.166.244:9104&#39;]        labels:          instance: server244-mysql-exporter  - job_name: server238-node-exporter    static_configs:      - targets: [&#39;192.168.166.238:9100&#39;]        labels:          instance: server238-node-exporter  - job_name: server232-node-exporter    static_configs:      - targets: [&#39;192.168.166.232:9100&#39;]        labels:          instance: server232-node-exporter  - job_name: server233-node-exporter    static_configs:      - targets: [&#39;192.168.166.233:9100&#39;]        labels:          instance: server233-node-exporter  - job_name: server234-node-exporter    static_configs:      - targets: [&#39;192.168.166.234:9100&#39;]        labels:          instance: server234-node-exporter  - job_name: server235-node-exporter    static_configs:      - targets: [&#39;192.168.166.235:9100&#39;]        labels:          instance: server235-node-exporter  - job_name: server239-node-exporter    static_configs:      - targets: [&#39;192.168.166.239:9100&#39;]        labels:          instance: server239-node-exporter  - job_name: server241-node-exporter    static_configs:      - targets: [&#39;192.168.166.241:9100&#39;]        labels:          instance: server241-node-exporter  - job_name: server245-node-exporter    static_configs:      - targets: [&#39;192.168.166.245:9100&#39;]        labels:          instance: server245-node-exporter  - job_name: server246-node-exporter    static_configs:      - targets: [&#39;192.168.166.246:9100&#39;]        labels:          instance: server246-node-exporter#  - job_name: server246-mongo-exporter#    static_configs:#      - targets: [&#39;192.168.166.246:9123&#39;]#        labels:#          instance: server246-mongo-exporter#  - job_name: server245-redis-exporter#    static_configs:#      - targets: [&#39;192.168.166.245:9121&#39;]#        labels:#          instance: server245-redis-exporter --&gt;  - job_name: server240-node-exporter    static_configs:      - targets: [&#39;192.168.166.240:9100&#39;]        labels:          instance: server240-node-exporter</code></pre><p>最后测试一下邮件是否能够发送，随便找一台部署了node-exporter的主机，停止node-exporter的运行来模拟故障（参考前面配置的<em>host-lost-connection-rule</em>名称的报警规则）。这里登录240服务器进行操作，如下：</p><pre><code>$ docker stop node-exporter</code></pre><p>查看prometheus的状态如下，间隔一分钟刷新一次，可以看到一个刷新周期走完后，即可获得停机的状态：</p><p><img src="Prometheus%E5%81%9C%E6%9C%BA%E7%8A%B6%E6%80%81%E6%9F%A5%E7%9C%8B.png" alt></p><p>停机后，根据配置信息，会在Prometheus页面中看到我们之前定义的报警规则，如下图：</p><p><img src="Prometheus%E6%8A%A5%E8%AD%A6%E8%A7%84%E5%88%99.png" alt></p><p>图中有三种状态的标识，即Inactive（未激活）、Pending（正在等待处理）、Firing（已处理），可以看到刚刚触发的报警信息变为Pending状态，当其状态转换为Firing后，意味着在Prometheus中处理完毕，这时候可以在alertmanager中看到报警信息，如下图：</p><p><img src="alertmanager%E6%8A%A5%E8%AD%A6%E4%BF%A1%E6%81%AF.png" alt></p><p>最后会在目标邮箱中看到邮件信息，如下：</p><p><img src="%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A6%E4%BF%A1%E6%81%AF.png" alt></p><h4 id="2-Webhook告警配置"><a href="#2-Webhook告警配置" class="headerlink" title="2. Webhook告警配置"></a>2. Webhook告警配置</h4><p>Webhook处理时，还需要加一层代理信息，需要将alertmanager中webhook的URL进行一次转发，再由转发程序配置企业微信的Webhook连接，转发到企业微信报警！</p><p>实际上，alertmanager不能直接调用企业微信的Webhook链接！</p><p>直接<a href="http://192.168.166.202:8181/test/common-utils/qywechat-webhook.git" target="_blank" rel="noopener">参考项目</a>中develop分支的实现方式。按照上述思路，接收到alertmanager的webhook信息，然后用线程池的方式对外调用企业微信的Webhook链接。</p><h2 id="监控告警指标的确定"><a href="#监控告警指标的确定" class="headerlink" title="监控告警指标的确定"></a>监控告警指标的确定</h2><h3 id="1-主机层面的监控"><a href="#1-主机层面的监控" class="headerlink" title="1. 主机层面的监控"></a>1. 主机层面的监控</h3><p>满足以下条件进行告警：</p><ul><li>CPU使用率，超过85%</li><li>内存使用率，超过85%</li><li>网络吞吐量，超过80MB/s</li><li>网络传输错误，出现一次</li><li>硬盘空间剩余，不足10%</li><li>硬盘读写量不正常，超过50MB/s</li><li>交换空间使用率，超过80%（限部分机器）</li></ul><h3 id="2-docker容器的告警"><a href="#2-docker容器的告警" class="headerlink" title="2. docker容器的告警"></a>2. docker容器的告警</h3><p>满足以下条件进行告警：</p><ul><li>容器被killed</li><li>CPU使用率，超过80%</li><li>内存使用率，超过80%</li><li>容器挂载的存储使用量，超过80%</li><li>容器磁盘读写使用率，超过80%</li></ul><h3 id="3-MySQL的告警"><a href="#3-MySQL的告警" class="headerlink" title="3. MySQL的告警"></a>3. MySQL的告警</h3><p>满足以下条件进行告警：</p><ul><li>MySQL宕机</li><li>连接数占用过多，超过80%</li><li>线程利用过高，超过60%</li><li>慢查询，出现1次</li></ul><p><em>*注意：</em> 目前MySQL按照单机模式进行监控，并未以集群方式监控。</p><h3 id="4-Redis的告警"><a href="#4-Redis的告警" class="headerlink" title="4. Redis的告警"></a>4. Redis的告警</h3><p>满足以下条件进行告警：</p><ul><li>Redis宕机</li><li>OOM，内存使用大于90%</li><li>连接数过多，超过100</li><li>连接数不足，小于5</li><li>存在拒绝连接的情况</li></ul><p><em>*注意：</em> 目前Redis按照单机模式进行监控，并未以集群方式监控。</p><h3 id="5-MongoDB的告警"><a href="#5-MongoDB的告警" class="headerlink" title="5. MongoDB的告警"></a>5. MongoDB的告警</h3><p>满足以下条件进行告警：</p><ul><li>游标（cursor）开启数，大于10k</li><li>游标访问超时的数量，大于100</li><li>连接数过多，大于500</li><li>虚拟内存使用量过高，超过1/3</li></ul><h3 id="6-进程告警（临时未添加）"><a href="#6-进程告警（临时未添加）" class="headerlink" title="6. 进程告警（临时未添加）"></a>6. 进程告警（临时未添加）</h3><h3 id="7-kafka告警"><a href="#7-kafka告警" class="headerlink" title="7. kafka告警"></a>7. kafka告警</h3><p>满足下面条件进行告警：</p><ul><li>处于同步状态中的topic数量，超过3个</li><li>消费者组超过50个实例</li></ul><h3 id="8-jmx告警配置"><a href="#8-jmx告警配置" class="headerlink" title="8. jmx告警配置"></a>8. jmx告警配置</h3><p>注意：</p><ul><li>参考的指标设定：awesome-prometheus-alerts.grep.to/rules.html</li><li>具体的配置信息，参考<a href="./doc/all_config.tar.gz">已上传的zip文件</a></li></ul><h3 id="9-fastdfs的监控告警"><a href="#9-fastdfs的监控告警" class="headerlink" title="9. fastdfs的监控告警"></a>9. fastdfs的监控告警</h3><h3 id="10-rocketMQ的监控告警"><a href="#10-rocketMQ的监控告警" class="headerlink" title="10. rocketMQ的监控告警"></a>10. rocketMQ的监控告警</h3><h3 id="11-ELK方面的监控告警"><a href="#11-ELK方面的监控告警" class="headerlink" title="11. ELK方面的监控告警"></a>11. ELK方面的监控告警</h3><p>### </p><h2 id="监控面板信息总结"><a href="#监控面板信息总结" class="headerlink" title="监控面板信息总结"></a>监控面板信息总结</h2><p>node-exporter相关：</p><p>MySQL相关：</p><p>Redis相关：</p><p>RocketMQ相关：</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>选择prometheus做监控，第一是容易建设，开箱即用，第二是生态组合优秀，监控面广。</p><p>经过建设后，在对主机管理层面，提升整体的管理水平，减少因硬件资源水平不足，例如硬盘空间不够，这样的问题带来对整个服务器集群的不良影响。</p><h2 id="剩余内容"><a href="#剩余内容" class="headerlink" title="剩余内容"></a>剩余内容</h2><p>目前已经完成了所有主机、docker容器的监控搭建，剩余以下内容未完成：</p><ul><li><del>process-exporter的使用</del></li><li><del>alertmanager中Webhook的使用，对比<a href="https://github.com/prometheus/alertmanager" target="_blank" rel="noopener">钉钉</a>Webhook调用进行改造</del></li><li><del>各项报警监控指标的确定</del></li><li><del>redis、mongodb、mysql监控面板搭建，redis、mongodb监控信息上报</del></li><li><del>Grafana监控面板的调整与合并</del></li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul><li>官方网站：<a href="https://prometheus.io/" target="_blank" rel="noopener">https://prometheus.io/</a></li><li><a href="https://blog.csdn.net/aixiaoyang168/article/details/98474494" target="_blank" rel="noopener">https://blog.csdn.net/aixiaoyang168/article/details/98474494</a></li><li><a href="https://blog.csdn.net/skh2015java/article/details/102572874" target="_blank" rel="noopener">https://blog.csdn.net/skh2015java/article/details/102572874</a></li><li><a href="https://www.jianshu.com/p/e3c9fc929d8a" target="_blank" rel="noopener">https://www.jianshu.com/p/e3c9fc929d8a</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于maven deploy出现连接超时的情况</title>
      <link href="/2021/06/15/guan-yu-maven-deploy-chu-xian-lian-jie-chao-shi-de-qing-kuang/"/>
      <url>/2021/06/15/guan-yu-maven-deploy-chu-xian-lian-jie-chao-shi-de-qing-kuang/</url>
      
        <content type="html"><![CDATA[<h1 id="关于maven-deploy命令出现连接超时的情况解决"><a href="#关于maven-deploy命令出现连接超时的情况解决" class="headerlink" title="关于maven deploy命令出现连接超时的情况解决"></a>关于maven deploy命令出现连接超时的情况解决</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在执行maven的编译构建操作后，将项目的公共依赖推送到甲方远端的maven仓库地址中。我首先在idea中进行操作，错误信息如下：</p><pre><code>Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project basestage-starter-web: Failed to retrieve remote metadata com.company:basestage-starter-web:0.0.1-SNAPSHOT/maven-metadata.xml: Could not transfer metadata com.company:basestage-starter-web:0.0.1-SNAPSHOT/maven-metadata.xml from/to my-company-snapshots (http://maven.company.com:8081/repository/maven-snapshots/): Connect to maven.company.com:8081 [maven.company.com/192.16.0.10] failed: Connection timed out: connect</code></pre><p>连续推送多次，问题如前。于是我又尝试使用命令行进行推送:</p><pre><code>mvn deploy -DskipTests.........Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project basestage-starter-web: Failed to retrieve remote metadata com.company:basestage-starter-web:0.0.1-SNAPSHOT/maven-metadata.xml: Could not transfer metadata com.company:basestage-starter-web:0.0.1-SNAPSHOT/maven-metadata.xml from/to my-company-snapshots (http://maven.company.com:8081/repository/maven-snapshots/): Connect to maven.company.com:8081 [maven.company.com/192.16.0.10] failed: Connection timed out: connect</code></pre><p><strong>注意：</strong>甲方的maven仓库地址为<em>maven.baddad.com:8081</em>，我们公司的maven地址为<em>maven.company.com:8081</em>。</p><h2 id="尝试解决：排除网络问题"><a href="#尝试解决：排除网络问题" class="headerlink" title="尝试解决：排除网络问题"></a>尝试解决：排除网络问题</h2><p>首先看网络问题，在公司使用vpn的方式进行推送，发现始终连不上甲方的网络环境，无法推送到甲方的nexus中。</p><p>其次在甲方的内网环境下，还是存在上述的依赖推送问题。不论是wifi环境下授权还是在内网服务器上操作，始终存在该问题。</p><h2 id="尝试解决：更换settings-xml配置文件"><a href="#尝试解决：更换settings-xml配置文件" class="headerlink" title="尝试解决：更换settings.xml配置文件"></a>尝试解决：更换settings.xml配置文件</h2><p>更换我们自己为甲方定制的settings.xml文件，具体内容如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>settings</span> <span class="token attr-name">xmlns</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/SETTINGS/1.0.0<span class="token punctuation">"</span></span>          <span class="token attr-name"><span class="token namespace">xmlns:</span>xsi</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://www.w3.org/2001/XMLSchema-instance<span class="token punctuation">"</span></span>          <span class="token attr-name"><span class="token namespace">xsi:</span>schemaLocation</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>localRepository</span><span class="token punctuation">></span></span>D:/Dev/Java/repository<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>localRepository</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginGroups</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginGroup</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginGroup</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginGroups</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>proxies</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>proxies</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>servers</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>server</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>username</span><span class="token punctuation">></span></span>admin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>username</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>password</span><span class="token punctuation">></span></span>admin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>password</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>server</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>server</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>username</span><span class="token punctuation">></span></span>admin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>username</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>password</span><span class="token punctuation">></span></span>admin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>password</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>server</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>server</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-public<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>username</span><span class="token punctuation">></span></span>admin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>username</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>password</span><span class="token punctuation">></span></span>admin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>password</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>server</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>servers</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mirrors</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mirrors</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>profiles</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>profile</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-nexus<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>activation</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>jdk</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>jdk</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>activation</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>maven.compiler.source</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>maven.compiler.source</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>maven.compiler.target</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>maven.compiler.target</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>maven.compiler.compilerVersion</span><span class="token punctuation">></span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>maven.compiler.compilerVersion</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repositories</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-public<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-public/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-snapshots/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-release<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-release/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repositories</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepositories</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepository</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-public<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-public/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>profile</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>profiles</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>activeProfiles</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>activeProfile</span><span class="token punctuation">></span></span>baddad-cloud-nexus<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>activeProfile</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>activeProfiles</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>settings</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>更换后在idea中进行设置，如下：</p><p><img src="maven%E5%9C%A8idea%E4%B8%AD%E7%9A%84%E9%85%8D%E7%BD%AE.png" alt></p><p>再进行尝试，问题如前。通过命令行指定settings.xml文件尝试，如下：</p><pre><code>$ mvn -s &#39;D:\settings\maven\settings-jiafang.xml&#39; clean install deploy .........</code></pre><p>问题如前。</p><h2 id="尝试解决：检查项目中是否包含外部的maven仓库地址并改正"><a href="#尝试解决：检查项目中是否包含外部的maven仓库地址并改正" class="headerlink" title="尝试解决：检查项目中是否包含外部的maven仓库地址并改正"></a>尝试解决：检查项目中是否包含外部的maven仓库地址并改正</h2><p>到这一步才想起来，前面错误中的nexus依赖仓库地址始终指向的我们公司自己的nexus3地址信息，而在maven的配置信息setttings.xml文件中，并未存在这个地址，在这种情况下推断，项目中存在我们公司自己的nexus3地址，于是查找项目的pom.xml文件如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>project</span> <span class="token attr-name">xmlns</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/POM/4.0.0<span class="token punctuation">"</span></span> <span class="token attr-name"><span class="token namespace">xmlns:</span>xsi</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://www.w3.org/2001/XMLSchema-instance<span class="token punctuation">"</span></span>  <span class="token attr-name"><span class="token namespace">xsi:</span>schemaLocation</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>modelVersion</span><span class="token punctuation">></span></span>4.0.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>modelVersion</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.test<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>basetest<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>0.0.1-SNAPSHOT<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>basestage<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>测试项目<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>packaging</span><span class="token punctuation">></span></span>pom<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>packaging</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>modules</span><span class="token punctuation">></span></span>        ........    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>modules</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">></span></span>       .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencyManagement</span><span class="token punctuation">></span></span>        .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencyManagement</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>        .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>        .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>distributionManagement</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.company.com:8081/repository/maven-releases/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshotRepository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.company.com:8081/repository/maven-snapshots/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshotRepository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>distributionManagement</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.company.com:8081/repository/maven-releases/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.company.com:8081/repository/maven-snapshots/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repositories</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>public<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>aliyun nexus<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.aliyun.com/nexus/content/groups/public/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepositories</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>project</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>显然这个地址指向不正确，于是修改如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>project</span> <span class="token attr-name">xmlns</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/POM/4.0.0<span class="token punctuation">"</span></span> <span class="token attr-name"><span class="token namespace">xmlns:</span>xsi</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://www.w3.org/2001/XMLSchema-instance<span class="token punctuation">"</span></span>  <span class="token attr-name"><span class="token namespace">xsi:</span>schemaLocation</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>modelVersion</span><span class="token punctuation">></span></span>4.0.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>modelVersion</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.test<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>basetest<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>0.0.1-SNAPSHOT<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>basestage<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">></span></span>测试项目<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>packaging</span><span class="token punctuation">></span></span>pom<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>packaging</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>modules</span><span class="token punctuation">></span></span>        ........    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>modules</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">></span></span>       .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencyManagement</span><span class="token punctuation">></span></span>        .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencyManagement</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">></span></span>        .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>        .......    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>distributionManagement</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-releases/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshotRepository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-snapshots/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshotRepository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>distributionManagement</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-releases/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>my-company-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-snapshots/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repositories</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>public<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>aliyun nexus<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.aliyun.com/nexus/content/groups/public/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepositories</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>project</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样再次测试deploy操作，得到一个401的结果，如下：</p><pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project xbnjava: Failed to deploy artifacts: Could not transfer artifact com.github.aliteralmind:xbnjava:pom:0.1.2 from/to sonatype-nexus-staging (https://oss.sonatype.org/service/local/staging/deploy/maven2/): Failed to transfer file: https://oss.sonatype.org/service/local/staging/deploy/maven2/com/github/aliteralmind/xbnjava/0.1.2/xbnjava-0.1.2.pom. Return code is: 401, ReasonPhrase: Unauthorized. -&gt; [Help 1]</code></pre><p>也就是说，目前地址修改正确了，但是登录信息无法进行对应，导致无法进行推送。</p><h2 id="尝试解决：检查settings-xml和项目中的pom-xml文件中授权信息是否对应"><a href="#尝试解决：检查settings-xml和项目中的pom-xml文件中授权信息是否对应" class="headerlink" title="尝试解决：检查settings.xml和项目中的pom.xml文件中授权信息是否对应"></a>尝试解决：检查settings.xml和项目中的pom.xml文件中授权信息是否对应</h2><p>根据第三步的信息，检查settings.xml文件和项目中pom.xml的对应关系，该对应关系决定着是否能正常从maven远程仓库中获得文件更新，在pom.xml中进行修改如下：</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>distributionManagement</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-releases/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshotRepository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-snapshots/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshotRepository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>distributionManagement</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test releases<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-releases/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>baddad-cloud-snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>test snapshots<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://maven.baddad.com:8081/repository/maven-snapshots/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>updatePolicy</span><span class="token punctuation">></span></span>always<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>updatePolicy</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repositories</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将pom文件中的id信息，修改为同settings.xml文件中相同的id信息。这样在访问nexus3镜像仓库时，就能匹配settings.xml中的授权信息，也就能正常登录并进行deploy操作！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p>settings.xml中的id信息一定要和pom.xml文件中设置的远程仓库地址的id要一一对应，且内部设置信息相同。重要的是，settings.xml中授权信息的id也要和settings.xml中的repository下的id信息相同！</p></li><li><p>注意先排查网络情况，一般网络情况不通是最大的问题。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>使用lanproxy在vpn环境下进行微服务的开发联调测试</title>
      <link href="/2021/05/26/shi-yong-lanproxy-zai-vpn-huan-jing-xia-jin-xing-wei-fu-wu-de-kai-fa-lian-diao-ce-shi/"/>
      <url>/2021/05/26/shi-yong-lanproxy-zai-vpn-huan-jing-xia-jin-xing-wei-fu-wu-de-kai-fa-lian-diao-ce-shi/</url>
      
        <content type="html"><![CDATA[<h1 id="使用lanproxy在vpn环境下进行微服务的开发联调测试"><a href="#使用lanproxy在vpn环境下进行微服务的开发联调测试" class="headerlink" title="使用lanproxy在vpn环境下进行微服务的开发联调测试"></a>使用lanproxy在vpn环境下进行微服务的开发联调测试</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在最近的项目中，我们需要通过vpn连接到公司内网进行开发。在启动vpn后，将工作机本地的服务启动，注册到nacos上。由于我们自己本机的ip地址发生了变化，在服务注册中心nacos中，发现我们自己的服务地址变更为vpn的地址，导致在其它服务轮询调用我们本机的服务时，出现了接口超时、服务调用不到的错误。这样就导致我们的开发工作受到阻碍，不能方便的进行联调。</p><p>同样在客户现场，我们也是通过vpn连接到客户现场的环境中，进行开发测试，由于vpn分配的ip地址到本机启动的服务上，无法被客户现场服务器中部署的服务调用到，造成了开发困难的情况。</p><p>这两种场景的不同在于，公司内网的服务器可以访问外部网路，而客户现场的服务器无法访问外部网络。</p><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><p>面对上述问题，直接考虑使用内网穿透（端口映射）的方式解决。简单地说，就是将本地端口映射到一个服务器上的端口，同一个网段上的服务器是互通的，通过在该网段内的服务器访问该端口，映射到本地端口，实现对本地端口的访问。这样就可以使服务以服务器的地址注册到nacos上，由网关或者OpenFeign进行服务调用时，可以访问到本地启动的微服务。示意图如下：</p><p><img src="LanProxy%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt></p><p>解决方案则选择使用<a href>LanProxy</a>，部署LanProxy服务端，在开发机启动LanProxy客户端，进行内部组网。通过面板创建客户端连接，配置端口映射，实现内网穿透。</p><h2 id="服务器部署"><a href="#服务器部署" class="headerlink" title="服务器部署"></a>服务器部署</h2><p>下载lanproxy服务端，从<a href="file.nioee.com/d/2e81550ebdbd416c933f/">这个地址</a>进行下载，上传到服务器，部署如下：</p><pre><code>// 解压服务端$ unzip proxy-server-0.1.zip // 配置启动信息$ cd proxy-server-0.1$ vim conf/config.properties // 启动项修改如下，由于开放端口只由55500-55599，所以将端口号限制在这个范围中server.bind=0.0.0.0server.port=55590server.ssl.enable=trueserver.ssl.bind=0.0.0.0server.ssl.port=55593server.ssl.jksPath=test.jksserver.ssl.keyStorePassword=123456server.ssl.keyManagerPassword=123456server.ssl.needsClientAuth=falseconfig.server.bind=0.0.0.0config.server.port=55591config.admin.username=adminconfig.admin.password=@WSXzaq1// :wq保存退出// 启动服务如下$ ./bin/startup.sh Starting the proxy server ...startedPID: 27660// 查看进程和端口号$ ps -ef | grep lanproxy$ sudo netstat -tnlp | grep 5559*tcp        0      0 0.0.0.0:55590           0.0.0.0:*               LISTEN      27660/java          tcp        0      0 0.0.0.0:55591           0.0.0.0:*               LISTEN      27660/java          tcp        0      0 0.0.0.0:55593           0.0.0.0:*               LISTEN      27660/java        </code></pre><p>注意：启动服务端需要java运行环境，没有的需要自行安装一下！</p><p>这样直接访问<a href="http://ip:55591即可，访问情况如下：" target="_blank" rel="noopener">http://ip:55591即可，访问情况如下：</a></p><p><img src="%E9%A6%96%E9%A1%B5%E8%AE%BF%E9%97%AE.png" alt></p><p>输入用户名密码访问即可登录。</p><h2 id="工作机开发"><a href="#工作机开发" class="headerlink" title="工作机开发"></a>工作机开发</h2><h3 id="0-（可选）域名设置"><a href="#0-（可选）域名设置" class="headerlink" title="0. （可选）域名设置"></a>0. （可选）域名设置</h3><p>如果线上服务器可以访问外网，则可以配置域名来进行访问。在线上服务器利用域名访问端口映射后的服务信息。</p><p>如果线上服务器不能访问外网，可以考虑将域名配置到/etc/hosts文件中，或者直接使用该服务器的ip地址。</p><h3 id="1-设置client信息"><a href="#1-设置client信息" class="headerlink" title="1. 设置client信息"></a>1. 设置client信息</h3><p>点击New Client，添加一个新的client信息，如下图：</p><p><img src="%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%9A%84client.png" alt></p><p>点击下面新的client信息，如下：</p><p><img src="%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%9A%84proxy-config.png" alt></p><p>添加端口信息的映射，如下：</p><p><img src="%E6%B7%BB%E5%8A%A0%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84.png" alt></p><p>注意这里需要，将最后一个Backend ID映射为本地的微服务所在的端口。这样在访问的时候就能访问到开发机上的端口信息。</p><p><strong>注意：</strong>在每一个新开服务时都需要在这里新增端口信息。</p><h3 id="2-开发机启动lanproxy客户端"><a href="#2-开发机启动lanproxy客户端" class="headerlink" title="2. 开发机启动lanproxy客户端"></a>2. 开发机启动lanproxy客户端</h3><p>在开发机<a href>下载</a>并启动lanproxy的客户端，使用带管理员权限的命令行进行启动如下：</p><pre><code>$ .\client_windows_amd64.exe -s 192.168.245.22 -p 55590 -k 04d60ab81091446b817f98aff38f9961</code></pre><p>命令解释如下：</p><ul><li>-s lanproxy所部署的服务器ip地址或者你自行添加的域名信息</li><li>-p config.properties中配置的server.port</li><li>-k 在创建client时生成的密钥信息</li></ul><p>这里临时不使用ssl的方式启动。如果需要可以参考<a href="https://github.com/ffay/lanproxy" target="_blank" rel="noopener">lanproxy</a>的内容。</p><p>普通端口连接</p><pre><code># mac 64位nohup ./client_darwin_amd64 -s SERVER_IP -p SERVER_PORT -k CLIENT_KEY &amp;# linux 64位nohup ./client_linux_amd64 -s SERVER_IP -p SERVER_PORT -k CLIENT_KEY &amp;# windows 64 位./client_windows_amd64.exe -s SERVER_IP -p SERVER_PORT -k CLIENT_KEY</code></pre><p>SSL端口连接</p><pre><code># mac 64位nohup ./client_darwin_amd64 -s SERVER_IP -p SERVER_SSL_PORT -k CLIENT_KEY -ssl true &amp;# linux 64位nohup ./client_linux_amd64 -s SERVER_IP -p SERVER_SSL_PORT -k CLIENT_KEY -ssl true &amp;# windows 64 位./client_windows_amd64.exe -s SERVER_IP -p SERVER_SSL_PORT -k CLIENT_KEY -ssl true</code></pre><h3 id="3-启动本地微服务注册到线上服务器"><a href="#3-启动本地微服务注册到线上服务器" class="headerlink" title="3. 启动本地微服务注册到线上服务器"></a>3. 启动本地微服务注册到线上服务器</h3><p>在intellij idea中启动本地微服务，在启动之前需要做以下设置，如图：</p><p><img src="%E8%AE%BE%E7%BD%AE%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0.png" alt></p><p>参数信息如下：</p><ul><li>-Dspring.cloud.client.ip-address=192.168.245.22</li></ul><p>这个信息设置了该服务启动时所属的ip信息，也可以是域名。</p><p>这样在注册到nacos的时候，该服务的地址就变为上面设置的地址，通过端口映射，可以从内网环境访问到在开发机启动的微服务。</p><h2 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h2><p>可以在nacos中看到注册的微服务信息，而且url为我们设定的ip或者域名，端口号对应我们自己设置的端口号。</p><h2 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h2><p>目前在甲方的服务器上已经部署了lanproxy服务端，在开发机上启动了甲方VPN工具，最后启动lanproxy的客户端去连接，客户端启动出现了连接不上的情况，日志信息包含“connection timeout”，还是端口开放的问题，具体日志忘记记录了。</p><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><ul><li><p><a href="https://nasge.com/archives/48.html" target="_blank" rel="noopener">https://nasge.com/archives/48.html</a></p></li><li><p><a href="https://blog.csdn.net/LLittleF/article/details/108712758" target="_blank" rel="noopener">https://blog.csdn.net/LLittleF/article/details/108712758</a></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> DevOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于nacos的问题排查</title>
      <link href="/2021/05/24/guan-yu-nacos-de-wen-ti-pai-cha/"/>
      <url>/2021/05/24/guan-yu-nacos-de-wen-ti-pai-cha/</url>
      
        <content type="html"><![CDATA[<h1 id="关于甲方方面在nacos下的超时问题处理"><a href="#关于甲方方面在nacos下的超时问题处理" class="headerlink" title="关于甲方方面在nacos下的超时问题处理"></a>关于甲方方面在nacos下的超时问题处理</h1><h2 id="问题背景描述"><a href="#问题背景描述" class="headerlink" title="问题背景描述"></a>问题背景描述</h2><p>起因源于甲方方面从防火墙开放端口后，通过vpn连接到甲方的网络中，在本地通过intellij idea启动uaa服务，看是否能注册到甲方的nacos中。</p><p>通过测试，并不能注册到甲方的nacos中，日志信息如下：</p><pre class="line-numbers language-log"><code class="language-log">D:\dev\Java\jdk1.8.0_201\bin\java.exe -Dnacos_ip=192.168.62.52:18848 -Dnacos_namespace=6e63890a-4d2c-4f6b-a2b8-529984c38b18 -XX:TieredStopAtLevel=1 -noverify -Dspring.profiles.active=test -Dspring.output.ansi.enabled=always -Dcom.sun.management.jmxremote -Dspring.jmx.enabled=true -Dspring.liveBeansView.mbeanDomain -Dspring.application.admin.enabled=true "-javaagent:D:\dev\Java\IntelliJ IDEA 2019.1.3\lib\idea_rt.jar=59908:D:\dev\Java\IntelliJ IDEA 2019.1.3\bin" -Dfile.encoding=UTF-8 -classpath C:\Users\ht\AppData\Local\Temp\classpath1285028913.jar com.company.test.testone.uaa.BaseStoneUaaServer           ______________        ________                __::  :: testone-uaa: test: v1.0.1-SNAPSHOT :::: Spring-Boot: 2.3.9.RELEASE :: [testone-uaa:10.0.93.182:0000] 2021-05-24 09:14:56.205 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:14:57.206 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:14:58.206 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:14:58.206 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : no available server[testone-uaa:10.0.93.182:0000] 2021-05-24 09:14:58.209 ERROR 13728 [] [main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.62.52_18848-6e63890a-4d2c-4f6b-a2b8-529984c38b18] [sub-server] get server config exception, dataId=testone-uaa, group=DEFAULT_GROUP, tenant=6e63890a-4d2c-4f6b-a2b8-529984c38b18java.net.ConnectException: no available server    at com.alibaba.nacos.client.config.http.ServerHttpAgent.httpGet(ServerHttpAgent.java:133)    at com.alibaba.nacos.client.config.http.MetricsHttpAgent.httpGet(MetricsHttpAgent.java:51)    at com.alibaba.nacos.client.config.impl.ClientWorker.getServerConfig(ClientWorker.java:298)    at com.alibaba.nacos.client.config.NacosConfigService.getConfigInner(NacosConfigService.java:149)    at com.alibaba.nacos.client.config.NacosConfigService.getConfig(NacosConfigService.java:97)    at com.alibaba.cloud.nacos.client.NacosPropertySourceBuilder.loadNacosData(NacosPropertySourceBuilder.java:85)    at com.alibaba.cloud.nacos.client.NacosPropertySourceBuilder.build(NacosPropertySourceBuilder.java:74)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadNacosPropertySource(NacosPropertySourceLocator.java:204)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadNacosDataIfPresent(NacosPropertySourceLocator.java:191)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadApplicationConfiguration(NacosPropertySourceLocator.java:142)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.locate(NacosPropertySourceLocator.java:103)    at org.springframework.cloud.bootstrap.config.PropertySourceLocator.locateCollection(PropertySourceLocator.java:52)    at org.springframework.cloud.bootstrap.config.PropertySourceLocator.locateCollection(PropertySourceLocator.java:47)    at org.springframework.cloud.bootstrap.config.PropertySourceBootstrapConfiguration.initialize(PropertySourceBootstrapConfiguration.java:98)    at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:626)    at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:370)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226)    at com.company.test.testone.uaa.BaseStoneUaaServer.main(BaseStoneUaaServer.java:35)[testone-uaa:10.0.93.182:0000] 2021-05-24 09:14:58.211 WARN 13728 [] [main] c.a.c.n.c.NacosPropertySourceBuilder     : Ignore the empty nacos configuration and get it based on dataId[testone-uaa] & group[DEFAULT_GROUP][testone-uaa:10.0.93.182:0000] 2021-05-24 09:14:59.212 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:00.212 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:01.212 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:02.212 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:02.212 ERROR 13728 [] [main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.62.52_18848-6e63890a-4d2c-4f6b-a2b8-529984c38b18] [sub-server] get server config exception, dataId=testone-uaa.yml, group=DEFAULT_GROUP, tenant=6e63890a-4d2c-4f6b-a2b8-529984c38b18java.net.ConnectException: [NACOS HTTP-GET] The maximum number of tolerable server reconnection errors has been reached    at com.alibaba.nacos.client.config.http.ServerHttpAgent.httpGet(ServerHttpAgent.java:124)    at com.alibaba.nacos.client.config.http.MetricsHttpAgent.httpGet(MetricsHttpAgent.java:51)    at com.alibaba.nacos.client.config.impl.ClientWorker.getServerConfig(ClientWorker.java:298)    at com.alibaba.nacos.client.config.NacosConfigService.getConfigInner(NacosConfigService.java:149)    at com.alibaba.nacos.client.config.NacosConfigService.getConfig(NacosConfigService.java:97)    at com.alibaba.cloud.nacos.client.NacosPropertySourceBuilder.loadNacosData(NacosPropertySourceBuilder.java:85)    at com.alibaba.cloud.nacos.client.NacosPropertySourceBuilder.build(NacosPropertySourceBuilder.java:74)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadNacosPropertySource(NacosPropertySourceLocator.java:204)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadNacosDataIfPresent(NacosPropertySourceLocator.java:191)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadApplicationConfiguration(NacosPropertySourceLocator.java:145)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.locate(NacosPropertySourceLocator.java:103)    at org.springframework.cloud.bootstrap.config.PropertySourceLocator.locateCollection(PropertySourceLocator.java:52)    at org.springframework.cloud.bootstrap.config.PropertySourceLocator.locateCollection(PropertySourceLocator.java:47)    at org.springframework.cloud.bootstrap.config.PropertySourceBootstrapConfiguration.initialize(PropertySourceBootstrapConfiguration.java:98)    at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:626)    at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:370)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226)    at com.company.test.testone.uaa.BaseStoneUaaServer.main(BaseStoneUaaServer.java:35)[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:02.212 WARN 13728 [] [main] c.a.c.n.c.NacosPropertySourceBuilder     : Ignore the empty nacos configuration and get it based on dataId[testone-uaa.yml] & group[DEFAULT_GROUP][testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:03.214 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:04.214 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.214 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : [NACOS SocketTimeoutException httpGet] currentServerAddr:http://192.168.62.52:18848， err : connect timed out[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.214 ERROR 13728 [] [main] c.a.n.c.config.http.ServerHttpAgent      : no available server[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.214 ERROR 13728 [] [main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.62.52_18848-6e63890a-4d2c-4f6b-a2b8-529984c38b18] [sub-server] get server config exception, dataId=testone-uaa-test.yml, group=DEFAULT_GROUP, tenant=6e63890a-4d2c-4f6b-a2b8-529984c38b18java.net.ConnectException: no available server    at com.alibaba.nacos.client.config.http.ServerHttpAgent.httpGet(ServerHttpAgent.java:133)    at com.alibaba.nacos.client.config.http.MetricsHttpAgent.httpGet(MetricsHttpAgent.java:51)    at com.alibaba.nacos.client.config.impl.ClientWorker.getServerConfig(ClientWorker.java:298)    at com.alibaba.nacos.client.config.NacosConfigService.getConfigInner(NacosConfigService.java:149)    at com.alibaba.nacos.client.config.NacosConfigService.getConfig(NacosConfigService.java:97)    at com.alibaba.cloud.nacos.client.NacosPropertySourceBuilder.loadNacosData(NacosPropertySourceBuilder.java:85)    at com.alibaba.cloud.nacos.client.NacosPropertySourceBuilder.build(NacosPropertySourceBuilder.java:74)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadNacosPropertySource(NacosPropertySourceLocator.java:204)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadNacosDataIfPresent(NacosPropertySourceLocator.java:191)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.loadApplicationConfiguration(NacosPropertySourceLocator.java:150)    at com.alibaba.cloud.nacos.client.NacosPropertySourceLocator.locate(NacosPropertySourceLocator.java:103)    at org.springframework.cloud.bootstrap.config.PropertySourceLocator.locateCollection(PropertySourceLocator.java:52)    at org.springframework.cloud.bootstrap.config.PropertySourceLocator.locateCollection(PropertySourceLocator.java:47)    at org.springframework.cloud.bootstrap.config.PropertySourceBootstrapConfiguration.initialize(PropertySourceBootstrapConfiguration.java:98)    at org.springframework.boot.SpringApplication.applyInitializers(SpringApplication.java:626)    at org.springframework.boot.SpringApplication.prepareContext(SpringApplication.java:370)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:314)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226)    at com.company.test.testone.uaa.BaseStoneUaaServer.main(BaseStoneUaaServer.java:35)[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.214 WARN 13728 [] [main] c.a.c.n.c.NacosPropertySourceBuilder     : Ignore the empty nacos configuration and get it based on dataId[testone-uaa-test.yml] & group[DEFAULT_GROUP][testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.215 INFO 13728 [] [main] b.c.PropertySourceBootstrapConfiguration : Located property source: [BootstrapPropertySource {name='bootstrapProperties-testone-uaa-test.yml,DEFAULT_GROUP'}, BootstrapPropertySource {name='bootstrapProperties-testone-uaa.yml,DEFAULT_GROUP'}, BootstrapPropertySource {name='bootstrapProperties-testone-uaa,DEFAULT_GROUP'}][testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.219 INFO 13728 [] [main] c.h.i.testone.uaa.BaseStoneUaaServer   : The following profiles are active: test[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.806 WARN 13728 [] [main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanDefinitionStoreException: Failed to process import candidates for configuration class [com.company.test.testone.uaa.BaseStoneUaaServer]; nested exception is java.lang.IllegalStateException: Error processing condition on org.apache.rocketmq.spring.autoconfigure.RocketMQAutoConfiguration[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.812 INFO 13728 [] [main] ConditionEvaluationReportLoggingListener : Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.[testone-uaa:10.0.93.182:0000] 2021-05-24 09:15:05.823 ERROR 13728 [] [main] o.s.boot.SpringApplication               : Application run failedorg.springframework.beans.factory.BeanDefinitionStoreException: Failed to process import candidates for configuration class [com.company.test.testone.uaa.BaseStoneUaaServer]; nested exception is java.lang.IllegalStateException: Error processing condition on org.apache.rocketmq.spring.autoconfigure.RocketMQAutoConfiguration    at org.springframework.context.annotation.ConfigurationClassParser.processImports(ConfigurationClassParser.java:610)    at org.springframework.context.annotation.ConfigurationClassParser.access$800(ConfigurationClassParser.java:111)    at org.springframework.context.annotation.ConfigurationClassParser$DeferredImportSelectorGroupingHandler.lambda$processGroupImports$1(ConfigurationClassParser.java:812)    at java.util.ArrayList.forEach(ArrayList.java:1257)    at org.springframework.context.annotation.ConfigurationClassParser$DeferredImportSelectorGroupingHandler.processGroupImports(ConfigurationClassParser.java:809)    at org.springframework.context.annotation.ConfigurationClassParser$DeferredImportSelectorHandler.process(ConfigurationClassParser.java:780)    at org.springframework.context.annotation.ConfigurationClassParser.parse(ConfigurationClassParser.java:193)    at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:319)    at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:236)    at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:280)    at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:96)    at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:707)    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:533)    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143)    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758)    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750)    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:405)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237)    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226)    at com.company.test.testone.uaa.BaseStoneUaaServer.main(BaseStoneUaaServer.java:35)Caused by: java.lang.IllegalStateException: Error processing condition on org.apache.rocketmq.spring.autoconfigure.RocketMQAutoConfiguration    at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:60)    at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:108)    at org.springframework.context.annotation.ConfigurationClassParser.processConfigurationClass(ConfigurationClassParser.java:226)    at org.springframework.context.annotation.ConfigurationClassParser.processImports(ConfigurationClassParser.java:600)    ... 20 common frames omittedCaused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'test.rocketmq.name-server' in value "${test.rocketmq.name-server}"    at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:178)    at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:124)    at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239)    at org.springframework.core.env.AbstractPropertyResolver.rhMck9sCCSTtRdH84JvGe1yR2Sqg5tE82E(AbstractPropertyResolver.java:210)    at org.springframework.core.env.AbstractPropertyResolver.rhMck9sCCSTtRdH84JvGe1yR2Sqg5tE82E(AbstractPropertyResolver.java:230)    at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:88)    at org.springframework.core.env.PropertySourcesPropertyResolver.getProperty(PropertySourcesPropertyResolver.java:62)    at org.springframework.core.env.AbstractEnvironment.getProperty(AbstractEnvironment.java:535)    at org.springframework.boot.autoconfigure.condition.OnPropertyCondition$Spec.collectProperties(OnPropertyCondition.java:140)    at org.springframework.boot.autoconfigure.condition.OnPropertyCondition$Spec.access$000(OnPropertyCondition.java:105)    at org.springframework.boot.autoconfigure.condition.OnPropertyCondition.determineOutcome(OnPropertyCondition.java:91)    at org.springframework.boot.autoconfigure.condition.OnPropertyCondition.getMatchOutcome(OnPropertyCondition.java:55)    at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:47)    ... 23 common frames omittedProcess finished with exit code 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同网络断开时的情况一样，出现了超时的情况。</p><h2 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h2><p>病急乱投医，首先测试问题，尝试排除ide的影响，排除依赖信息的影响，最后排除网络的情况。</p><h3 id="0-从命令行启动jar包"><a href="#0-从命令行启动jar包" class="headerlink" title="0. 从命令行启动jar包"></a>0. 从命令行启动jar包</h3><p>参考Dockerfile中的启动命令，进行到target目录下，执行：</p><pre class="line-numbers language-shell"><code class="language-shell">java -Dloader.path="libs/" -Dspring.profiles.active=test -Dnacos_ip=192.168.62.52:18848 -Dnacos_namespace=6e63890a-4d2c-4f6b-a2b8-529984c38b18 -jar testone-uaa-1.0.1-SNAPSHOT.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>错误日志相同。</p><h3 id="1-从命令行发起请求"><a href="#1-从命令行发起请求" class="headerlink" title="1. 从命令行发起请求"></a>1. 从命令行发起请求</h3><pre><code>$ curl -XGET -k -i &quot;http://192.168.62.52:18848/nacos/v1/cs/configs?dataId=testone-uaa-test.yml&amp;group=DEFAULT_GROUP&amp;tenant=6e63890a-4d2c-4f6b-a2b8-529984c38b18&quot;HTTP/1.1 200Config-Type: yamlContent-MD5: eb5fb35c428480018fe894695c82f596Pragma: no-cacheExpires: Thu, 01 Jan 1970 00:00:00 GMTCache-Control: no-cache,no-storeLast-Modified: Thu, 29 Apr 2021 08:17:24 GMTContent-Type: text/plain;charset=UTF-8Transfer-Encoding: chunkedDate: Mon, 24 May 2021 02:50:17 GMTserver:  port: 20111  http2:      enabled: truespring:  cloud:    nacos:      discovery:        service: ${spring.application.name}        ip: ${spring.cloud.client.ip-address}        port: ${server.port}......</code></pre><p>从命令行请求发现，HTTP协议内容可以进行传递。排除接口不通的问题。</p><h3 id="2-排除其它依赖的影响"><a href="#2-排除其它依赖的影响" class="headerlink" title="2. 排除其它依赖的影响"></a>2. 排除其它依赖的影响</h3><p>尝试构建一个最小的服务，排除其它的依赖信息。以之前给甲方方面的例子为测试对象。错误日志还是相同。</p><h3 id="3-抓包"><a href="#3-抓包" class="headerlink" title="3. 抓包"></a>3. 抓包</h3><h4 id="3-1-有无VPN的情况下"><a href="#3-1-有无VPN的情况下" class="headerlink" title="3.1 有无VPN的情况下"></a>3.1 有无VPN的情况下</h4><p>在vpn运行和vpn不运行的情况下，进行抓包测试，得到相同的结果。发现如下：</p><p><img src="error.jpg" alt></p><p>图片展示的信息为重试心跳请求的抓包信息，看到无ACK返回值。请求协议走TCP，而不是HTTP。</p><h4 id="3-2-甲方方面地址和乙方内部地址对比"><a href="#3-2-甲方方面地址和乙方内部地址对比" class="headerlink" title="3.2 甲方方面地址和乙方内部地址对比"></a>3.2 甲方方面地址和乙方内部地址对比</h4><p>切换到乙方方面的nacos请求信息，抓包信息如下：</p><p><img src="success.jpg" alt></p><p>结果发现，在请求接口之前客户端先发起了一个心跳请求，然后再开始请求接口。前面是TCP的心跳连接，心跳达成后，才进行接口的请求。这是这个部分的请求逻辑。</p><h2 id="最终解决方式"><a href="#最终解决方式" class="headerlink" title="最终解决方式"></a>最终解决方式</h2><p>于是得出一个结论，在心跳无法连接或者说心跳没有ACK应答的时候，nacos客户端不会发起对HTTP的请求。</p><p>这样只能解决端口的TCP访问问题，需要开放端口的TCP访问权限，否则无法使用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>抓包是个好办法，尤其是解决网络问题，优先考虑抓包的情况。wireshark用起来还是比较麻烦的，需要找个更易用更直观的工具。</li><li>感觉还是瘸腿，尤其是对HTTP协议和TCP/IP协议了解不够，需要补充这部分内容。</li><li>甲方的在开放端口的时候并未告诉我是否开放的TCP访问还是HTTP访问，所以这事儿后续在做WAF的时候要注意！</li><li>遇事不决，先抓包看代码，然后考虑各个链路上的问题！</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>两个gitlab之间的仓库同步问题解决</title>
      <link href="/2021/04/16/liang-ge-gitlab-zhi-jian-de-cang-ku-tong-bu-wen-ti-jie-jue/"/>
      <url>/2021/04/16/liang-ge-gitlab-zhi-jian-de-cang-ku-tong-bu-wen-ti-jie-jue/</url>
      
        <content type="html"><![CDATA[<h1 id="两个gitlab之间的仓库同步问题解决"><a href="#两个gitlab之间的仓库同步问题解决" class="headerlink" title="两个gitlab之间的仓库同步问题解决"></a>两个gitlab之间的仓库同步问题解决</h1><h2 id="问题场景"><a href="#问题场景" class="headerlink" title="问题场景"></a>问题场景</h2><p>由于我们在用户现场进行开发，而公司本部这边存放有我们自己的基础平台代码，而两边开发的过程中，不可避免的存在代码同步的问题。中间还存在VPN的连接问题，于是在这样的环境下实现代码同步，而且是尽可能的自动同步。</p><p>经过了解，查找了各项资料，发现一个问题，就是绝大部分都是自有或者本地gitlab和github之间进行的同步，不好找gitlab和gitlab之间的同步方式。虽然理论上应该是一致的，但实际操作上还有不同。</p><p>根据我英明领导的要求，要实现全双工通信，也就是说，在两个gitlab上创建互为镜像的项目，这两个项目可以定期的互相同步各自的代码。现在我只能实现单向通信，就是只有一侧进行同步，实现的是一种主从模式，如果有朋友能实现互相同步的，请联系我。但是我想了下，两个项目间互为镜像，是不是会造成先有鸡后有蛋的问题？就是必须先有一个项目，才能将另一个项目作为该项目的镜像？</p><h2 id="实现方案介绍"><a href="#实现方案介绍" class="headerlink" title="实现方案介绍"></a>实现方案介绍</h2><h3 id="环境以及必备工具介绍"><a href="#环境以及必备工具介绍" class="headerlink" title="环境以及必备工具介绍"></a>环境以及必备工具介绍</h3><ul><li><p>两个gitlab</p><ul><li>公司本部gitlab版本：12.2.5，ip地址（内网）：192.168.166.202:8181。</li><li>客户现场gitlab版本：12.4.2，ip地址（内网）：192.168.229.52:8181。</li></ul></li></ul><p>操作时均使用root账号操作，这一步比较危险，最好控制下权限，尽可能使用该项目的developer角色或者reporter角色的用户来进行操作。</p><p>在公司本部创建项目，studentmanagement2，以该项目进行测试。</p><ul><li>两个开源项目</li></ul><p>借助两个开源项目实现，分别是：</p><ul><li><a href="https://github.com/samrocketman/gitlab-mirrors.git" target="_blank" rel="noopener">git-mirrors</a>  </li><li><a href="https://github.com/doctormo/python-gitlab3.git" target="_blank" rel="noopener">python-gitlab3</a></li></ul><p>在python 2.7环境下运行，api兼容测试的gitlab版本。将这个中转程序放到客户现场的gitlab所在的机器上。</p><h3 id="基本概念介绍"><a href="#基本概念介绍" class="headerlink" title="基本概念介绍"></a>基本概念介绍</h3><h4 id="1-git-mirrors"><a href="#1-git-mirrors" class="headerlink" title="1. git mirrors"></a>1. git mirrors</h4><p>引用开源中国的解释如下：</p><pre><code>$ git clone --mirror $URL$ git clone --bare $URL等同于$ (cd $(basename $URL) &amp;&amp; git remote add --mirror=fetch origin $URL)当前的man-page如何表达：相比之下--bare，--mirror不仅将源的本地分支映射到目标的本地分支，它还映射所有引用（包括远程分支，注释等）并设置refspec配置，以便所有这些引用都被git remote update目标存储库中的a覆盖。</code></pre><p>简单来说，–bare就是裸克隆，只克隆该分支的信息。而–mirror是完整的镜像克隆，克隆所有相关的提交信息，包含不同的分支！</p><h4 id="2-整个实现流程介绍"><a href="#2-整个实现流程介绍" class="headerlink" title="2. 整个实现流程介绍"></a>2. 整个实现流程介绍</h4><p>核心是依靠git-mirrors项目进行中转，利用python-gitlab3依赖进行gitlab接口的调用。通过脚本利用git clone –mirror选项从公司本部的gitlab库，同步到客户现场的gitlab库中，并在中转机器上设置定时任务，进行定时同步。</p><p>示意图如下：</p><p><img src="%E5%90%8C%E6%AD%A5%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt></p><h3 id="实际操作流程"><a href="#实际操作流程" class="headerlink" title="实际操作流程"></a>实际操作流程</h3><h4 id="0-为客户现场中转主机进行环境配置"><a href="#0-为客户现场中转主机进行环境配置" class="headerlink" title="0. 为客户现场中转主机进行环境配置"></a>0. 为客户现场中转主机进行环境配置</h4><p>默认都在centos用户下安装，生成证书信息等操作。</p><p>安装git</p><pre><code>$ sudo yum install -y git</code></pre><p>已有python2.7的情况下，配置虚拟环境：</p><pre><code>// 安装pip$ curl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip.py$ sudo python get-pip.py// 安装virtualenv$ pip install virtualenv// 生成虚拟环境$ mkdir git_repository_sync &amp;&amp; virtualenv venv --python=python2.7</code></pre><p>安装python-gitlab</p><pre><code>$ yum install python-setuptools$ git clone https://github.com/doctormo/python-gitlab3.git$ cd python-gitlab3$ git checkout v0.5.8$ python setup.py install</code></pre><h4 id="1-在客户现场的gitlab中配置账号"><a href="#1-在客户现场的gitlab中配置账号" class="headerlink" title="1. 在客户现场的gitlab中配置账号"></a>1. 在客户现场的gitlab中配置账号</h4><ul><li><p>登录GitLab</p></li><li><p>创建一个用户</p></li><li><p>为该用户赋予管理员权限。简单起见，笔者使用root 这个GitLab的内置账户。</p></li><li><p>在GitLab创建一个Group，名字为test，该名称在下面的配置文件中会用到。</p></li></ul><h4 id="2-在虚拟机中配置用户并生成ssh-key"><a href="#2-在虚拟机中配置用户并生成ssh-key" class="headerlink" title="2. 在虚拟机中配置用户并生成ssh key"></a>2. 在虚拟机中配置用户并生成ssh key</h4><p>这一步需要在root用户下执行</p><pre><code>// 创建新用户# adduser gitmirror// 切换用户# su - gitmirror// 生成ssh密钥$ ssh-keygen -t rsa -C// 或者加入公司本部的gitlab所在的用户邮箱信息$ ssh-keygen -t rsa -C &quot;admin@example.com&quot;// 查看生成的密钥信息$ more /home/gitmirror/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDUdvDGqmvboURWy1YV1rexD+vRhkt4eEDoQuUBbQ6ceH6Ta/NRDPK6o/BnBC9n0AoSrXtHem5PYPnYKkLkv0yQo3JwtN/p07lH4SoN8/v+aHzCP87A5j1wSHA/6NB8CKefeZcR87llP/3P0v2gQukCcr3G1CaJeAhQ2lvLdimftgh83vvMv+28IJlt5UsJK0MqeQHw7uOA8//mgbEvZVHzzOGNpX/xkcj4lbCXhnLCyCr1/Q7LIKneJuxqZG1oyongIDU+yq3WFtNDGn8ORwIGVzXvA7sfTQF0npSsgKIJJKRkmn5+6pQtSB8WAYfrHc10qBzieFRlV5mZ7KYtySql admin@example.com</code></pre><p><strong>注意：</strong>如果切换gitmirror用户后生成了密钥信息，而python的虚拟环境建在centos用户下，那么需要将生成的证书信息导入到centos用户下，否则在执行git clone操作时，会出现找不到证书的情况。拷贝证书的操作如下：</p><pre><code>// 记得备份原有证书// 拷贝证书操作# cp /home/gitmirror/.ssh/id_rsa* /home/centos/.ssh/ </code></pre><h4 id="3-为公司本部的gitlab设置免密码登录"><a href="#3-为公司本部的gitlab设置免密码登录" class="headerlink" title="3. 为公司本部的gitlab设置免密码登录"></a>3. 为公司本部的gitlab设置免密码登录</h4><p>将上面生成的密钥信息拷贝的公司本部的gitlab用户中，操作如下：</p><p><img src="gitlab%E6%B7%BB%E5%8A%A0ssh-key%E4%BF%A1%E6%81%AF.png" alt></p><p>同样将该密钥信息也添加到客户现场的gitlab中。</p><p>测试ssh访问情况</p><pre><code>// 测试公司本部的gitlab访问$ ssh -T -p 2222 git@192.168.166.202// 这里gitlab使用docker部署，ssh端口为2222// 测试客户现场的gitlab访问$ ssh -T git@192.168.229.52</code></pre><h4 id="4-为客户现场的gitlab设置access-token（设置private-token）"><a href="#4-为客户现场的gitlab设置access-token（设置private-token）" class="headerlink" title="4. 为客户现场的gitlab设置access_token（设置private token）"></a>4. 为客户现场的gitlab设置access_token（设置private token）</h4><p>添加access_token的步骤如下：</p><p><img src="%E6%B7%BB%E5%8A%A0access_token.png" alt></p><p>点击绿色按钮后将会生成一个新的token，复制该token（只会出现一次）。</p><p><img src="%E5%A4%8D%E5%88%B6access_token.png" alt></p><p>复制完成后，填写到客户现场中转服务器中，如下：</p><pre><code># su - gitmirror$ touch private_token &amp;&amp; echo &#39;你复制的密钥信息&#39; &gt; private_token</code></pre><h4 id="5-创建本地仓库路径"><a href="#5-创建本地仓库路径" class="headerlink" title="5. 创建本地仓库路径"></a>5. 创建本地仓库路径</h4><p>GitLab Mirrors会将GitHub上的代码clone到本地，默认是~/repositories ，因此我们得创建该目录。</p><pre><code># su - gitmirror$ mkdir ~/repositories</code></pre><h4 id="6-下载并配置gitlab-mirrors"><a href="#6-下载并配置gitlab-mirrors" class="headerlink" title="6. 下载并配置gitlab-mirrors"></a>6. 下载并配置gitlab-mirrors</h4><p>切换到centos用户执行，如下：</p><pre><code># su - centos$ git clone https://github.com/samrocketman/gitlab-mirrors.git$ cd gitlab-mirrors$ chmod 755 *.sh// 添加配置文件$ cp config.sh.SAMPLE config.sh</code></pre><p>下面开始修改配置文件：</p><pre><code>$ cd gitlab-mirrors &amp;&amp; vim config.sh#Environment file## gitlab-mirrors settings##The user git-mirrors will run as.system_user=&quot;gitmirror&quot;#The home directory path of the $system_useruser_home=&quot;/home/${system_user}&quot;#The repository directory where gitlab-mirrors will contain copies of mirrored#repositories before pushing them to gitlab.repo_dir=&quot;${user_home}/repositories&quot;#colorize output of add_mirror.sh, update_mirror.sh, and git-mirrors.sh#commands.enable_colors=true#These are additional options which should be passed to git-svn.  On the command#line type &quot;git help svn&quot;git_svn_additional_options=&quot;-s&quot;#Force gitlab-mirrors to not create the gitlab remote so a remote URL must be#provided. (superceded by no_remote_set)no_create_set=false#Force gitlab-mirrors to only allow local remotes only.no_remote_set=false#Enable force fetching and pushing.  Will overwrite references if upstream#forced pushed.  Applies to git projects only.force_update=false#This option is for pruning mirrors.  If a branch is deleted upstream then that#change will propagate into your GitLab mirror.  Aplies to git projects only.prune_mirrors=false## Gitlab settings#// 注意这下面的内容#This is the Gitlab group where all project mirrors will be grouped.// 在客户现场gitlab中创建的group名称gitlab_namespace=&quot;test&quot;#This is the base web url of your Gitlab server.// 在客户现场gitlab地址gitlab_url=&quot;http://192.168.229.52:8181&quot;#Special user you created in Gitlab whose only purpose is to update mirror sites#and admin the $gitlab_namespace group.// 在客户现场gitlab中使用的用户名gitlab_user=&quot;root&quot;#Generate a token for your $gitlab_user and set it here.// 默认的获取access_token的文件gitlab_user_token_secret=&quot;$(head -n1 &quot;${user_home}/private_token&quot; 2&gt; /dev/null || echo &quot;&quot;)&quot;#Sets the Gitlab API version, either 3 or 4gitlab_api_version=4#Verify signed SSL certificates?ssl_verify=false#Push to GitLab over http?  Otherwise will push projects via SSH.http_remote=false## Gitlab new project default settings.  If a project needs to be created by# gitlab-mirrors then it will assign the following values as defaults.##values must be true or falseissues_enabled=falsewall_enabled=falsewiki_enabled=falsesnippets_enabled=falsemerge_requests_enabled=falsepublic=false// :wq保存退出</code></pre><h4 id="7-添加仓库同步配置并开始同步"><a href="#7-添加仓库同步配置并开始同步" class="headerlink" title="7. 添加仓库同步配置并开始同步"></a>7. 添加仓库同步配置并开始同步</h4><p>在centos用户下执行最后的操作，添加同步的公司本部仓库信息如下：</p><pre><code># su - centos$ ./add_mirror.sh --git --project-name test --mirror ssh://git@192.168.166.202:2222/test-cloud/demo-business/express/studentmanage2.git</code></pre><p><strong>注意：</strong>这里使用的ssh格式的地址信息，而不是使用http链接的信息。</p><p><strong>注意：</strong>执行./add_mirror.sh脚本出现的问题</p><pre><code># ./add_mirror.sh --git --project-name studentmanager2 --mirror http://10.0.66.202:8181/icp-cloud/demo-business/express/studentmanage2.gitResolving gitlab remote.Traceback (most recent call last):  File &quot;lib/manage_gitlab_project.py&quot;, line 12, in &lt;module&gt;    raise ImportError(&quot;python-gitlab module is not installed.  You probably didn&#39;t read the install instructions closely enough.  See docs/prerequisites.md.&quot;)ImportError: python-gitlab module is not installed.  You probably didn&#39;t read the install instructions closely enough.  See docs/prerequisites.md.There was an unknown issue with manage_gitlab_project.py</code></pre><p>解决方式:</p><pre><code># pip install python-gitlab</code></pre><p>添加命令执行无错误，即可进行同步，执行如下：</p><pre><code>$ cd ~/git_repository_sync/gitlab-mirrors &amp;&amp; ./git_mirrors.sh</code></pre><p>执行完成后，则可以看到客户现场的gitlab中有了我们自己的项目，如下：</p><p><img src="%E5%90%8C%E6%AD%A5%E9%A1%B9%E7%9B%AE%E6%88%AA%E5%9B%BE.png" alt></p><p>如果需要定时同步，直接编写crontab规则即可，如下：</p><pre><code>$ crontab -e*/5 * * * * /home/centos/git_repository_sync/gitlab-mirrors/git_mirrors.sh</code></pre><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在上述操作下，可以实现两地间gitlab仓库的单向同步</p><h2 id="参考连接"><a href="#参考连接" class="headerlink" title="参考连接"></a>参考连接</h2><ul><li><a href="http://www.itmuch.com/work/git-repo-sync-with-gitlab-mirrors/" target="_blank" rel="noopener">http://www.itmuch.com/work/git-repo-sync-with-gitlab-mirrors/</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s 1.20.5 二进制安装教程</title>
      <link href="/2021/04/04/k8s-1-20-5-er-jin-zhi-an-zhuang-jiao-cheng/"/>
      <url>/2021/04/04/k8s-1-20-5-er-jin-zhi-an-zhuang-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="k8s-1-20-5-二进制安装教程"><a href="#k8s-1-20-5-二进制安装教程" class="headerlink" title="k8s 1.20.5 二进制安装教程"></a>k8s 1.20.5 二进制安装教程</h1><h2 id="资源准备以及ip地址分配"><a href="#资源准备以及ip地址分配" class="headerlink" title="资源准备以及ip地址分配"></a>资源准备以及ip地址分配</h2><p>准备了六台机器进行安装，全部部署kubelet以及kube-porxy，不是在特定机器上进行部署，通过标签的方式控制业务服务的部署范围。</p><p>机器列表以及安装内容如下:</p><table><thead><tr><th>机器名</th><th>ip地址</th><th>配置信息</th><th>集群角色</th><th>操作系统</th><th>安装组件</th></tr></thead><tbody><tr><td>k8s-master-01</td><td>192.168.123.61</td><td>4核心 16G 80G SSD 千兆网络</td><td>master</td><td>CentOS 7.9 内核版本4.19</td><td>kube-apiserver、kube-controller-manager、kube-scheduler、etcd、kubelet、kube-proxy、keepalived、haproxy</td></tr><tr><td>k8s-master-02</td><td>192.168.123.62</td><td>4核心 16G 80G SSD 千兆网络</td><td>master</td><td>CentOS 7.9 内核版本4.19</td><td>kube-apiserver、kube-controller-manager、kube-scheduler、etcd、kubelet、kube-proxy、keepalived、haproxy</td></tr><tr><td>k8s-master-03</td><td>192.168.123.63</td><td>4核心 16G 80G SSD 千兆网络</td><td>master</td><td>CentOS 7.9 内核版本4.19</td><td>kube-apiserver、kube-controller-manager、kube-scheduler、etcd、kubelet、kube-proxy、keepalived、haproxy</td></tr><tr><td>k8s-worker-01</td><td>192.168.123.71</td><td>4核心 16G 100G 普盘 千兆网络</td><td>worker</td><td>CentOS 7.9 内核版本4.19</td><td>kubelet、kube-proxy</td></tr><tr><td>k8s-worker-02</td><td>192.168.123.72</td><td>4核心 16G 100G 普盘 千兆网络</td><td>worker</td><td>CentOS 7.9 内核版本4.19</td><td>kubelet、kube-proxy</td></tr><tr><td>k8s-worker-03</td><td>192.168.123.73</td><td>4核心 16G 100G 普盘 千兆网络</td><td>worker</td><td>CentOS 7.9 内核版本4.19</td><td>kubelet、kube-proxy</td></tr></tbody></table><p>网段信息如下：</p><table><thead><tr><th>网络地址</th><th>作用</th></tr></thead><tbody><tr><td>10.96.0.0/16</td><td>k8s Service 网段</td></tr><tr><td>10.96.0.2</td><td>CoreDNS地址</td></tr><tr><td>172.16.0.0/16</td><td>k8s pod 网段</td></tr><tr><td>192.168.123.60</td><td>VIP，由keepalived提供，代理apiserver端口</td></tr></tbody></table><p>使用了keepalived+haproxy实现kube-apiserver的高可用，设置虚拟ip为192.168.123.60，端口号为16443，通过这两个组件对apiserver服务进行代理，统一访问入口。</p><p>安装的组件版本信息如下：</p><table><thead><tr><th>组件信息</th><th>版本信息</th></tr></thead><tbody><tr><td>kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy</td><td>1.20.5</td></tr><tr><td>etcd</td><td>v3.4.13</td></tr><tr><td>calico</td><td>v3.14</td></tr><tr><td>coredns</td><td>1.8.0</td></tr></tbody></table><p><strong>注意：</strong>选择k8s版本时，版本号后面小版本大于5时再去选择使用该版本运行在生产环境中，例如1.18.9、1.19.5这样版本，尽量不要选择如1.19.1、1.18.3这样的版本。</p><h2 id="初始化操作"><a href="#初始化操作" class="headerlink" title="初始化操作"></a>初始化操作</h2><p>注意：这里绝大部分操作，均在root用户下执行，务必注意。</p><p>安装前注意：</p><ul><li>不要使用中文目录或者克隆过的虚拟机（网卡问题）</li><li>生产环境建议使用二进制安装</li><li>VIP不要和内网ip地址重复，需要和主机ip在同一个局域网段内</li></ul><h3 id="第一部分：初始化"><a href="#第一部分：初始化" class="headerlink" title="第一部分：初始化"></a>第一部分：初始化</h3><p>在所有机器上同步执行，设置hosts信息：</p><pre><code># vim /etc/hosts// 在文件后追加信息192.168.123.61 k8s-master-01192.168.123.62 k8s-master-02192.168.123.63 k8s-master-03192.168.123.60 k8s-master-lb192.168.123.71 k8s-worker-01192.168.123.72 k8s-worker-02192.168.123.73 k8s-worker-01</code></pre><p>修改yum源，提供docker和kubernetes的安装源信息，以及准备docker的安装环境。</p><pre><code># mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# yum install -y yum-utils \                    device-mapper-persistent-data \                    lvm2# yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF// 替换软件源# sed -i -e &#39;/mirrors.cloud.aliyuncs.com/d&#39; -e &#39;/mirrors.aliyuncs.com/d&#39; /etc/yum.repos.d/CentOS-Base.repo// 更新绕过内核信息，内核我们后续自行安装# yum update -y --exclude=kernel* </code></pre><p>安装必备工具</p><pre><code># yum install -y wget git unzip net-tools curl vim telnet psmisc  tree sshpass jq</code></pre><p>关闭防火墙，selinux、dnsmasq、NetworkManager</p><pre><code># systemctl disable --now firewalld# systemctl disable --now dnsmasq# systemctl disable --now NetworkManager# setenforce 0# sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux# sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config</code></pre><p>注意：selinux关闭时必须两个同时修改</p><p>关闭swap分区</p><pre><code># swapoff -a &amp;&amp; sysctl -w vm.swappiness=0# sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab</code></pre><p>时钟同步,时钟不一致可能会导致证书出现问题</p><pre><code># rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm# yum install ntpdate -y</code></pre><p>设置时区</p><pre><code># ln -df /usr/share/zoneinfo/Asia/Shanghai /etc/localtime# echo &#39;Asia/Shanghai&#39; &gt; /etc/timezone# ntpdate time2.aliyun.com// 设置crontab进行时间同步# crontab -e*/5 * * * * ntpdate time2.aliyun.com</code></pre><p>设置文件打开数量信息</p><pre><code>// 临时设置ulimit# ulimit -SHn 65535永久生效方式# vim /etc/security/limits.conf// 追加以下内容* soft nofile 655360* hard nofile 131072* soft nproc 655350* hard nproc 655350* soft memlock unlimited* hard memlock unlimited</code></pre><p>在k8s-master-01节点上生成ssh证书并传输到其它机器上，保证从k8s-master-01到各个节点都能无密码访问，如下：</p><pre><code># ssh-keygen -t rsa# for i in k8s-master-02 k8s-master-03 k8s-worker-01 k8s-worker-02 k8s-worker-03; do ssh-copy-id -i /root/.ssh/id_rsa.pub $i;done</code></pre><p>注意：k8s-master-01作为管理节点免密钥登录其它节点，安装过程中生成配置文件和证书，均在k8s-master-01上操作，集群管理也在k8s-master-01上进行。如果使用的是阿里云或者AWS，其后端服务器是不能反向连接SLB的，所以需要单独一台kubectl服务器。</p><h3 id="第二部分：内核配置"><a href="#第二部分：内核配置" class="headerlink" title="第二部分：内核配置"></a>第二部分：内核配置</h3><p>更新并重启机器，排除内核信息</p><pre><code># yum update -y --exclude=kernel* &amp;&amp; reboot</code></pre><p>在生产环境中必须进行内核升级，CentOS 7需要内核版本在4.18+。 这里使用4.19版本</p><pre><code># cd /root# wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm# wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm</code></pre><p>可以使用idm下载后再上传到各个服务器上。或者先上传到k8s-master-01节点上，将内核文件传输到各个机器上</p><pre><code># for i in k8s-master-02 k8s-master-03 k8s-worker-01 k8s-worker-02 k8s-worker-03; do scp kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm $i:/root/ ;done</code></pre><p>批量在各个机器上执行以下命令</p><pre><code># cd /root/ &amp;&amp; yum localinstall kernel-ml* -y</code></pre><p>更改内核启动顺序</p><pre><code># grub2-set-default 0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfg# grubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;</code></pre><p>检查内核版本</p><pre><code># grubby --default-kernel/boot/vmlinuz-4.19.12-1.el7.elrepo.x86_64</code></pre><p>所有节点重启，再重新检查内核信息，如果和之前设定的版本一致，则内核升级成功。</p><h3 id="第三部分：安装ipvs工具以及内核参数设置"><a href="#第三部分：安装ipvs工具以及内核参数设置" class="headerlink" title="第三部分：安装ipvs工具以及内核参数设置"></a>第三部分：安装ipvs工具以及内核参数设置</h3><p>生产环境中推荐使用ipvs工具，不推荐使用iptables</p><pre><code># yum install ipvsadm ipset sysstat conntrack libseccomp -y</code></pre><p>所有内核节点配置ipvs模块，4.19+版本内核需要把nf_conntrack_ipv4更改为nf_conntrack，4.18及以下可以继续使用。</p><pre><code># vim /etc/modules-load.d/ipvs.conf//添加以下配置信息ip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lbtcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipip# systemctl enable --now systemd-modules-load.service// 校验是否开启，如果无输出信息，需要重启机器# lsmod | grep -e ip_vs -e nf_conntrack</code></pre><p>开启内核参数</p><pre><code># cat &gt;&gt; /etc/sysctl.d/k8s.conf&lt;&lt;EOFnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1fs.may_datch_mounts=1vm.overcommit_memory=1vm.panic_on_oom=1fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time=600net.ipv4.tcp_keepalive_probes=3net.ipv4.tcp_keepalive_intvl=15net.ipv4.tcp_max_tw_buckets=36000net.ipv4.tcp_tw_reuse=1net.ipv4.tcp_max_orphans=327680net.ipv4.tcp_orphan_retries=3net.ipv4.tcp_syncookies=1net.ipv4.tcp_max_syn_backlog=16384net.ipv4.ip_conntrack_max=65536net.ipv4.tcp_timestamps=0net.core.somaxconn=16384EOF// 更新内核信息# sysctl --system</code></pre><p>注意：net.ipv4.ip_forward一定要打开，否则跨主机的通讯是不通的。</p><p>重启后进行验证</p><pre><code># lsmod | grep -e ip_vs -e nf_conntrackip_vs_ftp              16384  0nf_nat                 32768  1 ip_vs_ftpip_vs_sed              16384  0ip_vs_nq               16384  0ip_vs_fo               16384  0ip_vs_sh               16384  0ip_vs_dh               16384  0ip_vs_lblcr            16384  0ip_vs_wrr              16384  0ip_vs_rr               16384  0ip_vs_wlc              16384  0ip_vs_lc               16384  0ip_vs                 151552  22 ip_vs_wlc,ip_vs_rr,ip_vs_dh,ip_vs_lblcr,ip_vs_sh,ip_vs_fo,ip_vs_nq,ip_vs_wrr,ip_vs_lc,ip_vs_sed,ip_vs_ftpnf_conntrack          143360  2 nf_nat,ip_vsnf_defrag_ipv6         20480  1 nf_conntracknf_defrag_ipv4         16384  1 nf_conntracklibcrc32c              16384  4 nf_conntrack,nf_nat,xfs,ip_vs</code></pre><h3 id="第四部分：docker安装"><a href="#第四部分：docker安装" class="headerlink" title="第四部分：docker安装"></a>第四部分：docker安装</h3><p>安装docker 19.03.*，注意这里使用19.03版本的docker服务，不要使用更高版本的docker服务，否则容易导致很多问题。</p><pre><code># yum install docker-ce-19.03.* -y</code></pre><p>修改cgroup运行方式，修改为systemd方式运行，注意后续在安装时，kubelet的cgroupDriver和该处一致。</p><pre><code># mkdir /etc/docker# cat &gt; /etc/docker/daemon.json &lt;&lt;EOF{    &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],    &quot;registry-mirrors&quot;: [        &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,        &quot;https://docker.mirrors.ustc.edu.cn&quot;,        &quot;https://registry.docker-cn.com&quot;    ],    &quot;insecure-registries&quot;: [        // 私有的harbor仓库地址        &quot;192.168.123.105&quot;    ],    &quot;live-restore&quot;: true,    &quot;log-driver&quot;: &quot;json-file&quot;,    &quot;log-opts&quot;: {        &quot;max-size&quot;: &quot;50m&quot;,        &quot;max-file&quot;: &quot;3&quot;    }}EOF# systemctl daemon-reload &amp;&amp; systemctl enable --now docker</code></pre><p>注意：由于新版kubelet建议使用systemd，可以把docker的CgroupDriver更改为systemd。</p><h3 id="第五部分：下载etcd、k8s安装组件以及证书签发组件"><a href="#第五部分：下载etcd、k8s安装组件以及证书签发组件" class="headerlink" title="第五部分：下载etcd、k8s安装组件以及证书签发组件"></a>第五部分：下载etcd、k8s安装组件以及证书签发组件</h3><p>在所有节点上，创建以下文件夹信息，如下：</p><pre><code># mkdir -p /opt/cni/bin  /etc/etcd/ssl /etc/kubernetes/pki  /etc/kubernetes/pki/etcd /etc/kubernetes/manifests/  /etc/systemd/system/kubelet.service.d /var/lib/kubelet /var/log/kubernetes /root/.kube </code></pre><p>在k8s-master-01节点上进行操作。</p><p>下载证书签发组件，如下：</p><pre><code># wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64// 赋予可执行权限，并拷贝到/usr/local/bin/下，可以直接使用# chmod +x cfssl*# mv cfssl_linux-amd64 /usr/local/bin/cfssl# mv cfssljson_linux-amd64 /usr/local/bin/cfssljson# mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo</code></pre><p>下载k8s的Server二进制安装包，访问<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md" target="_blank" rel="noopener">该地址</a>，下载1.20.5版本的Server binaries，点击链接后选择<em>kubernetes-server-linux-amd64.tar.gz</em>压缩包，点击下载。也可以直接从下载链接下载：</p><pre><code># wget https://dl.k8s.io/v1.20.5/kubernetes-server-linux-amd64.tar.gz</code></pre><p>解压文件夹，并将用到的组件放置到执行路径下：</p><pre><code># tar -xf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}# chomod +x /usr/local/bin/kube*</code></pre><p>下载etcd安装包，二进制安装etcd 3.4.13，从<a href="https://github.com/etcd-io/etcd/releases/tag/v3.4.13" target="_blank" rel="noopener">该地址</a>进行下载，或者直接下载：</p><pre><code># wget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz# tar -zxvf etcd-v3.4.13-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.4.13-linux-amd64/etcd{,ctl}# chomod +x /usr/local/bin/etcd*</code></pre><p>将这些组件从k8s-master-01节点上，统一传输到各个节点，操作如下：</p><pre><code>MasterNodes=&#39;k8s-master-02 k8s-master-03&#39;WorkerNodes=&#39;k8s-worker-01 k8s-worker-02 k8s-worker-03&#39;for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube{let,-apiserver,-controller-manager,-scheduler,-proxy} $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; donefor NODE in $WorkerNodes; do scp /usr/local/bin/kube{let,-proxy} $NODE:/usr/local/bin/; done</code></pre><p>最后查看版本</p><pre><code># kubelet --versionKubernetes v1.20.5# etcdctl versionetcdctl version: 3.4.13API version: 3.4</code></pre><h2 id="kubernetes安装"><a href="#kubernetes安装" class="headerlink" title="kubernetes安装"></a>kubernetes安装</h2><p>所有生成证书的操作，均在/root/workspace目录下执行。</p><h3 id="etcd集群部署"><a href="#etcd集群部署" class="headerlink" title="etcd集群部署"></a>etcd集群部署</h3><h4 id="1-证书生成"><a href="#1-证书生成" class="headerlink" title="1. 证书生成"></a>1. 证书生成</h4><p>首先生成etcd的证书信息，配置文件如下：</p><pre><code># vim ca-csr.json{  &quot;CN&quot;: &quot;kubernetes&quot;,  &quot;key&quot;: {      &quot;algo&quot;: &quot;rsa&quot;,      &quot;size&quot;: 2048  },  &quot;names&quot;: [    {      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Shandong&quot;,      &quot;L&quot;: &quot;Jinan&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;system&quot;    }  ],  &quot;ca&quot;: {      &quot;expiry&quot;: &quot;87600h&quot;  }}</code></pre><p>注：</p><ul><li>CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；</li><li>O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)</li></ul><p>创建CA证书：</p><pre><code># cfssl gencert -initca ca-csr.json  | cfssljson -bare ca</code></pre><p>配置ca证书策略:</p><pre><code># vim ca-config.json{  &quot;signing&quot;: {      &quot;default&quot;: {          &quot;expiry&quot;: &quot;87600h&quot;        },      &quot;profiles&quot;: {          &quot;kubernetes&quot;: {              &quot;usages&quot;: [                  &quot;signing&quot;,                  &quot;key encipherment&quot;,                  &quot;server auth&quot;,                  &quot;client auth&quot;              ],              &quot;expiry&quot;: &quot;87600h&quot;          }      }  }}</code></pre><p>配置etcd请求csr文件：</p><pre><code># vim etcd-csr.json{  &quot;CN&quot;: &quot;etcd&quot;,  &quot;hosts&quot;: [    &quot;127.0.0.1&quot;,    &quot;192.168.123.61&quot;,    &quot;192.168.123.62&quot;,    &quot;192.168.123.63&quot;,    &quot;192.168.123.60&quot;  ],  &quot;key&quot;: {    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  },  &quot;names&quot;: [{    &quot;C&quot;: &quot;CN&quot;,    &quot;ST&quot;: &quot;Shandong&quot;,    &quot;L&quot;: &quot;Jinan&quot;,    &quot;O&quot;: &quot;k8s&quot;,    &quot;OU&quot;: &quot;system&quot;  }]}</code></pre><p>生成证书</p><pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson  -bare etcd# ls etcd*.pemetcd-key.pem  etcd.pem</code></pre><h4 id="2-etcd服务部署以及验证"><a href="#2-etcd服务部署以及验证" class="headerlink" title="2. etcd服务部署以及验证"></a>2. etcd服务部署以及验证</h4><p>一共需要配置三个etcd节点，分别位于k8s-master-01、k8s-master-02、k8s-master-03。etcd配置文件在各个节点上分别进行设置如下：</p><p>k8s-master-01节点</p><pre><code># vim /etc/etcd/etcd.conf#[Member]ETCD_NAME=&quot;k8s-master-01&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.123.61:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.123.61:2379,http://127.0.0.1:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.123.61:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.123.61:2379&quot;ETCD_INITIAL_CLUSTER=&quot;k8s-master-01=https://192.168.123.61:2380,k8s-master-02=https://192.168.123.62:2380,k8s-master-03=https://192.168.123.63:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</code></pre><p>k8s-master-02节点</p><pre><code>#  vim /etc/etcd/etcd.conf#[Member]ETCD_NAME=&quot;k8s-master-02&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.123.62:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.123.62:2379,http://127.0.0.1:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.123.62:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.123.62:2379&quot;ETCD_INITIAL_CLUSTER=&quot;k8s-master-01=https://192.168.123.61:2380,k8s-master-02=https://192.168.123.62:2380,k8s-master-03=https://192.168.123.63:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</code></pre><p>k8s-master-03节点</p><pre><code>#  vim /etc/etcd/etcd.conf#[Member]ETCD_NAME=&quot;k8s-master-03&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.123.63:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.123.63:2379,http://127.0.0.1:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.123.63:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.123.63:2379&quot;ETCD_INITIAL_CLUSTER=&quot;k8s-master-01=https://192.168.123.61:2380,k8s-master-02=https://192.168.123.62:2380,k8s-master-03=https://192.168.123.63:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</code></pre><p>配置项解析如下：</p><ul><li>ETCD_NAME：节点名称，集群中唯一</li><li>ETCD_DATA_DIR：数据目录</li><li>ETCD_LISTEN_PEER_URLS：集群通信监听地址</li><li>ETCD_LISTEN_CLIENT_URLS：客户端访问监听地址</li><li>ETCD_INITIAL_ADVERTISE_PEER_URLS：集群通告地址</li><li>ETCD_ADVERTISE_CLIENT_URLS：客户端通告地址</li><li>ETCD_INITIAL_CLUSTER：集群节点地址</li><li>ETCD_INITIAL_CLUSTER_TOKEN：集群Token</li><li>ETCD_INITIAL_CLUSTER_STATE：加入集群的当前状态，new是新集群，existing表示加入已有集群</li></ul><p>在这三个节点上，同时编写etcd服务，如下：</p><pre><code># vim /usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=-/etc/etcd/etcd.confWorkingDirectory=/var/lib/etcd/ExecStart=/usr/local/bin/etcd \  --cert-file=/etc/etcd/ssl/etcd.pem \  --key-file=/etc/etcd/ssl/etcd-key.pem \  --trusted-ca-file=/etc/etcd/ssl/ca.pem \  --peer-cert-file=/etc/etcd/ssl/etcd.pem \  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \  --peer-client-cert-auth \  --client-cert-authRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><p>从k8s-master-01节点开始设置etcd证书，并将证书发送到各个节点，如下：</p><pre><code># cp ca*.pem /etc/etcd/ssl/# cp etcd*.pem /etc/etcd/ssl/# cp etcd.conf /etc/etcd/# cp etcd.service /usr/lib/systemd/system/# for i in k8s-master-02 k8s-master-03;do rsync -vaz etcd*.pem ca*.pem $i:/etc/etcd/ssl/;done// 如果之前编写统一的配置文件，发送到不同节点的操作如下，注意修改配置文件中的ip地址信息# for i in k8s-master-02 k8s-master-03;do rsync -vaz etcd.conf $i:/etc/etcd/;done# for i in k8s-master-02 k8s-master-03;do rsync -vaz etcd.service $i:/usr/lib/systemd/system/;done</code></pre><p>在各个节点同时执行etcd服务启动，如下：</p><pre><code># mkdir -p /var/lib/etcd/default.etcd# systemctl daemon-reload# systemctl enable etcd.service# systemctl start etcd.service# systemctl status etcd</code></pre><p>注：第一次启动可能会卡一段时间，因为节点会等待其他节点启动</p><p>在k8s-master-01节点执行，查看集群状态：</p><pre><code># ETCDCTL_API=3 /usr/local/bin/etcdctl --write-out=table --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.123.61:2379,https://192.168.123.62:2379,https://192.168.123.63:2379 endpoint health+-------------------------+--------+-------------+-------+|        ENDPOINT         | HEALTH |    TOOK     | ERROR |+-------------------------+--------+-------------+-------+| https://192.168.123.61:2379 |   true |   7.71134ms |       || https://192.168.123.62:2379 |   true |  8.459485ms |       || https://192.168.123.63:2379 |   true | 11.253605ms |       |+-------------------------+--------+-------------+-------+</code></pre><h3 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h3><p><em>在apiserver安装时，注意其ip地址配置为127.0.0.1，在健康检查时可能无法通过，出现请求400 bad request的情况，无需担心，可正常使用。</em></p><p>首先创建工作目录如下：</p><pre><code>// kubernetes组件配置文件存放目录# mkdir -p /etc/kubernetes/         // kubernetes组件证书文件存放目录# mkdir -p /etc/kubernetes/ssl     // kubernetes组件日志文件存放目录# mkdir /var/log/kubernetes        </code></pre><h4 id="1-签发证书"><a href="#1-签发证书" class="headerlink" title="1. 签发证书"></a>1. 签发证书</h4><p>在k8s-master-01节点上，创建csr请求文件:</p><pre><code># vim kube-apiserver-csr.json{  &quot;CN&quot;: &quot;kubernetes&quot;,  &quot;hosts&quot;: [    &quot;127.0.0.1&quot;,    &quot;192.168.123.61&quot;,    &quot;192.168.123.62&quot;,    &quot;192.168.123.63&quot;,    &quot;192.168.123.71&quot;,    &quot;192.168.123.72&quot;,    &quot;192.168.123.73&quot;,    &quot;192.168.123.60&quot;,    &quot;10.96.0.1&quot;,    &quot;kubernetes&quot;,    &quot;kubernetes.default&quot;,    &quot;kubernetes.default.svc&quot;,    &quot;kubernetes.default.svc.cluster&quot;,    &quot;kubernetes.default.svc.cluster.local&quot;  ],  &quot;key&quot;: {    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  },  &quot;names&quot;: [    {      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Shandong&quot;,      &quot;L&quot;: &quot;Jinan&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;system&quot;    }  ]}</code></pre><p>如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表。由于该证书后续被 kubernetes master 集群使用，需要将master节点的IP都填上，同时还需要填写 service 网络的首个IP。(一般是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.96.0.1)</p><p>生成证书和token文件如下：</p><pre><code>// 生成证书信息# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver# ls kube-apiserver*.pemkube-apiserver-key.pem  kube-apiserver.pem// 生成token文件# cat &gt; token.csv &lt;&lt; EOF$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;),kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;EOF# cat token.csv492a8adfbae98956d54a4592366a14a9,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</code></pre><h4 id="2-创建配置和服务启动文件"><a href="#2-创建配置和服务启动文件" class="headerlink" title="2. 创建配置和服务启动文件"></a>2. 创建配置和服务启动文件</h4><p>在k8s-master-01节点上，创建配置文件如下：</p><pre><code># vim /etc/kubernetes/kube-apiserver.confKUBE_APISERVER_OPTS=&quot;--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \  --anonymous-auth=false \  --bind-address=192.168.123.61 \  --secure-port=6443 \  --advertise-address=192.168.123.61 \  --insecure-port=0 \  --authorization-mode=Node,RBAC \  --runtime-config=api/all=true \  --enable-bootstrap-token-auth \  --service-cluster-ip-range=10.96.0.0/16 \  --token-auth-file=/etc/kubernetes/token.csv \  --service-node-port-range=30000-50000 \  --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem  \  --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \  --client-ca-file=/etc/kubernetes/ssl/ca.pem \  --kubelet-client-certificate=/etc/kubernetes/ssl/kube-apiserver.pem \  --kubelet-client-key=/etc/kubernetes/ssl/kube-apiserver-key.pem \  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \  --service-account-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \  --service-account-issuer=https://kubernetes.default.svc.cluster.local \  --etcd-cafile=/etc/etcd/ssl/ca.pem \  --etcd-certfile=/etc/etcd/ssl/etcd.pem \  --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \  --etcd-servers=https://192.168.123.61:2379,https://192.168.123.62:2379,https://192.168.123.63:2379 \  --enable-swagger-ui=true \  --allow-privileged=true \  --apiserver-count=3 \  --audit-log-maxage=30 \  --audit-log-maxbackup=3 \  --audit-log-maxsize=100 \  --audit-log-path=/var/log/kube-apiserver-audit.log \  --event-ttl=1h \  --alsologtostderr=true \  --logtostderr=false \  --log-dir=/var/log/kubernetes \  --v=4&quot;</code></pre><p><em>注意：</em>在不同节点上，需要变更的是<em>bind-address</em>、<em>advertise-address</em>，需要变更为该节点的真实的ip地址。</p><p>在k8s-master-02节点上，创建配置文件如下：</p><pre><code># vim /etc/kubernetes/kube-apiserver.confKUBE_APISERVER_OPTS=&quot;--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \  --anonymous-auth=false \  --bind-address=192.168.123.62 \  --secure-port=6443 \  --advertise-address=192.168.123.62 \  --insecure-port=0 \  --authorization-mode=Node,RBAC \  --runtime-config=api/all=true \  --enable-bootstrap-token-auth \  --service-cluster-ip-range=10.96.0.0/16 \  --token-auth-file=/etc/kubernetes/token.csv \  --service-node-port-range=30000-50000 \  --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem  \  --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \  --client-ca-file=/etc/kubernetes/ssl/ca.pem \  --kubelet-client-certificate=/etc/kubernetes/ssl/kube-apiserver.pem \  --kubelet-client-key=/etc/kubernetes/ssl/kube-apiserver-key.pem \  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \  --service-account-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \  --service-account-issuer=https://kubernetes.default.svc.cluster.local \  --etcd-cafile=/etc/etcd/ssl/ca.pem \  --etcd-certfile=/etc/etcd/ssl/etcd.pem \  --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \  --etcd-servers=https://192.168.123.61:2379,https://192.168.123.62:2379,https://192.168.123.63:2379 \  --enable-swagger-ui=true \  --allow-privileged=true \  --apiserver-count=3 \  --audit-log-maxage=30 \  --audit-log-maxbackup=3 \  --audit-log-maxsize=100 \  --audit-log-path=/var/log/kube-apiserver-audit.log \  --event-ttl=1h \  --alsologtostderr=true \  --logtostderr=false \  --log-dir=/var/log/kubernetes \  --v=4&quot;</code></pre><p>在k8s-master-03节点上，创建配置文件如下：</p><pre><code># vim /etc/kubernetes/kube-apiserver.confKUBE_APISERVER_OPTS=&quot;--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \  --anonymous-auth=false \  --bind-address=192.168.123.63 \  --secure-port=6443 \  --advertise-address=192.168.123.63 \  --insecure-port=0 \  --authorization-mode=Node,RBAC \  --runtime-config=api/all=true \  --enable-bootstrap-token-auth \  --service-cluster-ip-range=10.96.0.0/16 \  --token-auth-file=/etc/kubernetes/token.csv \  --service-node-port-range=30000-50000 \  --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem  \  --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \  --client-ca-file=/etc/kubernetes/ssl/ca.pem \  --kubelet-client-certificate=/etc/kubernetes/ssl/kube-apiserver.pem \  --kubelet-client-key=/etc/kubernetes/ssl/kube-apiserver-key.pem \  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \  --service-account-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \  --service-account-issuer=https://kubernetes.default.svc.cluster.local \  --etcd-cafile=/etc/etcd/ssl/ca.pem \  --etcd-certfile=/etc/etcd/ssl/etcd.pem \  --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \  --etcd-servers=https://192.168.123.61:2379,https://192.168.123.62:2379,https://192.168.123.63:2379 \  --enable-swagger-ui=true \  --allow-privileged=true \  --apiserver-count=3 \  --audit-log-maxage=30 \  --audit-log-maxbackup=3 \  --audit-log-maxsize=100 \  --audit-log-path=/var/log/kube-apiserver-audit.log \  --event-ttl=1h \  --alsologtostderr=true \  --logtostderr=false \  --log-dir=/var/log/kubernetes \  --v=4&quot;</code></pre><p>各个配置项解释如下：</p><pre><code>--logtostderr：启用日志--v：日志等级--log-dir：日志目录--etcd-servers：etcd集群地址--bind-address：监听地址--secure-port：https安全端口--advertise-address：集群通告地址--allow-privileged：启用授权--service-cluster-ip-range：Service虚拟IP地址段--enable-admission-plugins：准入控制模块--authorization-mode：认证授权，启用RBAC授权和节点自管理--enable-bootstrap-token-auth：启用TLS bootstrap机制--token-auth-file：bootstrap token文件--service-node-port-range：Service nodeport类型默认分配端口范围--kubelet-client-xxx：apiserver访问kubelet客户端证书--tls-xxx-file：apiserver https证书--etcd-xxxfile：连接Etcd集群证书--audit-log-xxx：审计日志</code></pre><p>在所有节点上，创建服务启动文件如下：</p><pre><code># vim /usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=etcd.serviceWants=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/kube-apiserver.confExecStart=/usr/local/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h4 id="3-同步证书到其它节点并启动服务"><a href="#3-同步证书到其它节点并启动服务" class="headerlink" title="3. 同步证书到其它节点并启动服务"></a>3. 同步证书到其它节点并启动服务</h4><p>在k8s-master-01节点上，同步证书操作如下：</p><pre><code># cp ca*.pem /etc/kubernetes/ssl/# cp kube-apiserver*.pem /etc/kubernetes/ssl/# for i in k8s-master-02 k8s-master-03; do rsync -vaz ca*.pem kube-apiserver*.pem $i:/etc/kubernetes/ssl/;done</code></pre><p>注意检查好k8s-master-02、k8s-master-03节点kube-apiserver服务配置文件所在ip地址。</p><p>最后，在三个节点上同时进行服务启动操作，如下：</p><pre><code># systemctl daemon-reload# systemctl enable kube-apiserver# systemctl start kube-apiserver# systemctl status kube-apiserver</code></pre><p>简单测试api-server启动情况，一个是查看日志信息：</p><pre><code># tail -f /var/log/messages// 如果出现以下信息，说明启动成功transport: loopyWriter.run returning. connection error: desc = &quot;transport is closing&quot;</code></pre><p>另一个方式是通过curl请求apiserver的地址</p><pre><code># curl --insecure https://192.168.123.61:6443/{  &quot;kind&quot;: &quot;Status&quot;,  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;metadata&quot;: {  },  &quot;status&quot;: &quot;Failure&quot;,  &quot;message&quot;: &quot;Unauthorized&quot;,  &quot;reason&quot;: &quot;Unauthorized&quot;,  &quot;code&quot;: 401}</code></pre><p>只要请求有返回即说明启动成功。</p><h3 id="keepalived-haproxy安装配置"><a href="#keepalived-haproxy安装配置" class="headerlink" title="keepalived+haproxy安装配置"></a>keepalived+haproxy安装配置</h3><p>利用keepalived实现虚拟ip，利用haproxy提供针对apiserver中6443端口的代理。通过虚拟ip加端口号的方式，实现统一访问kube-apiserver服务。</p><p>如果不是高可用集群，无需安装haproxy或者keepalived，如果在云上安装，尽可能直接使用云上的loadbalancer，例如阿里云slb（可能会存在问题，无法代理自己的机器流量）或者腾讯云的elb等。</p><h4 id="1-工具安装与配置"><a href="#1-工具安装与配置" class="headerlink" title="1. 工具安装与配置"></a>1. 工具安装与配置</h4><p>在所有master节点安装keepalived和haproxy：</p><pre><code># yum install keepalived haproxy -y</code></pre><p>在所有master节点编辑haproxy的配置文件，所有节点配置相同，如下：</p><pre><code># cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak# vim /etc/haproxy/haproxy.cfgglobal    maxconn  2000    ulimit-n  16384    log  127.0.0.1 local0 err    stats timeout 30s#---------------------------------------------------------------------defaults    mode                    http    log                     global    option                  httplog    timeout http-request    15s    timeout connect         5000    timeout client          50000    timeout server          50000    timeout http-keep-alive 15s#---------------------------------------------------------------------# main frontend which proxys to the backends#---------------------------------------------------------------------frontend monitor-in    bind *:33305    mode http    option httplog    monitor-uri /monitorlisten stats    bind    *:8006    mode    http    stats   enable    stats   hide-version    stats   uri       /stats    stats   refresh   30s    stats   realm     Haproxy\ Statistics    stats   auth      admin:adminfrontend k8s-master    bind 0.0.0.0:16443    bind 127.0.0.1:16443    mode tcp    option tcplog    tcp-request inspect-delay 5s    default_backend k8s-master#---------------------------------------------------------------------# round robin balancing between the various backends#---------------------------------------------------------------------backend k8s-master    mode tcp    option tcplog    option tcp-check    balance roundrobin    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100    server  k8s-master-01 192.168.123.61:6443 check    server  k8s-master-02 192.168.123.62:6443 check    server  k8s-master-03 192.168.123.63:6443 check</code></pre><p>编辑完成后可以使用以下命令检查配置文件合法性：</p><pre><code># haproxy -c -f /etc/haproxy/haproxy.cfg</code></pre><p>配置keepalived的内容，<em>所有Master几点的配置不同，注意区分。</em></p><p>在k8s-master-01上配置以下内容</p><pre><code># vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {   router_id LVS_DEVEL}vrrp_script chk_apiserver {   script &quot;/etc/keepalived/check_apiserver.sh&quot;   interval 5   weight 5   fall 2   rise 1}vrrp_instance VI_1 {    state MASTER    interface ens192    virtual_router_id 51    mcat_src_ip 192.168.123.61    priority 101    nopreempt    advert_int 2    authentication {        auth_type PASS        auth_pass K8SHA_KA_AUTH    }    virtual_ipaddress {        192.168.123.60    }    track_script {       chk_apiserver    }}</code></pre><p>在k8s-master-02上配置以下内容</p><pre><code># vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {   router_id LVS_DEVEL}vrrp_script chk_apiserver {   script &quot;/etc/keepalived/check_apiserver.sh&quot;   interval 5   weight 5   fall 2   rise 1}vrrp_instance VI_1 {    state MASTER    interface ens192    virtual_router_id 51    mcat_src_ip 192.168.123.62    priority 100    nopreempt    advert_int 2    authentication {        auth_type PASS        auth_pass K8SHA_KA_AUTH    }    virtual_ipaddress {        192.168.123.60    }    track_script {       chk_apiserver    }}</code></pre><p>在k8s-master-03上配置以下内容</p><pre><code># vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {   router_id LVS_DEVEL}vrrp_script chk_apiserver {   script &quot;/etc/keepalived/check_apiserver.sh&quot;   interval 5   weight 5   fall 2   rise 1}vrrp_instance VI_1 {    state BACKUP    interface ens192    virtual_router_id 51    mcat_src_ip 192.168.123.63    priority 100    nopreempt    advert_int 2    authentication {        auth_type PASS        auth_pass K8SHA_KA_AUTH    }    virtual_ipaddress {        192.168.123.60    }    track_script {       chk_apiserver    }}</code></pre><p>然后在每一个master节点添加检查脚本，编辑check_apiserver.sh，如下：</p><pre><code># vim /etc/keepalived/check_apiserver.sh#!/bin/basherr=0for k in $(seq 1 5)do    check_code=$(pgrep kube-apiserver)    if [[ $check_code == &quot;&quot; ]]; then        err=$(expr $err + 1)        sleep 5        continue    else        err=0        break    fidoneif [[ $err != &quot;0&quot; ]]; then    echo &quot;systemctl stop keepalived&quot;    /usr/bin/systemctl stop keepalived    exit 1else    exit 0fi</code></pre><p>执行该脚本时，检查apiserver是否运行，如果apiserver进程不存在，在25秒后停用keepalived，服务不可用，进行报警操作。</p><p>编写完成后，变更文件可执行权限，如下：</p><pre><code># chmod +x /etc/keepalived/check_apiserver.sh</code></pre><p>在所有master节点启动haproxy和keepalived，如下：</p><pre><code># systemctl daemon-reload# systemctl enable --now haproxy# systemctl enable --now keepalived</code></pre><p>启动完成之后，查看vip信息，然后ping一下该vip，自此keepalived和haproxy就安装完成了。</p><pre><code># ip a | grep 60    inet 192.168.123.60/32 scope global ens192# ping 192.168.123.60PING 192.168.123.60 (192.168.123.60) 56(84) bytes of data.64 bytes from 192.168.123.60: icmp_seq=1 ttl=64 time=0.244 ms64 bytes from 192.168.123.60: icmp_seq=2 ttl=64 time=0.124 ms64 bytes from 192.168.123.60: icmp_seq=3 ttl=64 time=0.115 ms64 bytes from 192.168.123.60: icmp_seq=4 ttl=64 time=0.091 ms# telnet 192.168.123.60 16443Trying 192.168.123.60...Connected to 192.168.123.60.Escape character is &#39;^]&#39;.Connection closed by foreign host.// 访问查看是否生效#  curl --insecure https://192.168.123.60:16443/{  &quot;kind&quot;: &quot;Status&quot;,  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;metadata&quot;: {  },  &quot;status&quot;: &quot;Failure&quot;,  &quot;message&quot;: &quot;Unauthorized&quot;,  &quot;reason&quot;: &quot;Unauthorized&quot;,  &quot;code&quot;: 401}</code></pre><p><strong>注意：</strong>该VIP信息有可能只在某台机器上出现，如果从其他机器上能ping通，不需要在意这个问题。</p><h3 id="kubectl设置"><a href="#kubectl设置" class="headerlink" title="kubectl设置"></a>kubectl设置</h3><h4 id="1-创建csr请求文件并生成证书"><a href="#1-创建csr请求文件并生成证书" class="headerlink" title="1. 创建csr请求文件并生成证书"></a>1. 创建csr请求文件并生成证书</h4><pre><code># vim admin-csr.json{  &quot;CN&quot;: &quot;admin&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: {    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  },  &quot;names&quot;: [    {      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Shandong&quot;,      &quot;L&quot;: &quot;Jinan&quot;,      &quot;O&quot;: &quot;system:masters&quot;,      &quot;OU&quot;: &quot;system&quot;    }  ]}</code></pre><p>说明：</p><ul><li>后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；</li><li>kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；</li><li>O指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；</li><li>注意后续证书的设置，所有的<em>names</em>下的设置信息各不相同。</li></ul><p>注意：这个admin 证书，是将来生成管理员用的kube config 配置文件用的，现在我们一般建议使用RBAC 来对kubernetes 进行角色权限控制， kubernetes 将证书中的CN 字段 作为User， O 字段作为 Group；<br>“O”: “system:masters”, 必须是system:masters，否则后面kubectl create clusterrolebinding报错。</p><p>生成证书操作如下：</p><pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin# ls admin*.pemadmin-key.pem  admin.pem// 将证书拷贝到指定路径下# cp admin*.pem /etc/kubernetes/ssl/</code></pre><h4 id="2-创建kubeconfig配置文件"><a href="#2-创建kubeconfig配置文件" class="headerlink" title="2. 创建kubeconfig配置文件"></a>2. 创建kubeconfig配置文件</h4><p>kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书。创建过程如下：</p><pre><code>// 设置集群参数，--server为虚拟ip地址# kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.123.60:16443 --kubeconfig=kube.config// 设置客户端认证参数# kubectl config set-credentials admin --client-certificate=admin.pem --client-key=admin-key.pem --embed-certs=true --kubeconfig=kube.config// 设置上下文参数# kubectl config set-context kubernetes --cluster=kubernetes --user=admin --kubeconfig=kube.config// 设置默认上下文# kubectl config use-context kubernetes --kubeconfig=kube.config// 创建文件夹并拷贝文件# mkdir ~/.kube# cp kube.config ~/.kube/config// 授权kubernetes证书访问kubelet api权限# kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes</code></pre><p>设置完成后，就可以与kube-apiserver通信了。这里要注意，我们只把kubectl工具放到k8s-master-01节点上，通过该节点进行k8s集群的命令行操作。在其它节点上严禁使用kubectl工具。更稳妥的办法是将~/.kube/config文件拷贝到其它机器上，运行查看信息。</p><p>上述步骤完成后，kubectl就可以与kube-apiserver通信了。操作如下：</p><pre><code># kubectl cluster-infoKubernetes control plane is running at https://192.168.123.60:16443To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.# kubectl get componentstatusesWarning: v1 ComponentStatus is deprecated in v1.19+NAME                 STATUS      MESSAGE                                                                                       ERRORscheduler            Unhealthy   Get &quot;http://127.0.0.1:10251/healthz&quot;: dial tcp 127.0.0.1:10251: connect: connection refusedcontroller-manager   Unhealthy   Get &quot;http://127.0.0.1:10252/healthz&quot;: dial tcp 127.0.0.1:10252: connect: connection refusedetcd-2               Healthy     {&quot;health&quot;:&quot;true&quot;}                                                                      etcd-0               Healthy     {&quot;health&quot;:&quot;true&quot;}                                                                      etcd-1               Healthy     {&quot;health&quot;:&quot;true&quot;}                                                                      # kubectl get all --all-namespacesNAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGEdefault     service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   20m</code></pre><p>（可选项）配置kubectl子命令补全</p><pre><code># yum install -y bash-completion# source /usr/share/bash-completion/bash_completion# source &lt;(kubectl completion bash)# kubectl completion bash &gt; ~/.kube/completion.bash.inc# source &#39;/root/.kube/completion.bash.inc&#39;  # source $HOME/.bash_profile</code></pre><p><strong>注意：</strong>配置自动补全，在使用的时候会有明显的卡顿的情况，使用的时候要注意。</p><h3 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h3><h4 id="1-创建csr请求文件以及生成证书"><a href="#1-创建csr请求文件以及生成证书" class="headerlink" title="1. 创建csr请求文件以及生成证书"></a>1. 创建csr请求文件以及生成证书</h4><p>在k8s-master-01节点上，创建csr请求文件如下：</p><pre><code># vim kube-controller-manager-csr.json{    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,    &quot;key&quot;: {        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    },    &quot;hosts&quot;: [      &quot;127.0.0.1&quot;,      &quot;192.168.123.61&quot;,      &quot;192.168.123.62&quot;,      &quot;192.168.123.63&quot;    ],    &quot;names&quot;: [      {        &quot;C&quot;: &quot;CN&quot;,        &quot;ST&quot;: &quot;Shandong&quot;,        &quot;L&quot;: &quot;Jinan&quot;,        &quot;O&quot;: &quot;system:kube-controller-manager&quot;,        &quot;OU&quot;: &quot;system&quot;      }    ]}</code></pre><p>注：</p><ul><li>hosts 列表包含所有 kube-controller-manager 节点 IP；</li><li>CN 为 system:kube-controller-manager、O 为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。</li></ul><p>生成证书操作如下：</p><pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager# ls kube-controller-manager*.pemkube-controller-manager-key.pem  kube-controller-manager.pem</code></pre><h4 id="2-创建kube-controller-manager的kubeconfig"><a href="#2-创建kube-controller-manager的kubeconfig" class="headerlink" title="2.创建kube-controller-manager的kubeconfig"></a>2.创建kube-controller-manager的kubeconfig</h4><p>在k8s-master-01节点上，创建kubeconfig文件</p><pre><code>// 设置集群参数# kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.123.60:16443 --kubeconfig=kube-controller-manager.kubeconfig// 设置客户端认证参数# kubectl config set-credentials system:kube-controller-manager --client-certificate=kube-controller-manager.pem --client-key=kube-controller-manager-key.pem --embed-certs=true --kubeconfig=kube-controller-manager.kubeconfig// 设置上下文参数# kubectl config set-context system:kube-controller-manager --cluster=kubernetes --user=system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig// 设置默认上下文# kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig</code></pre><h4 id="3-创建kube-controller-manager的配置文件和启动文件"><a href="#3-创建kube-controller-manager的配置文件和启动文件" class="headerlink" title="3. 创建kube-controller-manager的配置文件和启动文件"></a>3. 创建kube-controller-manager的配置文件和启动文件</h4><p>在k8s-master-01节点上，创建kube-controller-manager的配置文件如下：</p><pre><code># vim kube-controller-manager.confKUBE_CONTROLLER_MANAGER_OPTS=&quot;--port=0 \  --secure-port=10252 \  --bind-address=127.0.0.1 \  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \  --service-cluster-ip-range=10.96.0.0/16 \  --cluster-name=kubernetes \  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \  --allocate-node-cidrs=true \  --cluster-cidr=172.16.0.0/16 \  --experimental-cluster-signing-duration=87600h \  --root-ca-file=/etc/kubernetes/ssl/ca.pem \  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \  --leader-elect=true \  --feature-gates=RotateKubeletServerCertificate=true \  --controllers=*,bootstrapsigner,tokencleaner \  --horizontal-pod-autoscaler-use-rest-clients=true \  --horizontal-pod-autoscaler-sync-period=10s \  --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \  --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \  --use-service-account-credentials=true \  --alsologtostderr=true \  --logtostderr=false \  --log-dir=/var/log/kubernetes \  --v=2&quot;</code></pre><p>创建启动文件如下：</p><pre><code># vim kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/kube-controller-manager.confExecStart=/usr/local/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.target</code></pre><h4 id="3-同步相关文件到各个节点"><a href="#3-同步相关文件到各个节点" class="headerlink" title="3. 同步相关文件到各个节点"></a>3. 同步相关文件到各个节点</h4><p>从k8s-master-01创建的配置文件，不需要修改，直接传输到k8s-master-02、k8s-master-03节点上。</p><pre><code>// copy到k8s-master-01节点路径# cp kube-controller-manager*.pem /etc/kubernetes/ssl/# cp kube-controller-manager.kubeconfig /etc/kubernetes/# cp kube-controller-manager.conf /etc/kubernetes/# cp kube-controller-manager.service /usr/lib/systemd/system/// 同步到其它节点# for i in k8s-master-02 k8s-master-03; do rsync -vaz kube-controller-manager*.pem $i:/etc/kubernetes/ssl/; rsync -vaz kube-controller-manager.kubeconfig kube-controller-manager.conf $i:/etc/kubernetes/; rsync -vaz kube-controller-manager.service $i:/usr/lib/systemd/system/;done</code></pre><h4 id="4-所有节点启动服务"><a href="#4-所有节点启动服务" class="headerlink" title="4. 所有节点启动服务"></a>4. 所有节点启动服务</h4><p>在所有kube-controller-manager部署节点启动服务：</p><pre><code># systemctl daemon-reload # systemctl enable kube-controller-manager# systemctl start kube-controller-manager# systemctl status kube-controller-manager</code></pre><h3 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h3><p>在k8s-master-01节点创建kube-scheduler相关内容，再统一复制到其它节点。</p><h4 id="1-创建csr请求文件并生成证书-1"><a href="#1-创建csr请求文件并生成证书-1" class="headerlink" title="1. 创建csr请求文件并生成证书"></a>1. 创建csr请求文件并生成证书</h4><p>创建csr请求文件：</p><pre><code># vim kube-scheduler-csr.json{    &quot;CN&quot;: &quot;system:kube-scheduler&quot;,    &quot;hosts&quot;: [      &quot;127.0.0.1&quot;,      &quot;192.168.123.61&quot;,      &quot;192.168.123.62&quot;,      &quot;192.168.123.63&quot;    ],    &quot;key&quot;: {        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    },    &quot;names&quot;: [      {        &quot;C&quot;: &quot;CN&quot;,        &quot;ST&quot;: &quot;Shandong&quot;,        &quot;L&quot;: &quot;Jinan&quot;,        &quot;O&quot;: &quot;system:kube-scheduler&quot;,        &quot;OU&quot;: &quot;system&quot;      }    ]}</code></pre><p><em>注意：</em></p><ul><li>hosts 列表包含所有 kube-scheduler 节点 IP；</li><li>CN 为 system:kube-scheduler、O 为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限。</li></ul><p>生成证书信息：</p><pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler# ls kube-scheduler*.pemkube-scheduler-key.pem  kube-scheduler.pem</code></pre><h4 id="2-创建kube-scheduler的kubeconfig"><a href="#2-创建kube-scheduler的kubeconfig" class="headerlink" title="2. 创建kube-scheduler的kubeconfig"></a>2. 创建kube-scheduler的kubeconfig</h4><pre><code>// 设置集群参数# kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.123.60:16443 --kubeconfig=kube-scheduler.kubeconfig// 设置客户端认证参数# kubectl config set-credentials system:kube-scheduler --client-certificate=kube-scheduler.pem --client-key=kube-scheduler-key.pem --embed-certs=true --kubeconfig=kube-scheduler.kubeconfig// 设置上下文参数# kubectl config set-context system:kube-scheduler --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig// 设置默认上下文# kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig</code></pre><h4 id="3-创建配置文件和服务启动文件并同步到各个master节点"><a href="#3-创建配置文件和服务启动文件并同步到各个master节点" class="headerlink" title="3. 创建配置文件和服务启动文件并同步到各个master节点"></a>3. 创建配置文件和服务启动文件并同步到各个master节点</h4><p>创建配置文件：</p><pre><code># vim kube-scheduler.confKUBE_SCHEDULER_OPTS=&quot;--address=127.0.0.1 \--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \--leader-elect=true \--alsologtostderr=true \--logtostderr=false \--log-dir=/var/log/kubernetes \--v=2&quot;</code></pre><p>创建服务启动文件：</p><pre><code># vim kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/kube-scheduler.confExecStart=/usr/local/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.target</code></pre><p>同步相关文件到各个节点上：</p><pre><code>// copy到k8s-master-01节点路径# cp kube-scheduler*.pem /etc/kubernetes/ssl/# cp kube-scheduler.kubeconfig /etc/kubernetes/# cp kube-scheduler.conf /etc/kubernetes/# cp kube-scheduler.service /usr/lib/systemd/system/// 同步到其它节点# for i in k8s-master-02 k8s-master-03; do rsync -vaz kube-scheduler*.pem $i:/etc/kubernetes/ssl/; rsync -vaz kube-scheduler.kubeconfig kube-scheduler.conf $i:/etc/kubernetes/; rsync -vaz kube-scheduler.service $i:/usr/lib/systemd/system/;done</code></pre><h4 id="4-启动服务"><a href="#4-启动服务" class="headerlink" title="4. 启动服务"></a>4. 启动服务</h4><p>在所有master节点上同时执行以下命令，启动kube-scheduler服务：</p><pre><code># systemctl daemon-reload# systemctl enable kube-scheduler# systemctl start kube-scheduler# systemctl status kube-scheduler</code></pre><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><p>在安装之前，需要在所有节点上执行拉取镜像的操作，如下：</p><pre><code># docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0</code></pre><p>在拉取完成后，记得在后续的配置文件中更改镜像的名称。由于网络的问题，导致我们使用国内镜像源进行拉取。如果配置文件中不改名，可以在拉取镜像后进行打tag操作，如下：</p><pre><code># docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 k8s.gcr.io/pause:3.2# docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0 k8s.gcr.io/coredns:1.7.0# docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0</code></pre><p>在这里我们使用国内镜像源的容器进行运行。</p><h4 id="1-创建kubelet-bootstrap-kubeconfig"><a href="#1-创建kubelet-bootstrap-kubeconfig" class="headerlink" title="1. 创建kubelet-bootstrap.kubeconfig"></a>1. 创建kubelet-bootstrap.kubeconfig</h4><p>在k8s-master-01节点上执行，创建配置文件，如下：</p><pre><code>// 获取token信息# BOOTSTRAP_TOKEN=$(awk -F &quot;,&quot; &#39;{print $1}&#39; /etc/kubernetes/token.csv)// 设置集群参数# kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.123.60:16443 --kubeconfig=kubelet-bootstrap.kubeconfig// 设置客户端认证参数# kubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=kubelet-bootstrap.kubeconfig// 设置上下文参数# kubectl config set-context default --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=kubelet-bootstrap.kubeconfig// 设置默认上下文# kubectl config use-context default --kubeconfig=kubelet-bootstrap.kubeconfig// 创建角色绑定# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap</code></pre><h4 id="2-创建配置文件以及启动文件"><a href="#2-创建配置文件以及启动文件" class="headerlink" title="2. 创建配置文件以及启动文件"></a>2. 创建配置文件以及启动文件</h4><p>创建配置文件：</p><pre><code># vim /etc/kubernetes/kubelet.json{  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;,  &quot;authentication&quot;: {    &quot;x509&quot;: {      &quot;clientCAFile&quot;: &quot;/etc/kubernetes/ssl/ca.pem&quot;    },    &quot;webhook&quot;: {      &quot;enabled&quot;: true,      &quot;cacheTTL&quot;: &quot;2m0s&quot;    },    &quot;anonymous&quot;: {      &quot;enabled&quot;: false    }  },  &quot;authorization&quot;: {    &quot;mode&quot;: &quot;Webhook&quot;,    &quot;webhook&quot;: {      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;    }  },  &quot;address&quot;: &quot;192.168.123.61&quot;,  &quot;port&quot;: 10250,  &quot;readOnlyPort&quot;: 10255,  &quot;cgroupDriver&quot;: &quot;systemd&quot;,  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,  &quot;serializeImagePulls&quot;: false,  &quot;featureGates&quot;: {    &quot;RotateKubeletClientCertificate&quot;: true,    &quot;RotateKubeletServerCertificate&quot;: true  },  &quot;clusterDomain&quot;: &quot;cluster.local.&quot;,  &quot;clusterDNS&quot;: [&quot;10.96.0.2&quot;],  &quot;resolvConf&quot;: &quot;/etc/resolv.conf&quot;}</code></pre><p>注意：在不同节点上，需要修改<em>address</em>的值，更换为该节点的真实ip地址。</p><p>创建启动文件：</p><pre><code># vim kubelet.service[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/kubernetes/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/usr/local/bin/kubelet \  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \  --cert-dir=/etc/kubernetes/ssl \  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \  --config=/etc/kubernetes/kubelet.json \  --network-plugin=cni \  --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 \  --alsologtostderr=true \  --logtostderr=false \  --log-dir=/var/log/kubernetes \  --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.target</code></pre><p>注：</p><ul><li>–hostname-override：显示名称，集群中唯一</li><li>–network-plugin：启用CNI</li><li>–kubeconfig：空路径，会自动生成，后面用于连接apiserver</li><li>–bootstrap-kubeconfig：首次启动向apiserver申请证书</li><li>–config：配置参数文件</li><li>–cert-dir：kubelet证书生成目录</li><li>–pod-infra-container-image：管理Pod网络容器的镜像</li></ul><p>注意：在该启动文件中，<em>pod-infra-container-image</em>容器地址已经变更为阿里云镜像源，不是gcr.io，也就是更换为在kubelet安装步骤开始前，拉取的容器镜像。</p><h4 id="3-将各个配置信息传输到各个节点上"><a href="#3-将各个配置信息传输到各个节点上" class="headerlink" title="3. 将各个配置信息传输到各个节点上"></a>3. 将各个配置信息传输到各个节点上</h4><pre><code>// k8s-master-01上复制文件信息# cp kubelet-bootstrap.kubeconfig /etc/kubernetes/# cp kubelet.json /etc/kubernetes/# cp kubelet.service /usr/lib/systemd/system/// 以上步骤，如果master节点不安装kubelet，则剔除k8s-master-02、k8s-master-03# for i in k8s-master-02 k8s-master-03 k8s-worker-01 k8s-worker-02 k8s-worker-03;do rsync -vaz kubelet-bootstrap.kubeconfig kubelet.json $i:/etc/kubernetes/;done# for i in k8s-master-02 k8s-master-03 k8s-worker-01 k8s-worker-02 k8s-worker-03;do rsync -vaz ca.pem $i:/etc/kubernetes/ssl/;done# for i in k8s-master-02 k8s-master-03 k8s-worker-01 k8s-worker-02 k8s-worker-03;do rsync -vaz kubelet.service $i:/usr/lib/systemd/system/;done</code></pre><p><strong>注意：</strong>传输完成后，kubelete.json配置文件中的<strong>address</strong>改为各个节点的ip地址。</p><h4 id="4-启动服务-1"><a href="#4-启动服务-1" class="headerlink" title="4. 启动服务"></a>4. 启动服务</h4><p>在所有节点上启动kubelet服务，如下：</p><pre><code># mkdir /var/lib/kubelet# mkdir /var/log/kubernetes# systemctl daemon-reload# systemctl enable kubelet# systemctl start kubelet# systemctl status kubelet</code></pre><p>确认kubelet服务启动成功后，接着到k8s-master-01节点上Approve一下bootstrap请求。执行如下命令可以看到三个worker节点分别发送了三个 CSR 请求：</p><pre><code># kubectl get csrNAME                                                   AGE     SIGNERNAME                                    REQUESTOR           CONDITIONnode-csr-5SulW-Pto5JW8k2nAvKA2dZ10XDmq1Z9UKHP0CVZnGU   60s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pendingnode-csr-L7awluOFTHXec6YgmtcAohKdfdXUf4d-M4IRN2J__ko   60s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pendingnode-csr-OsjPbL0Ga9lfcop6Cz8E8CI7-OWSolxC7JKk_PdUsaI   60s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pendingnode-csr-PcUb3KmMkTuj--mSXvLIvTp5b6WHjBgqkSewQSSk8Uw   59s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pendingnode-csr-hUy9THVqXtVonH6-2FnqQEOSgyrLKXFaITCJM2EpyMs   60s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pendingnode-csr-hVSmkR5zdpCxcPVGJVAmHMTB5MXoMOzN-xZb8QT_v_c   3m49s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending# kubectl certificate approve node-csr-5SulW-Pto5JW8k2nAvKA2dZ10XDmq1Z9UKHP0CVZnGUcertificatesigningrequest.certificates.k8s.io/node-csr-5SulW-Pto5JW8k2nAvKA2dZ10XDmq1Z9UKHP0CVZnGU approved# kubectl certificate approve node-csr-L7awluOFTHXec6YgmtcAohKdfdXUf4d-M4IRN2J__kocertificatesigningrequest.certificates.k8s.io/node-csr-L7awluOFTHXec6YgmtcAohKdfdXUf4d-M4IRN2J__ko approvedYou have new mail in /var/spool/mail/root# kubectl certificate approve node-csr-OsjPbL0Ga9lfcop6Cz8E8CI7-OWSolxC7JKk_PdUsaIcertificatesigningrequest.certificates.k8s.io/node-csr-OsjPbL0Ga9lfcop6Cz8E8CI7-OWSolxC7JKk_PdUsaI approved# kubectl certificate approve node-csr-PcUb3KmMkTuj--mSXvLIvTp5b6WHjBgqkSewQSSk8Uwcertificatesigningrequest.certificates.k8s.io/node-csr-PcUb3KmMkTuj--mSXvLIvTp5b6WHjBgqkSewQSSk8Uw approved# kubectl certificate approve node-csr-hUy9THVqXtVonH6-2FnqQEOSgyrLKXFaITCJM2EpyMscertificatesigningrequest.certificates.k8s.io/node-csr-hUy9THVqXtVonH6-2FnqQEOSgyrLKXFaITCJM2EpyMs approved# kubectl certificate approve node-csr-hVSmkR5zdpCxcPVGJVAmHMTB5MXoMOzN-xZb8QT_v_ccertificatesigningrequest.certificates.k8s.io/node-csr-hVSmkR5zdpCxcPVGJVAmHMTB5MXoMOzN-xZb8QT_v_c approved# kubectl get csrNAME                                                   AGE     SIGNERNAME                                    REQUESTOR           CONDITIONnode-csr-5SulW-Pto5JW8k2nAvKA2dZ10XDmq1Z9UKHP0CVZnGU   3m13s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issuednode-csr-L7awluOFTHXec6YgmtcAohKdfdXUf4d-M4IRN2J__ko   3m13s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issuednode-csr-OsjPbL0Ga9lfcop6Cz8E8CI7-OWSolxC7JKk_PdUsaI   3m13s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issuednode-csr-PcUb3KmMkTuj--mSXvLIvTp5b6WHjBgqkSewQSSk8Uw   3m12s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issuednode-csr-hUy9THVqXtVonH6-2FnqQEOSgyrLKXFaITCJM2EpyMs   3m13s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issuednode-csr-hVSmkR5zdpCxcPVGJVAmHMTB5MXoMOzN-xZb8QT_v_c   6m2s    kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issued# kubectl get nodesNAME            STATUS     ROLES    AGE   VERSIONk8s-master-01   NotReady   &lt;none&gt;   12s   v1.20.5k8s-master-02   NotReady   &lt;none&gt;   45s   v1.20.5k8s-master-03   NotReady   &lt;none&gt;   60s   v1.20.5k8s-worker-01   NotReady   &lt;none&gt;   93s   v1.20.5k8s-worker-02   NotReady   &lt;none&gt;   29s   v1.20.5k8s-worker-03   NotReady   &lt;none&gt;   74s   v1.20.5</code></pre><h3 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h3><h4 id="1-创建配置文件并生成证书"><a href="#1-创建配置文件并生成证书" class="headerlink" title="1. 创建配置文件并生成证书"></a>1. 创建配置文件并生成证书</h4><p>在k8s-master-01节点上执行。</p><p>创建csr请求文件：</p><pre><code># vim kube-proxy-csr.json{  &quot;CN&quot;: &quot;system:kube-proxy&quot;,  &quot;key&quot;: {    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  },  &quot;names&quot;: [    {      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Shandong&quot;,      &quot;L&quot;: &quot;Jinan&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;system&quot;    }  ]}</code></pre><p>生成证书操作如下：</p><pre><code># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy# ls kube-proxy*.pemkube-proxy-key.pem  kube-proxy.pem</code></pre><h4 id="2-创建kubeconfig文件"><a href="#2-创建kubeconfig文件" class="headerlink" title="2. 创建kubeconfig文件"></a>2. 创建kubeconfig文件</h4><p>在k8s-master-01节点上执行。</p><pre><code># kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.123.60:16443 --kubeconfig=kube-proxy.kubeconfig# kubectl config set-credentials kube-proxy --client-certificate=kube-proxy.pem --client-key=kube-proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig# kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</code></pre><h4 id="3-创建配置文件以及服务启动文件"><a href="#3-创建配置文件以及服务启动文件" class="headerlink" title="3. 创建配置文件以及服务启动文件"></a>3. 创建配置文件以及服务启动文件</h4><p>在k8s-master-01节点上创建配置文件：</p><pre><code># vim /etc/kubernetes/kube-proxy.yamlapiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 192.168.123.61clientConnection:  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfigclusterCIDR: 172.16.0.0/16healthzBindAddress: 192.168.123.61:10256kind: KubeProxyConfigurationmetricsBindAddress: 192.168.123.61:10249mode: &quot;ipvs&quot;</code></pre><p><strong>注意：</strong>在复制到各个节点上时，除了<em>clusterCIDR</em>不需要变更之外，其它的ip地址均需要变更为节点的ip地址。</p><p>创建服务启动文件如下：</p><pre><code># vim /usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/usr/local/bin/kube-proxy \  --config=/etc/kubernetes/kube-proxy.yaml \  --alsologtostderr=true \  --logtostderr=false \  --log-dir=/var/log/kubernetes \  --v=2Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h4 id="4-将各个配置信息传输到各个节点上"><a href="#4-将各个配置信息传输到各个节点上" class="headerlink" title="4. 将各个配置信息传输到各个节点上"></a>4. 将各个配置信息传输到各个节点上</h4><pre><code>// k8s-master-01上复制文件信息# cp kube-proxy*.pem /etc/kubernetes/ssl/# cp kube-proxy.kubeconfig kube-proxy.yaml /etc/kubernetes/# cp kube-proxy.service /usr/lib/systemd/system/// 以上步骤，如果master节点不安装kubelet，则剔除k8s-master-02、k8s-master-03# for i in k8s-master-02 k8s-master-03 k8s-worker-01 k8s-worker-02 k8s-worker-03;do rsync -vaz kube-proxy.kubeconfig kube-proxy.yaml $i:/etc/kubernetes/;done# for i in k8s-master-02 k8s-master-03 k8s-worker-01 k8s-worker-02 k8s-worker-03;do rsync -vaz kube-proxy.service $i:/usr/lib/systemd/system/;done</code></pre><p><strong>注意：</strong>传输完成后，kube-proxy.yaml配置文件中的<strong>bindAddress</strong>、<strong>healthzBindAddress</strong>、<strong>metricsBindAddress</strong>改为各个节点的ip地址，端口号不变。</p><h4 id="5-启动服务"><a href="#5-启动服务" class="headerlink" title="5. 启动服务"></a>5. 启动服务</h4><p>在所有节点上执行以下命令，启动kube-proxy。如下：</p><pre><code># mkdir -p /var/lib/kube-proxy# systemctl daemon-reload# systemctl enable kube-proxy# systemctl restart kube-proxy# systemctl status kube-proxy</code></pre><h3 id="网络组件calico以及CoreDNS部署"><a href="#网络组件calico以及CoreDNS部署" class="headerlink" title="网络组件calico以及CoreDNS部署"></a>网络组件calico以及CoreDNS部署</h3><h4 id="1-calico网络组件安装"><a href="#1-calico网络组件安装" class="headerlink" title="1. calico网络组件安装"></a>1. calico网络组件安装</h4><p>首先下载calico的部署文件，修改如下：</p><pre><code># wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml# vim calico.yaml......---# Source: calico/templates/calico-node.yaml# This manifest installs the calico-node container, as well# as the CNI plugins and network config on# each master and worker node in a Kubernetes cluster.kind: DaemonSetapiVersion: apps/v1metadata:  name: calico-node  namespace: kube-system  labels:    k8s-app: calico-nodespec:  selector:    matchLabels:      k8s-app: calico-node  updateStrategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 1  template:    spec:      ......      containers:        - name: calico-node          image: calico/node:v3.14.2          env:            ......            # The default IPv4 pool to create on startup if none exists. Pod IPs will be            # chosen from this range. Changing this value after installation will have            # no effect. This should fall within `--cluster-cidr`.            # 在这个位置添加pod网段的信息            - name: CALICO_IPV4POOL_CIDR              value: &quot;172.16.0.0/16&quot;            # Disable file logging so `kubectl logs` works.            - name: CALICO_DISABLE_FILE_LOGGING              value: &quot;true&quot;</code></pre><p>修改完成后，执行安装操作如下：</p><pre><code># kubectl apply -f calico.yaml</code></pre><p>查看安装后的效果</p><pre><code># kubectl get nodesNAME            STATUS   ROLES    AGE   VERSIONk8s-master-01   Ready    &lt;none&gt;   75m   v1.20.5k8s-master-02   Ready    &lt;none&gt;   76m   v1.20.5k8s-master-03   Ready    &lt;none&gt;   76m   v1.20.5k8s-worker-01   Ready    &lt;none&gt;   76m   v1.20.5k8s-worker-02   Ready    &lt;none&gt;   75m   v1.20.5k8s-worker-03   Ready    &lt;none&gt;   76m   v1.20.5# kubectl get pods -ANAMESPACE     NAME                                       READY   STATUS              RESTARTS   AGEkube-system   calico-kube-controllers-6dfcd885bf-dgjtl   1/1     Running             0          10mkube-system   calico-node-crrsq                          1/1     Running             0          10mkube-system   calico-node-rs96n                          1/1     Running             0          10mkube-system   calico-node-rtf8b                          1/1     Running             0          10mkube-system   calico-node-sjd6j                          1/1     Running             0          10mkube-system   calico-node-x4qfn                          1/1     Running             0          10mkube-system   calico-node-zhnv4                          1/1     Running             0          10m</code></pre><p>查看各个节点，均为Ready的状态。</p><h4 id="2-CoreDNS组件安装"><a href="#2-CoreDNS组件安装" class="headerlink" title="2. CoreDNS组件安装"></a>2. CoreDNS组件安装</h4><p>下载CoreDNS配置文件如下：</p><pre><code># wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed# mv coredns.yaml.sed coredns.yaml# vim coredns.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: coredns  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsrules:- apiGroups:  - &quot;&quot;  resources:  - endpoints  - services  - pods  - namespaces  verbs:  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  annotations:    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:corednssubjects:- kind: ServiceAccount  name: coredns  namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata:  name: coredns  namespace: kube-systemdata:  Corefile: |    .:53 {        errors        health {          lameduck 5s        }        ready        # 1. kubernetes 修改这个位置的配置        kubernetes cluster.local  in-addr.arpa ip6.arpa {          fallthrough in-addr.arpa ip6.arpa        }        prometheus :9153        # 2. 设置forward . /etc/resolv.conf        forward . /etc/resolv.conf {          max_concurrent 1000        }        cache 30        loop        reload        loadbalance    }---apiVersion: apps/v1kind: Deploymentmetadata:  name: coredns  namespace: kube-system  labels:    k8s-app: kube-dns    kubernetes.io/name: &quot;CoreDNS&quot;spec:  # replicas: not specified here:  # 1. Default is 1.  # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on.  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 1  selector:    matchLabels:      k8s-app: kube-dns  template:    metadata:      labels:        k8s-app: kube-dns    spec:      priorityClassName: system-cluster-critical      serviceAccountName: coredns      tolerations:        - key: &quot;CriticalAddonsOnly&quot;          operator: &quot;Exists&quot;      nodeSelector:        kubernetes.io/os: linux      affinity:         podAntiAffinity:           preferredDuringSchedulingIgnoredDuringExecution:           - weight: 100             podAffinityTerm:               labelSelector:                 matchExpressions:                   - key: k8s-app                     operator: In                     values: [&quot;kube-dns&quot;]               topologyKey: kubernetes.io/hostname      containers:      - name: coredns        image: coredns/coredns:1.8.0        imagePullPolicy: IfNotPresent        resources:          limits:            memory: 170Mi          requests:            cpu: 100m            memory: 70Mi        args: [ &quot;-conf&quot;, &quot;/etc/coredns/Corefile&quot; ]        volumeMounts:        - name: config-volume          mountPath: /etc/coredns          readOnly: true        ports:        - containerPort: 53          name: dns          protocol: UDP        - containerPort: 53          name: dns-tcp          protocol: TCP        - containerPort: 9153          name: metrics          protocol: TCP        securityContext:          allowPrivilegeEscalation: false          capabilities:            add:            - NET_BIND_SERVICE            drop:            - all          readOnlyRootFilesystem: true        livenessProbe:          httpGet:            path: /health            port: 8080            scheme: HTTP          initialDelaySeconds: 60          timeoutSeconds: 5          successThreshold: 1          failureThreshold: 5        readinessProbe:          httpGet:            path: /ready            port: 8181            scheme: HTTP      dnsPolicy: Default      volumes:        - name: config-volume          configMap:            name: coredns            items:            - key: Corefile              path: Corefile---apiVersion: v1kind: Servicemetadata:  name: kube-dns  namespace: kube-system  annotations:    prometheus.io/port: &quot;9153&quot;    prometheus.io/scrape: &quot;true&quot;  labels:    k8s-app: kube-dns    kubernetes.io/cluster-service: &quot;true&quot;    kubernetes.io/name: &quot;CoreDNS&quot;spec:  selector:    k8s-app: kube-dns  # 3. clusterIP为：10.96.0.2（kubelet配置文件中的clusterDNS）  clusterIP: 10.96.0.2  ports:  - name: dns    port: 53    protocol: UDP  - name: dns-tcp    port: 53    protocol: TCP  - name: metrics    port: 9153    protocol: TCP</code></pre><p>部署coredns组件，如下：</p><pre><code># kubectl apply -f coredns.yaml </code></pre><p>查看部署效果如下：</p><pre><code># kubectl get pods -ANAMESPACE     NAME                                       READY   STATUS              RESTARTS   AGEkube-system   calico-kube-controllers-6dfcd885bf-dgjtl   1/1     Running             0          10mkube-system   calico-node-crrsq                          1/1     Running             0          10mkube-system   calico-node-rs96n                          1/1     Running             0          10mkube-system   calico-node-rtf8b                          1/1     Running             0          10mkube-system   calico-node-sjd6j                          1/1     Running             0          10mkube-system   calico-node-x4qfn                          1/1     Running             0          10mkube-system   calico-node-zhnv4                          1/1     Running             0          10mkube-system   coredns-6ccb5d565f-p8mbf                   1/1     Running             0          4h51m   </code></pre><p>到这个位置开始，整个k8s集群就安装完成了。</p><h3 id="nginx部署验证"><a href="#nginx部署验证" class="headerlink" title="nginx部署验证"></a>nginx部署验证</h3><p>nginx部署文件如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># vim nginx.yaml </span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>deployment<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>service<span class="token punctuation">-</span>nodeport<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">30001</span>      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>文件部署如下：</p><pre><code># kubectl apply -f nginx.yaml# kubectl get svcNAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGEnginx-service-nodeport   NodePort    10.96.161.177   &lt;none&gt;        80:30001/TCP   26h# kubectl get podNAME                                READY   STATUS    RESTARTS   AGEnginx-deployment-559d658b74-hx64t   1/1     Running   0          25h// 访问测试# curl 192.168.123.61:30001&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;    body {        width: 35em;        margin: 0 auto;        font-family: Tahoma, Verdana, Arial, sans-serif;    }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><p>这样验证也完成了，整个集群已经成功运行了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>一波三折，结局尚好。</p><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><ul><li><a href="https://www.cnblogs.com/technology178/p/14295776.html" target="_blank" rel="noopener">https://www.cnblogs.com/technology178/p/14295776.html</a></li><li><a href="https://www.cnblogs.com/dukuan/p/12104600.html" target="_blank" rel="noopener">https://www.cnblogs.com/dukuan/p/12104600.html</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s一阶段学习记录</title>
      <link href="/2021/03/19/k8s-yi-jie-duan-xue-xi-ji-lu/"/>
      <url>/2021/03/19/k8s-yi-jie-duan-xue-xi-ji-lu/</url>
      
        <content type="html"><![CDATA[<p>k8s学习</p><h2 id="第一章-kubeadm方式安装k8s集群"><a href="#第一章-kubeadm方式安装k8s集群" class="headerlink" title="第一章 kubeadm方式安装k8s集群"></a>第一章 kubeadm方式安装k8s集群</h2><ol><li>安装部分</li></ol><p>安装前注意：</p><ol><li>不要使用中文目录或者克隆过的虚拟机（网卡问题）</li><li>生产环境建议使用二进制安装</li><li>VIP不要和内网ip地址重复，需要和主机ip在同一个局域网段内</li></ol><p>1.1 容器化安装</p><p>第一部分：初始化</p><p>修改各个机器的名称</p><pre><code># hostnamectl set-hostname k8s-master-01</code></pre><p>在所有机器上同步执行</p><pre><code># vim /etc/hosts192.168.229.51 k8s-master-01192.168.229.52 k8s-master-02192.168.229.53 k8s-master-03192.168.229.60 k8s-master-lb192.168.229.54 k8s-node-01192.168.229.55 k8s-node-02</code></pre><p>修改yum源</p><pre><code>mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bakwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum install -y yum-utils \                    device-mapper-persistent-data \                    lvm2yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repocat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFsed -i -e &#39;/mirrors.cloud.aliyuncs.com/d&#39; -e &#39;/mirrors.aliyuncs.com/d&#39; /etc/yum.repos.d/CentOS-Base.repoyum update -y --exclude=kernel* </code></pre><p>安装必备工具</p><pre><code>yum install -y wget git unzip net-tools curl vim telnet psmisc  tree sshpass jq</code></pre><p>关闭防火墙，selinux、dnsmasq、NetworkManager</p><pre><code>systemctl disable --now firewalldsystemctl disable --now dnsmasqsystemctl disable --now NetworkManagersetenforce 0sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinuxsed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config</code></pre><p>注意：selinux关闭时必须两个同时修改</p><p>关闭swap分区<br>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0<br>sed -ri ‘/^[^#]*swap/s@^@#@’ /etc/fstab</p><p>时钟同步,时钟不一致可能会导致证书出现问题</p><p>rpm -ivh <a href="http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm" target="_blank" rel="noopener">http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm</a><br>yum install ntpdate -y</p><p>设置时区<br>ln -df /usr/share/zoneinfo/Asia/Shanghai /etc/localtime<br>echo ‘Asia/Shanghai’ &gt; /etc/timezone<br>ntpdate time2.aliyun.com<br>crontab -e<br>*/5 * * * * ntpdate time2.aliyun.com</p><p>临时设置ulimit<br>ulimit -SHn 65535</p><p>永久生效方式<br>vim /etc/security/limits.conf</p><ul><li>soft nofile 655360</li><li>hard nofile 131072</li><li>soft nproc 655350</li><li>hard nproc 655350</li><li>soft memlock unlimited</li><li>hard memlock unlimited</li></ul><p>在master01上生成证书并传输到其它机器上</p><p>ssh-keygen -t rsa<br>for i in k8s-master-02 k8s-master-03 k8s-node-01 k8s-node-02; do ssh-copy-id -i /root/.ssh/id_rsa.pub $i;done</p><p>第二部分：内核配置</p><p>更新并重启机器，排除内核信息<br>yum update -y –exclude=kernel* &amp;&amp; reboot</p><p>在生产环境中必须进行内核升级，CentOS 7需要内核版本在4.18+。 这里使用4.19版本</p><p>cd /root<br>wget <a href="http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm" target="_blank" rel="noopener">http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm</a><br>wget <a href="http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm" target="_blank" rel="noopener">http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm</a></p><p>可以使用idm下载后再上传到各个服务器上。</p><p>将内核文件传输到各个机器上<br>for i in k8s-master-02 k8s-master-03 k8s-node-01 k8s-node-02; do scp kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm $i:/root/ ;done</p><p>批量在各个机器上执行以下命令<br>cd /root/ &amp;&amp; yum localinstall kernel-ml* -y</p><p>更改内核启动顺序<br>grub2-set-default 0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfg<br>grubby –args=”user_namespace.enable=1” –update-kernel=”$(grubby –default-kernel)”</p><p>检查内核版本<br>grubby –default-kernel<br>/boot/vmlinuz-4.19.12-1.el7.elrepo.x86_64</p><p>所有节点重启，再检查内核信息</p><p>第三部分：安装ipvs工具</p><p>生产环境中推荐使用ipvs工具，不推荐使用iptables<br>yum install ipvsadm ipset sysstat conntrack libseccomp -y</p><p>所有内核节点配置ipvs模块，4.19+版本内核需要把nf_conntrack_ipv4更改为nf_conntrack，4.18及以下可以继续使用。<br>vim /etc/modules-load.d/ipvs.conf<br>//添加<br>ip_vs<br>ip_vs_lc<br>ip_vs_wlc<br>ip_vs_rr<br>ip_vs_wrr<br>ip_vs_lbtc<br>ip_vs_lblcr<br>ip_vs_dh<br>ip_vs_sh<br>ip_vs_fo<br>ip_vs_nq<br>ip_vs_sed<br>ip_vs_ftp<br>ip_vs_sh<br>nf_conntrack<br>ip_tables<br>ip_set<br>xt_set<br>ipt_set<br>ipt_rpfilter<br>ipt_REJECT<br>ipip</p><p>systemctl enable –now systemd-modules-load.service</p><p>注意：ip_vs_lbtc这个配置项可能导致无法启动systemd-modules-load.service。</p><p>// 校验是否开启，如果无输出信息，需要重启机器<br>lsmod | grep -e ip_vs -e nf_conntrack</p><p>开启内核参数</p><p>cat &gt;&gt; /etc/sysctl.d/k8s.conf&lt;&lt;EOF<br>net.ipv4.ip_forward=1<br>net.bridge.bridge-nf-call-iptables=1<br>net.bridge.bridge-nf-call-ip6tables=1<br>fs.may_datch_mounts=1<br>vm.overcommit_memory=1<br>vm.panic_on_oom=1<br>fs.inotify.max_user_watches=89100<br>fs.file-max=52706963<br>fs.nr_open=52706963<br>net.netfilter.nf_conntrack_max=2310720</p><p>net.ipv4.tcp_keepalive_time=600<br>net.ipv4.tcp_keepalive_probes=3<br>net.ipv4.tcp_keepalive_intvl=15<br>net.ipv4.tcp_max_tw_buckets=36000<br>net.ipv4.tcp_tw_reuse=1<br>net.ipv4.tcp_max_orphans=327680<br>net.ipv4.tcp_orphan_retries=3<br>net.ipv4.tcp_syncookies=1<br>net.ipv4.tcp_max_syn_backlog=16384<br>net.ipv4.ip_conntrack_max=65536<br>net.ipv4.tcp_timestamps=0<br>net.core.somaxconn=16384<br>EOF</p><p>sysctl –system</p><p>重启后进行验证<br>lsmod | grep -e ip_vs -e nf_conntrack<br>ip_vs_ftp              16384  0<br>nf_nat                 32768  1 ip_vs_ftp<br>ip_vs_sed              16384  0<br>ip_vs_nq               16384  0<br>ip_vs_fo               16384  0<br>ip_vs_sh               16384  0<br>ip_vs_dh               16384  0<br>ip_vs_lblcr            16384  0<br>ip_vs_wrr              16384  0<br>ip_vs_rr               16384  0<br>ip_vs_wlc              16384  0<br>ip_vs_lc               16384  0<br>ip_vs                 151552  22 ip_vs_wlc,ip_vs_rr,ip_vs_dh,ip_vs_lblcr,ip_vs_sh,ip_vs_fo,ip_vs_nq,ip_vs_wrr,ip_vs_lc,ip_vs_sed,ip_vs_ftp<br>nf_conntrack          143360  2 nf_nat,ip_vs<br>nf_defrag_ipv6         20480  1 nf_conntrack<br>nf_defrag_ipv4         16384  1 nf_conntrack<br>libcrc32c              16384  4 nf_conntrack,nf_nat,xfs,ip_vs</p><p>第四部分：docker以及kubenetes组件安装</p><p>安装docker<br>yum install docker-ce-19.03.* -y</p><p>修改cgroup运行方式<br>mkdir /etc/docker<br>cat &gt; /etc/docker/daemon.json &lt;&lt;EOF<br>{<br>    “exec-opts”: [“native.cgroupdriver=systemd”]<br>}<br>EOF</p><p>systemctl daemon-reload &amp;&amp; systemctl enable –now docker</p><p>安装k8s组件</p><p>注意，版本号后面小版本大于5时再去选择使用该版本运行在生产环境中，例如1.18.9、1.19.5这样版本，尽量不要选择如1.19.1、1.18.3这样的版本。</p><p>查看最新版本的组件信息<br>yum list kubeadm –showduplicates |sort -r</p><p>安装kubeadm<br>yum install kubeadm -y</p><p>安装完成后，需要进行设置。注意：–cgroup-driver 一定要指定为和docker相同的systemd，否则容易导致k8s无法启动。</p><p>默认配置的pause镜像使用gcr.io仓库，国内访问科学上网问题，需要配置使用阿里云的镜像。</p><p>cat &gt;/etc/sysconfig/kubelet&lt;&lt;EOF<br>KUBELET_EXTRA_ARGS=”–cgroup-driver=systemd –pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2”<br>EOF</p><p>配置开机启动。未初始化之前可能存在启动失败的问题，临时不需要关心，由于kubelet没有配置文件，所以无法启动，待初始化后可用<br>systemctl daemon-reload &amp;&amp; systemctl enable –now kubelet</p><p>第五部分：高可用组件安装</p><p>下面操作针对所有master节点，进行安装。</p><p>安装HAProxy和keepalived</p><p>yum install haproxy keepalived -y</p><p>编写HAProxy的配置文件，三个节点的配置文件相同</p><p>vim /etc/haproxy/haproxy.cfg</p><p>#———————————————————————<br>defaults<br>    mode                    http<br>    log                     global<br>    option                  httplog<br>    timeout http-request    15s<br>    timeout connect         5000<br>    timeout client          50000<br>    timeout server          50000<br>    timeout http-keep-alive 15s</p><p>#———————————————————————</p><h1 id="main-frontend-which-proxys-to-the-backends"><a href="#main-frontend-which-proxys-to-the-backends" class="headerlink" title="main frontend which proxys to the backends"></a>main frontend which proxys to the backends</h1><p>#———————————————————————<br>frontend monitor-in<br>    bind *:33305<br>    mode http<br>    option httplog<br>    monitor-uri /monitor</p><p>frontend k8s-master<br>    bind 0.0.0.0:16443<br>    bind 127.0.0.1:16443<br>    mode tcp<br>    option tcplog<br>    tcp-request inspect-delay 5s<br>    default_backend k8s-master</p><p>#———————————————————————</p><h1 id="round-robin-balancing-between-the-various-backends"><a href="#round-robin-balancing-between-the-various-backends" class="headerlink" title="round robin balancing between the various backends"></a>round robin balancing between the various backends</h1><p>#———————————————————————<br>backend k8s-master<br>    mode tcp<br>    option tcplog<br>    option tcp-check<br>    balance roundrobin<br>    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100<br>    server  k8s-master01 192.168.229.51:6443 check<br>    server  k8s-master02 192.168.229.52:6443 check<br>    server  k8s-master03 192.168.229.53:6443 check</p><p>编写keepalived的配置，注意区分每个节点的ip地址和网卡信息：</p><p>vim /etc/keepalived/keepalived.conf<br>! Configuration File for keepalived</p><p>global_defs {<br>   router_id LVS_DEVEL<br>   script_user root<br>   enable_script_security<br>}</p><p>vrrp_script chk_apiserver {<br>   script “/etc/keepalived/check_apiserver.sh”<br>   interval 5<br>   weight -5<br>   fall 2<br>   rise 1<br>}</p><p>vrrp_instance VI_1 {<br>    state MASTER<br>    interface ens33               # 这个位置为网卡信息，需要更改<br>    mcast_src_ip 192.168.229.51   # 这个位置是宿主机的真实IP地址，需要更改<br>    virtual_router_id 51<br>    priority 101<br>    advert_int 2<br>    authentication {<br>        auth_type PASS<br>        auth_pass K8SHA_KA_AUTH<br>    }<br>    virtual_ipaddress {<br>        192.168.229.60    # 注意这个位置是设置的VIP<br>    }<br>    track_script {<br>        chk_apiserver<br>    }<br>}</p><p>注意健康检查track_script部分要打开。</p><p>所有的master节点需要配置keepalived的健康检查脚本</p><p>vim /etc/keepalived/check_apiserver.sh</p><p>#!/bin/bash</p><p>err=0<br>for i in $(seq 1 3)<br>do<br>  check_code=$(pgrep haproxy)<br>  if [[ $check_code == “” ]]; then<br>    err=$(expr $err+1)<br>    sleep 1<br>    continue<br>  else<br>    err=0<br>    break<br>  fi<br>done</p><p>if [[ $err != “0” ]]; then<br>  echo “systemctl stop keepalived”<br>  /usr/bin/systemctl stop keepalived<br>  exit 1<br>else<br>  exit 0<br>fi</p><p>赋予脚本可执行权限<br>chmod +x /etc/keepalived/check_apiserver.sh</p><p>启动haproxy和keepalived<br>systemctl daemon-reload &amp;&amp; systemctl enable –now haproxy &amp;&amp; keepalived</p><p>查看生效状况<br>ip addr | grep 60<br>    inet 192.168.229.60/32 scope global ens33<br>    inet6 fe80::20c:29ff:febf:7608/64 scope link</p><p>telnet 192.168.229.51 16443<br>Trying 192.168.229.51…<br>Connected to 192.168.229.51.<br>Escape character is ‘^]’.<br>Connection closed by foreign host.</p><p>问题排查：VIP ping不通，而且telnet没有出现]标志<br>查看keepalived 是否存在问题，查看keepalived运行状态，查看selinux、haproxy和keepalived的状态和监听端口</p><p>第五部分：集群初始化</p><p>下面的命令在master节点上执行</p><p>创建kubeadm初始化的yaml文件，最好使用配置文件去执行，否则使用命令时指定的参数过多容易导致执行失败。</p><p>vim kubeadm-config.yaml</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> kubeadm.k8s.io/v1beta2<span class="token comment" spellcheck="true"># 设置加入集群的token信息，24小时内有效，过期后需要处理</span><span class="token key atrule">bootstrapTokens</span><span class="token punctuation">:</span><span class="token punctuation">-</span> <span class="token key atrule">groups</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> system<span class="token punctuation">:</span>bootstrappers<span class="token punctuation">:</span>kubeadm<span class="token punctuation">:</span>default<span class="token punctuation">-</span>node<span class="token punctuation">-</span>token  <span class="token key atrule">token</span><span class="token punctuation">:</span> 7t2weq.bjbawausm0jaxury  <span class="token key atrule">ttl</span><span class="token punctuation">:</span> 24h0m0s  <span class="token key atrule">usages</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> signing  <span class="token punctuation">-</span> authentication <span class="token key atrule">kind</span><span class="token punctuation">:</span> InitConfiguration<span class="token comment" spellcheck="true"># 本机信息配置，ip地址需要配置为本机的ip地址</span><span class="token key atrule">localAPIEndpoint</span><span class="token punctuation">:</span>  <span class="token key atrule">advertiseAddress</span><span class="token punctuation">:</span> 192.168.229.51  <span class="token key atrule">bindPort</span><span class="token punctuation">:</span> <span class="token number">6443</span><span class="token key atrule">nodeRegistration</span><span class="token punctuation">:</span>  <span class="token key atrule">criSocket</span><span class="token punctuation">:</span> /var/run/dockershim.sock  <span class="token key atrule">name</span><span class="token punctuation">:</span> k8s<span class="token punctuation">-</span>master<span class="token punctuation">-</span><span class="token number">01</span>  <span class="token comment" spellcheck="true"># 添加一个容忍，配置master节点不允许部署容器</span>  <span class="token key atrule">taints</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule    <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubenetes.io/master<span class="token punctuation">---</span><span class="token key atrule">apiServer</span><span class="token punctuation">:</span>  <span class="token key atrule">certSANs</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> 192.168.229.60  <span class="token key atrule">timeoutForControlPlane</span><span class="token punctuation">:</span> 4m0s<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> kubeadm.k8s.io/v1beta2<span class="token key atrule">certificatesDir</span><span class="token punctuation">:</span> /etc/kubernetes/pki<span class="token key atrule">clusterName</span><span class="token punctuation">:</span> kubernetes<span class="token key atrule">controlPlaneEndpoint</span><span class="token punctuation">:</span> 192.168.229.60<span class="token punctuation">:</span><span class="token number">16443</span><span class="token key atrule">controllerManager</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token key atrule">dns</span><span class="token punctuation">:</span>   <span class="token key atrule">type</span><span class="token punctuation">:</span> CoreDNS<span class="token key atrule">etcd</span><span class="token punctuation">:</span>  <span class="token key atrule">local</span><span class="token punctuation">:</span>        <span class="token key atrule">dataDir</span><span class="token punctuation">:</span> /var/lib/etcd<span class="token key atrule">imageRepository</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>hangzhou.aliyuncs.com/google_containers<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterConfiguration<span class="token key atrule">kubernetesVersion</span><span class="token punctuation">:</span> v1.20.1<span class="token key atrule">networking</span><span class="token punctuation">:</span>   <span class="token key atrule">dnsDomain</span><span class="token punctuation">:</span> cluster.local    <span class="token key atrule">podSubnet</span><span class="token punctuation">:</span> 172.168.0.0/12  <span class="token key atrule">serviceSubnet</span><span class="token punctuation">:</span> 10.96.0.0/12<span class="token key atrule">scheduler</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>更新kubadm-config.yaml<br>kubeadm config migrate –old-config kubeadm-config.yaml –new-config new.yaml</p><p>更新完成后将其它文件复制到其它master节点<br>for i in k8s-master-02 k8s-master-03 ; do scp new.yaml $i:/root/ ;done</p><p>下载镜像<br>kubeadm config images pull –config /root/new.yaml</p><p>在master01节点执行k8s集群的初始化<br>kubeadm init –config /root/new.yaml –upload-certs</p><p>[init] Using Kubernetes version: v1.20.1<br>[preflight] Running pre-flight checks<br>[preflight] Pulling images required for setting up a Kubernetes cluster<br>[preflight] This might take a minute or two, depending on the speed of your internet connection<br>[preflight] You can also perform this action in beforehand using ‘kubeadm config images pull’<br>[certs] Using certificateDir folder “/etc/kubernetes/pki”<br>[certs] Generating “ca” certificate and key<br>[certs] Generating “apiserver” certificate and key<br>[certs] apiserver serving cert is signed for DNS names [k8s-master-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.229.51 192.168.229.60]<br>[certs] Generating “apiserver-kubelet-client” certificate and key<br>[certs] Generating “front-proxy-ca” certificate and key<br>[certs] Generating “front-proxy-client” certificate and key<br>[certs] Generating “etcd/ca” certificate and key<br>[certs] Generating “etcd/server” certificate and key<br>[certs] etcd/server serving cert is signed for DNS names [k8s-master-01 localhost] and IPs [192.168.229.51 127.0.0.1 ::1]<br>[certs] Generating “etcd/peer” certificate and key<br>[certs] etcd/peer serving cert is signed for DNS names [k8s-master-01 localhost] and IPs [192.168.229.51 127.0.0.1 ::1]<br>[certs] Generating “etcd/healthcheck-client” certificate and key<br>[certs] Generating “apiserver-etcd-client” certificate and key<br>[certs] Generating “sa” key and public key<br>[kubeconfig] Using kubeconfig folder “/etc/kubernetes”<br>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address<br>[kubeconfig] Writing “admin.conf” kubeconfig file<br>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address<br>[kubeconfig] Writing “kubelet.conf” kubeconfig file<br>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address<br>[kubeconfig] Writing “controller-manager.conf” kubeconfig file<br>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address<br>[kubeconfig] Writing “scheduler.conf” kubeconfig file<br>[kubelet-start] Writing kubelet environment file with flags to file “/var/lib/kubelet/kubeadm-flags.env”<br>[kubelet-start] Writing kubelet configuration to file “/var/lib/kubelet/config.yaml”<br>[kubelet-start] Starting the kubelet<br>[control-plane] Using manifest folder “/etc/kubernetes/manifests”<br>[control-plane] Creating static Pod manifest for “kube-apiserver”<br>[control-plane] Creating static Pod manifest for “kube-controller-manager”<br>[control-plane] Creating static Pod manifest for “kube-scheduler”<br>[etcd] Creating static Pod manifest for local etcd in “/etc/kubernetes/manifests”<br>[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory “/etc/kubernetes/manifests”. This can take up to 4m0s<br>[apiclient] All control plane components are healthy after 30.040077 seconds<br>[upload-config] Storing the configuration used in ConfigMap “kubeadm-config” in the “kube-system” Namespace<br>[kubelet] Creating a ConfigMap “kubelet-config-1.20” in namespace kube-system with the configuration for the kubelets in the cluster<br>[upload-certs] Storing the certificates in Secret “kubeadm-certs” in the “kube-system” Namespace<br>[upload-certs] Using certificate key:<br>afe317744fa848eceadb7df6960133e77c96f9c282656d361706b7e515a4286f<br>[mark-control-plane] Marking the node k8s-master-01 as control-plane by adding the labels “node-role.kubernetes.io/master=’’” and “node-role.kubernetes.io/control-plane=’’ (deprecated)”<br>[mark-control-plane] Marking the node k8s-master-01 as control-plane by adding the taints [node-role.kubemetes.io/naster:NoSchedule]<br>[bootstrap-token] Using token: 7t2weq.bjbawausm0jaxury<br>[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles<br>[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes<br>[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials<br>[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token<br>[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster<br>[bootstrap-token] Creating the “cluster-info” ConfigMap in the “kube-public” namespace<br>[kubelet-finalize] Updating “/etc/kubernetes/kubelet.conf” to point to a rotatable kubelet client certificate and key<br>[addons] Applied essential addon: CoreDNS<br>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address<br>[addons] Applied essential addon: kube-proxy</p><p>Your Kubernetes control-plane has initialized successfully!</p><p>To start using your cluster, you need to run the following as a regular user:</p><p>  mkdir -p $HOME/.kube<br>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>  sudo chown $(id -u):$(id -g) $HOME/.kube/config</p><p>Alternatively, if you are the root user, you can run:</p><p>  export KUBECONFIG=/etc/kubernetes/admin.conf</p><p>You should now deploy a pod network to the cluster.<br>Run “kubectl apply -f [podnetwork].yaml” with one of the options listed at:<br>  <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/cluster-administration/addons/</a></p><p>You can now join any number of the control-plane node running the following command on each as root:</p><p>  kubeadm join 192.168.229.60:16443 –token 7t2weq.bjbawausm0jaxury <br>    –discovery-token-ca-cert-hash sha256:246203ceb832292331df2fedae4e1db305859ce79cab901873d4410af5403242 <br>    –control-plane –certificate-key afe317744fa848eceadb7df6960133e77c96f9c282656d361706b7e515a4286f</p><p>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!<br>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use<br>“kubeadm init phase upload-certs –upload-certs” to reload certs afterward.</p><p>Then you can join any number of worker nodes by running the following on each as root:</p><p>kubeadm join 192.168.229.60:16443 –token 7t2weq.bjbawausm0jaxury <br>    –discovery-token-ca-cert-hash sha256:246203ceb832292331df2fedae4e1db305859ce79cab901873d4410af5403242</p><p>在master01节点配置环境变量，用于访问kubernetes集群：<br>cat &lt;<eof>&gt; /root/.bashrc<br>export KUBECONFIG=/etc/kubernetes/admin.conf<br>EOF</eof></p><p>source /root/.bashrc</p><p>这时候配置完成，可以使用kubectl命令来进行集群的管理<br>kubectl get node<br>NAME            STATUS     ROLES                  AGE     VERSION<br>k8s-master-01   NotReady   control-plane,master   4m36s   v1.20.1</p><p>kubectl get svc<br>NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE<br>kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5m12s</none></p><p>kubectl get pod -n kube-system -o wide<br>NAME                                    READY   STATUS    RESTARTS   AGE     IP               NODE            NOMINATED NODE   READINESS GATES<br>coredns-54d67798b7-66z7x                0/1     Pending   0          6m27s   <none>           <none>          <none>           <none><br>coredns-54d67798b7-jzql9                0/1     Pending   0          6m27s   <none>           <none>          <none>           <none><br>etcd-k8s-master-01                      1/1     Running   0          6m21s   192.168.229.51   k8s-master-01   <none>           <none><br>kube-apiserver-k8s-master-01            1/1     Running   0          6m21s   192.168.229.51   k8s-master-01   <none>           <none><br>kube-controller-manager-k8s-master-01   1/1     Running   0          6m21s   192.168.229.51   k8s-master-01   <none>           <none><br>kube-proxy-pqh28                        1/1     Running   0          6m26s   192.168.229.51   k8s-master-01   <none>           <none><br>kube-scheduler-k8s-master-01            1/1     Running   0          6m21s   192.168.229.51   k8s-master-01   <none>           <none></none></none></none></none></none></none></none></none></none></none></none></none></none></none></none></none></none></none></p><p>第六部分：master节点高可用</p><p>注意master节点加入命令和node节点加入命令有所不同</p><p>master节点加入命令：</p><p>  kubeadm join 192.168.229.60:16443 –token 7t2weq.bjbawausm0jaxury <br>    –discovery-token-ca-cert-hash sha256:246203ceb832292331df2fedae4e1db305859ce79cab901873d4410af5403242 <br>    –control-plane –certificate-key afe317744fa848eceadb7df6960133e77c96f9c282656d361706b7e515a4286f</p><p>node节点加入命令</p><p>  kubeadm join 192.168.229.60:16443 –token 7t2weq.bjbawausm0jaxury <br>    –discovery-token-ca-cert-hash sha256:246203ceb832292331df2fedae4e1db305859ce79cab901873d4410af5403242</p><p>区别在于是否指定了control-plane相关的正式信息</p><p>首先将k8s-master-02机器加入master节点，加入后在k8s-master-01中执行</p><p>kubectl get node<br>NAME            STATUS     ROLES                  AGE    VERSION<br>k8s-master-01   NotReady   control-plane,master   14m    v1.20.1<br>k8s-master-02   NotReady   control-plane,master   3m9s   v1.20.1</p><p>如果出现了token过期，造成master节点无法加入的情况，处理如下：</p><p>一、 生成新的token信息<br>kubeadm token create –print-join-command<br>kubeadm join 192.168.229.60:16443 –token dvm83r.k0gpb2vqpsg49zak     –discovery-token-ca-cert-hash sha256:246203ceb832292331df2fedae4e1db305859ce79cab901873d4410af5403242</p><p>二、 生成新的certificate-key<br>kubeadm init phase upload-certs –upload-certs<br>[upload-certs] Storing the certificates in Secret “kubeadm-certs” in the “kube-system” Namespace<br>[upload-certs] Using certificate key:<br>8222ab7c36a228960e05327d352e1cf12716d3fe536abac8620ec4dd250c2098</p><p>将k8s-master-03加入k8s集群<br>kubeadm join 192.168.229.60:16443 –token dvm83r.k0gpb2vqpsg49zak     –discovery-token-ca-cert-hash sha256:246203ceb832292331df2fedae4e1db305859ce79cab901873d4410af5403242 –control-plane –certificate-key 8222ab7c36a228960e05327d352e1cf12716d3fe536abac8620ec4dd250c2098</p><p>查看所有节点信息<br>kubectl get node<br>NAME            STATUS     ROLES                  AGE   VERSION<br>k8s-master-01   NotReady   control-plane,master   35m   v1.20.1<br>k8s-master-02   NotReady   control-plane,master   23m   v1.20.1<br>k8s-master-03   NotReady   control-plane,master   81s   v1.20.1</p><p>同理，将k8s-node-01、k8s-node-02作为node节点加入，<br>kubeadm join 192.168.229.60:16443 –token dvm83r.k0gpb2vqpsg49zak     –discovery-token-ca-cert-hash sha256:246203ceb832292331df2fedae4e1db305859ce79cab901873d4410af5403242</p><p>最终查看所有节点信息<br> kubectl get node<br>NAME            STATUS     ROLES                  AGE     VERSION<br>k8s-master-01   NotReady   control-plane,master   39m     v1.20.1<br>k8s-master-02   NotReady   control-plane,master   27m     v1.20.1<br>k8s-master-03   NotReady   control-plane,master   5m42s   v1.20.1<br>k8s-node-01     NotReady   <none>                 28s     v1.20.1<br>k8s-node-02     NotReady   <none>                 16s     v1.20.1</none></none></p><p>最后通过配置文件安装calico网络组件</p><p>编辑calico.yaml文件</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/calico-etcd-secrets.yaml</span><span class="token comment" spellcheck="true"># The following contains k8s Secrets for use with a TLS enabled etcd cluster.</span><span class="token comment" spellcheck="true"># For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>etcd<span class="token punctuation">-</span>secrets  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token key atrule">data</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Populate the following with etcd TLS configuration if desired, but leave blank if</span>  <span class="token comment" spellcheck="true"># not using TLS for etcd.</span>  <span class="token comment" spellcheck="true"># The keys below should be uncommented and the values populated with the base64</span>  <span class="token comment" spellcheck="true"># encoded contents of each file that would be associated with the TLS data.</span>  <span class="token comment" spellcheck="true"># Example command for encoding a file contents: cat &lt;file> | base64 -w 0</span>  <span class="token comment" spellcheck="true"># 修改2：etcd的证书文件信息添加</span>  <span class="token key atrule">etcd-key</span><span class="token punctuation">:</span> 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  <span class="token key atrule">etcd-cert</span><span class="token punctuation">:</span> LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURRekNDQWl1Z0F3SUJBZ0lJT2dtLzhHRDliQzB3RFFZSktvWklodmNOQVFFTEJRQXdFakVRTUE0R0ExVUUKQXhNSFpYUmpaQzFqWVRBZUZ3MHlNVEF4TURreE1EUXpNalJhRncweU1qQXhNRGt4TURRek1qVmFNQmd4RmpBVQpCZ05WQkFNVERXczRjeTF0WVhOMFpYSXRNREV3Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUR3RitQcXFoNHZsRHl0WkNtSGJYdVREK3B1N21MSnhrZE9RalRDbXBqWWRaQjdaS3JoMXBmbFZwN0YKaWs2Q3dyMEFzMENlUWtua0hGajVxQXJnK0xpUHZKajduZWxVR1BqcXhuMnMyVjBMbWtxWHFFd2diRSt5ZWNpcwpNL0dMcUR3MGJLZGVKVVR3ZkV3c1d0R2NGbWlGU05hQUxZUnFOeTFpamRoby9meE12OUw3NDdzNnV6a1JacUJxCnRvY2hDNnlQN2YyUm5GcTkweWRYL3JZcVFjNklHcUlVVkpoT05qelg1NkN2S0Rua21uem8rQXNsNkFuNnFPWDYKTXRQNmVyVm96VkdiQnZhUGNVUkZRRytSZVlOUTY1aUlSOUFCN3krSFdKQlRheEIyZ0ZCMzAzK2llellOdkZycwpZbHc1U1cydmdsRlNvN01mb2JiY3NreFprRHdEQWdNQkFBR2pnWll3Z1pNd0RnWURWUjBQQVFIL0JBUURBZ1dnCk1CMEdBMVVkSlFRV01CUUdDQ3NHQVFVRkJ3TUJCZ2dyQmdFRkJRY0RBakFmQmdOVkhTTUVHREFXZ0JTNERCbE8KeEFVVFVQQjdyYXJzeGE0eGtNenAxekJCQmdOVkhSRUVPakE0Z2cxck9ITXRiV0Z6ZEdWeUxUQXhnZ2xzYjJOaApiR2h2YzNTSEJNQ281VE9IQkg4QUFBR0hFQUFBQUFBQUFBQUFBQUFBQUFBQUFBRXdEUVlKS29aSWh2Y05BUUVMCkJRQURnZ0VCQUJTVjV4NGZRVCtvN1JiUnYrZWxCd1VUc3A0dmVWbFVZNVB2LzJjUWE2b0h1dUZ3eHFlYmZjRkMKcTVlbHIzUHMxUVUrTERtMUFXeFhTUTRqaHhiTUZ0ZEVtdGxaR0I4YUhaWU95dnpDTVhOcGVWRVArUTNTWUZKQwpSdHNuSmFrQ0ZTS2ZTcVliMzYxZHRUOCtpc2tzL0ZSTUlNMFVOZ1J1U2xRM0prdmRlOGNleitQWUliR0RkQUlTCkIxZ25WWTRNRVowNXVOSmxNU05jcXZyQy9JNHdjdXV6YURkdVBZNlVQMjJLYjZKay9CdWlkKzdHVmF5QUdBNksKN0E3RDJiRUR6a3RtQm1Hei9zeHBGTW5URnh1V3F1eEVybnRWanVkV085T1pSREJadTBWVHA3aHBHUG9KTkJqVgpCZG1tVzZ6QU4xd0ZsNlJpWk9rbzRoanNEUVVHeXVNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==  <span class="token key atrule">etcd-ca</span><span class="token punctuation">:</span> LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM0VENDQWNtZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFTTVJBd0RnWURWUVFERXdkbGRHTmsKTFdOaE1CNFhEVEl4TURFd09URXdORE15TkZvWERUTXhNREV3TnpFd05ETXlORm93RWpFUU1BNEdBMVVFQXhNSApaWFJqWkMxallUQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUpQcUsxUVVQNzBFCmJYOWpXYysyQ0FTVi9qRVo2T1ZrTkVyTmE0TC9POThZWmtKOWN6a3ZvTS9nSUIzcGFVemc1SmViazZFUnpFTjAKV0dCdU9vWnJuNHNUOERDWDc4U1N0TkxmOXpaOERvbjBVZXNRVUdnRnpQOE4vWTdlckxUT3dLMEhNVmNxSE5NaQpLb2labEFuc1piWGhxYnh0MDRJemIwckxxejZXcmcxdXZoWS9Iam52MDVjUjRZN094UTFLalBIZDR2cHhzZ1pZCjl6Lzhnem9FV3pTSlY4cDB4TzUzejNFN3NTcTBzOWJQdWtvOWp1Vm1Ra0RhQXE0aStBOGR3R25lU2hQRWJXZFMKTGVKMTNlYlpiRVN1OTZ4VVBXRTlTQndqNGpWVExqSi9XZ1VXZ043TXFNc2RtWGkxWGhDL2pIRFErNmEyV2FRVwpRRG1EL3lpUUtBOENBd0VBQWFOQ01FQXdEZ1lEVlIwUEFRSC9CQVFEQWdLa01BOEdBMVVkRXdFQi93UUZNQU1CCkFmOHdIUVlEVlIwT0JCWUVGTGdNR1U3RUJSTlE4SHV0cXV6RnJqR1F6T25YTUEwR0NTcUdTSWIzRFFFQkN3VUEKQTRJQkFRQ0ZPVkdRekZYbXRXVFFlNVZqRE9GTmZVSWVvbkZ6RDJRaE5FZ21zak9MMWdQcThmQlhQWDB4cFFMcwpvcTgza2lNL1hYZnI5cmxaR1A2NXJZVXYvcHJOS3kyZlZFYmJOYUxGRi83WUdPV3EwU1RUSk1vVXdBS1ltS0IvCnE0U2QvNXk2U1hvYmEzTmZqeDhzQnNKcTNBRXBXcmlyd3JyWGtoVHMzdW1RMTVBb0ZQdGpYaG5WTkt5UTNjbSsKVk9PSk81U0M0bnpaQndwbms2bUFtMXhaZmlXY2xXbDgyVmRKWjkxYm9qVEJlWUoxeGRyMlhIUHZsYllNMjk5NAp3V0M2V05PRG5lamZ2U0RMenJoYVJOdkJVTytzUGE1cENTQW5jRlNVUG1Xemg1d0QyVVdJbmMveGlqSkZSVVFWCjVGV3NQUWtIV0VmM3daa2hrYlR0NXFXTHpmcjUKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=<span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/calico-config.yaml</span><span class="token comment" spellcheck="true"># This ConfigMap is used to configure a self-hosted Calico installation.</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token key atrule">data</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Configure this with the location of your etcd cluster.</span>  <span class="token comment" spellcheck="true"># 修改1：修改为ETCD的节点</span>  <span class="token comment" spellcheck="true"># 记住这个位置一定要使用https，否则会无法连接到etcd</span>  <span class="token key atrule">etcd_endpoints</span><span class="token punctuation">:</span> <span class="token string">"https://192.168.229.51:2379,https://192.168.229.52:2379,https://192.168.229.53:2379"</span>  <span class="token comment" spellcheck="true"># If you're using TLS enabled etcd uncomment the following.</span>  <span class="token comment" spellcheck="true"># You must also populate the Secret below with these files.</span>  <span class="token key atrule">etcd_ca</span><span class="token punctuation">:</span> <span class="token string">"/calico-secrets/etcd-ca"</span>   <span class="token comment" spellcheck="true"># "/calico-secrets/etcd-ca"</span>  <span class="token key atrule">etcd_cert</span><span class="token punctuation">:</span> <span class="token string">"/calico-secrets/etcd-cert"</span> <span class="token comment" spellcheck="true"># "/calico-secrets/etcd-cert"</span>  <span class="token key atrule">etcd_key</span><span class="token punctuation">:</span> <span class="token string">"/calico-secrets/etcd-key"</span>  <span class="token comment" spellcheck="true"># "/calico-secrets/etcd-key"</span>  <span class="token comment" spellcheck="true"># Typha is disabled.</span>  <span class="token key atrule">typha_service_name</span><span class="token punctuation">:</span> <span class="token string">"none"</span>  <span class="token comment" spellcheck="true"># Configure the backend to use.</span>  <span class="token key atrule">calico_backend</span><span class="token punctuation">:</span> <span class="token string">"bird"</span>  <span class="token comment" spellcheck="true"># Configure the MTU to use for workload interfaces and tunnels.</span>  <span class="token comment" spellcheck="true"># - If Wireguard is enabled, set to your network MTU - 60</span>  <span class="token comment" spellcheck="true"># - Otherwise, if VXLAN or BPF mode is enabled, set to your network MTU - 50</span>  <span class="token comment" spellcheck="true"># - Otherwise, if IPIP is enabled, set to your network MTU - 20</span>  <span class="token comment" spellcheck="true"># - Otherwise, if not using any encapsulation, set to your network MTU.</span>  <span class="token key atrule">veth_mtu</span><span class="token punctuation">:</span> <span class="token string">"1440"</span>  <span class="token comment" spellcheck="true"># The CNI network configuration to install on each node. The special</span>  <span class="token comment" spellcheck="true"># values in this config will be automatically populated.</span>  <span class="token key atrule">cni_network_config</span><span class="token punctuation">:</span> <span class="token punctuation">|</span><span class="token punctuation">-</span>    <span class="token punctuation">{</span>      <span class="token key atrule">"name"</span><span class="token punctuation">:</span> <span class="token string">"k8s-pod-network"</span><span class="token punctuation">,</span>      <span class="token key atrule">"cniVersion"</span><span class="token punctuation">:</span> <span class="token string">"0.3.1"</span><span class="token punctuation">,</span>      <span class="token key atrule">"plugins"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">{</span>          <span class="token key atrule">"type"</span><span class="token punctuation">:</span> <span class="token string">"calico"</span><span class="token punctuation">,</span>          <span class="token key atrule">"log_level"</span><span class="token punctuation">:</span> <span class="token string">"info"</span><span class="token punctuation">,</span>          <span class="token key atrule">"etcd_endpoints"</span><span class="token punctuation">:</span> <span class="token string">"__ETCD_ENDPOINTS__"</span><span class="token punctuation">,</span>          <span class="token key atrule">"etcd_key_file"</span><span class="token punctuation">:</span> <span class="token string">"__ETCD_KEY_FILE__"</span><span class="token punctuation">,</span>          <span class="token key atrule">"etcd_cert_file"</span><span class="token punctuation">:</span> <span class="token string">"__ETCD_CERT_FILE__"</span><span class="token punctuation">,</span>          <span class="token key atrule">"etcd_ca_cert_file"</span><span class="token punctuation">:</span> <span class="token string">"__ETCD_CA_CERT_FILE__"</span><span class="token punctuation">,</span>          <span class="token key atrule">"mtu"</span><span class="token punctuation">:</span> __CNI_MTU__<span class="token punctuation">,</span>          <span class="token key atrule">"ipam"</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>              <span class="token key atrule">"type"</span><span class="token punctuation">:</span> <span class="token string">"calico-ipam"</span>          <span class="token punctuation">}</span><span class="token punctuation">,</span>          <span class="token key atrule">"policy"</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>              <span class="token key atrule">"type"</span><span class="token punctuation">:</span> <span class="token string">"k8s"</span>          <span class="token punctuation">}</span><span class="token punctuation">,</span>          <span class="token key atrule">"kubernetes"</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>              <span class="token key atrule">"kubeconfig"</span><span class="token punctuation">:</span> <span class="token string">"__KUBECONFIG_FILEPATH__"</span>          <span class="token punctuation">}</span>        <span class="token punctuation">}</span><span class="token punctuation">,</span>        <span class="token punctuation">{</span>          <span class="token key atrule">"type"</span><span class="token punctuation">:</span> <span class="token string">"portmap"</span><span class="token punctuation">,</span>          <span class="token key atrule">"snat"</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token punctuation">,</span>          <span class="token key atrule">"capabilities"</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token key atrule">"portMappings"</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token punctuation">}</span>        <span class="token punctuation">}</span><span class="token punctuation">,</span>        <span class="token punctuation">{</span>          <span class="token key atrule">"type"</span><span class="token punctuation">:</span> <span class="token string">"bandwidth"</span><span class="token punctuation">,</span>          <span class="token key atrule">"capabilities"</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token key atrule">"bandwidth"</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token punctuation">}</span>        <span class="token punctuation">}</span>      <span class="token punctuation">]</span>    <span class="token punctuation">}</span><span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/calico-kube-controllers-rbac.yaml</span><span class="token comment" spellcheck="true"># Include a clusterrole for the kube-controllers component,</span><span class="token comment" spellcheck="true"># and bind it to the calico-kube-controllers serviceaccount.</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Pods are monitored for changing labels.</span>  <span class="token comment" spellcheck="true"># The node controller monitors Kubernetes nodes.</span>  <span class="token comment" spellcheck="true"># Namespace and serviceaccount labels are used for policy.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> pods      <span class="token punctuation">-</span> nodes      <span class="token punctuation">-</span> namespaces      <span class="token punctuation">-</span> serviceaccounts    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> watch      <span class="token punctuation">-</span> list      <span class="token punctuation">-</span> get  <span class="token comment" spellcheck="true"># Watch for changes to Kubernetes NetworkPolicies.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"networking.k8s.io"</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> networkpolicies    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> watch      <span class="token punctuation">-</span> list<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers<span class="token key atrule">subjects</span><span class="token punctuation">:</span><span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/calico-node-rbac.yaml</span><span class="token comment" spellcheck="true"># Include a clusterrole for the calico-node DaemonSet,</span><span class="token comment" spellcheck="true"># and bind it to the calico-node serviceaccount.</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># The CNI plugin needs to get pods, nodes, and namespaces.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> pods      <span class="token punctuation">-</span> nodes      <span class="token punctuation">-</span> namespaces    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> get  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> endpoints      <span class="token punctuation">-</span> services    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># Used to discover service IPs for advertisement.</span>      <span class="token punctuation">-</span> watch      <span class="token punctuation">-</span> list  <span class="token comment" spellcheck="true"># Pod CIDR auto-detection on kubeadm needs access to config maps.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> configmaps    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> get  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> nodes/status    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># Needed for clearing NodeNetworkUnavailable flag.</span>      <span class="token punctuation">-</span> patch<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node<span class="token key atrule">subjects</span><span class="token punctuation">:</span><span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/calico-node.yaml</span><span class="token comment" spellcheck="true"># This manifest installs the calico-node container, as well</span><span class="token comment" spellcheck="true"># as the CNI plugins and network config on</span><span class="token comment" spellcheck="true"># each master and worker node in a Kubernetes cluster.</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node  <span class="token key atrule">updateStrategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">kubernetes.io/os</span><span class="token punctuation">:</span> linux      <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Make sure calico-node gets scheduled on all nodes.</span>        <span class="token punctuation">-</span> <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule          <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists        <span class="token comment" spellcheck="true"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> CriticalAddonsOnly          <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists        <span class="token punctuation">-</span> <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoExecute          <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node      <span class="token comment" spellcheck="true"># Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force</span>      <span class="token comment" spellcheck="true"># deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span>      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">0</span>      <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> system<span class="token punctuation">-</span>node<span class="token punctuation">-</span>critical      <span class="token key atrule">initContainers</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># This container installs the CNI binaries</span>        <span class="token comment" spellcheck="true"># and CNI network config file on each node.</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> install<span class="token punctuation">-</span>cni          <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>beijing.aliyuncs.com/dotbalo/cni<span class="token punctuation">:</span>v3.15.3          <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/install-cni.sh"</span><span class="token punctuation">]</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Name of the CNI config file to create.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CNI_CONF_NAME              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"10-calico.conflist"</span>            <span class="token comment" spellcheck="true"># The CNI network config to install on each node.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CNI_NETWORK_CONFIG              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> cni_network_config            <span class="token comment" spellcheck="true"># The location of the etcd cluster.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_ENDPOINTS              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_endpoints            <span class="token comment" spellcheck="true"># CNI MTU Config variable</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CNI_MTU              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> veth_mtu            <span class="token comment" spellcheck="true"># Prevents the container from sleeping forever.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SLEEP              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"false"</span>          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /host/opt/cni/bin              <span class="token key atrule">name</span><span class="token punctuation">:</span> cni<span class="token punctuation">-</span>bin<span class="token punctuation">-</span>dir            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /host/etc/cni/net.d              <span class="token key atrule">name</span><span class="token punctuation">:</span> cni<span class="token punctuation">-</span>net<span class="token punctuation">-</span>dir            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /calico<span class="token punctuation">-</span>secrets              <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>certs          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token comment" spellcheck="true"># Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes</span>        <span class="token comment" spellcheck="true"># to communicate with Felix over the Policy Sync API.</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> flexvol<span class="token punctuation">-</span>driver          <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>beijing.aliyuncs.com/dotbalo/pod2daemon<span class="token punctuation">-</span>flexvol<span class="token punctuation">:</span>v3.15.3          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> flexvol<span class="token punctuation">-</span>driver<span class="token punctuation">-</span>host            <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /host/driver          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Runs calico-node container on each Kubernetes node. This</span>        <span class="token comment" spellcheck="true"># container programs network policy and routes on each</span>        <span class="token comment" spellcheck="true"># host.</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node          <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>beijing.aliyuncs.com/dotbalo/node<span class="token punctuation">:</span>v3.15.3          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># The location of the etcd cluster.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_ENDPOINTS              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_endpoints            <span class="token comment" spellcheck="true"># Location of the CA certificate for etcd.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_CA_CERT_FILE              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_ca            <span class="token comment" spellcheck="true"># Location of the client key for etcd.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_KEY_FILE              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_key            <span class="token comment" spellcheck="true"># Location of the client certificate for etcd.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_CERT_FILE              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_cert            <span class="token comment" spellcheck="true"># Set noderef for node controller.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CALICO_K8S_NODE_REF              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> spec.nodeName            <span class="token comment" spellcheck="true"># Choose the backend to use.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CALICO_NETWORKING_BACKEND              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> calico_backend            <span class="token comment" spellcheck="true"># Cluster type to identify the deployment type</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CLUSTER_TYPE              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"k8s,bgp"</span>            <span class="token comment" spellcheck="true"># Auto-detect the BGP IP address.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> IP              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"autodetect"</span>            <span class="token comment" spellcheck="true"># Enable IPIP</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CALICO_IPV4POOL_IPIP              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"Always"</span>            <span class="token comment" spellcheck="true"># Enable or Disable VXLAN on the default IP pool.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CALICO_IPV4POOL_VXLAN              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"Never"</span>            <span class="token comment" spellcheck="true"># Set MTU for tunnel device used if ipip is enabled</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FELIX_IPINIPMTU              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> veth_mtu            <span class="token comment" spellcheck="true"># Set MTU for the VXLAN tunnel device.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FELIX_VXLANMTU              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> veth_mtu            <span class="token comment" spellcheck="true"># Set MTU for the Wireguard tunnel device.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FELIX_WIREGUARDMTU              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> veth_mtu            <span class="token comment" spellcheck="true"># The default IPv4 pool to create on startup if none exists. Pod IPs will be</span>            <span class="token comment" spellcheck="true"># chosen from this range. Changing this value after installation will have</span>            <span class="token comment" spellcheck="true"># no effect. This should fall within `--cluster-cidr`.</span>            <span class="token comment" spellcheck="true"># 修改3：添加ip地址信息</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CALICO_IPV4POOL_CIDR              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"172.168.0.0/12"</span>            <span class="token comment" spellcheck="true"># Disable file logging so `kubectl logs` works.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CALICO_DISABLE_FILE_LOGGING              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"true"</span>            <span class="token comment" spellcheck="true"># Set Felix endpoint to host default action to ACCEPT.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FELIX_DEFAULTENDPOINTTOHOSTACTION              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"ACCEPT"</span>            <span class="token comment" spellcheck="true"># Disable IPv6 on Kubernetes.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FELIX_IPV6SUPPORT              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"false"</span>            <span class="token comment" spellcheck="true"># Set Felix logging to "info"</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FELIX_LOGSEVERITYSCREEN              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"info"</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> FELIX_HEALTHENABLED              <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"true"</span>          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>          <span class="token key atrule">resources</span><span class="token punctuation">:</span>            <span class="token key atrule">requests</span><span class="token punctuation">:</span>              <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 250m          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">exec</span><span class="token punctuation">:</span>              <span class="token key atrule">command</span><span class="token punctuation">:</span>              <span class="token punctuation">-</span> /bin/calico<span class="token punctuation">-</span>node              <span class="token punctuation">-</span> <span class="token punctuation">-</span>felix<span class="token punctuation">-</span>live              <span class="token punctuation">-</span> <span class="token punctuation">-</span>bird<span class="token punctuation">-</span>live            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">6</span>          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">exec</span><span class="token punctuation">:</span>              <span class="token key atrule">command</span><span class="token punctuation">:</span>              <span class="token punctuation">-</span> /bin/calico<span class="token punctuation">-</span>node              <span class="token punctuation">-</span> <span class="token punctuation">-</span>felix<span class="token punctuation">-</span>ready              <span class="token punctuation">-</span> <span class="token punctuation">-</span>bird<span class="token punctuation">-</span>ready            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /lib/modules              <span class="token key atrule">name</span><span class="token punctuation">:</span> lib<span class="token punctuation">-</span>modules              <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /run/xtables.lock              <span class="token key atrule">name</span><span class="token punctuation">:</span> xtables<span class="token punctuation">-</span>lock              <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/run/calico              <span class="token key atrule">name</span><span class="token punctuation">:</span> var<span class="token punctuation">-</span>run<span class="token punctuation">-</span>calico              <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/calico              <span class="token key atrule">name</span><span class="token punctuation">:</span> var<span class="token punctuation">-</span>lib<span class="token punctuation">-</span>calico              <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /calico<span class="token punctuation">-</span>secrets              <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>certs            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> policysync              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/run/nodeagent      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Used by calico-node.</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> lib<span class="token punctuation">-</span>modules          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /lib/modules        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> var<span class="token punctuation">-</span>run<span class="token punctuation">-</span>calico          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/run/calico        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> var<span class="token punctuation">-</span>lib<span class="token punctuation">-</span>calico          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/lib/calico        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> xtables<span class="token punctuation">-</span>lock          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /run/xtables.lock            <span class="token key atrule">type</span><span class="token punctuation">:</span> FileOrCreate        <span class="token comment" spellcheck="true"># Used to install CNI.</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> cni<span class="token punctuation">-</span>bin<span class="token punctuation">-</span>dir          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /opt/cni/bin        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> cni<span class="token punctuation">-</span>net<span class="token punctuation">-</span>dir          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/cni/net.d        <span class="token comment" spellcheck="true"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="token comment" spellcheck="true"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>certs          <span class="token key atrule">secret</span><span class="token punctuation">:</span>            <span class="token key atrule">secretName</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>etcd<span class="token punctuation">-</span>secrets            <span class="token key atrule">defaultMode</span><span class="token punctuation">:</span> <span class="token number">0400</span>        <span class="token comment" spellcheck="true"># Used to create per-pod Unix Domain Sockets</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> policysync          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">type</span><span class="token punctuation">:</span> DirectoryOrCreate            <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/run/nodeagent        <span class="token comment" spellcheck="true"># Used to install Flex Volume Driver</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> flexvol<span class="token punctuation">-</span>driver<span class="token punctuation">-</span>host          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">type</span><span class="token punctuation">:</span> DirectoryOrCreate            <span class="token key atrule">path</span><span class="token punctuation">:</span> /usr/libexec/kubernetes/kubelet<span class="token punctuation">-</span>plugins/volume/exec/nodeagent~uds<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>node  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/calico-kube-controllers.yaml</span><span class="token comment" spellcheck="true"># See https://github.com/projectcalico/kube-controllers</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># The controllers can only have a single active instance.</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> Recreate  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers      <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">kubernetes.io/os</span><span class="token punctuation">:</span> linux      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> CriticalAddonsOnly          <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master          <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers      <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> system<span class="token punctuation">-</span>cluster<span class="token punctuation">-</span>critical      <span class="token comment" spellcheck="true"># The controllers must run in the host network namespace so that</span>      <span class="token comment" spellcheck="true"># it isn't governed by policy that would prevent it from working.</span>      <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers          <span class="token comment" spellcheck="true"># 修改4：镜像地址变更</span>          <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>beijing.aliyuncs.com/dotbalo/kube<span class="token punctuation">-</span>controllers<span class="token punctuation">:</span>v3.15.3          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># The location of the etcd cluster.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_ENDPOINTS              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_endpoints            <span class="token comment" spellcheck="true"># Location of the CA certificate for etcd.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_CA_CERT_FILE              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_ca            <span class="token comment" spellcheck="true"># Location of the client key for etcd.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_KEY_FILE              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_key            <span class="token comment" spellcheck="true"># Location of the client certificate for etcd.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ETCD_CERT_FILE              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>config                  <span class="token key atrule">key</span><span class="token punctuation">:</span> etcd_cert            <span class="token comment" spellcheck="true"># Choose which controllers to run.</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ENABLED_CONTROLLERS              <span class="token key atrule">value</span><span class="token punctuation">:</span> policy<span class="token punctuation">,</span>namespace<span class="token punctuation">,</span>serviceaccount<span class="token punctuation">,</span>workloadendpoint<span class="token punctuation">,</span>node          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Mount in the etcd TLS secrets.</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /calico<span class="token punctuation">-</span>secrets              <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>certs          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">exec</span><span class="token punctuation">:</span>              <span class="token key atrule">command</span><span class="token punctuation">:</span>              <span class="token punctuation">-</span> /usr/bin/check<span class="token punctuation">-</span>status              <span class="token punctuation">-</span> <span class="token punctuation">-</span>r      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="token comment" spellcheck="true"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> etcd<span class="token punctuation">-</span>certs          <span class="token key atrule">secret</span><span class="token punctuation">:</span>            <span class="token key atrule">secretName</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>etcd<span class="token punctuation">-</span>secrets            <span class="token key atrule">defaultMode</span><span class="token punctuation">:</span> <span class="token number">0400</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> calico<span class="token punctuation">-</span>kube<span class="token punctuation">-</span>controllers  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/calico-typha.yaml</span><span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/configure-canal.yaml</span><span class="token punctuation">---</span><span class="token comment" spellcheck="true"># Source: calico/templates/kdd-crds.yaml</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行命令开始安装<br>kubectl apply -f calico-etcd.yaml</p><p>需要有一个漫长的等待才能完成安装，完成后查看集群状态以及pod装填<br>kubectl -n kube-system get pod<br>NAME                                       READY   STATUS    RESTARTS   AGE<br>calico-kube-controllers-5f6d4b864b-pr5zl   1/1     Running   0          2m4s<br>calico-node-bwdmj                          1/1     Running   0          2m4s<br>calico-node-ddh6r                          1/1     Running   0          2m4s<br>calico-node-dhfb2                          1/1     Running   0          2m4s<br>calico-node-jx79z                          1/1     Running   0          2m4s<br>calico-node-x4nzm                          1/1     Running   0          2m4s<br>coredns-54d67798b7-66z7x                   1/1     Running   0          4h9m<br>coredns-54d67798b7-jzql9                   1/1     Running   0          4h9m<br>etcd-k8s-master-01                         1/1     Running   0          4h9m<br>etcd-k8s-master-02                         1/1     Running   0          3h57m<br>etcd-k8s-master-03                         1/1     Running   0          3h35m<br>kube-apiserver-k8s-master-01               1/1     Running   0          4h9m<br>kube-apiserver-k8s-master-02               1/1     Running   0          3h57m<br>kube-apiserver-k8s-master-03               1/1     Running   0          3h35m<br>kube-controller-manager-k8s-master-01      1/1     Running   1          4h9m<br>kube-controller-manager-k8s-master-02      1/1     Running   0          3h57m<br>kube-controller-manager-k8s-master-03      1/1     Running   0          3h35m<br>kube-proxy-f56vn                           1/1     Running   0          3h35m<br>kube-proxy-hzc6n                           1/1     Running   0          3h30m<br>kube-proxy-pqh28                           1/1     Running   0          4h9m<br>kube-proxy-r66cr                           1/1     Running   0          3h30m<br>kube-proxy-t7dvm                           1/1     Running   0          3h57m<br>kube-scheduler-k8s-master-01               1/1     Running   1          4h9m<br>kube-scheduler-k8s-master-02               1/1     Running   0          3h57m<br>kube-scheduler-k8s-master-03               1/1     Running   0          3h35m</p><p>这样整个集群算是告一段落了。</p><p>第7部分：metrics和dashboard的安装</p><p>执行metrics安装之前，需要先把下面配置文件中的证书拷贝到其它节点，否则会存在找不到证书的情况<br>for i in k8s-master-02 k8s-master-03 k8s-node-01 k8s-node-02; do scp /etc/kubernetes/pki/front-proxy-ca.crt $i:/etc/kubernetes/pki/front-proxy-ca.crt ;done</p><p>metrics的配置文件如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server    <span class="token key atrule">rbac.authorization.k8s.io/aggregate-to-admin</span><span class="token punctuation">:</span> <span class="token string">"true"</span>    <span class="token key atrule">rbac.authorization.k8s.io/aggregate-to-edit</span><span class="token punctuation">:</span> <span class="token string">"true"</span>    <span class="token key atrule">rbac.authorization.k8s.io/aggregate-to-view</span><span class="token punctuation">:</span> <span class="token string">"true"</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> system<span class="token punctuation">:</span>aggregated<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>reader<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> metrics.k8s.io    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> pods      <span class="token punctuation">-</span> nodes    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> get      <span class="token punctuation">-</span> list      <span class="token punctuation">-</span> watch<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> system<span class="token punctuation">:</span>metrics<span class="token punctuation">-</span>server<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token string">""</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> pods      <span class="token punctuation">-</span> nodes      <span class="token punctuation">-</span> nodes/stats      <span class="token punctuation">-</span> namespaces      <span class="token punctuation">-</span> configmaps    <span class="token key atrule">verbs</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> get      <span class="token punctuation">-</span> list      <span class="token punctuation">-</span> watch<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> RoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server<span class="token punctuation">-</span>auth<span class="token punctuation">-</span>reader  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> Role  <span class="token key atrule">name</span><span class="token punctuation">:</span> extension<span class="token punctuation">-</span>apiserver<span class="token punctuation">-</span>authentication<span class="token punctuation">-</span>reader<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server<span class="token punctuation">:</span>system<span class="token punctuation">:</span>auth<span class="token punctuation">-</span>delegator<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> system<span class="token punctuation">:</span>auth<span class="token punctuation">-</span>delegator<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> system<span class="token punctuation">:</span>metrics<span class="token punctuation">-</span>server<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> system<span class="token punctuation">:</span>metrics<span class="token punctuation">-</span>server<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> https      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">443</span>      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> https  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">args</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>cert<span class="token punctuation">-</span>dir=/tmp            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>secure<span class="token punctuation">-</span>port=4443            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>metric<span class="token punctuation">-</span>resolution=30s            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>kubelet<span class="token punctuation">-</span>insecure<span class="token punctuation">-</span>tls            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>kubelet<span class="token punctuation">-</span>preferred<span class="token punctuation">-</span>address<span class="token punctuation">-</span>types=InternalIP<span class="token punctuation">,</span>ExternalIP<span class="token punctuation">,</span>Hostname            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>requestheader<span class="token punctuation">-</span>client<span class="token punctuation">-</span>ca<span class="token punctuation">-</span>file=/etc/kubernetes/pki/front<span class="token punctuation">-</span>proxy<span class="token punctuation">-</span>ca.crt <span class="token comment" spellcheck="true"># change to front-proxy-ca.crt for kubeadm， 如果不添加证书，可能会造成监测信息获取不到的情况</span>            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>requestheader<span class="token punctuation">-</span>username<span class="token punctuation">-</span>headers=X<span class="token punctuation">-</span>Remote<span class="token punctuation">-</span>User            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>requestheader<span class="token punctuation">-</span>group<span class="token punctuation">-</span>headers=X<span class="token punctuation">-</span>Remote<span class="token punctuation">-</span>Group            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>requestheader<span class="token punctuation">-</span>extra<span class="token punctuation">-</span>headers<span class="token punctuation">-</span>prefix=X<span class="token punctuation">-</span>Remote<span class="token punctuation">-</span>Extra<span class="token punctuation">-</span>          <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>beijing.aliyuncs.com/dotbalo/metrics<span class="token punctuation">-</span>server<span class="token punctuation">:</span>v0.4.1          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">3</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /livez              <span class="token key atrule">port</span><span class="token punctuation">:</span> https              <span class="token key atrule">scheme</span><span class="token punctuation">:</span> HTTPS            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>          <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">4443</span>              <span class="token key atrule">name</span><span class="token punctuation">:</span> https              <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">3</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /readyz              <span class="token key atrule">port</span><span class="token punctuation">:</span> https              <span class="token key atrule">scheme</span><span class="token punctuation">:</span> HTTPS            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">runAsNonRoot</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">1000</span>          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /tmp              <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>dir            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ca<span class="token punctuation">-</span>ssl              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/kubernetes/pki      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">kubernetes.io/os</span><span class="token punctuation">:</span> linux      <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> system<span class="token punctuation">-</span>cluster<span class="token punctuation">-</span>critical      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>          <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>dir        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> ca<span class="token punctuation">-</span>ssl          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/kubernetes/pki<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apiregistration.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> APIService<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server  <span class="token key atrule">name</span><span class="token punctuation">:</span> v1beta1.metrics.k8s.io<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">group</span><span class="token punctuation">:</span> metrics.k8s.io  <span class="token key atrule">groupPriorityMinimum</span><span class="token punctuation">:</span> <span class="token number">100</span>  <span class="token key atrule">insecureSkipTLSVerify</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">service</span><span class="token punctuation">:</span>    <span class="token key atrule">name</span><span class="token punctuation">:</span> metrics<span class="token punctuation">-</span>server    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system  <span class="token key atrule">version</span><span class="token punctuation">:</span> v1beta1  <span class="token key atrule">versionPriority</span><span class="token punctuation">:</span> <span class="token number">100</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>安装命令<br>kubectl apply -f comp.yaml</p><p>安装完成后，查看指标信息<br>kubectl top node<br>NAME            CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%<br>k8s-master-01   161m         8%     1363Mi          35%<br>k8s-master-02   154m         7%     1310Mi          34%<br>k8s-master-03   152m         7%     1272Mi          33%<br>k8s-node-01     86m          4%     1090Mi          28%<br>k8s-node-02     92m          4%     1073Mi          28%</p><p>安装dashboard，配置文件有两个</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># dashboard.yaml</span><span class="token comment" spellcheck="true"># Copyright 2017 The Kubernetes Authors.</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># Licensed under the Apache License, Version 2.0 (the "License");</span><span class="token comment" spellcheck="true"># you may not use this file except in compliance with the License.</span><span class="token comment" spellcheck="true"># You may obtain a copy of the License at</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># Unless required by applicable law or agreed to in writing, software</span><span class="token comment" spellcheck="true"># distributed under the License is distributed on an "AS IS" BASIS,</span><span class="token comment" spellcheck="true"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="token comment" spellcheck="true"># See the License for the specific language governing permissions and</span><span class="token comment" spellcheck="true"># limitations under the License.</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Namespace<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">443</span>      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8443</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>csrf  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token key atrule">data</span><span class="token punctuation">:</span>  <span class="token key atrule">csrf</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>key<span class="token punctuation">-</span>holder  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>settings  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Role<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Allow Dashboard to get, update and delete Dashboard exclusive secrets.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"secrets"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"kubernetes-dashboard-key-holder"</span><span class="token punctuation">,</span> <span class="token string">"kubernetes-dashboard-certs"</span><span class="token punctuation">,</span> <span class="token string">"kubernetes-dashboard-csrf"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">,</span> <span class="token string">"update"</span><span class="token punctuation">,</span> <span class="token string">"delete"</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"configmaps"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"kubernetes-dashboard-settings"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">,</span> <span class="token string">"update"</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Allow Dashboard to get metrics.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"services"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"heapster"</span><span class="token punctuation">,</span> <span class="token string">"dashboard-metrics-scraper"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"proxy"</span><span class="token punctuation">]</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"services/proxy"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"heapster"</span><span class="token punctuation">,</span> <span class="token string">"http:heapster:"</span><span class="token punctuation">,</span> <span class="token string">"https:heapster:"</span><span class="token punctuation">,</span> <span class="token string">"dashboard-metrics-scraper"</span><span class="token punctuation">,</span> <span class="token string">"http:dashboard-metrics-scraper"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">]</span><span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Allow Metrics Scraper to get metrics from the Metrics server</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"metrics.k8s.io"</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"pods"</span><span class="token punctuation">,</span> <span class="token string">"nodes"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">,</span> <span class="token string">"list"</span><span class="token punctuation">,</span> <span class="token string">"watch"</span><span class="token punctuation">]</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> RoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> Role  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard          <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>beijing.aliyuncs.com/dotbalo/dashboard<span class="token punctuation">:</span>v2.0.4          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8443</span>              <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP          <span class="token key atrule">args</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>auto<span class="token punctuation">-</span>generate<span class="token punctuation">-</span>certificates            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>namespace=kubernetes<span class="token punctuation">-</span>dashboard            <span class="token comment" spellcheck="true"># Uncomment the following line to manually specify Kubernetes API server Host</span>            <span class="token comment" spellcheck="true"># If not specified, Dashboard will attempt to auto discover the API server and connect</span>            <span class="token comment" spellcheck="true"># to it. Uncomment only if the default does not work.</span>            <span class="token comment" spellcheck="true"># - --apiserver-host=http://my-address:port</span>          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /certs              <span class="token comment" spellcheck="true"># Create on-disk volume to store exec logs</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /tmp              <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">scheme</span><span class="token punctuation">:</span> HTTPS              <span class="token key atrule">path</span><span class="token punctuation">:</span> /              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8443</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">1001</span>            <span class="token key atrule">runAsGroup</span><span class="token punctuation">:</span> <span class="token number">2001</span>      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs          <span class="token key atrule">secret</span><span class="token punctuation">:</span>            <span class="token key atrule">secretName</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">"kubernetes.io/os"</span><span class="token punctuation">:</span> linux      <span class="token comment" spellcheck="true"># Comment the following tolerations if Dashboard must not be deployed on master</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master          <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">name</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8000</span>      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8000</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">name</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper      <span class="token key atrule">annotations</span><span class="token punctuation">:</span>        <span class="token key atrule">seccomp.security.alpha.kubernetes.io/pod</span><span class="token punctuation">:</span> <span class="token string">'runtime/default'</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper          <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.cn<span class="token punctuation">-</span>beijing.aliyuncs.com/dotbalo/metrics<span class="token punctuation">-</span>scraper<span class="token punctuation">:</span>v1.0.4          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8000</span>              <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">scheme</span><span class="token punctuation">:</span> HTTP              <span class="token key atrule">path</span><span class="token punctuation">:</span> /              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8000</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /tmp              <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">1001</span>            <span class="token key atrule">runAsGroup</span><span class="token punctuation">:</span> <span class="token number">2001</span>      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">"kubernetes.io/os"</span><span class="token punctuation">:</span> linux      <span class="token comment" spellcheck="true"># Comment the following tolerations if Dashboard must not be deployed on master</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master          <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># dashboard-user.yaml</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> admin<span class="token punctuation">-</span>user  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1 <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding <span class="token key atrule">metadata</span><span class="token punctuation">:</span>   <span class="token key atrule">name</span><span class="token punctuation">:</span> admin<span class="token punctuation">-</span>user  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>    <span class="token key atrule">rbac.authorization.kubernetes.io/autoupdate</span><span class="token punctuation">:</span> <span class="token string">"true"</span><span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> cluster<span class="token punctuation">-</span>admin<span class="token key atrule">subjects</span><span class="token punctuation">:</span><span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount  <span class="token key atrule">name</span><span class="token punctuation">:</span> admin<span class="token punctuation">-</span>user  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两个配置文件放在同一个文件夹下，所以运行时不指定具体文件<br>kubectl apply -f .</p><p>安装完成后，为了本地可以进行访问，在google浏览器中加入启动参数，用于解决无法访问Dashboard的问题</p><pre><code>--test-type --ignore-certificate-errors</code></pre><p>最后需要针对dashboard修改svc设置，变更ClusterIP为nodePort</p><pre><code>kubectl get svc -n kubernetes-dashboardNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGEdashboard-metrics-scraper   ClusterIP   10.110.87.54     &lt;none&gt;        8000/TCP   3m21skubernetes-dashboard        ClusterIP   10.104.150.211   &lt;none&gt;        443/TCP    3m21skubectl edit svc kubernetes-dashboard -n kubernetes-dashboard# Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1kind: Servicemetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;k8s-app&quot;:&quot;kubernetes-dashboard&quot;},&quot;name&quot;:&quot;kubernetes-dashboard&quot;,&quot;namespace&quot;:&quot;kubernetes-dashboard&quot;},&quot;spec&quot;:{&quot;ports&quot;:[{&quot;port&quot;:443,&quot;targetPort&quot;:8443}],&quot;selector&quot;:{&quot;k8s-app&quot;:&quot;kubernetes-dashboard&quot;}}}  creationTimestamp: &quot;2021-01-09T15:31:54Z&quot;  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboard  resourceVersion: &quot;34046&quot;  uid: ec006a73-60fd-412f-a4fe-19e4b0726a9bspec:  clusterIP: 10.104.150.211  clusterIPs:  - 10.104.150.211  ports:  - port: 443    protocol: TCP    targetPort: 8443  selector:    k8s-app: kubernetes-dashboard  sessionAffinity: None  type: nodePort   # 在此处更改status:  loadBalancer: {}// :wq保存退出kubectl get svc -n kubernetes-dashboardNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGEdashboard-metrics-scraper   ClusterIP   10.110.87.54     &lt;none&gt;        8000/TCP        7m33skubernetes-dashboard        NodePort    10.104.150.211   &lt;none&gt;        443:32056/TCP   7m33s</code></pre><p>根据已有的端口号，通过任意安装了kube-proxy的宿主机，或者VIP的IP+端口即可访问到dashboard。例如<a href="https://192.168.229.60:32056" target="_blank" rel="noopener">https://192.168.229.60:32056</a>.</p><p>选择使用token登录，首先要先获取token值</p><pre><code> kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &#39;{print $1}&#39;)Name:         admin-user-token-wgsnsNamespace:    kube-systemLabels:       &lt;none&gt;Annotations:  kubernetes.io/service-account.name: admin-user              kubernetes.io/service-account.uid: b13eceff-722e-4c3c-bcce-1a001a0a698cType:  kubernetes.io/service-account-tokenData====ca.crt:     1066 bytesnamespace:  11 bytestoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlN4QXVXRThDdGlNMkQ0MlByVjJueElaOUtYdXhqQ0JiVjVadkFUMmVtajQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXdnc25zIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiMTNlY2VmZi03MjJlLTRjM2MtYmNjZS0xYTAwMWEwYTY5OGMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.bjITkXMzLzGTJqKScN48TSVgA4cjj7dfcTOPKlEt87hZyg9tifp3WR1bbtNowdDlEcVK2anuHXm6Z9XV6OWVBWp845b-LZPcsOkLHHcYXd16YpLfAL3_SXLQpboq-_ilQU6M4IV2zWeBYXWc03aAqSkQVS8WlE8yuTm7UqRPpNs1C3YB92c3OjlXujS38gsdkvud5Wk8Mnr8lNEyYM39HiiM8SXVdDtN1kzLQgRpFMMQq75rGtohQ4cATkla0a6VHxRh9VFXdgaytutU_hAFetA6Oia7i0j56TbJBwguJNFTTm2-sSvMnWqAPIelu-LKVoC-aYMiSAODslyXVE-L-g</code></pre><p>将token值复制进去即可进行访问！</p><p>第8部分：必须要改的内容</p><p>8.1 修改kube-proxy的运行模式为ipvs</p><p>查看原来的运行模式</p><p>curl 127.0.0.1:10249/proxyMode<br>iptables</p><p>在master01节点执行，修改为ipvs<br>kubectl edit cm kube-proxy -n kube-system<br>// 找到mode位置，修改<br>mode: “ipvs”</p><p>修改完成后，进行kube-proxy的滚动更新<br>kubectl patch daemonset kube-proxy -p “{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;date&quot;:&quot;<code>date +&#39;%s&#39;</code>&quot;}}}}}” -n kube-system</p><p>更新完成后再查看运行方式<br>curl 127.0.0.1:10249/proxyMode<br>ipvs</p><p>查看ipvs规则信息<br>ipvsadm -ln<br>IP Virtual Server version 1.2.1 (size=4096)<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  127.0.0.1:32056 rr<br>  -&gt; 172.163.98.133:8443          Masq    1      0          0<br>TCP  172.17.0.1:32056 rr<br>  -&gt; 172.163.98.133:8443          Masq    1      0          0<br>TCP  172.174.107.64:32056 rr<br>  -&gt; 172.163.98.133:8443          Masq    1      0          0<br>TCP  192.168.229.51:32056 rr<br>  -&gt; 172.163.98.133:8443          Masq    1      0          0<br>TCP  10.96.0.1:443 rr<br>  -&gt; 192.168.229.51:6443          Masq    1      0          0<br>  -&gt; 192.168.229.52:6443          Masq    1      0          0<br>  -&gt; 192.168.229.53:6443          Masq    1      0          0<br>TCP  10.96.0.10:53 rr<br>  -&gt; 172.163.98.129:53            Masq    1      0          0<br>  -&gt; 172.175.44.1:53              Masq    1      0          0<br>TCP  10.96.0.10:9153 rr<br>  -&gt; 172.163.98.129:9153          Masq    1      0          0<br>  -&gt; 172.175.44.1:9153            Masq    1      0          0<br>TCP  10.104.150.211:443 rr<br>  -&gt; 172.163.98.133:8443          Masq    1      0          0<br>TCP  10.110.87.54:8000 rr<br>  -&gt; 172.175.44.2:8000            Masq    1      0          0<br>TCP  10.111.142.190:443 rr<br>  -&gt; 172.163.98.132:4443          Masq    1      1          0<br>UDP  10.96.0.10:53 rr<br>  -&gt; 172.163.98.129:53            Masq    1      0          0<br>  -&gt; 172.175.44.1:53              Masq    1      0          0</p><p>8.2 master节点可以部署非系统pod，通过对污点（taints）进行修改</p><p>查看节点的污点信息<br>kubectl describe node -l node-role.kubernetes.io/master= | grep Taints<br>Taints:             node-role.kubemetes.io/naster:NoSchedule<br>Taints:             node-role.kubernetes.io/master:NoSchedule<br>Taints:             node-role.kubernetes.io/master:NoSchedule</p><p>去除污点<br>kubectl taint node -l node-role.kubernetes.io/master node-role.kubernetes.io/master:NoSchedule-</p><p>再次查看<br>kubectl describe node -l node-role.kubernetes.io/master= | grep Taints<br>Taints:             <none><br>Taints:             <none><br>Taints:             <none></none></none></none></p><p>注意污点去除时，要和查找到的污点信息一一对应即可。</p><p>最后一部分：集群验证</p><p>注意：</p><ol><li>所有的镜像，尽量在本地镜像仓库内备份留存，这样集群一旦出现问题，在自愈的过程中拉取镜像时更快</li><li>在执行kubectl edit svc 命令时，可以按住shift+zz(按住shift连击两下z键)进行保存退出</li></ol><p>1.2 二进制安装</p><h2 id="第二章-基本概念介绍"><a href="#第二章-基本概念介绍" class="headerlink" title="第二章 基本概念介绍"></a>第二章 基本概念介绍</h2><p>2.1 为什么要使用k8s</p><p>2.1.1 健康检查，原生提供健康检查的功能</p><p>2.1.2 服务的动态扩容缩容</p><p>2.1.3 大规模容器管理（1000个容器），端口资源的简化管理（内部Service，不直接对外暴露的端口），减少端口冲突</p><p><img src="%E6%9E%B6%E6%9E%84%E5%9B%BE%E5%B1%95%E7%A4%BA.png" alt></p><p>2.2 master节点</p><p>整个集群的控制中枢</p><ul><li><p>kube-apiserver: 集群控制核心。各个模块之间的信息交互，集群管理、资源配置、安全机制的入口</p></li><li><p>controller-manager：集群状态管理器，保证pod和其它资源达到设定值，同apiserver通信，在需要的时候进行资源的增删改查</p></li><li><p>Scheduler: 集群的调度中心，会根据指定的条件，选择一个或一批符合条件的节点</p></li><li><p>etcd：键值数据库，保存集群信息，一般要部署奇数个节点（至少三个以上，避免脑裂的情况）。做了存储层面的内容，尽可能将etcd独立出来，不要和master节点部署到一台服务器上。</p></li></ul><p>最重要的，etcd集群一定要部署在SSD硬盘上，强制性要求，不能缩水。否则会导致整个集群运行缓慢。</p><p>注意：尽量将master节点设置污点，不允许master节点部署应用程序！</p><p>2.3 node节点（工作节点）</p><ul><li>kubelet： 负责监听节点上的pod状态，同时负责上报节点和节点上的Pod信息，负责与master节点进行通信，管理节点上的pod</li><li>kube-proxy：负责Pod之间的通信和负载均衡，将指定的流量分发到后端正确的机器上。</li></ul><p>查看kube-proxy的运行模式<br>curl 127.0.0.1:10249/proxyMode</p><ul><li>ipvs：监听master节点增加和删除Service和Endpoint的消息，调用Netlink接口创建相应的ipvs规则。例如，通过该规则关联物理端口与集群内Pod的访问地址信息。通过ipvs规则，将流量转发到相应的Pod上。</li><li>iptables：同ipvs功能相同，不同点在于，对于每一个Service都要创建一条iptables规则，将service的ClusterIP代理到后端对应的pod上。在iptables规则增多后，其性能会急剧下降。ipvs为内核级转发。</li></ul><p>2.4 常用Pod</p><ul><li><p>Calico：符合CNI标准的网络插件，给每个Pod生成一个唯一的IP地址，并且把每个节点当作一个路由器。支持网络流量策略。对比Cilium eBPF</p></li><li><p>CoreDNS：用于kubernetes集群内部Service的解析，可以让Pod把Service名称解析成IP地址，然后通过Service的IP地址连接到对应的应用上。</p></li></ul><p>一般认为Service网段中的第一个地址是留给kubernetes使用的，而第10个地址是留给CoreDNS使用的。</p><p>查：如何控制CoreDNS的个数？工具是哪个？</p><ul><li>Docker：容器引擎</li></ul><p>2.5 Pod</p><p>Pod具有隔离性，通过namespace进行隔离。</p><p>无隔离性的资源：PV、RBAC（clusterrole、clusterrolebinding）、storageclass、ingressclass</p><p>2.5.1 Pod概念</p><p>k8s中最小的单元，由一个或者多个容器组成，每个pod还包含一个Pause容器，Pause容器是pod的父容器，主要负责僵尸进程的回收管理， 通过Pause容器可以使同一个Pod中的多个容器共享存储（volume）、网络、PID、IPC等资源。</p><p>注意：对于docker容器，一个Container中最多启动一个进程，这样在退出、删除、杀死进程操作时，不会对其它的进程造成影响，事实上的进程隔离。</p><p>为何使用Pod这个概念？</p><pre><code>1. 一个Pod中多个容器，一个系统由多个微服务进行支撑，微服务中服务的强依赖性，通信延迟低或者数据依赖性2. 兼容符合CRI标准的不同的容器技术，例如containerd、CRI-O等</code></pre><p>2.5.2 Pod编写解析</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1 <span class="token comment" spellcheck="true"># 必选, API的版本号</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod <span class="token comment" spellcheck="true"># 必选,类型Pod</span><span class="token key atrule">metadata</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 必选,元数据</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token comment" spellcheck="true"># 必选,符合RFC 1035规范的Pod名称</span>  <span class="token comment" spellcheck="true"># namespace: default # 可选，Pod所在的命名空间，不指定默认为default, 可以使用 -n 指定namespace 注意：一般手动指定目标namespace，使用-n参数</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,标签选择器,一般用于过滤和区分Pod</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">role</span><span class="token punctuation">:</span> frontend <span class="token comment" spellcheck="true"># 可以写多个</span>  <span class="token key atrule">annotations</span> <span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,注释列表,可以写多个</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token key atrule">spec</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 必选,用于定义容器的详細信息</span>  <span class="token key atrule">initContainers</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 初始化容器,在容器启动之前执行的一些初始化操作，可以有多个，类似list或者数组，使用"-"进行区分</span>  <span class="token punctuation">-</span> <span class="token key atrule">command</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> sh    <span class="token punctuation">-</span> <span class="token punctuation">-</span>C    <span class="token punctuation">-</span> echo "I am InitContainer for init some configuration"    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent    <span class="token key atrule">name</span><span class="token punctuation">:</span> init<span class="token punctuation">-</span>container  <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">30 </span><span class="token comment" spellcheck="true"># 设置Pod终止之前的等待时间，等待资源处理的时间</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 必选,容器列表</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx <span class="token comment" spellcheck="true"># 必选,符合RFC 1035规范的容器名称</span>    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1 <span class="token comment" spellcheck="true"># 必选,容器所用的镜像的地址，注意需要添加为自有的容器管理地址，例如10.0.98.90:5000/test/nginx:1.16.1</span>    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always <span class="token comment" spellcheck="true"># 可选,镜像拉取策略,ifNotPresent，如果宿主机有该镜像就不再拉取；Always：总是拉取；Never：不管是否存在都不拉取</span>    <span class="token key atrule">command</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 可选,容器启动执行的命令，该处command相当于docker容器内的 ENTRYPOINT ，arg相当于docker容器中的 CMD命令</span>    <span class="token punctuation">-</span> nginx    <span class="token punctuation">-</span> <span class="token punctuation">-</span>g    <span class="token punctuation">-</span> <span class="token string">"daemon off;"</span>    <span class="token key atrule">workingDir</span><span class="token punctuation">:</span> /usr/share/nginx/html <span class="token comment" spellcheck="true"># 可选,容器的工作目录，在登录容器后会定位到该目录下</span>    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,存储卷配置,可以配置多个</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> webroot <span class="token comment" spellcheck="true"># 存储卷名称</span>      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/nginx/html <span class="token comment" spellcheck="true"># 挂载目录</span>      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true  </span><span class="token comment" spellcheck="true"># 只读</span>    <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 可选,容器需要暴露的端口号列表</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http  <span class="token comment" spellcheck="true"># 端口名称</span>      <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80 </span><span class="token comment" spellcheck="true"># 端口号</span>      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP <span class="token comment" spellcheck="true"># 端口协议,默认TCP</span>    <span class="token key atrule">env</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 可选，环境变量配置列表</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> TZ <span class="token comment" spellcheck="true"># 变量名</span>      <span class="token key atrule">value</span><span class="token punctuation">:</span> Asia/Shanghai <span class="token comment" spellcheck="true"># 变量的值</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> LANG      <span class="token key atrule">value</span><span class="token punctuation">:</span> en US.utf8    <span class="token key atrule">resources</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 可选，资源限制和资源请求限制</span>      <span class="token key atrule">limits</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 资源的最大限制</span>        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 1000m        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1024Mi      <span class="token key atrule">requests</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 启动所需的资源</span>        <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m        <span class="token key atrule">memory</span><span class="token punctuation">:</span> 512Mi<span class="token comment" spellcheck="true"># 三种健康检查方式：startupProbe、readinessProbe、livenessProbe</span><span class="token comment" spellcheck="true">#    startupProbe: # 可选,检测容器内进程是否完成启动。注意三种检查方式同时只能使用一种:</span><span class="token comment" spellcheck="true">#      httpGet: # httpGet检测方式,生产环境建议使用httpGet实现接口级健崇栓套,健。检查由应用程序提供。</span><span class="token comment" spellcheck="true">#        path: /api/successStart # 检查路径</span><span class="token comment" spellcheck="true">#        port: 80</span>    <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,健康检查。注可三种检查方式同时只能使用一者,</span>      <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># httpGet检测方式,生产环境建议使用HttpGet实现接口级健康检查,健康枪查应由应用程序提供。</span>            <span class="token key atrule">path</span><span class="token punctuation">:</span> / <span class="token comment" spellcheck="true"># 检查路径</span>            <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80 </span><span class="token comment" spellcheck="true">#</span>    <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,健康检套</span>      <span class="token comment" spellcheck="true">#exec:       #执行容器命令检测方式</span>           <span class="token comment" spellcheck="true">#command:</span>           <span class="token comment" spellcheck="true">#- cat</span>           <span class="token comment" spellcheck="true">#- /health</span>    <span class="token comment" spellcheck="true">#httpGet: #httpGet检测方式</span>    <span class="token comment" spellcheck="true">#   path: /_health # 检查路径</span>    <span class="token comment" spellcheck="true">#   port: 8088</span>    <span class="token comment" spellcheck="true">#   httpHeaders:  # 检查的请求头</span>    <span class="token comment" spellcheck="true">#     name: end-user   # 请求头的key值</span>    <span class="token comment" spellcheck="true">#     value: Jason    # 请求头的value值</span>      <span class="token key atrule">tcpSocket</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 端口检测方式</span>            <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">initialDelaySeconds</span> <span class="token punctuation">:</span> <span class="token number">60 </span><span class="token comment" spellcheck="true"># 初始化时间</span>      <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">2  </span><span class="token comment" spellcheck="true"># 超时时间</span>      <span class="token key atrule">periodSeconds</span> <span class="token punctuation">:</span> <span class="token number">5  </span><span class="token comment" spellcheck="true"># 检测间隔</span>      <span class="token key atrule">successThreshold</span> <span class="token punctuation">:</span> <span class="token number">1 </span><span class="token comment" spellcheck="true"># 检查成功为2次表示就绪</span>      <span class="token key atrule">failureThreshold</span> <span class="token punctuation">:</span> <span class="token number">2 </span><span class="token comment" spellcheck="true"># 检测失败1次表示未就绪</span>    <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 生命周期相关配置</span>      <span class="token key atrule">postStart</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 容器创建完成后执行的指令,可以是exec httpGet TCPSocket</span>        <span class="token key atrule">exec</span><span class="token punctuation">:</span>          <span class="token key atrule">command</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> sh          <span class="token punctuation">-</span> <span class="token punctuation">-</span>C          <span class="token punctuation">-</span> <span class="token string">'mkdir /data/'</span>      <span class="token key atrule">preStop</span><span class="token punctuation">:</span>        <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token comment" spellcheck="true">#  exec:</span>      <span class="token comment" spellcheck="true">#  command:</span>      <span class="token comment" spellcheck="true">#  - sh</span>      <span class="token comment" spellcheck="true">#  - -c</span>      <span class="token comment" spellcheck="true">#  - sleep 9</span>  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Always <span class="token comment" spellcheck="true"># 重启策略，可选,默认为Always：容器故障或者没有启动成功，自动重启  Onfailure：容器以不为0的状态终止，也就是异常终止的情况下，自动重启该容器，Never：总是不会重启</span>  <span class="token comment" spellcheck="true">#nodeSelector: # 可选,指定Node节点</span>  <span class="token comment" spellcheck="true">#      region: subnet7   # 匹配带有该标签的节点，将pod部署到该节点</span>  <span class="token key atrule">imagePullSecrets</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,拉取镜像使用的secret,可以配置多个，对应于私有镜像仓库的账号密码</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>dockercfg<span class="token punctuation">-</span><span class="token number">86258</span>  <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 可选,是否为主机模式,如是,会占用主机端口</span>  <span class="token key atrule">volumes</span> <span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 共享存储卷列表</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> webroot <span class="token comment" spellcheck="true"># 名称，与上述对应</span>    <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">#hostPath: # 挂载目录</span>        <span class="token comment" spellcheck="true">#  path: /etc/hosts # 挂载本机目录</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试使用脚本信息</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># pod课程所用到的配置文件</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1 <span class="token comment" spellcheck="true"># 必选, API的版本号</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod <span class="token comment" spellcheck="true"># 必选,类型Pod</span><span class="token key atrule">metadata</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 必选,元数据</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token comment" spellcheck="true"># 必选,符合RFC 1035规范的Pod名称</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,标签选择器,一般用于过滤和区分Pod</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">role</span><span class="token punctuation">:</span> frontend <span class="token comment" spellcheck="true"># 可以写多个</span>  <span class="token key atrule">annotations</span> <span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 可选,注释列表,可以写多个</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token key atrule">spec</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 必选,用于定义容器的详細信息</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 必选,容器列表</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx <span class="token comment" spellcheck="true"># 必选,符合RFC 1035规范的容器名称</span>    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1 <span class="token comment" spellcheck="true"># 必选,容器所用的镜像的地址，注意需要添加为自有的容器管理地址，例如10.0.98.90:5000/test/nginx:1.16.1</span>    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always <span class="token comment" spellcheck="true"># 可选,镜像拉取策略,ifNotPresent，如果宿主机有该镜像就不再拉取；Always：总是拉取；Never：不管是否存在都不拉取</span>    <span class="token key atrule">command</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 可选,容器启动执行的命令，该处command相当于docker容器内的 ENTRYPOINT ，arg相当于docker容器中的 CMD命令</span>    <span class="token punctuation">-</span> nginx    <span class="token punctuation">-</span> <span class="token punctuation">-</span>g    <span class="token punctuation">-</span> <span class="token string">"daemon off;"</span>    <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 可选,容器需要暴露的端口号列表</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http  <span class="token comment" spellcheck="true"># 端口名称</span>      <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80 </span><span class="token comment" spellcheck="true"># 端口号</span>      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP <span class="token comment" spellcheck="true"># 端口协议,默认TCP</span>    <span class="token key atrule">env</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 可选，环境变量配置列表</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> TZ <span class="token comment" spellcheck="true"># 变量名</span>      <span class="token key atrule">value</span><span class="token punctuation">:</span> Asia/Shanghai <span class="token comment" spellcheck="true"># 变量的值</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> LANG      <span class="token key atrule">value</span><span class="token punctuation">:</span> en US.utf8  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Always <span class="token comment" spellcheck="true"># 重启策略，可选,默认为Always：容器故障或者没有启动成功，自动重启  Onfailure：容器以不为0的状态终止，也就是异常终止的情况下，自动重启该容器，Never：总是不会重启</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>常用命令：</p><pre class="line-numbers language-shell"><code class="language-shell">kubectl create ns kube-public   # 创建命名空间kube-publickubectl get po --show-labels  # 查看pod上所携带的labelskubectl create -f pod.yaml -n kube-public  # 在kube-public命名空间上创建pod，-f指定创建资源的文件来源kubectl apply -f pod.yaml -n kube-public  # 运行已经创建的pod，如果pod.yaml文件未修改，运行该命令不对之前的镜像造成影响kubectl describe pod nginx    # 查看名称为nginx的pod当前状态以及配置信息kubectl delete pod nginx   # 删除名称为nginx的pod<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>遇到问题：</p><pre class="line-numbers language-log"><code class="language-log"># kubectl describe pod nginx.......  Warning  Failed     7m47s (x4 over 10m)    kubelet            Error: ErrImagePull  Warning  Failed     7m47s (x2 over 9m36s)  kubelet            Failed to pull image "nginx:1.16.1": rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/library/nginx/manifests/1.16.1: net/http: TLS handshake timeout  Normal   BackOff    7m35s (x6 over 10m)    kubelet            Back-off pulling image "nginx:1.16.1"  Warning  Failed     7m24s (x7 over 10m)    kubelet            Error: ImagePullBackOff# docker pull nginx:1.16.1Error response from daemon: Get https://registry-1.docker.io/v2/: net/http: TLS handshake timeout<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解决方式：重新配置docker服务的daemon.json文件，配置完成后重启docker服务，等待集群自动恢复。</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>    <span class="token property">"exec-opts"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"native.cgroupdriver=systemd"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"registry-mirrors"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token string">"https://registry.cn-hangzhou.aliyuncs.com"</span><span class="token punctuation">,</span>        <span class="token string">"https://docker.mirrors.ustc.edu.cn"</span><span class="token punctuation">,</span>        <span class="token string">"https://registry.docker-cn.com"</span><span class="token punctuation">,</span>        <span class="token string">"http://hub-mirror.c.163.com"</span><span class="token punctuation">,</span>        <span class="token string">"https://mirror.baidubce.com"</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"insecure-registries"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"harbor.xxx.cn:30002"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> // 这个配置项可以不用添加    <span class="token property">"live-restore"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>    <span class="token property">"log-driver"</span><span class="token operator">:</span> <span class="token string">"json-file"</span><span class="token punctuation">,</span>    <span class="token property">"log-opts"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"max-size"</span><span class="token operator">:</span> <span class="token string">"50m"</span><span class="token punctuation">,</span>        <span class="token property">"max-file"</span><span class="token operator">:</span> <span class="token string">"3"</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.5.3 Pod探针</p><p>探针分类：</p><ul><li>startupProbe：k8s 1.16版本后，用于判断容器内应用程序是否启用。如果配置了该项，就会禁止其它探针的运行，优先判断startupProbe的状态，只要探测成功后就不再进行探测。如果程序启动比较慢，建议使用。</li><li>livenessProbe：用于探测容器是否运行，如果探测失败，kubelet会根据配置的重启策略进行处理。如果没有配置该探针，默认就是success。决定容器是否重启</li><li>readinessProbe：用于探测容器内的程序是否健康，其返回值如果为success，那么就代表这个容器已经完全启动，并且程序已经是可以接收流量的状态。</li></ul><p>检测方式分类（每个探针都包含）：</p><ul><li>ExecAction：在容器内执行命令，如果返回值为0，则容器健康。</li><li>TCPSocketAction：通过tcp连接检查容器内端口是否连通，如果是通的，就认为容器健康。（可能存在端口已经启动，但pod中的容器无法连接的情况）</li><li>HTTPGetAction: 通过应用程序暴露的api地址来检测程序是否是正常的，如果返回的状态码为大于等于200到小于400之间，则认为容器健康。（最常用，生产）</li></ul><p>在公司日常开发中一定要求微服务暴露readiness和liveness相对应的接口信息。</p><p>常用命令：</p><pre><code>kubectl get deployment -n kube-systemkubectl edit deploy coredns -n kube-system  # 在线编辑名称为coredns的deployment配置信息watch kubectl get pod   # watch 循环查看某条命令的运行状态kubectl replace -f pod.yaml  # 根据pod.yaml中配置替换已有的资源</code></pre><p>参考问题：</p><p><a href="https://my.oschina.net/zhangshoufu/blog/4871452" target="_blank" rel="noopener">为什么引入startupProbe</a></p><p>（此观点错误） <del>startupProbe配置时，针对Spring Cloud微服务的场景，可以考虑使用ExecAction的方式，判断java进程是否存在，如果java进程已经存在了，可以认为pod正常运行了。</del> 存在进程存在而readiness不工作，造成pod不能重启</p><p>2.5.3.1 探针检查参数配置</p><ul><li>initialDelaySeconds : 60 # 初始化时间，在pod启动后等待60秒后进行检查，不建议设置太长，否则滚动发布周期会拉长，等待时间过长</li><li>timeoutSeconds: 2  # 超时时间，执行命令多长时间内能返回信息，一般设置1到2秒</li><li>periodSeconds : 5  # 检测间隔</li><li>successThreshold : 1 # 检查成功为1次表示就绪，例如接口成功返回信息一次代表成功</li><li>failureThreshold : 2 # 检测失败2次表示未就绪，不要设置为1次，可能存在网络波动的情况导致检测不成功</li></ul><p>pod有时不能直接使用apply和replace进行替换，可能会出现替换不成功的情况，但其它高级资源可以，例如deployment。所以针对pod的操作要先删除，再创建新的。</p><p>readiness和liveness在配置时都使用接口级别的健康检查，这两个接口由服务提供，尽量不使用linux命令进行，例如在command下执行pgrep java命令查看java进程是否存在来判断liveness状态。<br>如果遇到上述情况，会出现以下现象，readiness接口返回信息已经表示服务不可用，而liveness的判断中，java进程始终存在，造成整个pod处于0/1的状态，无法接受流量，也无法进行重启来恢复。</p><p>2.5.4 lifeCycle配置–postStart和preStop</p><p>同probe的区别时，lifeCycle中的配置没有健康检查的各个探针参数配置。</p><p>postStart使用风险：postStart并不能保证在initContainers中的command之后执行，，二者可能同时执行。如果面临一些高危权限的操作，建议将这些操作放在initContainers中的command下面设置并执行。</p><p>postStart常用于创建文件或者创建文件夹这样的操作。</p><p>2.5.4.1 preStop</p><p>terminationGracePeriodSeconds: 30  k8s预留的终止时间，默认30秒，可以自行改动，允许在该时间内处理容器终止后的清理或者收尾工作。<br>设置时，需要放在containers之上。但是和preStop中在</p><p>Pod终止流程</p><p>用户执行删除操作———-1. 变更pod状态为Terminating<br>                   |<br>                   ——2. （存在Service的情况下）Endpoint删除该Pod内的IP地址<br>                   |<br>                   ——3. 执行preStop的指令</p><p>以Spring Cloud中的微服务为例，Eureka组件进行描述。</p><p>pod删除前，预留的时间由terminationGracePeriodSeconds配置信息决定，而如果在preStop中配置命令，例如：</p><pre class="line-numbers language-yaml"><code class="language-yaml">      <span class="token key atrule">preStop</span><span class="token punctuation">:</span>        <span class="token key atrule">exec</span><span class="token punctuation">:</span>          <span class="token key atrule">command</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> sh            <span class="token punctuation">-</span> <span class="token punctuation">-</span>c            <span class="token punctuation">-</span> sleep 90<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>则pod删除前，不会真正sleep 90秒，通过time命令查看执行时间，接近于terminationGracePeriodSeconds设定的时间。</p><p>如果设置sleep低于terminationGracePeriodSeconds配置的时间，而任务恰好能在sleep设置的时间内可以完成，则sleep生效，否则还是以terminationGracePeriodSeconds为准。</p><p>常用命令：</p><pre><code>kubectl get event  # 查看默认namespace的事件time kubectl delete pod nginx  # 查看该条命令的执行时间</code></pre><p>注意：终止进程尽量使用kill <pid>或者kill -15 <pid>，禁止使用kill -9。最佳实践为kill <code>pgrep java</code>，前提是镜像支持pgrep命令。</pid></pid></p><p>2.6 RC &amp;&amp; RS</p><ul><li>Replication Controller : 可以保证Pod副本数达到期望值，可以确保一个Pod或者一组同类Pod总是可用。已经被废弃</li><li>ReplicaSet ：复制集。至此基于集合的标签选择器的下一代RC，主要用作Deployment协调创建、删除和更新Pod。和RC唯一区别是ReplicaSet支持标签选择器</li></ul><p>RC和RS不支持回滚，必须使用更高级资源（Deployment或者DaemonSet）进行管理，再通过RS去管理下面的Pod。</p><p>极少单独使用，主要是配合Deployment、StatefulSet以及DaemonSet去管理。一般建议使用Deployment来自动管理ReplicaSet。</p><p>2.7 Deployment</p><ul><li>Deployment：无状态服务</li><li>StatefulSet：有状态应用，例如Redis主从、RabbitMQ</li><li>DaemonSet：在每个节点上都会启动一个容器</li></ul><p>2.7.1 Deployment概念</p><p>用于部署无状态服务，最常用的控制器。一般用于管理维护企业内部无状态的微服务。</p><p>可以管理多个副本的Pod，实现无缝迁移、自动扩容缩容、自动灾难恢复、一键回滚等功能。</p><p>常用命令：</p><pre><code>kubectl create deployment nginx --image=nginx:1.16.1  # 手动创建deploymentkubectl create deployment nginx --image=nginx:1.16.1 -o yaml &gt; nginx-deployment.yaml   # 将创建的deployment文件导出kubectl replace -f nginx-deployment.yaml  # 通过配置文件替代当前namespace中的deployment配置kubectl edit deployment nginx        # 在线编辑名称为nginx的deployment# 重启deploykubectl patch deployment &lt;deployment-name&gt; \  -p &#39;{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;&lt;container-name&gt;&quot;,&quot;env&quot;:[{&quot;name&quot;:&quot;RESTART_&quot;,&quot;value&quot;:&quot;&#39;$(date +%s)&#39;&quot;}]}]}}}}&#39;# 示例kubectl patch deployment weichai-les-app \  -p &#39;{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;&lt;container-name&gt;&quot;,&quot;env&quot;:[{&quot;name&quot;:&quot;RESTART_&quot;,&quot;value&quot;:&quot;&#39;$(date +%s)&#39;&quot;}]}]}}}}&#39;</code></pre><p>配置文件：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token string">"2021-01-14T13:26:49Z"</span>  <span class="token key atrule">generation</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">progressDeadlineSeconds</span><span class="token punctuation">:</span> <span class="token number">600</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3            </span><span class="token comment" spellcheck="true"># 副本数</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10   </span><span class="token comment" spellcheck="true"># 历史记录保存个数</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>               <span class="token comment" spellcheck="true"># 滚动升级更新策略</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> 25%      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> 25%    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate  <span class="token key atrule">template</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># 从该节点往下配置的信息同pod中的配置项</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token null important">null</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        <span class="token key atrule">terminationMessagePath</span><span class="token punctuation">:</span> /dev/termination<span class="token punctuation">-</span>log        <span class="token key atrule">terminationMessagePolicy</span><span class="token punctuation">:</span> File      <span class="token key atrule">dnsPolicy</span><span class="token punctuation">:</span> ClusterFirst      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Always      <span class="token key atrule">schedulerName</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>scheduler      <span class="token key atrule">securityContext</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：尽量不要在deployment运行状态下直接修改配置中的labels信息，例如selector下matchLabels、template下的labels。无论是增加还是删除，如果修改后，可能会造成ReplicaSet资源无法删除的情况。修改labels参数，会造成启动新的ReplicaSet，而新的RS具有新的label，就无法管理之前的label了。</p><p>注意：可以在~/.kube/config中找到连接k8s的配置文件，配合kubectl命令在任意机器都可以连接k8s集群。</p><p>2.7.2 deployment状态解析</p><pre class="line-numbers language-shell"><code class="language-shell"># # kubectl get pod -o wide# NAME                     READY   STATUS    RESTARTS   AGE   IP               NODE          NOMINATED NODE   READINESS GATES# nginx-7f785d94f5-74fr8   1/1     Running   0          19m   172.175.44.13    k8s-node-01   <none>           <none># nginx-7f785d94f5-tmqfd   1/1     Running   0          18m   172.175.44.14    k8s-node-01   <none>           <none># nginx-7f785d94f5-vcsnk   1/1     Running   0          25m   172.163.98.145   k8s-node-02   <none>           <none>$ kubectl get deployment -o wideNAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTORnginx   3/3     3            3           29m   nginx        nginx:1.16.1   app=nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>NAME  — deployment名称，同一个namespace下不能使用相同的名称</li><li>READY — pod的状态，上面命令中存在3个副本，3个已经启动</li><li>UP-TO-DATE  —-  已经达到期望状态的被更新的副本数</li><li>AVAILABLE —–  已经可以用的副本数</li><li>AGE    ——-   显示应用程序运行的时间</li><li>CONTAINERS  —–  容器名称</li><li>IMAGES   —— 容器的镜像</li><li>SELECTOR  —— 管理Pod的标签</li></ul><p>常用命令</p><pre><code>kubectl get pod --show-labels # 查看pod的Labelskubectl get deploy -o yaml | grep image # 以yaml的方式输出配置信息，并且筛选查看image内容kubectl set image deploy nginx nginx=nginx:1.15.2 --record  # --record 记录当前更改的参数，根据record下的信息可以查看各个版本的参数信息kubectl rollout status deploy nginx   # 查看滚动更新的进度</code></pre><p>注意：deployment通过ReplicaSet来管理Pod。</p><p>如何触发deployment的更新，生成新的ReplicaSet？只有在修改配置文件下的spec节点下的templete配置信息后。</p><p>2.7.3 滚动更新示意</p><p>修改nginx的docker image版本后，重新生成新的ReplicaSet，然后更新Pod信息。</p><p>修改命令如下：</p><pre><code>kubectl set image deploy nginx nginx=nginx:1.15.2 --record</code></pre><p>通过describe命令查看如下：</p><pre><code>$ kubectl describe deploy nginx......Events:  Type    Reason             Age    From                   Message  ----    ------             ----   ----                   -------  Normal  ScalingReplicaSet  57m    deployment-controller  Scaled up replica set nginx-7f785d94f5 to 2  Normal  ScalingReplicaSet  56m    deployment-controller  Scaled up replica set nginx-7f785d94f5 to 3  Normal  ScalingReplicaSet  6m42s  deployment-controller  Scaled up replica set nginx-66bbc9fdc5 to 1  Normal  ScalingReplicaSet  6m41s  deployment-controller  Scaled down replica set nginx-7f785d94f5 to 2  Normal  ScalingReplicaSet  6m41s  deployment-controller  Scaled up replica set nginx-66bbc9fdc5 to 2  Normal  ScalingReplicaSet  6m40s  deployment-controller  Scaled down replica set nginx-7f785d94f5 to 1  Normal  ScalingReplicaSet  6m40s  deployment-controller  Scaled up replica set nginx-66bbc9fdc5 to 3  Normal  ScalingReplicaSet  6m38s  deployment-controller  Scaled down replica set nginx-7f785d94f5 to 0</code></pre><p>注意：在更新手段上，尽可能使用kubectl edit命令或者修改yaml格式的配置文件使用kubectl replace命令更新deployment。</p><p>微服务版本发布时，可以直接使用set命令进行更新。</p><p>2.7.4 回滚示意</p><p>查看滚动更新的历史记录</p><pre><code>$ kubectl rollout history deployment nginxdeployment.apps/nginxREVISION  CHANGE-CAUSE1         &lt;none&gt;2         kubectl set image deploy nginx nginx=nginx:1.15.2 --record=true3         kubectl set image deploy nginx nginx=nginx:1.17.7 --record=true</code></pre><p>回滚到上一个版本</p><pre><code>$ kubectl rollout undo deployment nginx</code></pre><p>查看指定版本的详细信息</p><pre><code>$ kubectl rollout history deployment nginx --revision=3deployment.apps/nginx with revision #3Pod Template:  Labels:       app=nginx        pod-template-hash=6d96fd594b  Annotations:  kubernetes.io/change-cause: kubectl set image deploy nginx nginx=nginx:1.17.7 --record=true  Containers:   nginx:    Image:      nginx:1.17.7    Port:       &lt;none&gt;    Host Port:  &lt;none&gt;    Environment:        &lt;none&gt;    Mounts:     &lt;none&gt;  Volumes:      &lt;none&gt;</code></pre><p>回滚到指定版本</p><pre><code>kubectl rollout history deployment nginx --to-revision=3</code></pre><p>2.7.5 扩容和缩容</p><p>可以对Deployment、ReplicaSet进行扩容</p><pre><code>kubectl scale --replicas=3 deployment nginx</code></pre><p>注意：扩容和缩容命令不会产生新的ReplicaSet（未修改spec.templete下的内容），但仅限于预期内的扩容操作。有关指标信息引起的自动扩容和缩容操作，使用HPA实现。</p><p>2.7.6 更新暂停和恢复</p><p>更新多出内容只触发一次更新。</p><p>第一种方式，直接通过kubectl edit命令修改deployment的配置信息</p><pre class="line-numbers language-shell"><code class="language-shell">kubectl edit deploy nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>第二种方式，更新暂停和恢复</p><pre class="line-numbers language-shell"><code class="language-shell">// 更新暂停命令kubectl rollout pause deploy nginx// 多次进行set// 修改镜像版本kubectl set image deploy nginx nginx=nginx:1.15.5 --record// 修改镜像pod数量kubectl scale --replicas=2 deployment nginx// 修改cpu配置kubectl set resources deploy nginx --limits=cpu=200m,memory=120Mi  --requests=cpu=10m,memory=16Mi// 查看配置信息kubectl get deploy -oyaml// 更新恢复命令kubectl rollout resume deploy nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.7.7 滚动更新策略</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token punctuation">...</span><span class="token punctuation">...</span>  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>      <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>        <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> 25%       <span class="token comment" spellcheck="true"># 可以超过期望值的最大Pod数，可选，默认25%，可设置数字或者百分比，若该值为0，则maxUnavailable不能为0</span>        <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> 25%   <span class="token comment" spellcheck="true"># 指定在回滚或者更新时最大不可用的Pod数量，可选，默认25%，可设置数字或者百分比，若该值为0，则maxSurge不能为0</span>      <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate   <span class="token comment" spellcheck="true"># 更新deployment的方式，默认RollingUpdate（滚动更新），其它还有Recreate（先删除旧的Pod，再创建新的Pod）</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其它：<br>.spec.revisionHistoryLimit: 设置保留ReplicaSet历史记录，即revision的数量，设置为0，则不保留历史记录。<br>.spec.minReadySeconds: 可选参数，指定新创建的Pod在没有任何容器崩溃的情况下，视为其状态已经Ready（准备完成）的最小的秒数，默认为0，即一旦被创建就视为Pod可用。</p><p>2.8 StatefulSet</p><p>StatefulSet (有状态集，缩写为sts）常用于部署有状态的且需要有序启动的应用程序，比如在进行SpringCloud项目容器化时，Eureka的部署是比较适合用StatefblSet部署方式的，可以给每个Eureka实例创建一个唯一且固定的标识符，并且每个Eureka实例无需配置多余的Service，其余Spring Boot应用可以直接通过Eureka的Headless Service即可进行注册。</p><p>2.8.1 基本概念</p><p>StatefblSet 主要用于管理有状态应用程序的工作负戟API对象。比如在生产环境中.可以部署ElasticSearch集群、MongoDB集群或者需要持久化的RabbitMQ集群、Redis集群、Kafka集群和zookeeper集群等。和Deployment类似，一个StatefulSet也同样管理着基于相同容器规范的Pod。不同的是，StatefulSet为每个Pod维护了一个粘性标识。这些Pod是根据相同的规范创建的，但是不可互换.每个 Pod 都有一个持久的标识符.在重新调度时也会保留，一般格式为 StatefulSetName-Number。比如定义一个名字是Redis-Sentinel的StatefulSet，指定创建三个Pod，那么创建出来的<br>Pod 名字就为 Redis-Sentinel-0、Redis-Sentinel-1、Redis-Sentinel-2。而 StatefiilSet 创建的 Pod 一般使用Headless Service (无头服务）进行通信。和普通的Service的区别在于 Headless Service没有ClusterIP，它使用的是 Endpoint进行互相通信。</p><p>Headless—般的格式为：</p><pre><code>statefulSetName-{0...N-1}.serviceName.namespace.svc.cluster.local</code></pre><p>说明：</p><ul><li>serviceName为Headless Service的名字，创建StatefulSet时，必须指定Headless Service的名称;</li><li>0…N-1为Pod所在的序号，从0开始到N-1;</li><li>statefulSetName为StatefulSet的名字:</li><li>namespace为服务所在的命名空间;</li><li>.cluster.local为Cluster Domain(集群域)。</li></ul><p>假如公司某个项目需要在Kubernetes中部署一个主从模式的Redis，此时使用StatefulSet部署就极为合适，因为StatefulSet启动时，只有当前一个容器完全启动时，后一个容器才会被调度，并且每个容器的标识符是固定的，那么就可以通过标识符来断定当前Pod的角色。</p><p>比如用一个名为redis-ms的StatefulSet部署主从架构的Redis，该StatefulSet部署在名为public-service的命名空间中。第一个容器启动时，它的标识符为redis-ms-0，并且Pod内主机名也为redis-ms-0，此时就可以根据主机名来判断，当主机名为redis-ms-0的容器作为Redis的主节点，其余容器作为从节点，那么Slave连接Master主机配置就可以使用不会更改的Master的Headless Service，此时Redis从节点(Slave)配置的链接信息为:</p><pre><code>redis-ms-0.redis-ms.public-service.svc.cluster.local</code></pre><p>2.8.2 注意事项</p><p>一般StatefulSet用于有以下一个或者多个需求的应用程序:</p><ul><li>需要稳定的独一无二的网络标识符。</li><li>需要持久化数据。</li><li>需要有序的、优雅的部署和扩展。</li><li>需要有序的自动滚动更新。</li></ul><p>如果应用程序不需要任何稳定的标识符或者有序的部署、删除或者扩展，应该使用无状态的控制器部署应用程序，比如Deployment或者ReplicaSet。</p><p>StatefulSet是Kubernetes 1.9版本之前的beta资源，在1.5版本之前的任何Kubernetes版本都没有。</p><p>Pod所用的存储必须由PersistentVolume Provisioner(持久化卷配置器)根据请求配置StorageClass，或者由管理员预先配置，当然也可以不配置存储。</p><p>为了确保数据安全，删除和缩放StatefulSet不会删除与StatefulSet关联的卷，可以手动选择性地删除PVC和PV(关于PV和PVC请参考2.2.12节)</p><p>StatefulSet目前使用Headless Service(无头服务)负责Pod的网络身份和通信，需要提前创建此服务。</p><p>删除一个StatefulSet时，不保证对Pod的终止，要在StatefulSet中实现Pod的有序和正常终止，可以在删除之前将StatefulSet的副本缩减为0。</p><p>2.8.3 创建一个StatefulSet</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>    <span class="token key atrule">name</span><span class="token punctuation">:</span> web  <span class="token key atrule">clusterIP</span><span class="token punctuation">:</span> None  <span class="token comment" spellcheck="true"># None--该Service不会存在ClusterIp，但是依旧可以通过Headless Service访问到StatefulSet</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> web<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx <span class="token comment" spellcheck="true"># has to match .spec.template.metadata.labels</span>  <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> <span class="token string">"nginx"</span>   <span class="token comment" spellcheck="true"># 注意：该serviceName必须指定一个已经存在的Service</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">2 </span><span class="token comment" spellcheck="true"># by default is 1</span>  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx <span class="token comment" spellcheck="true"># has to match .spec.selector.matchLabels</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>          <span class="token key atrule">name</span><span class="token punctuation">:</span> web<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建StatefulSet：</p><pre><code>kubectl create -f nginx-statefulset.yaml</code></pre><p>扩容StatefulSet：</p><pre><code>kubectl scale --replicas=3 sts web</code></pre><p>启动busybox访问我们前面创建的StatefulSet：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> busybox  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> app    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox<span class="token punctuation">:</span><span class="token number">1.28</span>    <span class="token key atrule">command</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> sleep      <span class="token punctuation">-</span> <span class="token string">"3600"</span>    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> ifNotPresent  <span class="token key atrule">restartPoicy</span><span class="token punctuation">:</span> Always<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>命令如下：</p><pre><code>kubectl apply -f busybox.yaml// 进入容器内执行命令的操作kubectl exec -it busybox -- sh/ # nslookup web-0.nginxServer:    10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName:      web-0.nginxAddress 1: 172.163.98.154 web-0.nginx.default.svc.cluster.local</code></pre><p>这里直接将Service解析到Pod的ip地址了，由于ClusterIP配置的为None，所以不需要转换即可解析。</p><p>2.8.4 StatefulSet扩容缩容</p><p>如果启动过程中，名称为web-0、web-1的Pod已经启动完毕，准备启动web-2的Pod时，web-0 Pod挂掉了，这时候web-2不会启动，而是等待web-0重新恢复后再进行web-2的启动。也就是说，在创建下一个pod之前，前面创建的pod存在任何的故障，下一个pod都不会被创建，而是等待所有的pod变为ready之后再启动下一个pod。</p><p>在Pod删除时，会先从web-2 pod开始，逐步删除web-1、web-0 这两个Pod。</p><p>查看所有的pod动态变化的状态：</p><pre><code>kubectl get pod -l app=nginx -w// 或者是使用watch kubectl get pod -l app=nginx</code></pre><p>通过改变副本数，来查看整个变化趋势！</p><pre><code>// 扩容操作kubectl scale --replicas=5 sts web// 缩容操作kubectl scale --replicas=3 sts web</code></pre><p>2.8.5 StatefulSet更新策略</p><p>deployment为随机无序更新</p><p>StatefulSet的更新方式–RollingUpdate、OnDelete</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token string">"2021-01-17T03:45:43Z"</span>  <span class="token key atrule">generation</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">podManagementPolicy</span><span class="token punctuation">:</span> OrderedReady  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token null important">null</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>          <span class="token key atrule">name</span><span class="token punctuation">:</span> web          <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP        <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        <span class="token key atrule">terminationMessagePath</span><span class="token punctuation">:</span> /dev/termination<span class="token punctuation">-</span>log        <span class="token key atrule">terminationMessagePolicy</span><span class="token punctuation">:</span> File      <span class="token key atrule">dnsPolicy</span><span class="token punctuation">:</span> ClusterFirst      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Always      <span class="token key atrule">schedulerName</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>scheduler      <span class="token key atrule">securityContext</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">updateStrategy</span><span class="token punctuation">:</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">partition</span><span class="token punctuation">:</span> <span class="token number">0</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate<span class="token key atrule">status</span><span class="token punctuation">:</span>  <span class="token key atrule">collisionCount</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">currentReplicas</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">currentRevision</span><span class="token punctuation">:</span> web<span class="token punctuation">-</span>78c4f54fd4  <span class="token key atrule">observedGeneration</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">readyReplicas</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">updateRevision</span><span class="token punctuation">:</span> web<span class="token punctuation">-</span>78c4f54fd4  <span class="token key atrule">updatedReplicas</span><span class="token punctuation">:</span> <span class="token number">5</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>重点在于，更新时，sts会先更新web-2 的Pod，待web-2更新完成后，再去更新web-1。如果这时候web-0被删除或者因意外不可用，则更新不会继续，会等待web-0恢复后再进行更新。也就是说，在RollingUpdate模式下，StatefulSet是正序去创建，倒序去更新！</p><p>查看更新状态：</p><pre><code>// 不太准确的方式kubectl rollout status sts web// 精准模式watch kubectl get pod -l app=nginx</code></pre><p>使用edit方式修改sts的镜像信息：</p><pre><code>kubectl edit sts web</code></pre><p>OnDelete方式更新如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml">  <span class="token punctuation">...</span><span class="token punctuation">...</span>  <span class="token key atrule">updateStrategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> OnDelete  <span class="token punctuation">...</span><span class="token punctuation">...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>更改为OnDelete模式后，只有把Pod进行删除操作，它才会去更新该Pod。</p><pre><code>kubectl delete pod web-2</code></pre><p>此时查看StatefulSet下的不同的Pod，查看修改的镜像版本信息:</p><pre><code>kubectl get pod web-2 -oyaml | grep imagekubectl get pod web-0 -oyaml | grep image</code></pre><p>RollingUpdate模式下设置partition信息，代表更新大于partition个数的Pod。</p><p>分段更新：partion设置为2，保留尾缀小于2的pod，也就是不更新web-0、web-1，更新web-2、web-3、web-4。</p><p>—————|<br>web-0   web-1  | web-2   web-3   web-4<br>—-不更新—–| ———更新———|</p><p>这样就实现了简单的灰度发布。</p><p>如果partition设置为0，则表示保留尾缀小于0的pod，由于尾缀小于0的pod不存在，故进行全部更新。</p><p>使用edit方式修改sts的镜像信息：</p><pre><code>kubectl edit sts web// 修改rollingUpdate并且修改partition为0</code></pre><p>2.8.6 StatefulSet级联删除和非级联删除</p><p>级联删除，删除StatefulSet时同时删除Pod。<br>非级联删除，删除StatefulSet时不删除Pod，日常用的少，仅仅作为了解。</p><p>默认使用级联删除</p><p>测试级联删除</p><pre><code>kubectl delete sts web</code></pre><p>测试非级联删除</p><pre><code>kubectl delete sts web --cascade=false// 新版本下使用kubectl delete sts web --cascade=orphan</code></pre><p>非级联删除下，Pod变成了孤儿Pod，此时删除Pod不会被重建。</p><p>2.9 守护进程服务DaemonSet</p><p>2.9.1 DaemonSet的概念和创建</p><p>DaemonSet：守护进程集，缩写为ds，在所有节点或者匹配的节点上都部署一个容器。</p><p>calico使用DaemonSet进行部署。</p><p>应用场景：</p><ul><li>运行集群存储的daemon，比如ceph、glusterd</li><li>节点的CNI网络插件，calico</li><li>节点日志的收集，fluentd或者filebeat</li><li>节点的监控，node-exporter</li><li>服务暴露：部署一个ingress nginx</li></ul><p>创建DaemonSet</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token null important">null</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        <span class="token key atrule">terminationMessagePath</span><span class="token punctuation">:</span> /dev/termination<span class="token punctuation">-</span>log        <span class="token key atrule">terminationMessagePolicy</span><span class="token punctuation">:</span> File      <span class="token key atrule">dnsPolicy</span><span class="token punctuation">:</span> ClusterFirst      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Always      <span class="token key atrule">schedulerName</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>scheduler      <span class="token key atrule">securityContext</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建命令：</p><pre><code>kubectl create -f nginx-daemonset.yaml</code></pre><p>对node打标签，将DaemonSet部署到特定节点上。（注意）</p><pre><code>// 对node打标签kubectl label node k8s-node-01 k8s-node-02 ds=true</code></pre><p>修改配置文件</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">template</span><span class="token punctuation">:</span>  <span class="token punctuation">...</span>..  <span class="token key atrule">spec</span><span class="token punctuation">:</span>    <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>      <span class="token key atrule">ds</span><span class="token punctuation">:</span> <span class="token string">"true"</span> <span class="token comment" spellcheck="true"># 这个true要写为字符串的形式</span>    <span class="token key atrule">containers</span><span class="token punctuation">:</span><span class="token punctuation">...</span><span class="token punctuation">...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>重新配置生效：</p><pre><code>kubectl replace -f nginx-daemonset.yaml</code></pre><p>配置完成后，DaemonSet会先进行停止多余的Pod（不含ds=true标签的node节点上的Pod将会被停止），然后再对已保留的Pod进行一次滚动更新，产生一次版本记录。</p><p>2.9.2 DaemonSet的更新和回滚</p><p>DaemonSet默认的更新配置如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml">  <span class="token key atrule">updateStrategy</span><span class="token punctuation">:</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">1  </span><span class="token comment" spellcheck="true"># 建议最大不可用pod数量写为1，如果升级或者回滚出现问题，可以缩小影响范围</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>更新DaemonSet操作：</p><pre><code>kubectl set image ds nginx nginx=nginx:1.15.2 --recordkubectl set image ds nginx nginx=nginx:1.16.1 --record</code></pre><p>注意：建议使用OnDelete方式去进行更新，这样在更新时，可以先操作单个节点的Pod去进行更新，不会影响其它节点正在运行的Pod。  </p><p>2.10 Label和Selector</p><p>Label：对k8s中各种资源进行分类分组，添加一个具有特别属性的一个标签。<br>Selector： 通过一些过滤的语法，查找到对应标签的资源。</p><p>对一个节点添加label：</p><pre><code>kubectl lable node k8s-node-02 region=subnet7</code></pre><p>通过label筛选出该节点：</p><pre><code>kubectl get node -l region=subnet7</code></pre><p>查看所有容器的labels</p><pre><code>kubectl get po -A --show-labels</code></pre><p>注意：谨慎修改经常变动资源的标签，例如Pod或者Deployment，如果修改后，再进行部署，该修改的标签无效。</p><p>labels实践：</p><pre><code>// Pod新增labelkubectl label pod busybox state=CN// Pod删除标签，标签名称后跟上&quot;-&quot;号kubectl label pod busybox state-// 根据标签筛选Podkubectl get pod -l app=busybox// 修改已有的标签信息，使用--overwritekubectl label pod busybox app=busybox-2 --overwrite</code></pre><p>selector实践：</p><pre><code>// 查看所有pod的labelskubectl get pod -A --show-labels// 多条件查找，查找带有k8s-app=metrics-server和k8s-app=dashboard-metrics-scraper的podkubectl get pod -A -l &quot;k8s-app in (metrics-server,dashboard-metrics-scraper)&quot;// 设置pod的label为version=1.0kubectl label pod web-0 version=1.0// 筛选pod，label中version不等于1.0且app为nginx的podkubectl get pod -l version!=1.0,app=nginx</code></pre><p>2.11 Service</p><p>2.11.1 南北流量和东西流量</p><p>在Service Mesh微服务架构中，我们常常会听到东西流量和南北流量两个术语。</p><p>南北流量（NORTH-SOUTH traffic）和东西流量（EAST-WEST traffic）是数据中心环境中的网络流量模式。下面我们通过一个例子来理解这两个术语。</p><p>假设我们尝试通过浏览器访问某些Web应用。Web应用部署在位于某个数据中心的应用服务器中。在多层体系结构中，典型的数据中心不仅包含应用服务器，还包含其他服务器，如负载均衡器、数据库等，以及路由器和交换机等网络组件。假设应用服务器是负载均衡器的前端。</p><p>当我们访问web应用时，会发生以下类型的网络流量：一个是客户端（位于数据中心一侧的浏览器）与负载均衡器（位于数据中心）之间的网络流量；另一个是负载均衡器、应用服务器、数据库等之间的网络流量，它们都位于数据中心。</p><ul><li>南北流量</li></ul><p>在这个例子中，前者即即客户端和服务器之间的流量被称为南北流量。简而言之，南北流量是server-client流量。</p><ul><li>东西流量</li></ul><p>不同服务器之间的流量与数据中心或不同数据中心之间的网络流被称为东西流量。简而言之，东西流量是server-server流量。</p><p>当下，东西流量远超南北流量，尤其是在当今的大数据生态系统中，比如Hadoop生态系统（大量server驻留在数据中心中，用map reduce处理），server-server流量远大于server-client流量。</p><p>大家可能会好奇，东西南北，为什么这么命名。</p><p>该命名来自于绘制典型network diagrams的习惯。在图表中，通常核心网络组件绘制在顶部（NORTH），客户端绘制在底部（SOUTH），而数据中心内的不同服务器水平（EAST-WEST）绘制。</p><p>2.11.2 传统架构服务访问方式和k8s中服务的访问方式</p><p>传统架构：</p><ul><li>nginx反向代理，upstream</li><li>Spring Cloud 注册中心，路由表</li></ul><p>k8s：</p><p>服务之间访问，通过Service层面进行访问，利用Service匹配后端Pod的label，来决定流量走向。外部访问整个系统，通过ingress代理到Service层面，再由Service层面匹配后向的Pod对外提供服务。</p><p>其实本质都是通过路由表的方式确定服务的访问，只是路由表的表达不同。</p><p>2.11.3 什么是Service</p><p>为什么不通过ip地址的方式访问pod？Pod被删除后，ip地址就会发生变化，重建后的pod就会更新ip地址，原有的访问ip就失效了。</p><p>Service可以简单的理解为逻辑上的一组Pod，一种可以访问Pod的策略，而且其它Pod可以通过Service访问到这个Service代理的Pod。相对于Pod而言，它会有一个固定的名称，一旦创建就固定不变。对比Pod而言更稳定。</p><p>常用命令：</p><pre><code>kubectl get svc -A // 获取集群中所有的Service信息</code></pre><p>当Service创建后，会创建一个同名的Endpoint，该Endpoint会指向Pod的ip地址信息。</p><p>2.11.3 定义一个Service</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> dns   <span class="token comment" spellcheck="true"># Service端口的名称，最好使用端口用途区分</span>    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80    </span><span class="token comment" spellcheck="true"># Service自己的端口, serviceA-->serviceB    http://serviceB</span>    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP  <span class="token comment" spellcheck="true"># UDP、TCP、STCP，默认TCP</span>    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">80 </span><span class="token comment" spellcheck="true"># 后端应用的端口，两个端口可以不一致，但是此处的端口必须保证填写正确</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> https    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">443</span>    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP    <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">443</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>       <span class="token comment" spellcheck="true"># 通过这个Selector来过滤后向的Pod信息</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">sessionAffinity</span><span class="token punctuation">:</span> None  <span class="token key atrule">type</span><span class="token punctuation">:</span> ClusterIP<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建Service：</p><pre><code>kubectl create -f Service.yaml</code></pre><p>查看pod的日志信息</p><pre><code>kubectl logs -f --tail=100 nginx-7f785d94f5-p9qxp</code></pre><p>注意：访问Service时，如果不在同一个namespace中，需要在访问时添加namespace名称，例如：<a href="http://nginx-svc.default。" target="_blank" rel="noopener">http://nginx-svc.default。</a></p><p>应用之间的调用，尽量不要使用跨namespace的调用。</p><p>Service创建之后，无需关心后端应用有什么变化，直接通过Service访问即可。</p><p>2.11.4 通过Service代理k8s外部应用</p><p>使用场景：</p><ul><li>希望在生产环境中使用某个固定的名称而非IP地址访问外部的中间件服务。</li><li>希望Service执行另一个namespace中或者其他集群中的服务。</li><li>某个项目正在迁移至k8s集群，但一部分服务仍然在集群外部，此时可以使用Service代理至k8s集群外部的服务。</li></ul><p>代理外部服务的Service</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc<span class="token punctuation">-</span>external  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc<span class="token punctuation">-</span>external<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http       <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80    </span>    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">80 </span>  <span class="token comment" spellcheck="true"># 未使用selector，需要自行创建endpoint</span>  <span class="token key atrule">sessionAffinity</span><span class="token punctuation">:</span> None  <span class="token key atrule">type</span><span class="token punctuation">:</span> ClusterIP<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建外部地址的endpoint，以代理百度首页为例：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Endpoints<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc<span class="token punctuation">-</span>external  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc<span class="token punctuation">-</span>external  <span class="token comment" spellcheck="true"># endpoint的名称必须和Service的名称一致，否则建立不了连接</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">subsets</span><span class="token punctuation">:</span><span class="token punctuation">-</span> <span class="token key atrule">addresses</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">ip</span><span class="token punctuation">:</span> 220.181.38.148 <span class="token comment" spellcheck="true"># 此处为外部服务的IP地址</span>  <span class="token comment" spellcheck="true"># 此处端口信息，必须与Service中设置的端口一致，协议一致，名称一致</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> http    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80      </span><span class="token comment" spellcheck="true"># 此处为外部服务的端口号</span>    <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建Service和Endpoint：</p><pre><code>kubectl create -f service-external.yamlkubectl create -f service-external-endpoint.yaml</code></pre><p>如果后续发生了地址变更，直接修改Endpoint的地址，不需要修改Service的配置。</p><p>注意：apply = replace + create</p><p>反向代理外部域名</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc<span class="token punctuation">-</span>external<span class="token punctuation">-</span>address  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc<span class="token punctuation">-</span>external<span class="token punctuation">-</span>address<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> ExternalName  <span class="token key atrule">externalName</span><span class="token punctuation">:</span> www.baidu.com<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>少用，出现了跨域的情况，访问<a href="http://nginx-svc-external-address，由于先解析到nginx-svc-external-address这个地址，再解析到百度官网地址，导致的跨域。" target="_blank" rel="noopener">http://nginx-svc-external-address，由于先解析到nginx-svc-external-address这个地址，再解析到百度官网地址，导致的跨域。</a></p><p>2.11.5 Service类型</p><ul><li>ClusterIP，在集群内部使用，也是默认值。集群内部可以进行访问，集群外部无法访问。</li><li>ExternalName，通过返回定义的CNAME别名，用的少</li><li>NodePort: 在所有安装了kube-proxy的节点上开启一个端口，此端口可以代理至后端Pod，然后集群外部可以使用节点的IP地址和NodePort端口号访问到集群Pod的服务。默认端口范围为：30000至32767。限制性使用，对外有限暴露端口号。常见于Redis、RabbitMQ等工具集群化部署后，对外提供服务。（NodePort性能有限）。</li><li>LoadBalancer：使用云提供商的负载均衡器公开服务。</li></ul><p>2.12 Ingress</p><p>为何少使用NodePort？在Service比较多的时候，NodePort的性能会急剧下降；过多的Pod带来端口管理的问题。</p><p>2.12.1 Ingress概念</p><p>Ingress用于实现用域名的方式访问k8s的内部应用。</p><p>服务发布的方式：负载均衡设施 –&gt; Ingress –&gt; Service –&gt; Deployment/Pod/DaemonSet/StatefulSet</p><p>番外：什么是eBPF？</p><p>2.12.2 helm安装Ingress</p><p>安装helm3：</p><pre><code>// 下载helm 3.5.0 的压缩包wget https://get.helm.sh/helm-v3.5.0-linux-amd64.tar.gz// 解压缩tar -zxvf helm-v3.5.0-linux-amd64.tar.gz// 将运行文件传入/usr/local/bin/中mv linux-amd64/helm /usr/local/bin/helm// 最后执行测试helm help</code></pre><p>安装ingress</p><pre><code>// 添加ingress仓库helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx// 查看已添加仓库helm repo list// 搜索ingress安装包helm search repo ingress-nginx// 下载安装包helm pull ingress-nginx/ingress-nginx// 解压安装包tar -zxvf ingress-nginx-3.20.1.tgz</code></pre><p>解压完成后需要修改values.yaml文件，修改后如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true">## nginx configuration</span><span class="token comment" spellcheck="true">## Ref: https://github.com/kubernetes/ingress-nginx/blob/master/controllers/nginx/configuration.md</span><span class="token comment" spellcheck="true">##</span><span class="token comment" spellcheck="true">## Overrides for generated resource names</span><span class="token comment" spellcheck="true"># See templates/_helpers.tpl</span><span class="token comment" spellcheck="true"># nameOverride:</span><span class="token comment" spellcheck="true"># fullnameOverride:</span><span class="token key atrule">controller</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> controller  <span class="token key atrule">image</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># repository: google_containers/ingress-nginx/controller</span>    <span class="token key atrule">repository</span><span class="token punctuation">:</span> pollyduan/ingress<span class="token punctuation">-</span>nginx<span class="token punctuation">-</span>controller    <span class="token key atrule">tag</span><span class="token punctuation">:</span> <span class="token string">"v0.43.0"</span>    <span class="token comment" spellcheck="true"># digest: sha256:9bba603b99bf25f6d117cf1235b6598c16033ad027b143c90fa5b3cc583c5713</span>    <span class="token key atrule">pullPolicy</span><span class="token punctuation">:</span> IfNotPresent    <span class="token comment" spellcheck="true"># www-data -> uid 101</span>    <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">101</span>    <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token comment" spellcheck="true"># Configures the ports the nginx-controller listens on</span>  <span class="token key atrule">containerPort</span><span class="token punctuation">:</span>    <span class="token key atrule">http</span><span class="token punctuation">:</span> <span class="token number">80</span>    <span class="token key atrule">https</span><span class="token punctuation">:</span> <span class="token number">443</span>  <span class="token comment" spellcheck="true"># Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/</span>  <span class="token key atrule">config</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">## Annotations to be added to the controller config configuration configmap</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">configAnnotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># Will add custom headers before sending traffic to backends according to https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/customization/custom-headers</span>  <span class="token key atrule">proxySetHeaders</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># Will add custom headers before sending response traffic to the client according to: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#add-headers</span>  <span class="token key atrule">addHeaders</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># Optionally customize the pod dnsConfig.</span>  <span class="token key atrule">dnsConfig</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># Optionally change this to ClusterFirstWithHostNet in case you have 'hostNetwork: true'.</span>  <span class="token comment" spellcheck="true"># By default, while using host network, name resolution uses the host's DNS. If you wish nginx-controller</span>  <span class="token comment" spellcheck="true"># to keep resolving names inside the k8s network, use ClusterFirstWithHostNet.</span>  <span class="token comment" spellcheck="true"># 使用hostNetwork需要修改为这个值</span>  <span class="token key atrule">dnsPolicy</span><span class="token punctuation">:</span> ClusterFirstWithHostNet  <span class="token comment" spellcheck="true"># Bare-metal considerations via the host network https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#via-the-host-network</span>  <span class="token comment" spellcheck="true"># Ingress status was blank because there is no Service exposing the NGINX Ingress controller in a configuration using the host network, the default --publish-service flag used in standard cloud setups does not apply</span>  <span class="token key atrule">reportNodeInternalIp</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token comment" spellcheck="true"># Required for use with CNI based kubernetes installations (such as ones set up by kubeadm),</span>  <span class="token comment" spellcheck="true"># since CNI and hostport don't mix yet. Can be deprecated once https://github.com/kubernetes/kubernetes/issues/23920</span>  <span class="token comment" spellcheck="true"># is merged</span>  <span class="token comment" spellcheck="true"># 推荐使用hostNetwork方式去部署，直接使用宿主机的端口号，性能更好。</span>  <span class="token comment" spellcheck="true"># 可以通过hostNetwork的方式部署到指定的节点上</span>  <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token comment" spellcheck="true">## Use host ports 80 and 443</span>  <span class="token comment" spellcheck="true">## Disabled by default</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">hostPort</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token key atrule">ports</span><span class="token punctuation">:</span>      <span class="token key atrule">http</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">https</span><span class="token punctuation">:</span> <span class="token number">443</span>  <span class="token comment" spellcheck="true">## Election ID to use for status update</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">electionID</span><span class="token punctuation">:</span> ingress<span class="token punctuation">-</span>controller<span class="token punctuation">-</span>leader  <span class="token comment" spellcheck="true">## Name of the ingress class to route through this controller</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">ingressClass</span><span class="token punctuation">:</span> nginx  <span class="token comment" spellcheck="true"># labels to add to the pod container metadata</span>  <span class="token key atrule">podLabels</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">#  key: value</span>  <span class="token comment" spellcheck="true">## Security Context policies for controller pods</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">podSecurityContext</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">## See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for</span>  <span class="token comment" spellcheck="true">## notes on enabling and using sysctls</span>  <span class="token comment" spellcheck="true">###</span>  <span class="token key atrule">sysctls</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># sysctls:</span>  <span class="token comment" spellcheck="true">#   "net.core.somaxconn": "8192"</span>  <span class="token comment" spellcheck="true">## Allows customization of the source of the IP address or FQDN to report</span>  <span class="token comment" spellcheck="true">## in the ingress status field. By default, it reads the information provided</span>  <span class="token comment" spellcheck="true">## by the service. If disable, the status field reports the IP address of the</span>  <span class="token comment" spellcheck="true">## node or nodes where an ingress controller pod is running.</span>  <span class="token key atrule">publishService</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token comment" spellcheck="true">## Allows overriding of the publish service to bind to</span>    <span class="token comment" spellcheck="true">## Must be &lt;namespace>/&lt;service_name></span>    <span class="token comment" spellcheck="true">##</span>    <span class="token key atrule">pathOverride</span><span class="token punctuation">:</span> <span class="token string">""</span>  <span class="token comment" spellcheck="true">## Limit the scope of the controller</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">scope</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> <span class="token string">""</span>   <span class="token comment" spellcheck="true"># defaults to .Release.Namespace</span>  <span class="token comment" spellcheck="true">## Allows customization of the configmap / nginx-configmap namespace</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">configMapNamespace</span><span class="token punctuation">:</span> <span class="token string">""</span>   <span class="token comment" spellcheck="true"># defaults to .Release.Namespace</span>  <span class="token comment" spellcheck="true">## Allows customization of the tcp-services-configmap</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">tcp</span><span class="token punctuation">:</span>    <span class="token key atrule">configMapNamespace</span><span class="token punctuation">:</span> <span class="token string">""</span>   <span class="token comment" spellcheck="true"># defaults to .Release.Namespace</span>    <span class="token comment" spellcheck="true">## Annotations to be added to the tcp config configmap</span>    <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">## Allows customization of the udp-services-configmap</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">udp</span><span class="token punctuation">:</span>    <span class="token key atrule">configMapNamespace</span><span class="token punctuation">:</span> <span class="token string">""</span>   <span class="token comment" spellcheck="true"># defaults to .Release.Namespace</span>    <span class="token comment" spellcheck="true">## Annotations to be added to the udp config configmap</span>    <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># Maxmind license key to download GeoLite2 Databases</span>  <span class="token comment" spellcheck="true"># https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases</span>  <span class="token key atrule">maxmindLicenseKey</span><span class="token punctuation">:</span> <span class="token string">""</span>  <span class="token comment" spellcheck="true">## Additional command line arguments to pass to nginx-ingress-controller</span>  <span class="token comment" spellcheck="true">## E.g. to specify the default SSL certificate you can use</span>  <span class="token comment" spellcheck="true">## extraArgs:</span>  <span class="token comment" spellcheck="true">##   default-ssl-certificate: "&lt;namespace>/&lt;secret_name>"</span>  <span class="token key atrule">extraArgs</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">## Additional environment variables to set</span>  <span class="token key atrule">extraEnvs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># extraEnvs:</span>  <span class="token comment" spellcheck="true">#   - name: FOO</span>  <span class="token comment" spellcheck="true">#     valueFrom:</span>  <span class="token comment" spellcheck="true">#       secretKeyRef:</span>  <span class="token comment" spellcheck="true">#         key: FOO</span>  <span class="token comment" spellcheck="true">#         name: secret-resource</span>  <span class="token comment" spellcheck="true">## DaemonSet or Deployment</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token comment" spellcheck="true"># 使用DaemonSet方式部署</span>  <span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet  <span class="token comment" spellcheck="true">## Annotations to be added to the controller Deployment or DaemonSet</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">#  keel.sh/pollSchedule: "@every 60m"</span>  <span class="token comment" spellcheck="true">## Labels to be added to the controller Deployment or DaemonSet</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">#  keel.sh/policy: patch</span>  <span class="token comment" spellcheck="true">#  keel.sh/trigger: poll</span>  <span class="token comment" spellcheck="true"># The update strategy to apply to the Deployment or DaemonSet</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">updateStrategy</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">#  rollingUpdate:</span>  <span class="token comment" spellcheck="true">#    maxUnavailable: 1</span>  <span class="token comment" spellcheck="true">#  type: RollingUpdate</span>  <span class="token comment" spellcheck="true"># minReadySeconds to avoid killing pods before we are ready</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token comment" spellcheck="true">## Node tolerations for server scheduling to nodes with taints</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">tolerations</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#  - key: "key"</span>  <span class="token comment" spellcheck="true">#    operator: "Equal|Exists"</span>  <span class="token comment" spellcheck="true">#    value: "value"</span>  <span class="token comment" spellcheck="true">#    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"</span>  <span class="token comment" spellcheck="true">## Affinity and anti-affinity</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">affinity</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true"># # An example of preferred pod anti-affinity, weight is in the range 1-100</span>    <span class="token comment" spellcheck="true"># podAntiAffinity:</span>    <span class="token comment" spellcheck="true">#   preferredDuringSchedulingIgnoredDuringExecution:</span>    <span class="token comment" spellcheck="true">#   - weight: 100</span>    <span class="token comment" spellcheck="true">#     podAffinityTerm:</span>    <span class="token comment" spellcheck="true">#       labelSelector:</span>    <span class="token comment" spellcheck="true">#         matchExpressions:</span>    <span class="token comment" spellcheck="true">#         - key: app.kubernetes.io/name</span>    <span class="token comment" spellcheck="true">#           operator: In</span>    <span class="token comment" spellcheck="true">#           values:</span>    <span class="token comment" spellcheck="true">#           - ingress-nginx</span>    <span class="token comment" spellcheck="true">#         - key: app.kubernetes.io/instance</span>    <span class="token comment" spellcheck="true">#           operator: In</span>    <span class="token comment" spellcheck="true">#           values:</span>    <span class="token comment" spellcheck="true">#           - ingress-nginx</span>    <span class="token comment" spellcheck="true">#         - key: app.kubernetes.io/component</span>    <span class="token comment" spellcheck="true">#           operator: In</span>    <span class="token comment" spellcheck="true">#           values:</span>    <span class="token comment" spellcheck="true">#           - controller</span>    <span class="token comment" spellcheck="true">#       topologyKey: kubernetes.io/hostname</span>    <span class="token comment" spellcheck="true"># # An example of required pod anti-affinity</span>    <span class="token comment" spellcheck="true"># podAntiAffinity:</span>    <span class="token comment" spellcheck="true">#   requiredDuringSchedulingIgnoredDuringExecution:</span>    <span class="token comment" spellcheck="true">#   - labelSelector:</span>    <span class="token comment" spellcheck="true">#       matchExpressions:</span>    <span class="token comment" spellcheck="true">#       - key: app.kubernetes.io/name</span>    <span class="token comment" spellcheck="true">#         operator: In</span>    <span class="token comment" spellcheck="true">#         values:</span>    <span class="token comment" spellcheck="true">#         - ingress-nginx</span>    <span class="token comment" spellcheck="true">#       - key: app.kubernetes.io/instance</span>    <span class="token comment" spellcheck="true">#         operator: In</span>    <span class="token comment" spellcheck="true">#         values:</span>    <span class="token comment" spellcheck="true">#         - ingress-nginx</span>    <span class="token comment" spellcheck="true">#       - key: app.kubernetes.io/component</span>    <span class="token comment" spellcheck="true">#         operator: In</span>    <span class="token comment" spellcheck="true">#         values:</span>    <span class="token comment" spellcheck="true">#         - controller</span>    <span class="token comment" spellcheck="true">#     topologyKey: "kubernetes.io/hostname"</span>  <span class="token comment" spellcheck="true">## Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in.</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">topologySpreadConstraints</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># - maxSkew: 1</span>    <span class="token comment" spellcheck="true">#   topologyKey: failure-domain.beta.kubernetes.io/zone</span>    <span class="token comment" spellcheck="true">#   whenUnsatisfiable: DoNotSchedule</span>    <span class="token comment" spellcheck="true">#   labelSelector:</span>    <span class="token comment" spellcheck="true">#     matchLabels:</span>    <span class="token comment" spellcheck="true">#       app.kubernetes.io/instance: ingress-nginx-internal</span>  <span class="token comment" spellcheck="true">## terminationGracePeriodSeconds</span>  <span class="token comment" spellcheck="true">## wait up to five minutes for the drain of connections</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">300</span>  <span class="token comment" spellcheck="true">## Node labels for controller pod assignment</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/user-guide/node-selection/</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>    <span class="token key atrule">kubernetes.io/os</span><span class="token punctuation">:</span> linux    <span class="token comment" spellcheck="true"># 指定部署的节点所含有的标签</span>    <span class="token key atrule">ingress</span><span class="token punctuation">:</span> <span class="token string">"true"</span>  <span class="token comment" spellcheck="true">## Liveness and readiness probe values</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>    <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>    <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>    <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>    <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">10254</span>  <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>    <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">3</span>    <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>    <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>    <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">10254</span>  <span class="token comment" spellcheck="true"># Path of the health check endpoint. All requests received on the port defined by</span>  <span class="token comment" spellcheck="true"># the healthz-port parameter are forwarded internally to this path.</span>  <span class="token key atrule">healthCheckPath</span><span class="token punctuation">:</span> <span class="token string">"/healthz"</span>  <span class="token comment" spellcheck="true">## Annotations to be added to controller pods</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">podAnnotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token key atrule">replicaCount</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">minAvailable</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token comment" spellcheck="true"># Define requests resources to avoid probe issues due to CPU utilization in busy nodes</span>  <span class="token comment" spellcheck="true"># ref: https://github.com/kubernetes/ingress-nginx/issues/4735#issuecomment-551204903</span>  <span class="token comment" spellcheck="true"># Ideally, there should be no limits.</span>  <span class="token comment" spellcheck="true"># https://engineering.indeedblog.com/blog/2019/12/cpu-throttling-regression-fix/</span>  <span class="token comment" spellcheck="true"># 临时使用默认的配置，在生产环境中需要重新设置</span>  <span class="token comment" spellcheck="true"># 如果是专有节点，需要尽可能配置的更大</span>  <span class="token key atrule">resources</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true">#  limits:</span>  <span class="token comment" spellcheck="true">#    cpu: 100m</span>  <span class="token comment" spellcheck="true">#    memory: 90Mi</span>    <span class="token key atrule">requests</span><span class="token punctuation">:</span>      <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m      <span class="token key atrule">memory</span><span class="token punctuation">:</span> 90Mi  <span class="token comment" spellcheck="true"># Mutually exclusive with keda autoscaling</span>  <span class="token key atrule">autoscaling</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token key atrule">minReplicas</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">maxReplicas</span><span class="token punctuation">:</span> <span class="token number">11</span>    <span class="token key atrule">targetCPUUtilizationPercentage</span><span class="token punctuation">:</span> <span class="token number">50</span>    <span class="token key atrule">targetMemoryUtilizationPercentage</span><span class="token punctuation">:</span> <span class="token number">50</span>  <span class="token key atrule">autoscalingTemplate</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># Custom or additional autoscaling metrics</span>  <span class="token comment" spellcheck="true"># ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics</span>  <span class="token comment" spellcheck="true"># - type: Pods</span>  <span class="token comment" spellcheck="true">#   pods:</span>  <span class="token comment" spellcheck="true">#     metric:</span>  <span class="token comment" spellcheck="true">#       name: nginx_ingress_controller_nginx_process_requests_total</span>  <span class="token comment" spellcheck="true">#     target:</span>  <span class="token comment" spellcheck="true">#       type: AverageValue</span>  <span class="token comment" spellcheck="true">#       averageValue: 10000m</span>  <span class="token comment" spellcheck="true"># Mutually exclusive with hpa autoscaling</span>  <span class="token key atrule">keda</span><span class="token punctuation">:</span>    <span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> <span class="token string">"keda.sh/v1alpha1"</span>  <span class="token comment" spellcheck="true"># apiVersion changes with keda 1.x vs 2.x</span>  <span class="token comment" spellcheck="true"># 2.x = keda.sh/v1alpha1</span>  <span class="token comment" spellcheck="true"># 1.x = keda.k8s.io/v1alpha1</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token key atrule">minReplicas</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">maxReplicas</span><span class="token punctuation">:</span> <span class="token number">11</span>    <span class="token key atrule">pollingInterval</span><span class="token punctuation">:</span> <span class="token number">30</span>    <span class="token key atrule">cooldownPeriod</span><span class="token punctuation">:</span> <span class="token number">300</span>    <span class="token key atrule">restoreToOriginalReplicaCount</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token key atrule">triggers</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#     - type: prometheus</span> <span class="token comment" spellcheck="true">#       metadata:</span> <span class="token comment" spellcheck="true">#         serverAddress: http://&lt;prometheus-host>:9090</span> <span class="token comment" spellcheck="true">#         metricName: http_requests_total</span> <span class="token comment" spellcheck="true">#         threshold: '100'</span> <span class="token comment" spellcheck="true">#         query: sum(rate(http_requests_total{deployment="my-deployment"}[2m]))</span>    <span class="token key atrule">behavior</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span> <span class="token comment" spellcheck="true">#     scaleDown:</span> <span class="token comment" spellcheck="true">#       stabilizationWindowSeconds: 300</span> <span class="token comment" spellcheck="true">#       policies:</span> <span class="token comment" spellcheck="true">#       - type: Pods</span> <span class="token comment" spellcheck="true">#         value: 1</span> <span class="token comment" spellcheck="true">#         periodSeconds: 180</span> <span class="token comment" spellcheck="true">#     scaleUp:</span> <span class="token comment" spellcheck="true">#       stabilizationWindowSeconds: 300</span> <span class="token comment" spellcheck="true">#       policies:</span> <span class="token comment" spellcheck="true">#       - type: Pods</span> <span class="token comment" spellcheck="true">#         value: 2</span> <span class="token comment" spellcheck="true">#         periodSeconds: 60</span>  <span class="token comment" spellcheck="true">## Enable mimalloc as a drop-in replacement for malloc.</span>  <span class="token comment" spellcheck="true">## ref: https://github.com/microsoft/mimalloc</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">enableMimalloc</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token comment" spellcheck="true">## Override NGINX template</span>  <span class="token key atrule">customTemplate</span><span class="token punctuation">:</span>    <span class="token key atrule">configMapName</span><span class="token punctuation">:</span> <span class="token string">""</span>    <span class="token key atrule">configMapKey</span><span class="token punctuation">:</span> <span class="token string">""</span>  <span class="token key atrule">service</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token key atrule">labels</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true"># clusterIP: ""</span>    <span class="token comment" spellcheck="true">## List of IP addresses at which the controller services are available</span>    <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips</span>    <span class="token comment" spellcheck="true">##</span>    <span class="token key atrule">externalIPs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># loadBalancerIP: ""</span>    <span class="token key atrule">loadBalancerSourceRanges</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token key atrule">enableHttp</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">enableHttps</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token comment" spellcheck="true">## Set external traffic policy to: "Local" to preserve source IP on</span>    <span class="token comment" spellcheck="true">## providers supporting it</span>    <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer</span>    <span class="token comment" spellcheck="true"># externalTrafficPolicy: ""</span>    <span class="token comment" spellcheck="true"># Must be either "None" or "ClientIP" if set. Kubernetes will default to "None".</span>    <span class="token comment" spellcheck="true"># Ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies</span>    <span class="token comment" spellcheck="true"># sessionAffinity: ""</span>    <span class="token comment" spellcheck="true"># specifies the health check node port (numeric port number) for the service. If healthCheckNodePort isn’t specified,</span>    <span class="token comment" spellcheck="true"># the service controller allocates a port from your cluster’s NodePort range.</span>    <span class="token comment" spellcheck="true"># Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip</span>    <span class="token comment" spellcheck="true"># healthCheckNodePort: 0</span>    <span class="token key atrule">ports</span><span class="token punctuation">:</span>      <span class="token key atrule">http</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">https</span><span class="token punctuation">:</span> <span class="token number">443</span>    <span class="token key atrule">targetPorts</span><span class="token punctuation">:</span>      <span class="token key atrule">http</span><span class="token punctuation">:</span> http      <span class="token key atrule">https</span><span class="token punctuation">:</span> https    <span class="token comment" spellcheck="true"># 自有资源部署，不使用LoadBalancer，使用clusterIP进行部署</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> ClusterIP    <span class="token comment" spellcheck="true"># type: NodePort</span>    <span class="token comment" spellcheck="true"># nodePorts:</span>    <span class="token comment" spellcheck="true">#   http: 32080</span>    <span class="token comment" spellcheck="true">#   https: 32443</span>    <span class="token comment" spellcheck="true">#   tcp:</span>    <span class="token comment" spellcheck="true">#     8080: 32808</span>    <span class="token key atrule">nodePorts</span><span class="token punctuation">:</span>      <span class="token key atrule">http</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">https</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">tcp</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">udp</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">## Enables an additional internal load balancer (besides the external one).</span>    <span class="token comment" spellcheck="true">## Annotations are mandatory for the load balancer to come up. Varies with the cloud service.</span>    <span class="token key atrule">internal</span><span class="token punctuation">:</span>      <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>      <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token comment" spellcheck="true"># loadBalancerIP: ""</span>      <span class="token comment" spellcheck="true">## Restrict access For LoadBalancer service. Defaults to 0.0.0.0/0.</span>      <span class="token key atrule">loadBalancerSourceRanges</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token comment" spellcheck="true">## Set external traffic policy to: "Local" to preserve source IP on</span>      <span class="token comment" spellcheck="true">## providers supporting it</span>      <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer</span>      <span class="token comment" spellcheck="true"># externalTrafficPolicy: ""</span>  <span class="token key atrule">extraContainers</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## Additional containers to be added to the controller pod.</span>  <span class="token comment" spellcheck="true">## See https://github.com/lemonldap-ng-controller/lemonldap-ng-controller as example.</span>  <span class="token comment" spellcheck="true">#  - name: my-sidecar</span>  <span class="token comment" spellcheck="true">#    image: nginx:latest</span>  <span class="token comment" spellcheck="true">#  - name: lemonldap-ng-controller</span>  <span class="token comment" spellcheck="true">#    image: lemonldapng/lemonldap-ng-controller:0.2.0</span>  <span class="token comment" spellcheck="true">#    args:</span>  <span class="token comment" spellcheck="true">#      - /lemonldap-ng-controller</span>  <span class="token comment" spellcheck="true">#      - --alsologtostderr</span>  <span class="token comment" spellcheck="true">#      - --configmap=$(POD_NAMESPACE)/lemonldap-ng-configuration</span>  <span class="token comment" spellcheck="true">#    env:</span>  <span class="token comment" spellcheck="true">#      - name: POD_NAME</span>  <span class="token comment" spellcheck="true">#        valueFrom:</span>  <span class="token comment" spellcheck="true">#          fieldRef:</span>  <span class="token comment" spellcheck="true">#            fieldPath: metadata.name</span>  <span class="token comment" spellcheck="true">#      - name: POD_NAMESPACE</span>  <span class="token comment" spellcheck="true">#        valueFrom:</span>  <span class="token comment" spellcheck="true">#          fieldRef:</span>  <span class="token comment" spellcheck="true">#            fieldPath: metadata.namespace</span>  <span class="token comment" spellcheck="true">#    volumeMounts:</span>  <span class="token comment" spellcheck="true">#    - name: copy-portal-skins</span>  <span class="token comment" spellcheck="true">#      mountPath: /srv/var/lib/lemonldap-ng/portal/skins</span>  <span class="token key atrule">extraVolumeMounts</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## Additional volumeMounts to the controller main container.</span>  <span class="token comment" spellcheck="true">#  - name: copy-portal-skins</span>  <span class="token comment" spellcheck="true">#   mountPath: /var/lib/lemonldap-ng/portal/skins</span>  <span class="token key atrule">extraVolumes</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## Additional volumes to the controller pod.</span>  <span class="token comment" spellcheck="true">#  - name: copy-portal-skins</span>  <span class="token comment" spellcheck="true">#    emptyDir: {}</span>  <span class="token key atrule">extraInitContainers</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## Containers, which are run before the app containers are started.</span>  <span class="token comment" spellcheck="true"># - name: init-myservice</span>  <span class="token comment" spellcheck="true">#   image: busybox</span>  <span class="token comment" spellcheck="true">#   command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']</span>  <span class="token comment" spellcheck="true"># 准入控制配置</span>  <span class="token comment" spellcheck="true"># 0.40以后可用</span>  <span class="token key atrule">admissionWebhooks</span><span class="token punctuation">:</span>    <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">failurePolicy</span><span class="token punctuation">:</span> Fail    <span class="token comment" spellcheck="true"># timeoutSeconds: 10</span>    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8443</span>    <span class="token key atrule">certificate</span><span class="token punctuation">:</span> <span class="token string">"/usr/local/certificates/cert"</span>    <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">"/usr/local/certificates/key"</span>    <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token key atrule">objectSelector</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token key atrule">service</span><span class="token punctuation">:</span>      <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token comment" spellcheck="true"># clusterIP: ""</span>      <span class="token key atrule">externalIPs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token comment" spellcheck="true"># loadBalancerIP: ""</span>      <span class="token key atrule">loadBalancerSourceRanges</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">443</span>      <span class="token key atrule">type</span><span class="token punctuation">:</span> ClusterIP    <span class="token key atrule">patch</span><span class="token punctuation">:</span>      <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token key atrule">image</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># repository: docker.io/jettech/kube-webhook-certgen</span>        <span class="token key atrule">repository</span><span class="token punctuation">:</span> jettech/kube<span class="token punctuation">-</span>webhook<span class="token punctuation">-</span>certgen        <span class="token key atrule">tag</span><span class="token punctuation">:</span> v1.5.0        <span class="token key atrule">pullPolicy</span><span class="token punctuation">:</span> IfNotPresent      <span class="token comment" spellcheck="true">## Provide a priority class name to the webhook patching job</span>      <span class="token comment" spellcheck="true">##</span>      <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">podAnnotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">2000</span>  <span class="token key atrule">metrics</span><span class="token punctuation">:</span>    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">10254</span>    <span class="token comment" spellcheck="true"># if this port is changed, change healthz-port: in extraArgs: accordingly</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token key atrule">service</span><span class="token punctuation">:</span>      <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token comment" spellcheck="true"># prometheus.io/scrape: "true"</span>      <span class="token comment" spellcheck="true"># prometheus.io/port: "10254"</span>      <span class="token comment" spellcheck="true"># clusterIP: ""</span>      <span class="token comment" spellcheck="true">## List of IP addresses at which the stats-exporter service is available</span>      <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips</span>      <span class="token comment" spellcheck="true">##</span>      <span class="token key atrule">externalIPs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token comment" spellcheck="true"># loadBalancerIP: ""</span>      <span class="token key atrule">loadBalancerSourceRanges</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">9913</span>      <span class="token key atrule">type</span><span class="token punctuation">:</span> ClusterIP      <span class="token comment" spellcheck="true"># externalTrafficPolicy: ""</span>      <span class="token comment" spellcheck="true"># nodePort: ""</span>    <span class="token key atrule">serviceMonitor</span><span class="token punctuation">:</span>      <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>      <span class="token key atrule">additionalLabels</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">namespace</span><span class="token punctuation">:</span> <span class="token string">""</span>      <span class="token key atrule">namespaceSelector</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token comment" spellcheck="true"># Default: scrape .Release.Namespace only</span>      <span class="token comment" spellcheck="true"># To scrape all, use the following:</span>      <span class="token comment" spellcheck="true"># namespaceSelector:</span>      <span class="token comment" spellcheck="true">#   any: true</span>      <span class="token key atrule">scrapeInterval</span><span class="token punctuation">:</span> 30s      <span class="token comment" spellcheck="true"># honorLabels: true</span>      <span class="token key atrule">targetLabels</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      <span class="token key atrule">metricRelabelings</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token key atrule">prometheusRule</span><span class="token punctuation">:</span>      <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>      <span class="token key atrule">additionalLabels</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token comment" spellcheck="true"># namespace: ""</span>      <span class="token key atrule">rules</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># # These are just examples rules, please adapt them to your needs</span>        <span class="token comment" spellcheck="true"># - alert: NGINXConfigFailed</span>        <span class="token comment" spellcheck="true">#   expr: count(nginx_ingress_controller_config_last_reload_successful == 0) > 0</span>        <span class="token comment" spellcheck="true">#   for: 1s</span>        <span class="token comment" spellcheck="true">#   labels:</span>        <span class="token comment" spellcheck="true">#     severity: critical</span>        <span class="token comment" spellcheck="true">#   annotations:</span>        <span class="token comment" spellcheck="true">#     description: bad ingress config - nginx config test failed</span>        <span class="token comment" spellcheck="true">#     summary: uninstall the latest ingress changes to allow config reloads to resume</span>        <span class="token comment" spellcheck="true"># - alert: NGINXCertificateExpiry</span>        <span class="token comment" spellcheck="true">#   expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800</span>        <span class="token comment" spellcheck="true">#   for: 1s</span>        <span class="token comment" spellcheck="true">#   labels:</span>        <span class="token comment" spellcheck="true">#     severity: critical</span>        <span class="token comment" spellcheck="true">#   annotations:</span>        <span class="token comment" spellcheck="true">#     description: ssl certificate(s) will expire in less then a week</span>        <span class="token comment" spellcheck="true">#     summary: renew expiring certificates to avoid downtime</span>        <span class="token comment" spellcheck="true"># - alert: NGINXTooMany500s</span>        <span class="token comment" spellcheck="true">#   expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"5.+"} ) / sum(nginx_ingress_controller_requests) ) > 5</span>        <span class="token comment" spellcheck="true">#   for: 1m</span>        <span class="token comment" spellcheck="true">#   labels:</span>        <span class="token comment" spellcheck="true">#     severity: warning</span>        <span class="token comment" spellcheck="true">#   annotations:</span>        <span class="token comment" spellcheck="true">#     description: Too many 5XXs</span>        <span class="token comment" spellcheck="true">#     summary: More than 5% of all requests returned 5XX, this requires your attention</span>        <span class="token comment" spellcheck="true"># - alert: NGINXTooMany400s</span>        <span class="token comment" spellcheck="true">#   expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"4.+"} ) / sum(nginx_ingress_controller_requests) ) > 5</span>        <span class="token comment" spellcheck="true">#   for: 1m</span>        <span class="token comment" spellcheck="true">#   labels:</span>        <span class="token comment" spellcheck="true">#     severity: warning</span>        <span class="token comment" spellcheck="true">#   annotations:</span>        <span class="token comment" spellcheck="true">#     description: Too many 4XXs</span>        <span class="token comment" spellcheck="true">#     summary: More than 5% of all requests returned 4XX, this requires your attention</span>  <span class="token comment" spellcheck="true">## Improve connection draining when ingress controller pod is deleted using a lifecycle hook:</span>  <span class="token comment" spellcheck="true">## With this new hook, we increased the default terminationGracePeriodSeconds from 30 seconds</span>  <span class="token comment" spellcheck="true">## to 300, allowing the draining of connections up to five minutes.</span>  <span class="token comment" spellcheck="true">## If the active connections end before that, the pod will terminate gracefully at that time.</span>  <span class="token comment" spellcheck="true">## To effectively take advantage of this feature, the Configmap feature</span>  <span class="token comment" spellcheck="true">## worker-shutdown-timeout new value is 240s instead of 10s.</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>    <span class="token key atrule">preStop</span><span class="token punctuation">:</span>      <span class="token key atrule">exec</span><span class="token punctuation">:</span>        <span class="token key atrule">command</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> /wait<span class="token punctuation">-</span>shutdown  <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token comment" spellcheck="true">## Rollback limit</span><span class="token comment" spellcheck="true">##</span><span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token comment" spellcheck="true">## Default 404 backend</span><span class="token comment" spellcheck="true">##</span><span class="token key atrule">defaultBackend</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> defaultbackend  <span class="token comment" spellcheck="true"># 去除外部连接，通过搜索dockerhub寻找同版本替代品，使用mirrorgooglecontainers替代原来的地址信息</span>  <span class="token key atrule">image</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># repository: k8s.gcr.io/defaultbackend-amd64</span>    <span class="token key atrule">repository</span><span class="token punctuation">:</span> mirrorgooglecontainers/defaultbackend<span class="token punctuation">-</span>amd64    <span class="token key atrule">tag</span><span class="token punctuation">:</span> <span class="token string">"1.5"</span>    <span class="token key atrule">pullPolicy</span><span class="token punctuation">:</span> IfNotPresent    <span class="token comment" spellcheck="true"># nobody user -> uid 65534</span>    <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">65534</span>    <span class="token key atrule">runAsNonRoot</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">extraArgs</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token key atrule">serviceAccount</span><span class="token punctuation">:</span>    <span class="token key atrule">create</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">name</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true">## Additional environment variables to set for defaultBackend pods</span>  <span class="token key atrule">extraEnvs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span>  <span class="token comment" spellcheck="true">## Readiness and liveness probes for default backend</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>    <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">3</span>    <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>    <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>    <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>    <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">6</span>    <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">0</span>    <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>    <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token comment" spellcheck="true">## Node tolerations for server scheduling to nodes with taints</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">tolerations</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#  - key: "key"</span>  <span class="token comment" spellcheck="true">#    operator: "Equal|Exists"</span>  <span class="token comment" spellcheck="true">#    value: "value"</span>  <span class="token comment" spellcheck="true">#    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"</span>  <span class="token key atrule">affinity</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">## Security Context policies for controller pods</span>  <span class="token comment" spellcheck="true">## See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for</span>  <span class="token comment" spellcheck="true">## notes on enabling and using sysctls</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">podSecurityContext</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># labels to add to the pod container metadata</span>  <span class="token key atrule">podLabels</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">#  key: value</span>  <span class="token comment" spellcheck="true">## Node labels for default backend pod assignment</span>  <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/user-guide/node-selection/</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">## Annotations to be added to default backend pods</span>  <span class="token comment" spellcheck="true">##</span>  <span class="token key atrule">podAnnotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token key atrule">replicaCount</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">minAvailable</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># limits:</span>  <span class="token comment" spellcheck="true">#   cpu: 10m</span>  <span class="token comment" spellcheck="true">#   memory: 20Mi</span>  <span class="token comment" spellcheck="true">#  requests:</span>  <span class="token comment" spellcheck="true">#    cpu: 100m</span>  <span class="token comment" spellcheck="true">#    memory: 120Mi</span>  <span class="token key atrule">autoscaling</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token key atrule">minReplicas</span><span class="token punctuation">:</span> <span class="token number">1</span>    <span class="token key atrule">maxReplicas</span><span class="token punctuation">:</span> <span class="token number">2</span>    <span class="token key atrule">targetCPUUtilizationPercentage</span><span class="token punctuation">:</span> <span class="token number">50</span>    <span class="token key atrule">targetMemoryUtilizationPercentage</span><span class="token punctuation">:</span> <span class="token number">50</span>  <span class="token key atrule">service</span><span class="token punctuation">:</span>    <span class="token key atrule">annotations</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true"># clusterIP: ""</span>    <span class="token comment" spellcheck="true">## List of IP addresses at which the default backend service is available</span>    <span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips</span>    <span class="token comment" spellcheck="true">##</span>    <span class="token key atrule">externalIPs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">#loadBalancerIP: ""</span>    <span class="token key atrule">loadBalancerSourceRanges</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>    <span class="token comment" spellcheck="true"># 修改为NodePort模式</span>    <span class="token comment" spellcheck="true">#type: ClusterIP</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">priorityClassName</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token comment" spellcheck="true">## Enable RBAC as per https://github.com/kubernetes/ingress/tree/master/examples/rbac/nginx and https://github.com/kubernetes/ingress/issues/266</span><span class="token key atrule">rbac</span><span class="token punctuation">:</span>  <span class="token key atrule">create</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">scope</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token comment" spellcheck="true"># If true, create &amp; use Pod Security Policy resources</span><span class="token comment" spellcheck="true"># https://kubernetes.io/docs/concepts/policy/pod-security-policy/</span><span class="token key atrule">podSecurityPolicy</span><span class="token punctuation">:</span>  <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token key atrule">serviceAccount</span><span class="token punctuation">:</span>  <span class="token key atrule">create</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">## Optional array of imagePullSecrets containing private registry credentials</span><span class="token comment" spellcheck="true">## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/</span><span class="token key atrule">imagePullSecrets</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># - name: secretName</span><span class="token comment" spellcheck="true"># TCP service key:value pairs</span><span class="token comment" spellcheck="true"># Ref: https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/tcp</span><span class="token comment" spellcheck="true">##</span><span class="token key atrule">tcp</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">#  8080: "default/example-tcp-svc:9000"</span><span class="token comment" spellcheck="true"># UDP service key:value pairs</span><span class="token comment" spellcheck="true"># Ref: https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/udp</span><span class="token comment" spellcheck="true">##</span><span class="token key atrule">udp</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">#  53: "kube-system/kube-dns:53"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>需要修改的位置：</p><p>a) Controller和admissionWebhook的镜像地址，需要将公网镜像同步至公司内网镜像仓库<br>b) hostNetwork 设置为 true<br>c) dnsPolicy设置为 ClusterFirstWithHostNet<br>d) NodeSelector添加ingress: “true”部署至指定节点<br>e) 类型更改为 kind: DaemonSet<br>f) service.type改为NodePort</p><p>注意：在yaml中写入true时一定要加引号，例如 nginx: “true”。</p><p>最后使用命令安装ingress-nginx：</p><pre><code>// 创建命名空间kubectl create ns ingress-nginx// 对节点打标签，将目标ingress部署到该节点kubectl label node k8s-master-03 ingress=true// 安装helm install ingress-nginx -n ingress-nginx .</code></pre><p>2.12.3 ingress的扩容缩容</p><p>对节点打标签，进行ingress的扩容操作</p><pre><code>kubectl label node k8s-master-02 ingress=true</code></pre><p>DaemonSet会监听各个节点机器的变化，如果有新的节点添加了<em>ingress=true</em>这个标签，DaemonSet会控制在该节点上新部署一个ingress的实例。</p><p>查看ingress信息：</p><pre><code>kubectl get pod -n ingress-nginx -o wideNAME                             READY   STATUS    RESTARTS   AGE     IP               NODE            NOMINATED NODE   READINESS GATESingress-nginx-controller-549t5   1/1     Running   0          4h33m   192.168.229.53   k8s-master-03   &lt;none&gt;           &lt;none&gt;ingress-nginx-controller-ktx9r   1/1     Running   0          5m4s    192.168.229.52   k8s-master-02   &lt;none&gt;           &lt;none&gt;</code></pre><p>在节点上删除<em>ingress=true</em>这个标签，DaemonSet将会删除在该节点上部署的ingress实例，如下：</p><pre><code>kubectl label node k8s-master-02 ingress-node/k8s-master-02 labeledkubectl get pod -n ingress-nginx -o wideNAME                             READY   STATUS        RESTARTS   AGE     IP               NODE            NOMINATED NODE   READINESS GATESingress-nginx-controller-549t5   1/1     Running       0          4h35m   192.168.229.53   k8s-master-03   &lt;none&gt;           &lt;none&gt;ingress-nginx-controller-ktx9r   1/1     Terminating   0          7m24s   192.168.229.52   k8s-master-02   &lt;none&gt;           &lt;none&gt;</code></pre><p><strong>注意：</strong>如果在外部存在负载均衡器，例如硬件F5，软件上云服务厂商的SLB，在添加时需要先启动新的ingress实例，然后在负载均衡器中添加该节点的访问信息；在删除时，需要先在负载均衡器中删除该ingress配置的访问信息，再停止该实例，避免出现服务宕机的情况。</p><p>ingress实质是在宿主机上使用了宿主机的端口和网络，并不是使用kube-proxy进行代理。</p><p>2.12.4 配置ingress进行服务发布</p><p>通过读取配置文件的annotations节点下的配置信息，可以通过ingress-controller动态生成nginx的配置文件，达成站点部署的操作。</p><p>rewrite示例，配置文件如下:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> networking.k8s.io/v1beta1   <span class="token comment" spellcheck="true"># 1.19以后建议使用networking.k8s.io/v1，但是ingress-controller有可能不支持v1，extensions/v1beta1不要再写</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Ingress<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">annotations</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># ingress的配置写在annotations中 </span>    <span class="token key atrule">kubernetes.io/ingress.class</span><span class="token punctuation">:</span> <span class="token string">"nginx"</span>  <span class="token comment" spellcheck="true"># 在创建ingress的时候通过ingressClass指定了该名称</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> example<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 一个ingress可以配置多个rule</span>  <span class="token punctuation">-</span> <span class="token key atrule">host</span><span class="token punctuation">:</span> foo.bar.com  <span class="token comment" spellcheck="true"># 域名配置，可以不写，匹配*, 例如*.bar.com</span>    <span class="token key atrule">http</span><span class="token punctuation">:</span>      <span class="token key atrule">paths</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 相当于nginx的location配置，同一个host可以配置多个path</span>      <span class="token punctuation">-</span> <span class="token key atrule">backend</span><span class="token punctuation">:</span>          <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>svc          <span class="token key atrule">servicePort</span><span class="token punctuation">:</span> <span class="token number">80</span>        <span class="token key atrule">path</span><span class="token punctuation">:</span> /      <span class="token comment" spellcheck="true"># - backend:</span>      <span class="token comment" spellcheck="true">#     serviceName: http-svc-abc</span>      <span class="token comment" spellcheck="true">#     servicePort: 80</span>      <span class="token comment" spellcheck="true">#   path: /abc</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过配置域名的方式反向代理到已部署的nginx上。使上述配置生效，如下：</p><pre><code>kubectl create -f ingress.yaml</code></pre><p><em>重大问题：访问504、502错误码的问题</em></p><p><strong>解决方式</strong>：配置了ClusterIP</p><pre><code>  service:    annotations: {}    # clusterIP: &quot;&quot;    ## List of IP addresses at which the default backend service is available    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips    ##    externalIPs: []    #loadBalancerIP: &quot;&quot;    loadBalancerSourceRanges: []    servicePort: 80    type: ClusterIP</code></pre><p>需要修改为NodePort，并且需要注意的是在访问域名时需要带上ingress服务的端口号。例如：</p><pre><code>$ curl http://foo.bar.com:30946</code></pre><p>注意：多域名配置时，在annotion中设置rewrite时，需要rewrite方式转发的域名写在一个配置文件中，不需要的写在另一个文件中。</p><p>2.12.5 ingress排错</p><p>安装kubectl工具的krew插件管理工具，以此为基础，安装ingress的调试工具。</p><p>首先安装krew，执行下面的脚本内容</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token punctuation">(</span>  <span class="token keyword">set</span> -x<span class="token punctuation">;</span> <span class="token function">cd</span> <span class="token string">"<span class="token variable"><span class="token variable">$(</span>mktemp -d<span class="token variable">)</span></span>"</span> <span class="token operator">&amp;&amp;</span>  curl -fsSLO <span class="token string">"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz"</span> <span class="token operator">&amp;&amp;</span>  <span class="token function">tar</span> zxvf krew.tar.gz <span class="token operator">&amp;&amp;</span>  KREW<span class="token operator">=</span>./krew-<span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">uname</span> <span class="token operator">|</span> <span class="token function">tr</span> '<span class="token punctuation">[</span>:upper:<span class="token punctuation">]</span>' '<span class="token punctuation">[</span>:lower:<span class="token punctuation">]</span>'<span class="token variable">)</span></span>_<span class="token variable"><span class="token variable">$(</span><span class="token function">uname</span> -m <span class="token operator">|</span> <span class="token function">sed</span> -e 's/x86_64/amd64/' -e 's/arm.*$/arm/'<span class="token variable">)</span></span>"</span> <span class="token operator">&amp;&amp;</span>  <span class="token string">"<span class="token variable">$KREW</span>"</span> <span class="token function">install</span> krew<span class="token punctuation">)</span>// （无科学上网的情况下）或者考虑提前下载好krew的安装包krew.tar.gz，以及配置文件krew.yaml，执行下面的命令<span class="token punctuation">(</span>  <span class="token keyword">set</span> -x<span class="token punctuation">;</span> <span class="token function">cd</span> <span class="token string">"<span class="token variable"><span class="token variable">$(</span>mktemp -d<span class="token variable">)</span></span>"</span> <span class="token operator">&amp;&amp;</span>  <span class="token function">cp</span> /root/k8s-practice/ingress/krew.tar.gz ./krew.tar.gz <span class="token operator">&amp;&amp;</span>   <span class="token function">cp</span> /root/k8s-practice/ingress/krew.yaml ./krew.yaml <span class="token operator">&amp;&amp;</span>   <span class="token function">tar</span> zxvf krew.tar.gz <span class="token operator">&amp;&amp;</span>  KREW<span class="token operator">=</span>./krew-<span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">uname</span> <span class="token operator">|</span> <span class="token function">tr</span> '<span class="token punctuation">[</span>:upper:<span class="token punctuation">]</span>' '<span class="token punctuation">[</span>:lower:<span class="token punctuation">]</span>'<span class="token variable">)</span></span>_<span class="token variable"><span class="token variable">$(</span><span class="token function">uname</span> -m <span class="token operator">|</span> <span class="token function">sed</span> -e 's/x86_64/amd64/' -e 's/arm.*$/arm/'<span class="token variable">)</span></span>"</span> <span class="token operator">&amp;&amp;</span>  <span class="token string">"<span class="token variable">$KREW</span>"</span> <span class="token function">install</span> --archive<span class="token operator">=</span>krew.tar.gz --manifest<span class="token operator">=</span>krew.yaml <span class="token operator">&amp;&amp;</span>   <span class="token string">"<span class="token variable">$KREW</span>"</span> update <span class="token punctuation">)</span>// 输出日志如下Installing plugin: krewInstalled plugin: krew\ <span class="token operator">|</span> Use this plugin: <span class="token operator">|</span>      kubectl krew <span class="token operator">|</span> Documentation: <span class="token operator">|</span>      https://krew.sigs.k8s.io/ <span class="token operator">|</span> Caveats: <span class="token operator">|</span> \ <span class="token operator">|</span>  <span class="token operator">|</span> krew is now installed<span class="token operator">!</span> To start using kubectl plugins, you need to add <span class="token operator">|</span>  <span class="token operator">|</span> krew's installation directory to your PATH: <span class="token operator">|</span>  <span class="token operator">|</span> <span class="token operator">|</span>  <span class="token operator">|</span>   * macOS/Linux: <span class="token operator">|</span>  <span class="token operator">|</span>     - Add the following to your ~/.bashrc or ~/.zshrc: <span class="token operator">|</span>  <span class="token operator">|</span>         <span class="token function">export</span> PATH<span class="token operator">=</span><span class="token string">"<span class="token variable">${KREW_ROOT:-$HOME/.krew}</span>/bin:<span class="token variable">$PATH</span>"</span> <span class="token operator">|</span>  <span class="token operator">|</span>     - Restart your shell. <span class="token operator">|</span>  <span class="token operator">|</span> <span class="token operator">|</span>  <span class="token operator">|</span>   * Windows: Add %USERPROFILE%\.krew\bin to your PATH environment variable <span class="token operator">|</span>  <span class="token operator">|</span> <span class="token operator">|</span>  <span class="token operator">|</span> To list krew commands and to get help, run: <span class="token operator">|</span>  <span class="token operator">|</span>   $ kubectl krew <span class="token operator">|</span>  <span class="token operator">|</span> For a full list of available plugins, run: <span class="token operator">|</span>  <span class="token operator">|</span>   $ kubectl krew search <span class="token operator">|</span>  <span class="token operator">|</span> <span class="token operator">|</span>  <span class="token operator">|</span> You can <span class="token function">find</span> documentation at <span class="token operator">|</span>  <span class="token operator">|</span>   https://krew.sigs.k8s.io/docs/user-guide/quickstart/. <span class="token operator">|</span> //<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>安装后需要进行配置环境变量</p><pre><code>$ cat &gt;&gt;EOF&lt; ~/.bashrcexport PATH=&quot;${KREW_ROOT:-$HOME/.krew}/bin:$PATH&quot;EOF</code></pre><p>安装krew完成后，需要退出终端重新进行登录。这时候使用krew安装ingress排查工具ingress-nginx，如下：</p><pre><code>kubectl krew updatekubectl krew install ingress-nginx// 或者直接通过压缩包进行安装，需要先下载该压缩包$ wget https://github.com/kubernetes/ingress-nginx/releases/download/controller-0.31.0/kubectl-ingress_nginx-linux-amd64.tar.gz// 还要下载yaml配置文件$ wget https://github.com/kubernetes-sigs/krew-index/blob/master/plugins/ingress-nginx.yaml// 通过yaml文件同压缩包一起安装$ kubectl krew install --manifest=ingress-nginx-plugin.yaml --archive=kubectl-ingress_nginx-linux-amd64.tar.gzInstalling plugin: ingress-nginxInstalled plugin: ingress-nginx\ | Use this plugin: |      kubectl ingress-nginx | Documentation: |      https://kubernetes.github.io/ingress-nginx/kubectl-plugin//// 执行命令进行验证kubectl ingress-nginx --help</code></pre><p>下面就可以进行问题排查了。</p><p><strong>参考文章</strong></p><ul><li><a href="https://segmentfault.com/a/1190000023088442" target="_blank" rel="noopener">https://segmentfault.com/a/1190000023088442</a></li><li><a href="https://segmentfault.com/a/1190000021784761" target="_blank" rel="noopener">https://segmentfault.com/a/1190000021784761</a></li></ul><p>2.12.6 ingress替代品：traefik</p><p>需要解决的问题是，如何让traefik使用ClusterIP的方式部署在node节点，通过80端口访问。</p><p>需要验证的问题是，如果ingress-nginx使用默认部署的方式，通过port-forward是否可以正常进行访问？</p><p>2.13 ConfigMap &amp;&amp; Secret</p><p>2.13.1 ConfigMap</p><p>ConfigMap管理配置文件或者一些大量的环境变量信息。ConfigMap将配置和Pod分离，更易于配置文件的更改和管理。</p><p>Secret更倾向于存储和共享敏感、加密的配置信息。</p><p>2.13.2 创建ConfigMap</p><p>通过本地目录下的多个配置文件创建配置信息</p><pre><code>mkdir -p configure-pod-container/configmaptouch game.properties &amp;&amp; touch user-interface.propertiescat &lt;&lt;EOF&gt; game.propertiesenemy.types=aliens,monstersplayer.maximum-lives=5 EOFcat &lt;&lt;EOF&gt;&lt;/EOF&gt; user-interface.propertiescolor.good=purplecolor.bad=yellowallow.textmode=trueEOF// 从文件中创建配置信息kubectl create configmap game-config --from-file=configure-pod-container/configmap/kubectl describe cm game-config// 通过yaml文件的方式创建kubectl get cm game-config -o yaml</code></pre><p><strong>注意：</strong>在实际使用过程中，尽可能使用配置文件进行创建，减少直接在命令行中编写配置信息的方式。</p><p>通过本地配置文件创建配置信息</p><pre><code>kubectl create configmap game-config-ui --from-file=configure-pod-container/configmap/user-interface.properties// 更改配置文件名称，在--from-file=后指定名称kubectl create configmap game-config-ui-rename --from-file=ui-pro=configure-pod-container/configmap/user-interface.properties</code></pre><p>创建key-value形式的配置文件，不展示配置文件名称。</p><pre><code>touch configure-pod-container/configmap/user-env-interface.propertiescat &lt;&lt;EOF&gt; configure-pod-container/configmap/user-env-interface.propertiescolor.good=redcolor.bad=yellowcolor.middle=blueallow.textmode=trueEOFkubectl create configmap game-config-env --from-env-file=configure-pod-container/configmap/user-env-interface.properties// 以key-value形式展示，不包含文件名称信息# kubectl describe configmap game-config-envName:         game-config-envNamespace:    defaultLabels:       &lt;none&gt;Annotations:  &lt;none&gt;Data====allow.textmode:----truecolor.bad:----yellowcolor.good:----redcolor.middle:----blueEvents:  &lt;none&gt;// 查看生成的配置信息kubectl get configmap game-config-env -o yaml</code></pre><p>可以通过上述方式生成配置文件信息，利用yaml格式进行创建。</p><p><strong>注意：</strong> 如果指定了多个–from-env-file后，只有最后一个指定的生效，前面指定的配置信息将会被覆盖掉。</p><p>从一个字段创建ConfigMap</p><pre><code>kubectl create cm bite-config --from-literal=special.how=very --from-literal=special.types=vtec --from-literal=special.who=JIM</code></pre><p>2.13.3 ConfigMap用途</p><p>2.13.3.1 作为环境变量的方式</p><p>在Pod中读取ConfigMap的信息，实现将上面创建的ConfigMap中的值作为环境变量在Pod中输出。Pod的配置文件如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> mypod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>container    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/bin/sh"</span><span class="token punctuation">,</span> <span class="token string">"-c"</span><span class="token punctuation">,</span> <span class="token string">"env"</span><span class="token punctuation">]</span>    <span class="token key atrule">env</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SPECIAL_LEVEL_KEY      <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>         <span class="token key atrule">configMapKeyRef</span><span class="token punctuation">:</span>          <span class="token key atrule">name</span><span class="token punctuation">:</span> bite<span class="token punctuation">-</span>config          <span class="token key atrule">key</span><span class="token punctuation">:</span> special.how  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建上面的Pod信息并查看输出的环境变量：</p><pre><code>kubectl create -f test-pod.yamlkubectl logs -f mypod | grep SPECIAL_LEVEL_KEYSPECIAL_LEVEL_KEY=very// 查看bite-config的key-value信息kubectl get cm bite-config -o yaml</code></pre><p>上述方式用的少，尽量使用一个ConfigMap就把所有环境变量包含进去，不要一个个设置。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> env<span class="token punctuation">-</span>config  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">data</span><span class="token punctuation">:</span>  <span class="token key atrule">SPECIAL_LEVEL</span><span class="token punctuation">:</span> very  <span class="token key atrule">SPECIAL_TYPE</span><span class="token punctuation">:</span> charm<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> mypod<span class="token punctuation">-</span>multi  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>container<span class="token punctuation">-</span>multi    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/bin/sh"</span><span class="token punctuation">,</span> <span class="token string">"-c"</span><span class="token punctuation">,</span> <span class="token string">"env"</span><span class="token punctuation">]</span>    <span class="token key atrule">envFrom</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">configMapRef</span><span class="token punctuation">:</span>        <span class="token key atrule">name</span><span class="token punctuation">:</span> env<span class="token punctuation">-</span>config <span class="token comment" spellcheck="true"># name should be indented under the secretRef or configMapRef fields</span>    <span class="token comment" spellcheck="true"># 手动添加</span>    <span class="token key atrule">env</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test1      <span class="token key atrule">value</span><span class="token punctuation">:</span> test1<span class="token punctuation">-</span>value    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> port      <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"3306"</span>  <span class="token comment" spellcheck="true"># value部分只能是字符串</span>  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以<em>envFrom</em>这种形式设置环境变量最常用。</p><p>2.13.3.2 作为配置文件的方式</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> mypod<span class="token punctuation">-</span>volume  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>container<span class="token punctuation">-</span>volume    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/bin/sh"</span><span class="token punctuation">,</span> <span class="token string">"-c"</span><span class="token punctuation">,</span> <span class="token string">"sleep 3600"</span><span class="token punctuation">]</span>    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>volume      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt/1    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>volume<span class="token punctuation">-</span>file      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt/2  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>volume    <span class="token key atrule">configMap</span><span class="token punctuation">:</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> env<span class="token punctuation">-</span>config  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> config<span class="token punctuation">-</span>volume<span class="token punctuation">-</span>file    <span class="token key atrule">configMap</span><span class="token punctuation">:</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> game<span class="token punctuation">-</span>config<span class="token punctuation">-</span>ui  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过延时来保证Pod运行，然后进入镜像中查看配置信息，如下：</p><pre><code># kubectl exec -it mypod-volume -- sh/ # lsbin   dev   etc   home  mnt   proc  root  sys   tmp   usr   var/ # cd /mnt//mnt # ls1  2/mnt # cd 1//mnt/1 # lsSPECIAL_LEVEL  SPECIAL_TYPE/mnt/1 # cat SPECIAL_LEVELvery/mnt/1 # cd ../mnt # ls1  2/mnt # cd 2//mnt/2 # lsuser-interface.properties/mnt/2 # cat user-interface.propertiescolor.good=purplecolor.bad=yellowallow.textmode=true</code></pre><p>key-value的形式一半用作环境变量，文件的形式一般通过挂载的方式作为配置文件使用。</p><p>2.13.4 Secret</p><p>Secret：用来保存敏感信息的，比如密码、令牌或者key值，例如Redis、MySQL的密码等。</p><p>2.13.4.1 Secret创建</p><p>通过配置文件来创建Secret</p><pre><code>echo &quot;admin&quot; &gt; ./admin.txtecho &quot;1234567&quot; &gt; ./password.txtkubectl create secret generic db-user-pass --from-file=admin.txt --from-file=password.txtkubectl get secret db-user-pass -o yamlapiVersion: v1data:  admin.txt: YWRtaW4K  password.txt: MTIzNDU2Nwo=kind: Secretmetadata:  creationTimestamp: &quot;2021-01-26T14:28:41Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: {}        f:admin.txt: {}        f:password.txt: {}      f:type: {}    manager: kubectl-create    operation: Update    time: &quot;2021-01-26T14:28:41Z&quot;  name: db-user-pass  namespace: default  resourceVersion: &quot;798263&quot;  uid: df3f2dd3-3f5d-4d90-993b-39a5c0ce7845echo &quot;MTIzNDU2Nwo=&quot; | base64 --decode</code></pre><p><strong>注意：</strong>在Secret存储时，使用base64进行了加密。</p><p>直接通过命令行创建，但是需要注意的是，在双引号字符串中，使用特殊字符的，必须使用命令行进行转译，例如$，\，*，!，否则无法创建。如果含有特殊字符时使用单引号，则特殊字符不需要转义。</p><pre><code>kubectl create secret generic db-user --from-literal=username=user001 --from-literal=passwd=&quot;\!\\\*QAZxsw2&quot;// 等同于kubectl create secret generic db-user-2 --from-literal=username=user001 --from-literal=passwd=&#39;!\*QAZxsw2&#39;</code></pre><p>可以通过配置文件的方式进行创建，但是创建之前需要注意，在data下所使用的字符串必须经过base64加密，而在stringData下使用的字符串则不需要加密，如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> db<span class="token punctuation">-</span>user<span class="token punctuation">-</span><span class="token number">3</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token key atrule">data</span><span class="token punctuation">:</span>  <span class="token key atrule">passwd</span><span class="token punctuation">:</span> XCFcXCpRQVp4c3cy  <span class="token key atrule">username</span><span class="token punctuation">:</span> dXNlcjAwMQ==<span class="token key atrule">stringData</span><span class="token punctuation">:</span>  <span class="token key atrule">center</span><span class="token punctuation">:</span> daioafiah  <span class="token key atrule">username</span><span class="token punctuation">:</span> myname<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行命令创建</p><pre><code>kubectl create -f secret.yaml</code></pre><p><strong>注意：</strong>stringData优先级高于data，如果stringData中存在和data中同样key值的字符串，以stringData中的为准。</p><p>默认类型为Opaque，</p><p>2.13.4.2 Secret使用</p><p>配置secretName的方式进行读写。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> mypod<span class="token punctuation">-</span>secret  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>container<span class="token punctuation">-</span>volume    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/bin/sh"</span><span class="token punctuation">,</span> <span class="token string">"-c"</span><span class="token punctuation">,</span> <span class="token string">"sleep 3600"</span><span class="token punctuation">]</span>    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> secret<span class="token punctuation">-</span>volume      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt/1      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true  </span><span class="token comment" spellcheck="true"># 只读模式</span>  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> secret<span class="token punctuation">-</span>volume    <span class="token key atrule">secret</span><span class="token punctuation">:</span>      <span class="token key atrule">secretName</span><span class="token punctuation">:</span> db<span class="token punctuation">-</span>user<span class="token punctuation">-</span><span class="token number">3</span>  <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>kubectl create -f pod-secret.yaml</code></pre><p><strong>注意：</strong>将信息挂载到镜像后，在对应路径的文件中，会以明文的形式保存。</p><p>典型用法：ImagePullSecret，Pod拉取私有镜像仓库时使用的账号密码，里面的账号信息会传递给kubelet，然后kubelet就可以拉取有用户名密码设置的镜像仓库中的镜像。</p><p>创建一个docker registry类型的secret</p><pre><code>kubectl create secret docker-registry docker-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAILkubectl get secret docker-secret -o yamlapiVersion: v1data:  .dockerconfigjson: eyJhdXRocyI6eyJET0NLRVJfUkVHSVNUUllfU0VSVkVSIjp7InVzZXJuYW1lIjoiRE9DS0VSX1VTRVIiLCJwYXNzd29yZCI6IkRPQ0tFUl9QQVNTV09SRCIsImVtYWlsIjoiRE9DS0VSX0VNQUlMIiwiYXV0aCI6IlJFOURTMFZTWDFWVFJWSTZSRTlEUzBWU1gxQkJVMU5YVDFKRSJ9fX0=kind: Secretmetadata:  creationTimestamp: &quot;2021-01-27T14:19:59Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: {}        f:.dockerconfigjson: {}      f:type: {}    manager: kubectl-create    operation: Update    time: &quot;2021-01-27T14:19:59Z&quot;  name: docker-secret  namespace: default  resourceVersion: &quot;811770&quot;  uid: 80501ce1-ead7-4b7f-af33-5df46b8674fatype: kubernetes.io/dockerconfigjson</code></pre><p>下面将该secret挂载到创建容器的配置文件上，如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">name</span><span class="token punctuation">:</span> tomcat<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">imagePullSecrets</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> docker<span class="token punctuation">-</span>secret  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> tomcat<span class="token punctuation">:</span>8.0.51<span class="token punctuation">-</span>alpine    <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent    <span class="token key atrule">name</span><span class="token punctuation">:</span> tomcat    <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span>      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token key atrule">terminationMessagePath</span><span class="token punctuation">:</span> /dev/termination<span class="token punctuation">-</span>log    <span class="token key atrule">terminationMessagePolicy</span><span class="token punctuation">:</span> File    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/run/secrets/kubernetes.io/serviceaccount      <span class="token key atrule">name</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>token<span class="token punctuation">-</span>fzgbs      <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">dnsPolicy</span><span class="token punctuation">:</span> ClusterFirst<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后执行生效，pod将会从指定的镜像地址中，通过验证信息确定拉取镜像！</p><p>也可以将Secret放在ServiceAccount中，pod挂载ServiceAccount来实现私有仓库拉取时的验证！</p><p><strong>补充：</strong> ServiceAccount</p><p>2.13.4.3 SubPath使用</p><p>由于将配置文件或者Secret挂载到容器内的某个目录时，会覆盖该目录下的所有文件，可能会引起容器工作不正常。利用Subpath来解决目录覆盖的问题。</p><p>以nginx容器作为示例，首先导出nginx容器的配置：</p><pre><code>kubectl exec -it nginx-app-76b5cd66f5-mvc5d -- cat /etc/nginx/nginx.conf &gt; nginx.conf</code></pre><p>然后通过该配置文件创建ConfigMap，</p><pre><code>kubectl create configmap nginx-conf --from-file=nginx.conf</code></pre><p>修改deployment为nginx-app的配置信息，挂载该配置文件，如下：</p><pre><code>kubectl edit deploy nginx-app// 修改配置      name: nginx      // 3. 修改启动命令      command: [&quot;sh&quot;,&quot;-c&quot;,&quot;sleep 3600&quot;]      ......      ......      containers:      - image: nginx        imagePullPolicy: Always        name: nginx        // 2. 添加volumeMounts挂载到容器的目录下        volumeMounts:        - name: config-volume          mountPath: /etc/nginx      ......      ......          dnsPolicy: ClusterFirst      restartPolicy: Always      schedulerName: default-scheduler      securityContext: {}      terminationGracePeriodSeconds: 30      // 1. 添加volume信息      volumes:      - name: config-volume        configMap:          name: nginx-conf</code></pre><p>最后来查看效果，会发现，原来nginx容器/etc/nginx目录下会被覆盖掉。</p><pre><code>kubectl exec -it       nginx-app-6dcd794b48-hx784 -- ls /etc/nginx// 只有一条结果，其它的配置文件不存在nginx.conf</code></pre><p>使用subpath来解决这个问题，如下：</p><pre><code>kubectl edit deploy nginx-app// 修改配置      name: nginx      command: [&quot;sh&quot;,&quot;-c&quot;,&quot;sleep 3600&quot;]      ......      ......      containers:      - image: nginx        imagePullPolicy: Always        name: nginx        volumeMounts:        - name: config-volume          // 1. 首先修改mountPath信息，加入文件的完整路径          mountPath: /etc/nginx/nginx.conf          // 3. 添加subPath          // 该处subPath路径和下面编写的items下的路径相同          subPath：etc/nginx/nginx.conf      ......      ......          dnsPolicy: ClusterFirst      restartPolicy: Always      schedulerName: default-scheduler      securityContext: {}      terminationGracePeriodSeconds: 30      volumes:      - name: config-volume        configMap:          name: nginx-conf          // 2. 添加items          items:          - key: nginx.conf            // 注意：path的位置不能以“/”开头            path: etc/nginx/nginx.conf</code></pre><p>这样配置完成后，在subPath中以文件形式挂载，就能正常在nginx配置中生效了，下面查看该configMap的信息如下：</p><pre><code>kubectl get cm nginx-conf -oyaml// 所有配置信息如下apiVersion: v1data:  nginx.conf: &quot;\r\nuser  nginx;\r\nworker_processes  1;\r\n\r\nerror_log  /var/log/nginx/error.log    warn;\r\npid        /var/run/nginx.pid;\r\n\r\n\r\nevents {\r\n    worker_connections    \ 1024;\r\n}\r\n\r\n\r\nhttp {\r\n    include       /etc/nginx/mime.types;\r\n    \   default_type  application/octet-stream;\r\n\r\n    log_format  main  &#39;$remote_addr    - $remote_user [$time_local] \&quot;$request\&quot; &#39;\r\n                      &#39;$status    $body_bytes_sent \&quot;$http_referer\&quot; &#39;\r\n                      &#39;\&quot;$http_user_agent\&quot;    \&quot;$http_x_forwarded_for\&quot;&#39;;\r\n\r\n    access_log  /var/log/nginx/access.log  main;\r\n\r\n    \   sendfile        on;\r\n    #tcp_nopush     on;\r\n\r\n    keepalive_timeout    \ 65;\r\n\r\n    #gzip  on;\r\n\r\n    include /etc/nginx/conf.d/*.conf;\r\n}\r\n&quot;kind: ConfigMapmetadata:  creationTimestamp: &quot;2021-02-09T02:24:49Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: {}        f:nginx.conf: {}    manager: kubectl-create    operation: Update    time: &quot;2021-02-09T02:24:49Z&quot;  name: nginx-conf  namespace: default  resourceVersion: &quot;835513&quot;  uid: 87638177-e623-40e9-96bd-eb39fb84c5c7// 查看该目录下的所有文件信息，发现目录下的文件不再被覆盖kubectl exec -it nginx-app-5cbd79f89c-2qwdf -- ls /etc/nginxconf.d  fastcgi_params  koi-utf  koi-win  mime.types  modules  nginx.conf  scgi_params  uwsgi_params  win-utf</code></pre><p>subPath也可以用在动态PV中，在一个目录下挂载两个path。</p><p><strong>补充：</strong> 以图形化的方式编写k8s的配置文件。</p><p>2.13.4.4 配置信息的热更新</p><p>如果ConfigMap和Secret如果是以subPath的形式挂载的，那么Pod是不会感知到ConfigMap和Secret的更新。也就是无法将配置信息同步到Pod中。</p><p>如果Pod的变量来自于ConfigMap和Secret中定义的内容，那么ConfigMap和Secret更新后，也不会更新Pod中的变量。</p><p>在上一节操作的基础上，添加一组对照的内容，如下：</p><pre><code>kubectl edit deploy nginx-app// 修改配置信息    ......    ......    spec:      containers:      - command:        - sh        - -c        - sleep 3600        image: nginx        imagePullPolicy: Always        name: nginx        ports:        - containerPort: 80          protocol: TCP        resources: {}        terminationMessagePath: /dev/termination-log        terminationMessagePolicy: File        // 2. 添加新的挂载点        volumeMounts:        - mountPath: /etc/nginx/nginx.conf          name: config-volume          subPath: etc/nginx/nginx.conf        // 添加mnt挂载新的路径        - mountPath: /mnt/          name: config-volume-non-subpath      dnsPolicy: ClusterFirst      restartPolicy: Always      schedulerName: default-scheduler      securityContext: {}      terminationGracePeriodSeconds: 30      volumes:      - configMap:          defaultMode: 420          items:          - key: nginx.conf            path: etc/nginx/nginx.conf          name: nginx-conf        name: config-volume      // 1. 添加新的配置信息      - configMap:          defaultMode: 420          name: nginx-conf        name: config-volume-non-subpath        ......        ......</code></pre><p>查看最终结果信息，如下：</p><pre><code>kubectl exec -it nginx-app-d87589bdc-jsc9r -- cat /etc/nginx/nginx.conf// 输出结果如下user  nginx;worker_processes  2;kubectl exec -it nginx-app-d87589bdc-jsc9r -- cat /mnt/nginx.conf// 输出结果如下user  nginx;worker_processes  2;</code></pre><p>修改ConfigMap，将work_processes的值改为3，然后再进行查看，如下：</p><pre><code>kubectl edit cm nginx-conf// 修改配置信息worker_processes 3;// 等待2分钟左右会在Pod中生效// 查看如下：kubectl exec -it nginx-app-d87589bdc-jsc9r -- cat /etc/nginx/nginx.conf// 输出结果如下user  nginx;worker_processes  2;kubectl exec -it nginx-app-d87589bdc-jsc9r -- cat /mnt/nginx.conf// 输出结果如下user  nginx;worker_processes  3;</code></pre><p>由上述结果对比，subPath的方式不能进行配置信息的热更新。</p><p>在热更新修改ConfigMap的时候，有以下三种方式，修改比较完善：</p><ol><li>（<strong>待补充</strong>）图形化资源管理平台进行修改–Ratel</li><li>kubectl edit cm ConfigMap名称，在线进行修改</li><li>修改配置文件后，使用create + –dry-run命令进行运行，如下：</li></ol><pre><code>kubectl create cm nginx-conf --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f-</code></pre><p><em>–dry-run</em>:–dry-run=’none’: Must be “none”, “server”, or “client”. If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource.</p><p>分两步区别的话，通过–dry-run命令生成配置文件的yaml信息，然后使用replace来进行替代。</p><p>热更新Secret时，将上述命令中的ConfigMap更换为Secret即可。</p><p><strong>注意：</strong>不可变的ConfigMap和Secret，immutable ConfigMap和immutable Secret。</p><p>在yaml配置文件中，添加<strong>immutable: true</strong>这一句配置信息。k8s 1.18版本以后可用。</p><p>2.14 存储卷和存储–Volumes</p><p>2.14.1 Volumes</p><p>解决问题：</p><ol><li>Container中的磁盘文件是短暂的，但容器崩溃时，kubelet会重新启动容器，但最初的文件会丢失。通过Volumes来保存这些文件。</li><li>当一个Pod运行多个Container时，各个容器可能需要共享一些文件，通过Volumes来进行共享文件的存储。同时可以支持多个Pod之间的文件共享。</li></ol><p>总结：一些需要持久化数据的程序或者需要共享数据的容器需要使用Volumes。例如：Redis-Cluster中的配置文件nodes.conf，日志收集的需求，需要在应用程序的容器里面加一个SideCar，这个容器专门负责收集日志，比如filebeat，它通过volumes共享应用程序的日志文件目录。</p><p>其它：nfs作为存储来说效率太低。使用其它的后端存储或者云存储代替。</p><p>2.14.2 HostPath模式</p><p>hostPath：将节点上的文件和目录挂载到Pod上，如果Pod被删除，将会hostPath指定的路径将会不变。</p><p>不推荐使用，无法保证Pod总是部署在特定的节点上，如果更换了节点，将会导致对应的文件找不到，导致运行失败的情况。</p><p>演示：通过hostPath挂载timezone文件信息</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># vim nginx-volumes.yaml</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> 25%      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> 25%    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> enter          <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>        <span class="token comment" spellcheck="true"># 2. 挂载该volume文件信息 </span>        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /etc/timezone          <span class="token key atrule">name</span><span class="token punctuation">:</span> timezone      <span class="token comment" spellcheck="true"># 1. 添加该Volume信息，挂载/etc/timezone文件</span>      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> timezone        <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>          <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/timezone          <span class="token key atrule">type</span><span class="token punctuation">:</span> File<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使配置信息生效，并且查看时区信息，如下：</p><pre><code>// 使配置信息生效# kubectl apply -f nginx-volumes.yaml# kubectl exec -it nginx-689f8fd586-mq57p -- cat /etc/timezoneAsia/Shanghai# cat /etc/timezoneAsia/Shanghai# kubectl exec -it nginx-app-d87589bdc-jsc9r -- cat /etc/timezoneEtc/UTC</code></pre><p>hostPath支持的类型如下：</p><table><thead><tr><th>类型</th><th>解释</th></tr></thead><tbody><tr><td></td><td>空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。</td></tr><tr><td>DirectoryOrCreate</td><td>如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。</td></tr><tr><td>Directory</td><td>在给定路径上必须存在的目录。</td></tr><tr><td>FileOrCreate</td><td>如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。</td></tr><tr><td>File</td><td>在给定路径上必须存在的文件。</td></tr><tr><td>Socket</td><td>在给定路径上必须存在的 UNIX 套接字。</td></tr><tr><td>CharDevice</td><td>在给定路径上必须存在的字符设备。</td></tr><tr><td>BlockDevice</td><td>在给定路径上必须存在的块设备。</td></tr></tbody></table><p>当使用这种类型的卷时要小心，因为：</p><ol><li>具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod 会由于节点上文件的不同 而在不同节点上有不同的行为。</li><li>下层主机上创建的文件或目录只能由 root 用户写入。你需要在 特权容器 中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 hostPath 卷。</li></ol><p>2.14.3 emptyDir模式</p><p>共享目录使用该模式进行，可以被挂载到相同或者不同的卷上。如果删除Pod，emptyDir卷中的数据也将被删除。</p><p>默认情况下，emptyDir卷支持节点上的任何介质，可能是SSD、磁盘或网络存储，具体取决于自身的环境。可以将emptyDir.medium字段设置为Memory,让Kubernetes使用tmpfs （内存支持的文件系统），虽然tmpfs非常快，但是tmpfs在节点重启时，数据同样会被清除，并且设置的大小会被计入到Container的内存限制当中。</p><p>在上一个hostPath的基础上，进行修改：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> 25%      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> 25%    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># 3. 创建一个非同名容器，挂载到/opt目录下</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> enter          <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /opt          <span class="token key atrule">name</span><span class="token punctuation">:</span> share<span class="token punctuation">-</span>volume      <span class="token comment" spellcheck="true"># 2. 创建一个非同名容器，挂载到/mnt目录下</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span><span class="token number">2</span>        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> enter          <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt          <span class="token key atrule">name</span><span class="token punctuation">:</span> share<span class="token punctuation">-</span>volume      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># 1. 创建一个emptyDir路径</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> share<span class="token punctuation">-</span>volume        <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>          <span class="token comment" spellcheck="true"># 如果使用Memory的设置，需要去除上面的花括号</span>          <span class="token comment" spellcheck="true">#medium: Memory</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> timezone        <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>          <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/timezone          <span class="token key atrule">type</span><span class="token punctuation">:</span> File<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在一个Pod中启动多个容器，其网络空间是共享的，而两个nginx容器，均使用了80端口，在启动第二个容器时出现了端口被占用的问题。</p><p>临时的解决方式是，修改启动的command命令，例如将第二个pod重新配置command命令信息，如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">...</span><span class="token punctuation">...</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span><span class="token number">2</span>        <span class="token key atrule">command</span><span class="token punctuation">:</span>         <span class="token punctuation">-</span> sh        <span class="token punctuation">-</span> <span class="token punctuation">-</span>c        <span class="token punctuation">-</span> sleep 3600        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> enter          <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt          <span class="token key atrule">name</span><span class="token punctuation">:</span> share<span class="token punctuation">-</span>volume<span class="token punctuation">...</span><span class="token punctuation">...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>彻底的解决办法是，尽可能不使用相同的pod进行部署，或者修改command启动命令，更换端口号信息。</p><p>这样修改完成后，在第一个名称为nginx的container中创建文件并添加内容如下：</p><pre><code># kubectl exec -it nginx-5d67cccf59-69fv8 -c nginx --  bashroot@nginx-5d67cccf59-69fv8:/# touch /mnt/123 &amp;&amp; echo &#39;111111&#39; &gt; 123// exit退出该容器</code></pre><p>第一个container中挂载的目录为/mnt，因此在/mnt目录下创建，而第二个container中挂载的目录为/opt，这样在第二个容器的opt目录中查看第一个容器中创建的文件信息</p><pre><code># kubectl exec -it nginx-5d67cccf59-69fv8 -c nginx2 --  bashroot@nginx-5d67cccf59-69fv8:/# cat /opt/123// 输出内容如下111111</code></pre><p>这样就实现了文件的共享。需要注意的是，在含有多个容器的Pod中，指定要访问的容器信息，需要使用”-c”来进行指定。</p><p>2.14.4 nfs安装与挂载</p><p>在192.168.229.54这台机器上安装nfs，如下：</p><pre><code># yum install nfs-utils -y# systemctl enable nfs-server &amp;&amp; systemctl start nfs-server</code></pre><p>这样就启动了nfs服务，查看当前nfs服务支持的nfs版本，如下：</p><pre><code># cat /proc/fs/nfsd/versions-2 +3 +4 +4.1 +4.2</code></pre><p>下面开始创建并配置共享目录，如下：</p><pre><code># mkdir -p /data/nfs// 修改export配置文件，加入该目录# vim /etc/exports// 添加以下内容/data/nfs 192.168.229.0/24(rw,sync,no_subtree_check,no_root_squash)// :wq保存退出// 配置完成后需要使配置生效# exportfs -r# systemctl restart nfs-server</code></pre><p>这样nfs就安装完成了，重要的是，需要给剩下的每个节点装一下nfs-utils，否则会出现挂载的时候识别不了nfs存储。全部安装完成后，下面回到192.168.229.51这台机器上，尝试挂载nfs，操作如下：</p><pre><code># mount -t nfs 192.168.229.54:/data/nfs /mnt/</code></pre><p>挂载完成后可以在nfs安装的机器上查看挂载信息，如下：</p><pre><code># showmount -eExport list for k8s-node-01:/data/nfs 192.168.229.0/24</code></pre><p>这时回到192.168.229.51机器上，创建文件，测试nfs是否可用，如下：</p><pre><code># cd /mnt &amp;&amp; touch 123 &amp;&amp; echo &quot;111111111111111111&quot; &gt; 123</code></pre><p>在nginx中挂载nfs服务，首先在192.168.229.51所在机器上，卸载nfs服务所在的文件夹，然后再进行挂载。</p><pre><code># umount /mnt// 找到之前的nginx配置文件，通过volumes挂载nfs服务# cd ~/k8s-practice/volumes &amp;&amp; vim nginx-volumes.yamlapiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx  namespace: defaultspec:  replicas: 3  revisionHistoryLimit: 10  selector:    matchLabels:      app: nginx  strategy:    rollingUpdate:      maxSurge: 25%      maxUnavailable: 25%    type: RollingUpdate  template:    metadata:      labels:        app: nginx    spec:      containers:      - image: nginx:1.16.1        imagePullPolicy: IfNotPresent        name: nginx        ports:        - name: enter          containerPort: 80        volumeMounts:        - mountPath: /mnt          name: share-volume      - image: nginx:1.15.2        imagePullPolicy: IfNotPresent        name: nginx2        command:        - sh        - -c        - sleep 3600        ports:        - name: enter2          containerPort: 81        volumeMounts:        - mountPath: /opt          name: share-volume        # 2. 挂载nfs服务        - mountPath: /mnt          name: nfs-volume                volumes:      - name: share-volume        emptyDir: {}          # 如果使用Memory的设置，需要去除上面的花括号          #medium: Memory      - name: timezone        hostPath:          path: /etc/timezone          type: File      # 1. 添加nfs目录      - name: nfs-volume        nfs:          server: 192.168.229.54          # 0. 记得在nfs服务器上首先创建该目录，并且在该目录下创建名称为123的文件          path: /data/nfs/test-dp// :wq保存退出// 使配置信息生效并进行查看# kubectl replace -f nginx-deployment.yaml// 查看结果# kubectl exec -it nginx-64fcb85cd9-5gk8x -- ls -al /optdrwxrwxrwx 2 root root 17 Feb 27 04:25 .drwxr-xr-x 1 root root 28 Feb 27 04:28 ..-rw-r--r-- 1 root root 19 Feb 27 04:25 123</code></pre><p><strong>注意：</strong>在生产环境中还是不推荐使用nfs存储的，由于nfs没有任何高可用的保障，难以实现高可用的架构。</p><p>一般情况下使用pv和pvc的方式挂载nfs。</p><p>2.14.5 持久化存储PV和PVC</p><p>Volume：NFS、Ceph、GFS</p><p>PV：PersistentVolume，NFS、Ceph、GFS，由k8s配置连接不同的存储，PV同样使集群的一类资源，可以用yaml进行定义，相当于一块存储。可以由管理员定义，由PV去连接后向的存储。</p><p>PVC：PersistentVolumeClaim，对PV的申请。将PVC挂载到容器中，由容器去使用。</p><p>Pod –&gt; PVC –&gt; PV –&gt; Storage</p><p>PV/PVC用来管理k8s的存储，降低了复杂度和简化了使用。PV通过不同的配置，连接对应的后向存储，但是PVC连接PV的时候，使用相同的方式连接。PV用作申请存储，而PVC用作绑定存储。</p><p>PV又区分为动态存储和静态存储。</p><p>2.14.5.1 使用PV连接nfs存储</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> pv0003<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># PV的容量</span>  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi  <span class="token key atrule">volumeMode</span><span class="token punctuation">:</span> Filesystem  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> ReadWriteOnce  <span class="token comment" spellcheck="true"># 回收策略</span>  <span class="token key atrule">persistentVolumeReclaimPolicy</span><span class="token punctuation">:</span> Recycle  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> slow  <span class="token key atrule">mountOptions</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> hard    <span class="token punctuation">-</span> nfsvers=4.1  <span class="token key atrule">nfs</span><span class="token punctuation">:</span>    <span class="token key atrule">path</span><span class="token punctuation">:</span> /tmp    <span class="token key atrule">server</span><span class="token punctuation">:</span> 172.17.0.2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>回收策略persistentVolumeReclaimPolicy: Retain、Recycle和Delete</p><ul><li>Recycle：回收并销毁文件，rm -rf</li><li>Retain: 保留</li><li>Delete：删除了PVC，同时也会删除PV，这一类的PV需要支持删除的功能（动态存储支持）</li></ul><p>Capacity：PV的容量</p><p>volumeMode： 挂载类型，FileSystem、block</p><p>accessModes：访问模式</p><ul><li>ReadWriteOnce： RWO，可以被单节点以读写模式挂载</li><li>ReadWriteMany： RWX，可以被多节点以读写模式挂载</li><li>ReadOnlyMany： ROX，可以被多个节点以只读模式挂载</li></ul><p>storageClassName：PV的类（类名），PVC和PV的这个名字一样才能被绑定，名字不同，读写权限不同是不能进行绑定的。</p><p>mountOptions：挂载参数</p><p><strong>注意：</strong>不需要记住所有的配置，只需要知道连接的原理即可。</p><p>实际操作，创建PV，配置文件如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolume<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> pv0001<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># PV的容量</span>  <span class="token key atrule">capacity</span><span class="token punctuation">:</span>    <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi  <span class="token key atrule">volumeMode</span><span class="token punctuation">:</span> Filesystem  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> ReadWriteMany  <span class="token comment" spellcheck="true"># 回收策略</span>  <span class="token key atrule">persistentVolumeReclaimPolicy</span><span class="token punctuation">:</span> Recycle  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> nfs<span class="token punctuation">-</span>slow  <span class="token key atrule">mountOptions</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> hard    <span class="token punctuation">-</span> nfsvers=4.1  <span class="token key atrule">nfs</span><span class="token punctuation">:</span>    <span class="token key atrule">path</span><span class="token punctuation">:</span> /data/nfs/test<span class="token punctuation">-</span>dp    <span class="token key atrule">server</span><span class="token punctuation">:</span> 192.168.229.54<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：PV没有namespace的限制，直接创建即可。但PVC有namespace限制！</p><pre><code>// 创建pv# kubectl create -f nginx-pv.yaml// 查看pv# kubectl get pvNAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGEpv0001   1Gi        RWX            Recycle          Available           nfs-slow                9s</code></pre><p>PV的状态：</p><ul><li>Available：空闲的，没有被任何pvc绑定</li><li>Bound: 已经被pvc绑定</li><li>Released： pvc被删除，但是资源未被重新使用</li><li>Failed：自动回收失败</li></ul><p>创建一个pvc去申请使用pv，配置文件如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> myclaim<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 访问策略要和pv中配置的权限一致</span>  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> ReadWriteMany  <span class="token key atrule">volumeMode</span><span class="token punctuation">:</span> Filesystem  <span class="token key atrule">resources</span><span class="token punctuation">:</span>    <span class="token key atrule">requests</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># 存储大小不能超过pv创建时配置的文件大小</span>      <span class="token comment" spellcheck="true"># 如果超过pv创建时配置的存储大小，将会导致找不到对应的pv</span>      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi  <span class="token comment" spellcheck="true"># 这个位置名称要和pv中配置的名称一致</span>  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> nfs<span class="token punctuation">-</span>slow<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建pv信息如下：</p><pre><code># kubectl create -f nginx-pvc.yaml# kubectl get pvcNAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGEmy-pvc   Bound    pv0001   1Gi        RWX            nfs-slow       19s# kubectl get pvNAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGEpv0001   1Gi        RWX            Recycle          Bound    default/my-pvc   nfs-slow                12m</code></pre><p>创建成功后，可以看到pv和pvc的状态，均为bound的状态，pv的claim下也显示了哪个pvc绑定了该pv。</p><p><strong>注意：</strong>PVC不允许使用kubectl edit命令进行更改</p><p>最后在deployment中使用这个pvc。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> 25%      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> 25%    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> enter          <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt          <span class="token key atrule">name</span><span class="token punctuation">:</span> share<span class="token punctuation">-</span>volume        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /opt          <span class="token key atrule">name</span><span class="token punctuation">:</span> nfs<span class="token punctuation">-</span>volume      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.15.2        <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx2        <span class="token key atrule">command</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> sh        <span class="token punctuation">-</span> <span class="token punctuation">-</span>c        <span class="token punctuation">-</span> sleep 3600        <span class="token key atrule">ports</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> enter2          <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">81</span>        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 2. 挂载pvc所在的目录</span>        <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /tmp/pvc          <span class="token key atrule">name</span><span class="token punctuation">:</span> pvc<span class="token punctuation">-</span>test      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> share<span class="token punctuation">-</span>volume        <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>          <span class="token comment" spellcheck="true"># 如果使用Memory的设置，需要去除上面的花括号</span>          <span class="token comment" spellcheck="true">#medium: Memory</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> timezone        <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>          <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/timezone          <span class="token key atrule">type</span><span class="token punctuation">:</span> File      <span class="token comment" spellcheck="true"># 1. 新增pvc的引用</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pvc<span class="token punctuation">-</span>test        <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 注意：claimName对应的是pvc的名称</span>          <span class="token key atrule">claimName</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>pvc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建pv的配置文件信息可能有不同，但是对于pvc和volume的挂载，使用方式是一致的。</p><p>最后使其生效，然后查看结果：</p><pre><code># kubectl create -f nginx-pv-pvc.yaml# kubectl get podNAME                          READY   STATUS      RESTARTS   AGEnginx-6976f57754-75chk        2/2     Running     0          4m42snginx-6976f57754-8fjzp        2/2     Running     0          4m42snginx-6976f57754-rp6p5        2/2     Running     0          4m42s# kubectl exec -it nginx-6976f57754-rp6p5 -c nginx2 -- df -ThFilesystem                       Type     Size  Used Avail Use% Mounted onoverlay                          overlay   17G  5.0G   13G  30% /tmpfs                            tmpfs     64M     0   64M   0% /devtmpfs                            tmpfs    2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root          xfs       17G  5.0G   13G  30% /opt192.168.229.54:/data/nfs/test-dp nfs4      17G  5.0G   13G  30% /tmp/pvcshm                              tmpfs     64M     0   64M   0% /dev/shmtmpfs                            tmpfs    2.0G   12K  2.0G   1% /run/secrets/kubernetes.io/serviceaccounttmpfs                            tmpfs    2.0G     0  2.0G   0% /proc/acpitmpfs                            tmpfs    2.0G     0  2.0G   0% /proc/scsitmpfs                            tmpfs    2.0G     0  2.0G   0% /sys/firmware</code></pre><p>可以看到<em>192.168.229.54:/data/nfs/test-dp nfs4      17G  5.0G   13G  30% /tmp/pvc</em>记录已经显示pvc挂载到该容器中。</p><p>这时候，可以进入到容器中对应的/tmp/pvc目录下，创建文件并添加内容，最终可以在nfs存储所在的机器上同时看到该文件。</p><p>2.14.5.2 常见问题</p><ul><li>创建PVC之后一致绑定不上PV</li></ul><ol><li>PVC的空间申请大小大于PV的大小</li><li>PVC的StorageClassName没有和PV中设定的一致</li><li>PVC的访问模式accessModes和PV中设定的不一致</li></ol><ul><li>创建挂载了PVC的Pod之后，一直处于Pending状态</li></ul><ol><li>PVC没有创建成功，或者没有被创建</li><li>PVC和Pod不在同一个namespace下</li></ol><p>存储删除时，如果要删除PVC，必须要先删除挂载该PVC的容器或者Deployment，之后再删除该PVC，否则PVC一直处于Terminal状态。</p><p>如果删除了PVC，再删除Pod，将会导致Deployment下的pod重新上线时，一直处在pending的状态，无法找到该PVC，也就无法挂载，导致Pod无法重新启动。这时候，PVC删除会出现超时的情况。</p><p>如果出现了上述情况，还要删除PVC，那么必须先删除使用该PVC的容器，因为PVC被删掉后，容器的运行状态不正常，无法对外提供服务。</p><p>删除时，必须先删除PV再删除PVC，如果先删除PVC，必须要把挂载该PVC的容器或者deployment删掉，</p><ul><li><p>PVC有namespace的区分，PV则是全局存在的。</p></li><li><p>Recycle模式下，回收PV时，会主动创建一个回收镜像来进行回收操作。删除PVC以后，k8s或创建一个用于回收的Pod，根据PV的回收策略进行PV的回收，回收完以后，PV的状态就会变成可被绑定的状态，也就是空闲状态，其它Pending状态的PVC如果匹配到这个PV，就可以和这个PV进行绑定。</p></li></ul><p>注意：PVC在任意一个namespace下都可以和对应的PV进行绑定。</p><ul><li><p>PV支持selector标签挂载。</p></li><li><p>自定义存储CSI的使用（待补充）</p></li></ul><p>2.14.6 cronjob计划任务</p><p>在k8s里面运行周期性的计划任务，类型linux上的crontab。</p><ul><li><p>“* * * * * 分 时 日 月 周”的编写方式，同crontab</p></li><li><p>计划任务可能需要调用应用的接口，可能需要依赖某些环境，例如php中运行cronjob，不需要在宿主机安装php环境，直接借助php的docker镜像（但是拉取镜像需要时间，需要考虑cronjob的运行调度）</p></li><li><p>cronjob被调用的时间，同controller-manager的时间一致。如果controller-manager是二进制进行启动，其时间和宿主机一致。如果部署在容器中，要保证容器的时间是正确的。</p></li></ul><p><strong>注意：</strong>cronjob的名字不能超过52个字符，创建时，会在该名称后面添加11个字符，如果超过63个字符，名称会过长导致不能解析该名称。</p><p>下面开始演示cronjob的编写方式。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1beta1<span class="token key atrule">kind</span><span class="token punctuation">:</span> CronJob<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> hello<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">schedule</span><span class="token punctuation">:</span> <span class="token string">"*/2 * * * *"</span>  <span class="token key atrule">jobTemplate</span><span class="token punctuation">:</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">template</span><span class="token punctuation">:</span>        <span class="token key atrule">spec</span><span class="token punctuation">:</span>          <span class="token key atrule">containers</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> hello            <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox            <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent            <span class="token key atrule">command</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> /bin/sh            <span class="token punctuation">-</span> <span class="token punctuation">-</span>c            <span class="token punctuation">-</span> date; echo Hello from the Kubernetes cluster          <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code># kubectl apply -f busybox-cronjob.yaml# kubectl get cj helloNAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGEhello   */2 * * * *   False     1        5s              10s// 查看创建后的cronjob信息# kubectl get cj hellp -oyaml</code></pre><p>查看具体的cronjob配置信息如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1beta1<span class="token key atrule">kind</span><span class="token punctuation">:</span> CronJob<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> hello  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> default<span class="token key atrule">spec</span><span class="token punctuation">:</span>   <span class="token key atrule">concurrencyPolicy</span><span class="token punctuation">:</span> Allow  <span class="token comment" spellcheck="true"># 并发调度控制，Allow：允许同时运行多个任务；Forbid：不进行并发执行多个任务；Replace：替换之前的任务</span>  <span class="token key atrule">failedJobsHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">1  </span><span class="token comment" spellcheck="true"># 保留失败的任务数，可以按需设置较大的数值，看到为何执行出错的问题</span>  <span class="token key atrule">jobTemplate</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token null important">null</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">template</span><span class="token punctuation">:</span>        <span class="token key atrule">metadata</span><span class="token punctuation">:</span>          <span class="token key atrule">creationTimestamp</span><span class="token punctuation">:</span> <span class="token null important">null</span>        <span class="token key atrule">spec</span><span class="token punctuation">:</span>          <span class="token key atrule">containers</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">command</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> /bin/sh            <span class="token punctuation">-</span> <span class="token punctuation">-</span>c            <span class="token punctuation">-</span> date; echo Hello from the Kubernetes cluster            <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox            <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent            <span class="token key atrule">name</span><span class="token punctuation">:</span> hello            <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            <span class="token key atrule">terminationMessagePath</span><span class="token punctuation">:</span> /dev/termination<span class="token punctuation">-</span>log            <span class="token key atrule">terminationMessagePolicy</span><span class="token punctuation">:</span> File          <span class="token key atrule">dnsPolicy</span><span class="token punctuation">:</span> ClusterFirst          <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure          <span class="token key atrule">schedulerName</span><span class="token punctuation">:</span> default<span class="token punctuation">-</span>scheduler          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>          <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>  <span class="token key atrule">schedule</span><span class="token punctuation">:</span> <span class="token string">'*/2 * * * *'</span>   <span class="token comment" spellcheck="true"># 调度策略</span>  <span class="token key atrule">successfulJobsHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">3  </span><span class="token comment" spellcheck="true"># 成功的job保留的次数</span>  <span class="token key atrule">suspend</span><span class="token punctuation">:</span> <span class="token boolean important">false  </span><span class="token comment" spellcheck="true"># 任务挂起，true，cronjob不会被执行</span>  <span class="token key atrule">startingDeadlineSeconds</span><span class="token punctuation">:</span> <span class="token number">30 </span><span class="token comment" spellcheck="true"># 在30秒内失败后会继续重新调用执行该cronjob，直到成功执行为止</span><span class="token key atrule">status</span><span class="token punctuation">:</span>  <span class="token key atrule">lastScheduleTime</span><span class="token punctuation">:</span> <span class="token string">"2021-03-07T01:58:00Z"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后，需要查看cronjob的执行内容，如下：</p><pre><code># kubectl get podNAME                          READY   STATUS      RESTARTS   AGEhello-1615082760-wpkzd        0/1     Completed   0          6mhello-1615082880-gvqg2        0/1     Completed   0          4mhello-1615083000-vzhc6        0/1     Completed   0          119s# kubectl logs -f hello-1615083000-vzhc6Sun Mar  7 02:10:05 UTC 2021Hello from the Kubernetes cluster</code></pre><p><strong>注意：</strong>1. cronjob有namespace隔离的概念。2. pod随着执行完成的job，保留的数量有限，在查看执行结果前要想查看该pod是否还存在。</p><p>2.14.7 Taint &amp; Toleration</p><p>类比NodeSelector的内容。</p><p>污点和容忍的理念</p><p>Taint：在一类服务器上打上污点，让不能容忍这个污点的Pod不能部署在打了污点的服务器上。</p><p>例如Master节点不应该部署系统Pod之外的任何Pod上。可以用Taint进行打污点，每个节点可以打多个污点。</p><p>举例子如下：</p><p>某个Node机器上存在<em>gpu-server:true</em>的Taint污点存在，该节点只能部署容忍该污点的容器存在，而这时某个Pod上添加了一个Toleration满足<em>gpu-server:true</em>，才能部署到这个Node机器上。</p><p><em>taint的结构为： key=value:effect</em>。</p><p>在没打污点之前，查看一下各个Pod在k8s中的部署情况</p><pre><code># kubectl get pod -o wideNAME                          READY   STATUS      RESTARTS   AGE     IP                NODE            NOMINATED NODE   READINESS GATESbusybox                       1/1     Running     163        48d     172.163.98.171    k8s-node-02     &lt;none&gt;           &lt;none&gt;hello-1615085760-6mx25        0/1     Completed   0          5m56s   172.165.176.216   k8s-master-02   &lt;none&gt;           &lt;none&gt;hello-1615085880-x2x4r        0/1     Completed   0          3m56s   172.164.183.161   k8s-master-03   &lt;none&gt;           &lt;none&gt;hello-1615086000-hbbzz        0/1     Completed   0          115s    172.164.183.162   k8s-master-03   &lt;none&gt;           &lt;none&gt;mypod                         0/1     Completed   0          40d     172.175.44.56     k8s-node-01     &lt;none&gt;           &lt;none&gt;mypod-multi                   0/1     Completed   0          40d     172.175.44.57     k8s-node-01     &lt;none&gt;           &lt;none&gt;mypod-secret                  0/1     Completed   0          39d     172.175.44.61     k8s-node-01     &lt;none&gt;           &lt;none&gt;mypod-volume                  0/1     Completed   0          40d     172.175.44.58     k8s-node-01     &lt;none&gt;           &lt;none&gt;nginx-6976f57754-75chk        2/2     Running     6          6d23h   172.163.98.187    k8s-node-02     &lt;none&gt;           &lt;none&gt;nginx-6976f57754-8fjzp        2/2     Running     6          6d23h   172.174.107.71    k8s-master-01   &lt;none&gt;           &lt;none&gt;nginx-6976f57754-rp6p5        2/2     Running     6          6d23h   172.175.44.19     k8s-node-01     &lt;none&gt;           &lt;none&gt;nginx-app-d87589bdc-bqpff     1/1     Running     71         24d     172.175.44.60     k8s-node-01     &lt;none&gt;           &lt;none&gt;nginx-app-d87589bdc-jsc9r     1/1     Running     71         24d     172.163.98.175    k8s-node-02     &lt;none&gt;           &lt;none&gt;tomcat-app-866957f847-cth22   1/1     Running     1          41d     172.175.44.2      k8s-node-01     &lt;none&gt;           &lt;none&gt;tomcat-app-866957f847-z87bw   1/1     Running     1          41d     172.163.98.176    k8s-node-02     &lt;none&gt;           &lt;none&gt;</code></pre><p>可以看到k8s-master-01上有部署的业务Pod。在master节点打一个污点如下：</p><pre><code># kubectl taint node k8s-master-01 master-test=test:NoSchedule </code></pre><p><strong>注意：</strong>NoSchedule：只是让该节点不会被调度，无法进行强制迁移；</p><pre><code># kubectl edit deploy nginx......      restartPolicy: Always      schedulerName: default-scheduler      # 添加下面的nodeSelector信息      nodeSelector:        kubernetes.io/hostname: k8s-master-01      securityContext: {}      terminationGracePeriodSeconds: 30      volumes:......# kubectl get pod -o wideNAME                          READY   STATUS      RESTARTS   AGE     IP                NODE            NOMINATED NODE   READINESS GATESnginx-7bd9f58674-w5fnp        0/2     Pending     0          3m22s   &lt;none&gt;            &lt;none&gt;          &lt;none&gt;           &lt;none&gt;# kubectl describe pod  nginx-7bd9f58674-w5fnp......Events:  Type     Reason            Age    From               Message  ----     ------            ----   ----               -------  Warning  FailedScheduling  4m46s  default-scheduler  0/5 nodes are available: 1 node(s) had taint {master-test: test}, that the pod didn&#39;t tolerate, 4 node(s) didn&#39;t match Pod&#39;s node affinity.  Warning  FailedScheduling  4m46s  default-scheduler  0/5 nodes are available: 1 node(s) had taint {master-test: test}, that the pod didn&#39;t tolerate, 4 node(s) didn&#39;t match Pod&#39;s node affinity.</code></pre><p>可以看到pod上没有master-test: test没有这个Toleration，不符合k8s-master-01节点上刚刚设置的Taint，导致pod无法被部署。</p><p>重新修改deploy，添加toleration测试：</p><pre><code># kubectl edit deploy nginx......      restartPolicy: Always      schedulerName: default-scheduler      # 添加下面的nodeSelector信息      nodeSelector:        kubernetes.io/hostname: k8s-master-01      tolerations:      - key: &quot;master-test&quot;        value: &quot;test&quot;        effect: &quot;NoSchedule&quot;        operator: &quot;Equal&quot;      securityContext: {}      terminationGracePeriodSeconds: 30      volumes:......# kubectl get pod -o wideNAME                          READY   STATUS        RESTARTS   AGE     IP                NODE            NOMINATED NODE   READINESS GATESnginx-6976f57754-75chk        2/2     Terminating   7          6d23h   172.163.98.187    k8s-node-02     &lt;none&gt;           &lt;none&gt;nginx-6976f57754-8fjzp        2/2     Terminating   7          6d23h   172.174.107.71    k8s-master-01   &lt;none&gt;           &lt;none&gt;nginx-6976f57754-rp6p5        2/2     Terminating   7          6d23h   172.175.44.19     k8s-node-01     &lt;none&gt;           &lt;none&gt;nginx-6dfccfcb68-dct2p        2/2     Running       0          8s      172.174.107.73    k8s-master-01   &lt;none&gt;           &lt;none&gt;nginx-6dfccfcb68-fm26r        2/2     Running       0          6s      172.174.107.74    k8s-master-01   &lt;none&gt;           &lt;none&gt;nginx-6dfccfcb68-vkvcx        2/2     Running       0          10s     172.174.107.72    k8s-master-01   &lt;none&gt;           &lt;none&gt;# kubectl describe pod nginx-6dfccfcb68-vkvcx ......Tolerations:     master-test=test:NoSchedule                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300sEvents:  Type    Reason     Age   From               Message  ----    ------     ----  ----               -------  Normal  Scheduled  60s   default-scheduler  Successfully assigned default/nginx-6dfccfcb68-vkvcx to k8s-master-01  Normal  Pulled     60s   kubelet            Container image &quot;nginx:1.16.1&quot; already present on machine  Normal  Created    59s   kubelet            Created container nginx  Normal  Started    59s   kubelet            Started container nginx  Normal  Pulled     59s   kubelet            Container image &quot;nginx:1.15.2&quot; already present on machine  Normal  Created    59s   kubelet            Created container nginx2  Normal  Started    59s   kubelet            Started container nginx2</code></pre><p>可以看到添加toleration以后，nginx的pod全部调度到k8s-master-01节点上。</p><pre class="line-numbers language-yaml"><code class="language-yaml">      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">"master-test"</span>        <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"test"</span>        <span class="token key atrule">effect</span><span class="token punctuation">:</span> <span class="token string">"NoSchedule"</span>        <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">"Equal"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>effect参数信息：</p><ul><li>NoSchedule：禁止调度</li><li>NoExecute：如果不符合这个污点，会立刻被驱逐</li></ul><pre class="line-numbers language-yaml"><code class="language-yaml">      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">"master-test"</span>        <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"test"</span>        <span class="token key atrule">effect</span><span class="token punctuation">:</span> <span class="token string">"NoExecute"</span>        <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">"Equal"</span>        <span class="token key atrule">tolerationSeconds</span><span class="token punctuation">:</span> <span class="token number">60 </span><span class="token comment" spellcheck="true"># 在节点上允许的停留时间，超过这个时间还是要被驱逐</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>PreferNoSchedule：尽量避免将Pod调度到指定的节点上（软污点，非强制性的）</li></ul><p>operator参数信息：</p><ul><li>Equal：完全匹配key、value、effect这三个taint才能被调度。</li><li>Exists：</li></ul><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># 只要匹配到key和effect都可以容忍，不管value是多少</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">"master-test"</span>        <span class="token key atrule">effect</span><span class="token punctuation">:</span> <span class="token string">"NoSchedule"</span>        <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">"Exists"</span><span class="token comment" spellcheck="true"># 或者如下所示，容忍所有的taint</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">"Exists"</span><span class="token comment" spellcheck="true"># 或者如下所示，只要节点存在matest-test的key，容忍匹配的taint</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">"Exists"</span>        <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">"master-test"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果node节点有多个Taint，在Pod/Deployment上设置每个Taint都需要容忍才能部署到该node上。</p><p>删除污点的操作如下：</p><pre><code># kubectl taint node k8s-master-01 master-test:NoExecute-node/k8s-master-01 untainted</code></pre><p>当某个节点不正常时，k8s中master会自动给不正常的节点，添加几个Taint。通过describe命令可以查看，排查节点问题。</p><pre><code># kubectl describe node k8s-master-01node.kubernetes.io/unreachable:NoExecutenode.kubernetes.io/unreachable:NoSchedulenode.kubernetes.io/not-ready:NoExecute for 300s   # 节点没有准备好node.kubernetes.io/unreachable:NoExecute for 300s</code></pre><p>参考连接：<a href="https://www.cnblogs.com/tylerzhou/p/11026364.html" target="_blank" rel="noopener">https://www.cnblogs.com/tylerzhou/p/11026364.html</a></p><p>周国通系列文章：<a href="https://www.cnblogs.com/tylerzhou/p/10969041.html" target="_blank" rel="noopener">https://www.cnblogs.com/tylerzhou/p/10969041.html</a></p><p>2.14.8 InitContainer初始化容器</p><p>在我们应用容器启动之前做的一些初始化操作。例如：初始化环境，执行一些内核变更的命令。保证一定会在容器启动之前运行。</p><p>postStart：在容器启动之前做一些操作。不能保证在你的container的EntryPoint之前运行。</p><p>可以指定多个initContainer，只有上面的运行成功之后，才能运行下一个container。当所有的initContainers执行完毕后，它才会运行真正要部署的container。</p><pre><code>$ kubectl edit deploy nginx// 修改如下内容spec:  progressDeadlineSeconds: 600  replicas: 3  revisionHistoryLimit: 10  selector:    matchLabels:      app: nginx  strategy:    rollingUpdate:      maxSurge: 25%      maxUnavailable: 25%    type: RollingUpdate  template:    metadata:      creationTimestamp: null      labels:        app: nginx    spec:      // 添加初始化容器的内容，向容器文件中添加内容      initContainers:      - command:        - sh        - -c        - echo &quot;InitAllContainers&quot; &gt;&gt; /mnt/init        image: nginx        imagePullPolicy: IfNotPresent        name: init1        resources: {}        terminationMessagePath: /dev/termination-log        terminationMessagePolicy: File        volumeMounts:        - mountPath: /mnt          name: share-volume      containers:      - image: nginx:1.16.1        imagePullPolicy: I。。。。。</code></pre><p>在容器三次初始化时，会发现往文件中追加了三次写入！</p><p>2.14.9 Affinity亲和力</p><ol><li><p>NodeAffinity: 节点亲和力</p><ul><li>requiredDuringSchedulingIgnoredDuringExecution：硬亲和力，必须。既支持必须部署在指定节点上，也支持必须不部署在指定的节点上。</li><li>preferredDuringSchedulingIgnoredDuringExecution：软亲和力，尽量。尽量部署在满足条件的节点上，尽量不要部署在被匹配的节点。</li></ul></li></ol><p>nodeSelector仅仅支持部署在指定节点上，不支持必须不部署在指定节点上。</p><ol start="2"><li><p>PodAffinity：Pod亲和力</p><p> A应用、B应用、C应用，将A应用根据某种策略尽量部署在一块。使用Label进行区分</p><ul><li>requiredDuringSchedulingIgnoredDuringExecution：将A应用和B应用部署在一块</li><li>preferredDuringSchedulingIgnoredDuringExecution：尽量将A应用和B应用部署在一块</li></ul></li><li><p>PodAntiAffinity：Pod反亲和力</p><p> A应用、B应用、C应用，将A应用根据某种策略尽量不部署在一块。</p><ul><li>requiredDuringSchedulingIgnoredDuringExecution：不要将A应用和与之匹配的应用部署在一块</li><li>preferredDuringSchedulingIgnoredDuringExecution：尽量不要将A应用和与之匹配的应用部署在一块</li></ul></li></ol><p>如果集群节点数超过1000，不建议使用该方式进行调度，由于需要计算的关系，会导致调度时间过长。优化的方式是，减少调度的节点，也就是对部分节点进行调度，这样可以加快调度的速度，而且不影响其它节点的运行。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>全链路灰度发布实践</title>
      <link href="/2021/03/06/quan-lian-lu-hui-du-fa-bu-shi-jian/"/>
      <url>/2021/03/06/quan-lian-lu-hui-du-fa-bu-shi-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="全链路灰度发布方案"><a href="#全链路灰度发布方案" class="headerlink" title="全链路灰度发布方案"></a>全链路灰度发布方案</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>根据目前公司现状，需要进行上云的操作，涉及到服务保障的部分，要求做到三个9的可用率，所以率先进行灰度发布的预研，保证发布过程中进行平滑升级的操作，及早暴露发布问题，做到尽可能的无损发布。</p><h2 id="方案综述"><a href="#方案综述" class="headerlink" title="方案综述"></a>方案综述</h2><p>目前平台这边整个技术栈后端以java为主，基于Spring Cloud体系搭建，所以在灰度发布时，也优先考虑java侧的技术选型，最终选择使用Nepxion Discovery框架来完成这个操作。该框架支持从前端到后端的完整发布体系，可以通过header、cookie或者params实现从前端来控制后端发布访问同类服务不同版本的能力，做到动态控制整个访问路径。</p><p>针对前端技术栈以Vue框架为主，部署在nginx，利用nginx+consul+upsync体系实现前端的灰度发布，实质上模拟了新服务的上线引流，旧服务的逐步下线。通过nginx进行负载均衡，调整流量的内容，使用consul进行发布信息的存储，主要是存储要访问的url信息以及访问配置策略，最后通过upsync(nginx-upsync-module)插件从consul中拉取配置信息，实现不需要重启nginx就可以实时变更nginx配置信息。通过组合的方式实现前端的灰度发布。</p><p>由于是进行方案验证阶段，仅以最小系统进行验证，选定omp相关服务进行验证，后端服务包括：</p><ul><li>gateway</li><li>base-opt</li><li>uaa</li><li>umc</li><li>dict</li><li>id-generator</li></ul><p>对于后端服务，只有gateway是单独部署的，其余服务均部署两个相同的镜像服务。在本次验证中，主要验证登录接口获取access_token信息，通过变更镜像服务的版本来实现不同版本的访问。</p><p>前端服务只有omp的前端项目（test-frontend1.0-ui），在两台机器上各部署一个服务，然后前置一个nginx进行反向代理，对访问nginx的ip地址进行筛选，确定要访问的不同的前端服务。针对不同的前端服务，设置不同的header信息，网关侧根据拦截到的header信息确定访问后端的服务的版本，进而实现访问链路的控制。</p><p>最终要实现的是，通过访问不同的前端，进而确定访问不通版本的微服务，实现全链路的灰度发布体系。</p><h2 id="资源准备"><a href="#资源准备" class="headerlink" title="资源准备"></a>资源准备</h2><p>这里一共准备了三台主机</p><ul><li>192.168.59.103：后端灰度发布环境1.0版本服务</li><li>192.168.59.104：后端灰度发布环境，uaa1.1版本，其它服务1.0版本</li><li>192.168.59.105：前端灰度发布环境</li></ul><h2 id="整体架构部署图"><a href="#整体架构部署图" class="headerlink" title="整体架构部署图"></a>整体架构部署图</h2><p><img src="gateway%E7%81%B0%E5%BA%A6%E6%B5%81%E7%A8%8B.png" alt></p><p><img src="%E5%89%8D%E7%AB%AF%E7%81%B0%E5%BA%A6%E6%B5%81%E7%A8%8B1.png" alt></p><h2 id="后端发布方案"><a href="#后端发布方案" class="headerlink" title="后端发布方案"></a>后端发布方案</h2><p>基于<a href="https://github.com/Nepxion/Discovery" target="_blank" rel="noopener">Nepxion Discovery</a>进行后端灰度发布的操作，以常用的登录接口为例子进行灰度发布的测试。首先采用硬编码配置的方式，手动实现流量的切换，然后使用前端访问的方式，通过header中配置变量，传递给网关再进行后向的服务访问。</p><p>首先简要介绍下Nepxion Discovery，同时支持Spring Cloud体系和Spring Cloud Alibaba体系，拥有广泛的框架层面支持。在执行灰度发布的时候，建立了5个层面的规则：</p><ul><li>版本匹配（version）</li><li>区域匹配（region）</li><li>环境匹配（env）</li><li>分组匹配（group），我们利用分组来拉取对应的配置文件信息</li><li>可用区匹配（zone）</li></ul><p>除了这五个层面的匹配规则，剩下的还支持：</p><ul><li>权重设定</li><li>ip地址黑名单屏蔽</li><li>header参数、param参数、Cookie参数传递</li><li>条件表达式的解析</li><li>全链路传递，通过全链路参数传递能够保证把前端项目也能纳入到整个体系</li></ul><p>等等。。。</p><p>整个服务治理架构图如下：</p><p><img src="Nepxion%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86.png" alt></p><p>说起来简单，但是使用起来非常复杂，尤其是超级多的配置项非常容易把人整晕了，所以还得好好查看文档！</p><h3 id="1-网关侧控制灰度发布"><a href="#1-网关侧控制灰度发布" class="headerlink" title="1. 网关侧控制灰度发布"></a>1. 网关侧控制灰度发布</h3><p>首先测试网关侧进行灰度发布的设定，对整个体系要进行改造，对包含在内的所有服务–gateway、uaa、umc、base-opt、id-gen、dict这六个项目进行改造，对其创建新的分支<em>feature.gray_deploy.nepxion</em>，进行修改。</p><p>这里主要演示通过版本信息进行灰度发布的内容，对登录接口进行灰度发布，上述提到的uaa、umc、base-opt、id-gen、dict这几个服务均部署两套，部署单个gateway服务，然后针对uaa服务添加1.0版本和1.1版本，利用postman进行接口测试，通过查看日志来判断登录到不同版本的服务。</p><h4 id="1-1-灰度发布配置信息"><a href="#1-1-灰度发布配置信息" class="headerlink" title="1.1 灰度发布配置信息"></a>1.1 灰度发布配置信息</h4><p>首先对nacos重新添加namespace为zgray-deploy-test，其签名为14f13d68-b0d8-44e6-9e06-1e0b4bbbab7c。将对应服务的配置文件均添加到该namespace下，包括服务也要注册到这里面。</p><p>然后确定服务的分组信息为gray_deploy_group，将灰度发布的配置文件的名称确定为gray_deploy_group，灰度发布配置信息如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>rule</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>strategy</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>{"test-uaa":"1.0","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>strategy</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>rule</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样配置以后，从gateway开始调用后向各个服务的版本为1.0，如果需要变更版本，直接修改配置文件即可。作为整个服务链路上的服务，将会根据配置分组信息（group），来确定拉取的灰度发布配置信息。</p><p>最后在灰度发布操作时，修改test-uaa的版本为1.1，重新发布该配置信息，则可以使调用链路发生改变。</p><h4 id="1-2-微服务源码改造部分"><a href="#1-2-微服务源码改造部分" class="headerlink" title="1.2 微服务源码改造部分"></a>1.2 微服务源码改造部分</h4><p>针对微服务源码，主要包含引入依赖项，配置信息变更，启动参数变更的内容。</p><h5 id="1-1-1-引入依赖项"><a href="#1-1-1-引入依赖项" class="headerlink" title="1.1.1 引入依赖项"></a>1.1.1 引入依赖项</h5><p>在项目的pom.xml中引入依赖信息，这里要注意的是，如果该项目下拥有多个module，将依赖信息一定要放到对应服务的module中的pom.xml文件下，否则容易导致在jenkins构建的时候找不到依赖信息而报错。</p><p>对pom.xml文件修改如下：</p><pre class="line-numbers language-xml"><code class="language-xml">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>discovery.version</span><span class="token punctuation">></span></span>6.6.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>discovery.version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">></span></span><span class="token comment" spellcheck="true">&lt;!-- 灰度发布依赖信息 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.nepxion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>discovery-plugin-register-center-starter-nacos<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${discovery.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.nepxion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>discovery-plugin-config-center-starter-nacos<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${discovery.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.nepxion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>discovery-plugin-admin-center-starter<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${discovery.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.nepxion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>discovery-plugin-strategy-starter-gateway<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${discovery.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>            <span class="token comment" spellcheck="true">&lt;!-- 如果出现构建冲突问题，遇到下面的错误，请剔除下面exclusions中的依赖信息  --></span>            <span class="token comment" spellcheck="true">&lt;!--***************************APPLICATION FAILED TO START***************************Description:Parameter 0 of method modifyRequestBodyGatewayFilterFactory in org.springframework.cloud.gateway.config.GatewayAutoConfiguration required a bean of type 'org.springframework.http.codec.ServerCodecConfigurer' that could not be found.The injection point has the following annotations:        - @org.springframework.beans.factory.annotation.Autowired(required=true)Action:Consider defining a bean of type 'org.springframework.http.codec.ServerCodecConfigurer' in your configuration. --></span>            <span class="token comment" spellcheck="true">&lt;!-- &lt;exclusions>            &lt;exclusion>                &lt;groupId>org.springframework.boot&lt;/groupId>                &lt;artifactId>spring-boot-starter-web&lt;/artifactId>            &lt;/exclusion>                &lt;exclusion>                    &lt;groupId>org.springframework.boot&lt;/groupId>                    &lt;artifactId>spring-boot-starter-webflux&lt;/artifactId>                &lt;/exclusion>        &lt;/exclusions> --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.nepxion<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>discovery-plugin-strategy-starter-skywalking<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${discovery.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在添加完成后，一定进行本地的构建验证。验证通过后，进行后续操作。</p><h5 id="1-1-2-配置信息变更"><a href="#1-1-2-配置信息变更" class="headerlink" title="1.1.2 配置信息变更"></a>1.1.2 配置信息变更</h5><p>对每一个服务的<em>bootstrap.yaml</em>配置文件进行修改，达成变量信息可配置的能力。</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token punctuation">...</span><span class="token punctuation">...</span>  <span class="token key atrule">cloud</span><span class="token punctuation">:</span>    <span class="token key atrule">nacos</span><span class="token punctuation">:</span>      <span class="token punctuation">...</span><span class="token punctuation">...</span>      <span class="token key atrule">discovery</span><span class="token punctuation">:</span>        <span class="token punctuation">...</span><span class="token punctuation">...</span>        <span class="token key atrule">metadata</span><span class="token punctuation">:</span>          <span class="token key atrule">group</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>gray_deploy_group<span class="token punctuation">:</span>discovery<span class="token punctuation">-</span>guide<span class="token punctuation">-</span>group<span class="token punctuation">}</span> <span class="token comment" spellcheck="true"># 分组信息，该处group信息和nacos中服务的配置文件所在group要区分</span>          <span class="token key atrule">version</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>gray_deploy_version<span class="token punctuation">:</span><span class="token number">1.0</span><span class="token punctuation">}</span> <span class="token comment" spellcheck="true"># 版本</span>          <span class="token key atrule">region</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>gray_deploy_region<span class="token punctuation">:</span>dev<span class="token punctuation">}</span> <span class="token comment" spellcheck="true"># 区域，dev、test、release等</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>gray_deploy_env<span class="token punctuation">:</span>env1<span class="token punctuation">}</span> <span class="token comment" spellcheck="true"># 环境</span>          <span class="token key atrule">zone</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>gray_deploy_zone<span class="token punctuation">:</span>zone1<span class="token punctuation">}</span> <span class="token comment" spellcheck="true"># 可用区，云服务器所在的可用区</span><span class="token comment" spellcheck="true"># config rule加载的配置，需要单独进行配置</span><span class="token key atrule">nacos</span><span class="token punctuation">:</span>  <span class="token key atrule">server-addr</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>nacos_ip<span class="token punctuation">:</span>192.168.66.206<span class="token punctuation">:</span><span class="token number">38848</span><span class="token punctuation">}</span>  <span class="token key atrule">plugin</span><span class="token punctuation">:</span>    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>nacos_namespace<span class="token punctuation">:</span>858b37b1<span class="token punctuation">-</span>35be<span class="token punctuation">-</span>4564<span class="token punctuation">-</span>a24e<span class="token punctuation">-</span>dc2c322d5784<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个配置内容理论上也可以放在具体服务的配置文件中，但是这里进行测试后发现，放在启动项的配置文件中更合适。</p><p>随后要区分网关和其它服务的配置，网关侧配置如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token key atrule">application</span><span class="token punctuation">:</span>    <span class="token key atrule">strategy</span><span class="token punctuation">:</span>      <span class="token key atrule">gateway</span><span class="token punctuation">:</span>        <span class="token key atrule">core</span><span class="token punctuation">:</span>          <span class="token key atrule">header</span><span class="token punctuation">:</span>            <span class="token key atrule">transmission</span><span class="token punctuation">:</span>              <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>        <span class="token key atrule">header</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true</span>          <span class="token key atrule">priority</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token key atrule">original</span><span class="token punctuation">:</span>          <span class="token key atrule">header</span><span class="token punctuation">:</span>            <span class="token key atrule">ignored</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其它服务配置信息如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token key atrule">application</span><span class="token punctuation">:</span>    <span class="token key atrule">strategy</span><span class="token punctuation">:</span>      <span class="token key atrule">service</span><span class="token punctuation">:</span>        <span class="token key atrule">header</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true</span>          <span class="token key atrule">priority</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里需要注意的是，header.priority以及service.priority均设置为true，在这个位置，只进行后端发布的验证。如果是前端加入灰度发布的时候，这两个参数要设置为false。</p><p>最后要切换一下配置信息中的admin服务的连接地址。</p><h5 id="1-1-3-启动参数变更"><a href="#1-1-3-启动参数变更" class="headerlink" title="1.1.3 启动参数变更"></a>1.1.3 启动参数变更</h5><p>这里的启动参数主要是指Dockerfile中的启动命令，主要添加了Nepxion的启动项，把下面的内容接入到Dockerfile中：</p><pre class="line-numbers language-txt"><code class="language-txt">-Dgray_deploy_group=${GRAY_DEPLOY_GROUP} -Dgray_deploy_version=${GRAY_DEPLOY_VERSION} -Dgray_deploy_region=${GRAY_DEPLOY_REGION} -Dgray_deploy_env=${GRAY_DEPLOY_ENV} -Dgray_deploy_zone=${GRAY_DEPLOY_ZONE} <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>以gateway为例，完整的Dockerfile信息如下：</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile"># 以基础镜像为底，执行FROM 192.168.93.105/test/base_backend:0.0.2# 安装字体库文件，解决验证码问题RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories &&\    apk add --no-cache ttf-dejavu fontconfigCOPY ./target/test-gateway-1.1.0-SNAPSHOT.jar /appCMD ["sh", "-c", "java -XX:MaxRAMPercentage=85.0 -XX:MaxRAM=2048m -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=8 -javaagent:/app/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.namespace=${SKYWALKING_NAMESPACE} -Dskywalking.agent.service_name=${SKYWALKING_TARGET_SERVICE_NAME} -Dskywalking.collector.backend_service=${SKYWALKING_IP_PORT}  -Dspring.profiles.active=${CHANNEL} -Dspring.cloud.client.ip-address=${IP_ADDR}  -Dnacos_ip=${NACOS_IP} -Dnacos_namespace=${NACOS_NAMESPACE} -Dgray_deploy_group=${GRAY_DEPLOY_GROUP} -Dgray_deploy_version=${GRAY_DEPLOY_VERSION} -Dgray_deploy_region=${GRAY_DEPLOY_REGION} -Dgray_deploy_env=${GRAY_DEPLOY_ENV} -Dgray_deploy_zone=${GRAY_DEPLOY_ZONE} -jar test-gateway-1.1.0-SNAPSHOT.jar"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>到此，所有的微服务就改造完成了。下面进行构建任务的改造。</p><h4 id="1-3-构建任务改造"><a href="#1-3-构建任务改造" class="headerlink" title="1.3 构建任务改造"></a>1.3 构建任务改造</h4><p>所有要改造的构建任务如图：</p><p><img src="%E6%9E%84%E5%BB%BA%E4%BB%BB%E5%8A%A1%E6%88%AA%E5%9B%BE.png" alt></p><p>针对构建任务进行下面的配置：</p><ul><li>添加参数信息，主要是添加灰度发布的参数，参数列表如下：</li></ul><table><thead><tr><th>参数名称</th><th>默认内容</th><th>参数说明</th></tr></thead><tbody><tr><td>DEPLOY_TO_SELF</td><td>0</td><td>判断是否在该机器上部署，0为第一台机器、1为第二台机器，2为第三台机器，在shell脚本中判断是否继续执行</td></tr><tr><td>GRAY_DEPLOY_GROUP</td><td>gray_deploy_group</td><td>灰度发布，分组信息</td></tr><tr><td>GRAY_DEPLOY_VERSION</td><td>1.0</td><td>灰度发布，版本信息</td></tr><tr><td>GRAY_DEPLOY_REGION</td><td>dev</td><td>灰度发布，区域，dev、test、release等</td></tr><tr><td>GRAY_DEPLOY_ENV</td><td>env1</td><td>灰度发布，部署环境信息</td></tr></tbody></table><ul><li>更换git的branch信息为*/feature.gray-deploy.nepxion</li><li>更换镜像名称，添加<em>gray-deploy-</em>的内容</li><li>更改发布的主机内容，这里包含两台主机，192.168.59.103和192.168.59.104，模拟两个环境</li><li>修改启动脚本信息，添加灰度发布的内容，修改配置的nacos中的namespace信息如下：</li></ul><pre class="line-numbers language-shell"><code class="language-shell">#--------------------------------------------------------------------------if [ ${DEPLOY_TO_SELF} == '0' ]; then    #--------------------------------------------------------------------------    # 判断是否存在镜像    # docker ps -a | grep -w test-data-dict-gray-deploy-develop &> /dev/null    # 如果存在先停止运行并删除镜像    if [ "$(docker ps -a | grep -w test-data-dict-gray-deploy-develop)" ]; then        echo "test-data-dict-gray-deploy-develop is exsited!!"        docker stop `docker ps -a | grep -w test-data-dict-gray-deploy-develop | awk '{print $1}'`        docker rm `docker ps -a | grep -w test-data-dict-gray-deploy-develop | awk '{print $1}'`        docker image rm `docker images | grep -w test-data-dict-gray-deploy-develop | awk '{print $3}'`    fi    echo "pull the test-data-dict-gray-deploy-develop image"    docker pull 192.168.93.105/test/test-data-dict-gray-deploy-develop:$BUILD_NUMBER    docker run -d -p 19090:19090 -e CHANNEL="standalone" -e IP_ADDR="192.168.59.103" -e NACOS_IP="192.168.66.206:38848" -e NACOS_NAMESPACE="14f13d68-b0d8-44e6-9e06-1e0b4bbbab7c"  -e SKYWALKING_NAMESPACE="test-dev" -e SKYWALKING_TARGET_SERVICE_NAME="test-data-dict-develop-2" -e SKYWALKING_IP_PORT="192.168.66.208:11800" -e GRAY_DEPLOY_ZONE=${GRAY_DEPLOY_ZONE} -e GRAY_DEPLOY_ENV=${GRAY_DEPLOY_ENV} -e GRAY_DEPLOY_REGION=${GRAY_DEPLOY_REGION} -e GRAY_DEPLOY_VERSION=${GRAY_DEPLOY_VERSION}  -e GRAY_DEPLOY_GROUP=${GRAY_DEPLOY_GROUP} --name test-dict-${DEPLOY_TO_SELF} 192.168.93.105/test/test-data-dict-gray-deploy-develop:$BUILD_NUMBERelse    echo "=================================不在该机器进行部署==============================================="fi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在jenkins构建时，通过修改DEPLOY_TO_SELF参数决定部署到哪台机器上，然后对GRAY_DEPLOY_GROUP统一使用gray_deploy_group名称，拉取该分组下的配置文件，注入到服务中。</p><p>各个微服务根据注入到服务中的灰度发布配置选择是否接受网关指定的流量，如果不接受则不对调用请求作出响应。</p><h4 id="1-4-微服务部署以及上线监控"><a href="#1-4-微服务部署以及上线监控" class="headerlink" title="1.4 微服务部署以及上线监控"></a>1.4 微服务部署以及上线监控</h4><p>手动点击jenkins的构建任务，DEPLOY_TO_SELF设为0时统一执行一遍构建，DEPLOY_TO_SELF设为1时再统一执行一遍构建。特别针对uaa服务，在DEPLOY_TO_SELF设为1时，将版本号GRAY_DEPLOY_VERSION设为1.1，所有的服务构建完成后，都将注册到nacos集群中，服务列表如下：</p><p><img src="%E6%9C%8D%E5%8A%A1%E5%88%97%E8%A1%A8%E4%BF%A1%E6%81%AF.png" alt></p><p>服务的metadata信息如下，以uaa服务为例子：</p><p><img src="uaa%E6%9C%8D%E5%8A%A1metadata%E4%BF%A1%E6%81%AF.png" alt></p><p>最后是admin中的服务面板信息，如下</p><p><img src="%E6%9C%8D%E5%8A%A1%E9%9D%A2%E6%9D%BF%E4%BF%A1%E6%81%AF.png" alt></p><p>这样所有的服务上线并正常运行了。下面开始简单的进行测试，使用postman进行模拟登录请求的操作。</p><h4 id="1-5-示例1：简单的灰度发布"><a href="#1-5-示例1：简单的灰度发布" class="headerlink" title="1.5 示例1：简单的灰度发布"></a>1.5 示例1：简单的灰度发布</h4><p>首先使用默认的灰度发布配置，进行请求，通过查看两个不同版本的uaa服务的日志，确定1.0版本的uaa承接了该登录请求。</p><p>然后打开1.1版本的uaa服务的日志信息，修改灰度发布的配置如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>rule</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>strategy</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>{"test-uaa":"1.1","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>strategy</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>rule</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>uaa服务的版本被修改为1.1版本，这时候继续进行请求，可以看到1.1版本的uaa服务承接了该登录请求。</p><p>这样初步实现了简单的灰度发布。</p><h4 id="1-6-示例2：通过header参数进行蓝绿部署"><a href="#1-6-示例2：通过header参数进行蓝绿部署" class="headerlink" title="1.6 示例2：通过header参数进行蓝绿部署"></a>1.6 示例2：通过header参数进行蓝绿部署</h4><p>首先对各个服务的配置信息进行修改，对gateway的配置修改如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token key atrule">application</span><span class="token punctuation">:</span>    <span class="token key atrule">strategy</span><span class="token punctuation">:</span>      <span class="token key atrule">gateway</span><span class="token punctuation">:</span>        <span class="token key atrule">header</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true</span>          <span class="token key atrule">priority</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对其它服务配置修改如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token key atrule">application</span><span class="token punctuation">:</span>    <span class="token key atrule">strategy</span><span class="token punctuation">:</span>      <span class="token key atrule">service</span><span class="token punctuation">:</span>        <span class="token key atrule">header</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true</span>          <span class="token key atrule">priority</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样修改后，就可以进行全链路的参数传递了，根据传递的header参数，可以匹配灰度发布的规则。</p><p>然后对灰度发布的配置文件进行修改，如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>rule</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>strategy</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>{"test-uaa":"1.1","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>strategy</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>strategy-customization</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>conditions</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>blue-green<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>condition</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>blue-condition<span class="token punctuation">"</span></span> <span class="token attr-name">expression</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>#H[<span class="token punctuation">'</span>a<span class="token punctuation">'</span>] <span class="token punctuation">=</span><span class="token punctuation">=</span> <span class="token punctuation">'</span>1<span class="token punctuation">'</span><span class="token punctuation">"</span></span> <span class="token attr-name">version-id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>blue-version-route<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>condition</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>green-condition<span class="token punctuation">"</span></span> <span class="token attr-name">expression</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>#H[<span class="token punctuation">'</span>a<span class="token punctuation">'</span>] <span class="token punctuation">=</span><span class="token punctuation">=</span> <span class="token punctuation">'</span>2<span class="token punctuation">'</span><span class="token punctuation">"</span></span> <span class="token attr-name">version-id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>green-version-route<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>conditions</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>routes</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>route</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>blue-version-route<span class="token punctuation">"</span></span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>version<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>{"test-uaa":"1.1","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>route</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>route</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>green-version-route<span class="token punctuation">"</span></span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>version<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>{"test-uaa":"1.0","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>route</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>routes</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>strategy-customization</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>rule</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对灰度发布的内容进行了筛选，默认最上面的strategy为兜底策略，在下面strategy-customization为蓝绿发布的策略，根据请求中的header参数信息为条件进行删选，匹配不同的版本链路信息。</p><p>最后在postman的请求中，添加header参数为a，变更a的值分别为空，1,2，可以查看运行结果。整体运行图如下：</p><p><img src="%E8%93%9D%E7%BB%BF%E9%83%A8%E7%BD%B2%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt></p><h2 id="后端发布问题"><a href="#后端发布问题" class="headerlink" title="后端发布问题"></a>后端发布问题</h2><ol><li>流量控制后，用户数据兜底问题，就是如何控制发布时出现问题的用户进行数据回滚的操作。</li><li>未能探索出将灰度发布的链路信息写入skywalking的方式</li><li>资源问题，两套环境的维护</li><li>jenkins构建项目的规划，目前一个项目构建两个版本，不太用的过来</li><li>k8s支持，如果以现有框架继续运行可以使用，如果将基础设施落到k8s或者istio上，就无法再使用该框架，例如将nacos和gateway均沉淀到k8s中</li><li>如果存在较多的小版本的相同服务，如何确定灰度时的版本？默认的灰度发布的版本是什么？</li><li>如果未指定默认版本时，也就是配置文件不存在时，应该怎么去设置？</li></ol><h2 id="前端发布方案"><a href="#前端发布方案" class="headerlink" title="前端发布方案"></a>前端发布方案</h2><p>针对前端的特点，使用nginx进行前端服务的反向代理，部署两套前端服务，仅仅配置在网络请求时的header信息不同。在这两套前端服务之前，使用openresty进行反向代理，外加使用nginx的插件upsync进行动态变更前端地址，通过upsync连接consul配置中心，获取前端地址。最后使用redis存储ip地址信息，利用openresty中的lua模块，读取redis中缓存的ip地址信息，再根据ip地址的匹配，决定要访问的前端地址信息。整体架构包括openresty+lua+consul+upsync+redis组件构成整个前端灰度发布体系。</p><h3 id="2-前端控制灰度发布"><a href="#2-前端控制灰度发布" class="headerlink" title="2. 前端控制灰度发布"></a>2. 前端控制灰度发布</h3><h4 id="2-1-前端项目改造"><a href="#2-1-前端项目改造" class="headerlink" title="2.1 前端项目改造"></a>2.1 前端项目改造</h4><p>针对前端项目的改造，使用omp项目的前端，在request.js中配置发送的header信息，如下：</p><pre class="line-numbers language-js"><code class="language-js"><span class="token comment" spellcheck="true">// request interceptor</span>service<span class="token punctuation">.</span>interceptors<span class="token punctuation">.</span>request<span class="token punctuation">.</span><span class="token function">use</span><span class="token punctuation">(</span>config <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//添加平台标识</span>    config<span class="token punctuation">.</span>headers<span class="token punctuation">[</span><span class="token string">'identification'</span><span class="token punctuation">]</span> <span class="token operator">=</span> identification        <span class="token comment" spellcheck="true">//在token中添加市区</span>    config<span class="token punctuation">.</span>headers<span class="token punctuation">[</span><span class="token string">'timezone'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getTimezoneOffset</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">60</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">//判断token是否过期，过期则携带刷新token重新请求</span>        <span class="token comment" spellcheck="true">// const refreshToken = Vue.ls.get(REFRESH_TOKEN)</span>        <span class="token comment" spellcheck="true">// if(refreshToken){</span>        <span class="token comment" spellcheck="true">//   judgeTokenOutTime()</span>        <span class="token comment" spellcheck="true">// }</span>        <span class="token comment" spellcheck="true">//多页面的时候会找不到ls所以需要进行判断</span>    <span class="token comment" spellcheck="true">// ======发送header请求内容======</span>    <span class="token comment" spellcheck="true">// 修改这个内容</span>    config<span class="token punctuation">.</span>headers<span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">"1"</span>    <span class="token keyword">let</span> token<span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>Vue<span class="token punctuation">.</span>ls<span class="token punctuation">)</span> <span class="token punctuation">{</span>        token <span class="token operator">=</span> Vue<span class="token punctuation">.</span>ls<span class="token punctuation">.</span><span class="token keyword">get</span><span class="token punctuation">(</span>ACCESS_TOKEN<span class="token punctuation">)</span>    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>window<span class="token punctuation">.</span>localStorage<span class="token punctuation">.</span><span class="token function">getItem</span><span class="token punctuation">(</span><span class="token string">'pro__Access-Token'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            token <span class="token operator">=</span> JSON<span class="token punctuation">.</span><span class="token function">parse</span><span class="token punctuation">(</span>window<span class="token punctuation">.</span>localStorage<span class="token punctuation">.</span><span class="token function">getItem</span><span class="token punctuation">(</span><span class="token string">'pro__Access-Token'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>value        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    config<span class="token punctuation">.</span>headers<span class="token punctuation">[</span><span class="token string">'Authorization'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">getBase64Code</span><span class="token punctuation">(</span>grantTypeUsername<span class="token punctuation">,</span> grantTypePassword<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//请求携带authorization</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>token<span class="token punctuation">)</span> <span class="token punctuation">{</span>        config<span class="token punctuation">.</span>headers<span class="token punctuation">[</span><span class="token string">'Access-Token'</span><span class="token punctuation">]</span> <span class="token operator">=</span> token <span class="token comment" spellcheck="true">// 让每个请求携带自定义 token 请根据实际情况自行修改</span>        config<span class="token punctuation">.</span>headers<span class="token punctuation">[</span><span class="token string">'Authorization'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'Bearer '</span> <span class="token operator">+</span> token    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//判断是不是登出请求</span>    <span class="token comment" spellcheck="true">// if(config.data === 'logout'){</span>    <span class="token comment" spellcheck="true">// config.headers['Authorization'] = token</span>    <span class="token comment" spellcheck="true">// }</span>    <span class="token keyword">return</span> config<span class="token punctuation">}</span><span class="token punctuation">,</span> err <span class="token operator">=</span><span class="token operator">></span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在不同版本的前端服务中，修改header信息，当a=1的时候，访问1.1版本的uaa服务；当a=2的时候，访问版本为1.0的uaa服务。</p><p>最后将这两个前端项目分别部署到192.168.59.103、192.168.59.104的服务器上。</p><h4 id="2-2-openresty-lua-upsync-redis环境搭建"><a href="#2-2-openresty-lua-upsync-redis环境搭建" class="headerlink" title="2.2 openresty+lua+upsync+redis环境搭建"></a>2.2 openresty+lua+upsync+redis环境搭建</h4><h5 id="2-2-1-安装luajit"><a href="#2-2-1-安装luajit" class="headerlink" title="2.2.1 安装luajit"></a>2.2.1 安装luajit</h5><pre><code>$ mkdir luajit$ wget https://github.com/openresty/luajit2/archive/v2.1-20201027.tar.gz$ tar -zxvf v2.1-20201027.tar.gz$ cd luajit2-2.1-20201027/// 编译安装lua环境$ make linux PREFIX=/opt/luajit$ sudo mkdir -p /opt/luajit &amp;&amp; sudo make install PREFIX=/opt/luajit// 配置环境变量$ sudo vim /etc/profile// 在文件末尾追加# luaexport LUAJIT_BIN=/opt/luajit/bin/export LUAJIT_LIB=/opt/luajit/lib/export LUAJIT_INC=/opt/luajit/include/luajit-2.1/export PATH=$LUAJIT_BIN:$PATHexport PATH=$LUAJIT_LIB:$PATHexport PATH=$LUAJIT_INC:$PATH// :wq保存退出// 配置so到配置文件中$ sudo echo &quot;/opt/luajit/lib/&quot; &gt;&gt; /etc/ld.so.conf// 配置生效$ sudo ldconfig// 或者使用短连接的方式$ sudo ln -s /opt/luajit/lib/libluajit-5.1.so.2 /usr/lib64/// 测试lua$ luajitLuaJIT 2.1.0-beta3 -- Copyright (C) 2005-2020 Mike Pall. https://luajit.org/JIT: ON SSE3 SSE4.1 BMI2 fold cse dce fwd dse narrow loop abc sink fuse&gt;</code></pre><h5 id="2-2-2-编译安装openresty"><a href="#2-2-2-编译安装openresty" class="headerlink" title="2.2.2 编译安装openresty"></a>2.2.2 编译安装openresty</h5><pre><code>// 安装依赖信息$ sudo yum install pcre-devel openssl-devel gcc curl$ sudo yum install -y yum-utils$ sudo yum -y install libpcre3 libpcre3-dev ruby zlib1g-dev patch$ wget https://openresty.org/download/openresty-1.17.8.2.tar.gz$ tar -zxvf openresty-1.17.8.2.tar.gz// 下载upsync插件$ mkdir /home/centos/nginx-modules$ wget https://github.com//weibocom/nginx-upsync-module/archive/v2.1.3.tar.gz$ tar -zxvf v2.1.3.tar.gz$ mv nginx-upsync-module-v2.1.3 nginx-upsync-module// 下载nginx_upstream_check_module插件$ git clone https://github.com/xiaokai-wang/nginx_upstream_check_module.git$ cd openresty-1.17.8.2// 对openresty添加patch$ patch -p1 &lt; /home/centos/nginx-modules/nginx_upstream_check_module/check_1.12.1+.patch// 编译并安装openresty$ ./configure --add-module=/home/centos/nginx-modules/nginx-upsync-module --add-module=/home/centos/nginx-modules/nginx_upstream_check_module --prefix=/opt/openresty --with-http_stub_status_module --with-http_ssl_module --with-http_flv_module --with-http_gzip_static_module --with-http_realip_module$ gmake &amp;&amp; sudo gmake install// 安装完成后进行启动$ sudo /opt/openresty/nginx/sbin/nginx// 访问nginx$ curl localhost</code></pre><h5 id="2-2-3-安装consul"><a href="#2-2-3-安装consul" class="headerlink" title="2.2.3 安装consul"></a>2.2.3 安装consul</h5><pre><code>// 下载consul组件$ cd ~ &amp;&amp; wget https://releases.hashicorp.com/consul/1.7.3/consul_1.7.3_linux_amd64.zip$ unzip consul_1.7.3_linux_amd64.zip$ cd consul_1.7.3_linux_amd64/$ sudo mkdir -p /opt/consul &amp;&amp; sudo cp -r ./* /opt/consul$ sudo mkdir -p /opt/consul/node1 /opt/consul/node2 /opt/consul/node3$ sudo mkdir -p /opt/consul/node1/logs /opt/consul/node2/logs /opt/consul/node3/logs$ sudo vim /opt/consul/node1// 添加第一个节点的配置信息{  &quot;datacenter&quot;: &quot;nginx-consul&quot;,  &quot;data_dir&quot;: &quot;/opt/consul/data/node1&quot;,  &quot;log_file&quot;: &quot;/opt/consul/data/node1/consul.log&quot;,  &quot;log_level&quot;: &quot;INFO&quot;,  &quot;server&quot;: true,  &quot;node_name&quot;: &quot;node1&quot;,  &quot;ui&quot;: true,  &quot;bind_addr&quot;: &quot;192.168.59.105&quot;,  &quot;client_addr&quot;: &quot;192.168.59.105&quot;,  &quot;advertise_addr&quot;: &quot;192.168.59.105&quot;,  &quot;bootstrap_expect&quot;: 3,  &quot;ports&quot;:{    &quot;http&quot;: 8510,    &quot;dns&quot;: 8610,    &quot;server&quot;: 8310,    &quot;serf_lan&quot;: 8311,    &quot;serf_wan&quot;: 8312   }}// :wq保存退出$ sudo vim /opt/consul/node2// 添加第一个节点的配置信息{  &quot;datacenter&quot;: &quot;nginx-consul&quot;,  &quot;data_dir&quot;: &quot;/opt/consul/data/node2&quot;,  &quot;log_file&quot;: &quot;/opt/consul/data/node2/consul.log&quot;,  &quot;log_level&quot;: &quot;INFO&quot;,  &quot;server&quot;: true,  &quot;node_name&quot;: &quot;node2&quot;,  &quot;ui&quot;: true,  &quot;bind_addr&quot;: &quot;192.168.59.105&quot;,  &quot;client_addr&quot;: &quot;192.168.59.105&quot;,  &quot;advertise_addr&quot;: &quot;192.168.59.105&quot;,  &quot;bootstrap_expect&quot;: 3,  &quot;ports&quot;:{    &quot;http&quot;: 8520,    &quot;dns&quot;: 8620,    &quot;server&quot;: 8320,    &quot;serf_lan&quot;: 8321,    &quot;serf_wan&quot;: 8322    }}// :wq保存退出$ sudo vim /opt/consul/node3// 添加第一个节点的配置信息{  &quot;datacenter&quot;: &quot;nginx-consul&quot;,  &quot;data_dir&quot;: &quot;/opt/consul/data/node3&quot;,  &quot;log_file&quot;: &quot;/opt/consul/data/node3/consul.log&quot;,  &quot;log_level&quot;: &quot;INFO&quot;,  &quot;server&quot;: true,  &quot;node_name&quot;: &quot;node3&quot;,  &quot;ui&quot;: true,  &quot;bind_addr&quot;: &quot;192.168.59.105&quot;,  &quot;client_addr&quot;: &quot;192.168.59.105&quot;,  &quot;advertise_addr&quot;: &quot;192.168.59.105&quot;,  &quot;bootstrap_expect&quot;: 3,  &quot;ports&quot;:{    &quot;http&quot;: 8530,    &quot;dns&quot;: 8630,    &quot;server&quot;: 8330,    &quot;serf_lan&quot;: 8331,    &quot;serf_wan&quot;: 8332    }}// :wq保存退出$ cd /opt/consul &amp;&amp; sudo vim consul_start.sh// 添加以下内容# !/bin/bashnohup /opt/consul/consul agent -config-file=/opt/consul/data/node1/consul_conf.json &gt; /dev/null 2&gt;&amp;1 &amp;sleep 10nohup /opt/consul/consul agent -config-file=/opt/consul/data/node2/consul_conf.json -retry-join=192.168.59.105:8311 &gt; /dev/null 2&gt;&amp;1 &amp;sleep 10nohup /opt/consul/consul agent -config-file=/opt/consul/data/node3/consul_conf.json -retry-join=192.168.59.105:8311 &gt; /dev/null 2&gt;&amp;1 &amp;// :wq保存退出// 启动整个consul的伪集群$ sudo sh +x consul_start.sh// 启动完成后直接访问$ curl http://192.168.59.105:8510/</code></pre><h5 id="2-2-4-安装redis"><a href="#2-2-4-安装redis" class="headerlink" title="2.2.4 安装redis"></a>2.2.4 安装redis</h5><pre><code>$ sudo yum install redis$ systemctl enable redis &amp;&amp; systemctl start redis$ redis-cli -h 127.0.0.1 -p 6379127.0.0.1:6379&gt;// ctrl+c退出</code></pre><p>这样整个环境就搭建完成了。</p><h4 id="2-3-前端运行策略"><a href="#2-3-前端运行策略" class="headerlink" title="2.3 前端运行策略"></a>2.3 前端运行策略</h4><h5 id="2-3-1-前端部署配置"><a href="#2-3-1-前端部署配置" class="headerlink" title="2.3.1 前端部署配置"></a>2.3.1 前端部署配置</h5><p>在192.168.59.103、192.168.59.104机器上进行部署前端服务，同上。</p><h5 id="2-3-2-openresty-nginx配置"><a href="#2-3-2-openresty-nginx配置" class="headerlink" title="2.3.2 openresty nginx配置"></a>2.3.2 openresty nginx配置</h5><pre><code>$ cd /opt/openresty/nginx/conf$ sudo vim nginx.conf</code></pre><p>nginx.conf配置修改如下：</p><pre class="line-numbers language-conf"><code class="language-conf">#user  nobody;worker_processes  1;#error_log  logs/error.log;#error_log  logs/error.log  notice;#error_log  logs/error.log  info;#pid        logs/nginx.pid;events {    worker_connections  1024;}http {    # 导入lua的开发包支持    lua_package_path "/opt/openresty/lualib/?.lua;;";    lua_package_cpath "/opt/openresty/lualib/?.so;;";    #include /opt/openresty/nginx/conf/lua/gray_deploy.conf;    include       mime.types;    default_type  application/octet-stream;    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '    #                  '$status $body_bytes_sent "$http_referer" '    #                  '"$http_user_agent" "$http_x_forwarded_for"';    #access_log  logs/access.log  main;    upstream baseClient {       #ip_hash;       # 这里是consul的leader节点的HTTP端点       upsync 192.168.59.105:8510/v1/kv/upstreams/base/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;       # consul访问不了的时候的备用配置       upsync_dump_path /opt/openresty/nginx/conf/app_backup.conf;       # 这里是为了兼容Nginx的语法检查       include /opt/openresty/nginx/conf/app_backup.conf;       # 下面三个配置是健康检查的配置       check interval=1000 rise=2 fall=2 timeout=3000 type=http default_down=false;       check_http_send "HEAD / HTTP/1.0\r\n\r\n";       check_http_expect_alive http_2xx http_3xx;    }    upstream grayClient {       #ip_hash;       # 这里是consul的leader节点的HTTP端点       upsync 192.168.59.105:8510/v1/kv/upstreams/gray/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;       # consul访问不了的时候的备用配置       upsync_dump_path /opt/openresty/nginx/conf/app_backup.conf;       # 这里是为了兼容Nginx的语法检查       include /opt/openresty/nginx/conf/app_backup.conf;       # 下面三个配置是健康检查的配置       check interval=1000 rise=2 fall=2 timeout=3000 type=http default_down=false;       check_http_send "HEAD / HTTP/1.0\r\n\r\n";       check_http_expect_alive http_2xx http_3xx;    }    sendfile        on;    #tcp_nopush     on;    #keepalive_timeout  0;    keepalive_timeout  65;    #gzip  on;    server {        listen       80;        server_name  localhost;        #charset koi8-r;        #access_log  logs/host.access.log  main;        location / {            #root   html;            #index  index.html index.htm;            proxy_set_header Host $host;            proxy_set_header X-Real-IP $remote_addr;            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;            # lua脚本处理ip地址的匹配            content_by_lua_file /opt/openresty/nginx/conf/lua/gray_deploy.lua;        }        # 灰度发布之前的路由信息        location @base{                proxy_pass http://baseClient;        }        # 灰度发布的路由信息        location @gray{                proxy_pass http://grayClient;        }        # 健康检查 - 查看负载均衡列表        location /upstream_list {            upstream_show;        }        # 健康检查 - 查看负载均衡的状态        location /upstream_status {            check_status;            access_log off;        }        location /lua {            default_type 'text/html';            content_by_lua_file conf/lua/test.lua;        }        #error_page  404              /404.html;        # redirect server error pages to the static page /50x.html        #        error_page   500 502 503 504  /50x.html;        location = /50x.html {            root   html;        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上述配置完成后，需要在命令行执行下测试命令，保证修改nginx.conf文件没有错误。</p><pre><code>$ sudo /opt/openresty/nginx/sbin/nginx -tnginx: the configuration file /opt/openresty/nginx/conf/nginx.conf syntax is oknginx: configuration file /opt/openresty/nginx/conf/nginx.conf test is successful</code></pre><h5 id="2-3-3-openresty-lua脚本编写"><a href="#2-3-3-openresty-lua脚本编写" class="headerlink" title="2.3.3 openresty lua脚本编写"></a>2.3.3 openresty lua脚本编写</h5><pre><code>$ cd /opt/openresty/nginx/conf &amp;&amp; mkdir lua &amp;&amp; touch gray_deploy.lua$ sudo vim lua/gray_deploy.lua// 编写lua脚本local redis = require &quot;resty.redis&quot;local cache = redis.new()cache:set_timeout(60000)local ok, err = cache.connect(cache, &#39;127.0.0.1&#39;, 6379)if not ok then    ngx.say(&quot;failed to connect:&quot;, err)    returnend--local red, err = cache:auth(&quot;foobared&quot;)--if not red then--    ngx.say(&quot;failed to authenticate: &quot;, err)--    return--endlocal local_ip = ngx.req.get_headers()[&quot;X-Real-IP&quot;]if local_ip == nil then    local_ip = ngx.req.get_headers()[&quot;x_forwarded_for&quot;]endif local_ip == nil then    local_ip = ngx.var.remote_addrend--ngx.say(&quot;local_ip is : &quot;, local_ip)-- 根据redis中缓存的ip地址确定访问的前端地址local intercept = cache:get(local_ip)if intercept == local_ip then    ngx.exec(&quot;@gray&quot;)    returnendngx.exec(&quot;@base&quot;)local ok, err = cache:close()if not ok then    ngx.say(&quot;failed to close:&quot;, err)    returnend// :wq保存退出</code></pre><h5 id="2-3-4-对consul中设置前端的url地址"><a href="#2-3-4-对consul中设置前端的url地址" class="headerlink" title="2.3.4 对consul中设置前端的url地址"></a>2.3.4 对consul中设置前端的url地址</h5><p>在consul中通过curl调用接口的方式添加配置信息，如下：</p><pre><code>$ curl -X PUT -d &#39;{&quot;weight&quot;:1, &quot;max_fails&quot;:2, &quot;fail_timeout&quot;:10}&#39; http://192.168.59.105:8510/v1/kv/upstreams/base/192.168.59.104:20080$ curl -X PUT -d &#39;{&quot;weight&quot;:1, &quot;max_fails&quot;:2, &quot;fail_timeout&quot;:10}&#39; http://192.168.59.105:8510/v1/kv/upstreams/base/192.168.59.103:20080</code></pre><p>添加后的结果如下：</p><p><img src="consul%E6%B7%BB%E5%8A%A0%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF1.png" alt></p><p><img src="consul%E6%B7%BB%E5%8A%A0%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF2.png" alt></p><h5 id="2-3-5-启动nginx并查看运行情况"><a href="#2-3-5-启动nginx并查看运行情况" class="headerlink" title="2.3.5 启动nginx并查看运行情况"></a>2.3.5 启动nginx并查看运行情况</h5><p>启动nginx服务：</p><pre><code>$ sudo /opt/openresty/nginx/conf/nginx</code></pre><h5 id="2-3-6-在redis中添加灰度发布的ip地址"><a href="#2-3-6-在redis中添加灰度发布的ip地址" class="headerlink" title="2.3.6 在redis中添加灰度发布的ip地址"></a>2.3.6 在redis中添加灰度发布的ip地址</h5><pre><code>$ redis-cli -h 127.0.0.1 -p 6379127.0.0.1:6379&gt; set 192.168.93.182 192.168.93.182</code></pre><p>添加完成后，当192.168.93.182进行访问的时候，走灰度发布的链路，当其他ip进行访问的时候，走普通的线路。</p><h4 id="2-4-示例：根据IP地址匹配蓝绿发布的内容"><a href="#2-4-示例：根据IP地址匹配蓝绿发布的内容" class="headerlink" title="2.4 示例：根据IP地址匹配蓝绿发布的内容"></a>2.4 示例：根据IP地址匹配蓝绿发布的内容</h4><p>在redis中添加ip地址，在consul配置前端的灰度发布的前端连接地址，通过不同的ip地址进行访问。</p><p>然后在不同的机器上使用selenium进行自动化访问测试，python脚本如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># coding = utf-8</span><span class="token comment" spellcheck="true"># 利用chromewebdriver模拟登陆系统，测试前端负载均衡效果</span><span class="token keyword">from</span> selenium <span class="token keyword">import</span> webdriver<span class="token keyword">import</span> time<span class="token comment" spellcheck="true">#from time import sleep</span><span class="token keyword">from</span> selenium<span class="token punctuation">.</span>webdriver<span class="token punctuation">.</span>chrome<span class="token punctuation">.</span>options <span class="token keyword">import</span> Options<span class="token comment" spellcheck="true">#from selenium.webdriver.firefox.options import Options</span><span class="token comment" spellcheck="true"># 无头模式</span><span class="token comment" spellcheck="true"># 无头模式不开启浏览器，也就是在程序里面运行的</span>chrome_options <span class="token operator">=</span> Options<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#chrome_options.add_argument("--headless")</span><span class="token comment" spellcheck="true"># 添加UserAgent</span>chrome_options<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'user-agent=Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36'</span><span class="token punctuation">)</span>chrome_options<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--disable-gpu'</span><span class="token punctuation">)</span> chrome_options<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'disable-infobars'</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 隐藏"Chrome正在受到自动软件的控制"</span>chrome_options<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'lang=zh_CN.UTF-8'</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 设置中文</span>chrome_options<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'window-size=1920x1080'</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 指定浏览器分辨率</span>chrome_options<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--hide-scrollbars'</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 隐藏滚动条, 应对一些特殊页面</span>chrome_options<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--remote-debugging-port=9222'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># #设置图片不加载</span><span class="token comment" spellcheck="true"># prefs = {</span><span class="token comment" spellcheck="true">#     'profile.default_content_setting_values': {</span><span class="token comment" spellcheck="true">#         'images': 2</span><span class="token comment" spellcheck="true">#     }</span><span class="token comment" spellcheck="true"># }</span><span class="token comment" spellcheck="true"># chrome_options.add_experimental_option('prefs', prefs)</span>browser <span class="token operator">=</span> webdriver<span class="token punctuation">.</span>Chrome<span class="token punctuation">(</span>executable_path<span class="token operator">=</span><span class="token punctuation">(</span>r<span class="token string">'C:\Users\administrator\AppData\Local\Google\Chrome\Application\chromedriver.exe'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> options<span class="token operator">=</span>chrome_options<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># browser = webdriver.Firefox()</span><span class="token comment" spellcheck="true"># 如果不用上面三行，那么就用下面这一行。运行的时候回自动的开启浏览器，并在浏览器中自动运行，你可以看到自动运行的过程</span><span class="token comment" spellcheck="true"># browser = webdriver.Chrome(executable_path=(r'C:\Users\administrator\AppData\Local\Google\Chrome\Application\chromedriver.exe'))</span><span class="token comment" spellcheck="true"># username_list = ["user002","user004","user005","user006","user007","user008","user009","user101","123","pangdezhen","xiaohong","lisa","lanwangji","weiwuxian","user20191126163930","user20191126164707","345"]</span>username_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"user002"</span><span class="token punctuation">,</span><span class="token string">"user004"</span><span class="token punctuation">,</span><span class="token string">"user008"</span><span class="token punctuation">]</span><span class="token keyword">for</span> username <span class="token keyword">in</span> username_list<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 设置访问连接</span>    <span class="token comment" spellcheck="true">#browser.execute_script( 'window.open("http://192.168.59.105");' )</span>    browser<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"http://192.168.59.105"</span><span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 查找用户名、密码、验证码输入框</span>    browser<span class="token punctuation">.</span>find_element_by_id<span class="token punctuation">(</span><span class="token string">"username"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>send_keys<span class="token punctuation">(</span>username<span class="token punctuation">)</span>    browser<span class="token punctuation">.</span>find_element_by_id<span class="token punctuation">(</span><span class="token string">"password"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>send_keys<span class="token punctuation">(</span><span class="token string">"123456"</span><span class="token punctuation">)</span>    browser<span class="token punctuation">.</span>find_element_by_id<span class="token punctuation">(</span><span class="token string">"checkCode"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>send_keys<span class="token punctuation">(</span><span class="token string">"1111"</span><span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 点击登录按钮进行登录(未完成)</span>    browser<span class="token punctuation">.</span>find_element_by_class_name<span class="token punctuation">(</span><span class="token string">"login-button"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"=====用户已登录======"</span><span class="token punctuation">)</span>    browser<span class="token punctuation">.</span>find_element_by_class_name<span class="token punctuation">(</span><span class="token string">"anticon-user"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    browser<span class="token punctuation">.</span>find_element_by_class_name<span class="token punctuation">(</span><span class="token string">"anticon-logout"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    browser<span class="token punctuation">.</span>find_element_by_class_name<span class="token punctuation">(</span><span class="token string">"ant-btn-primary"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"=====用户已退出======="</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 重新加载页面</span>    browser<span class="token punctuation">.</span>execute_script<span class="token punctuation">(</span><span class="token string">'window.location.reload("http://192.168.59.105")'</span><span class="token punctuation">)</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    browser<span class="token punctuation">.</span>refresh<span class="token punctuation">(</span><span class="token punctuation">)</span>time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>browser<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"test end!!!"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后执行这个脚本，查看服务中的日志信息。</p><h2 id="前端发布问题"><a href="#前端发布问题" class="headerlink" title="前端发布问题"></a>前端发布问题</h2><ol><li>前端cdn 307问题导致错误信息Uncaught SyntaxError: expected expression, got ‘&lt;’</li><li>nginx配置，从不同位置加载不同的文件信息，可能存在加载混乱的情况，前端文件不会同时来自同一个前端服务</li><li>无法精准控制用户访问哪个后向前端服务。</li><li>目前只能做到对访问ip进行筛选和分析，尚未进一步完善灰度发布规则。</li><li>前端尽可能的可配置，如何直接配置header参数而不是变更代码？</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>终于他妈的写完文档了！</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>前端发布部分：</p><ul><li><a href="https://www.cnblogs.com/throwable/p/13113620.html" target="_blank" rel="noopener">https://www.cnblogs.com/throwable/p/13113620.html</a></li><li><a href="https://www.jianshu.com/p/fadab3d092c5" target="_blank" rel="noopener">https://www.jianshu.com/p/fadab3d092c5</a></li><li><a href="https://www.cnblogs.com/nf01/articles/13614279.html" target="_blank" rel="noopener">https://www.cnblogs.com/nf01/articles/13614279.html</a></li><li><a href="https://blog.csdn.net/zhaoydzhaoyd/article/details/107916968" target="_blank" rel="noopener">https://blog.csdn.net/zhaoydzhaoyd/article/details/107916968</a></li><li><a href="https://segmentfault.com/a/1190000014621318" target="_blank" rel="noopener">https://segmentfault.com/a/1190000014621318</a></li></ul><p>后端发布部分：</p><ul><li><a href="http://nepxion.gitee.io/docs/web-doc/Discovery%E3%80%90%E6%8E%A2%E7%B4%A2%E3%80%91%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html" target="_blank" rel="noopener">http://nepxion.gitee.io/docs/web-doc/Discovery%E3%80%90%E6%8E%A2%E7%B4%A2%E3%80%91%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html</a></li><li><a href="https://github.com/Nepxion/Discovery" target="_blank" rel="noopener">https://github.com/Nepxion/Discovery</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>灰度部署预研和实践(一)</title>
      <link href="/2021/02/20/hui-du-bu-shu-yu-yan-he-shi-jian-yi/"/>
      <url>/2021/02/20/hui-du-bu-shu-yu-yan-he-shi-jian-yi/</url>
      
        <content type="html"><![CDATA[<h1 id="灰度部署预研和实践"><a href="#灰度部署预研和实践" class="headerlink" title="灰度部署预研和实践"></a>灰度部署预研和实践</h1><p>后端服务主要依赖于Nepxion Discovery框架，利用该框架中的固定参数进行配置，从分组、版本号、可用区、环境、地区五个方面进行区分配置。</p><p>在k8s环境中，借助Spinnaker工具实现服务的滚动更新，配合上述内容实现k8s中的部署配置。</p><p>这两种方式不会相互影响。</p><h2 id="发布方式对比"><a href="#发布方式对比" class="headerlink" title="发布方式对比"></a>发布方式对比</h2><h2 id="jmeter测试"><a href="#jmeter测试" class="headerlink" title="jmeter测试"></a>jmeter测试</h2><h2 id="从前端-网关-微服务链路查看整体效果"><a href="#从前端-网关-微服务链路查看整体效果" class="headerlink" title="从前端-网关-微服务链路查看整体效果"></a>从前端-网关-微服务链路查看整体效果</h2><h2 id="前端动态切换部署–OpenRestry-Lua的开发"><a href="#前端动态切换部署–OpenRestry-Lua的开发" class="headerlink" title="前端动态切换部署–OpenRestry+Lua的开发"></a>前端动态切换部署–OpenRestry+Lua的开发</h2><h2 id="流量染色"><a href="#流量染色" class="headerlink" title="流量染色"></a>流量染色</h2><h2 id="istio实践"><a href="#istio实践" class="headerlink" title="istio实践"></a>istio实践</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CentOS 7 备份与恢复</title>
      <link href="/2021/02/04/centos-7-bei-fen-yu-hui-fu/"/>
      <url>/2021/02/04/centos-7-bei-fen-yu-hui-fu/</url>
      
        <content type="html"><![CDATA[<h1 id="CentOS-7-备份与恢复"><a href="#CentOS-7-备份与恢复" class="headerlink" title="CentOS 7 备份与恢复"></a>CentOS 7 备份与恢复</h1><h2 id="迁移环境介绍"><a href="#迁移环境介绍" class="headerlink" title="迁移环境介绍"></a>迁移环境介绍</h2><p>当前使用的是CentOS 7.6版本的服务器镜像，由于出现了机房故障的情况，待恢复后，公司要求进行整个系统进行迁移的工作，并且重新进行域名备份的工作。该服务器承载我司内部即时通讯系统的运行，因此需要整体备份和恢复，并且重新启动服务。</p><h2 id="系统备份"><a href="#系统备份" class="headerlink" title="系统备份"></a>系统备份</h2><p>在原服务器使用tar命令进行打包，使用tar.bz2的格式压缩文件，如下：</p><pre><code>// 切换管理员$ sudo su# cd /# tar cvpjf backup.tar.bz2 –-exclude=/proc –exclude=/lost+found -–exclude=/backup.tar.bz2 –-exclude=/mnt –exclude=/sys /</code></pre><p>注意，需要安装依赖bzip2，如下：</p><pre><code>$ sudo yum install -y bzip2</code></pre><h2 id="系统迁移"><a href="#系统迁移" class="headerlink" title="系统迁移"></a>系统迁移</h2><h3 id="待迁入机器的前期准备"><a href="#待迁入机器的前期准备" class="headerlink" title="待迁入机器的前期准备"></a>待迁入机器的前期准备</h3><ol><li>ip设置</li></ol><p>设置外网访问以及ip地址，这里要求可以连通外网即可，因为后续还原完成后还要重新修改ip地址。</p><ol start="2"><li>yum安装bzip2</li></ol><pre><code>$ sudo yum install -y bzip2</code></pre><ol start="3"><li>拷贝进入服务器</li></ol><pre><code>$ scp ./backup/backup.tar.bz2 root@192.168.164.104:/</code></pre><h3 id="开始还原"><a href="#开始还原" class="headerlink" title="开始还原"></a>开始还原</h3><pre><code>$ sudo su# cd /# tar xvpfj backup.tar.bz2 -C /</code></pre><p>还原完成后出现以下错误，可以忽略，继续操作。</p><pre><code>tar: Exiting with failure status due to previous errors</code></pre><h3 id="还原后设置"><a href="#还原后设置" class="headerlink" title="还原后设置"></a>还原后设置</h3><h4 id="1-执行selinux修复-restorecon-Rv"><a href="#1-执行selinux修复-restorecon-Rv" class="headerlink" title="1. 执行selinux修复 restorecon -Rv /"></a>1. 执行selinux修复 restorecon -Rv /</h4><pre><code># restorecon -Rv /</code></pre><h4 id="2-grub2修复引导，-boot目录"><a href="#2-grub2修复引导，-boot目录" class="headerlink" title="2. grub2修复引导，/boot目录"></a>2. grub2修复引导，/boot目录</h4><pre><code>// 查看硬盘UUID信息# blkid/dev/mapper/centos-root: UUID=&quot;5fadecf2-0b26-4140-a666-227c360d322d&quot; TYPE=&quot;xfs&quot;/dev/sda2: UUID=&quot;BUfxBc-OTHF-rgY4-He23-vjpe-Pt9p-cEWrt0&quot; TYPE=&quot;LVM2_member&quot;/dev/sda1: UUID=&quot;2f65063a-f7d7-438e-be8d-e1b86568b225&quot; TYPE=&quot;xfs&quot;/dev/mapper/centos-swap: UUID=&quot;86b641f6-236d-4158-8972-f90d2a243b65&quot; TYPE=&quot;swap&quot;/dev/mapper/centos-home: UUID=&quot;4ad2ac7f-b244-41a0-9d7b-f8116bc92162&quot; TYPE=&quot;xfs&quot;// 修改启动的硬盘信息# vim /etc/fstab// 修改为2f65063a-f7d7-438e-be8d-e1b86568b225信息## /etc/fstab# Created by anaconda on Wed Aug 21 14:54:48 2019## Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root /                       xfs     defaults        0 0UUID=2f65063a-f7d7-438e-be8d-e1b86568b225 /boot                   xfs     defaults        0 0/dev/mapper/centos-home /home                   xfs     defaults        0 0/dev/mapper/centos-swap swap                    swap    defaults        0 0// :wq保存退出// # grub2-install /dev/sda// 生成grub2启动的配置文件# grub2-mkconfig -o /boot/grub2/grub.cfg</code></pre><p>这样引导修复了一半，剩下的需要修改开机默认启动的内核信息。</p><h4 id="3-修改ip地址并测试ssh连接（注意密码的变更）"><a href="#3-修改ip地址并测试ssh连接（注意密码的变更）" class="headerlink" title="3. 修改ip地址并测试ssh连接（注意密码的变更）"></a>3. 修改ip地址并测试ssh连接（注意密码的变更）</h4><p>这里修改时要注意，ip要变更为目标机器的ip地址，而且要注意，恢复以后root密码发生了变化，会被之前备份的机器上的数据进行覆盖。</p><pre><code># vim /etc/sysconfig/network-scripts/ifcfg-ens192// 修改静态地址TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens192UUID=1b3d8c8c-e041-4193-8f04-7fec654320daDEVICE=ens192ONBOOT=yesIPADDR=192.168.164.104GATEWAY=192.168.164.254NETMASK=255.255.255.0DNS1=202.102.152.3ZONE=public// :wq保存退出// 重启网络服务# systemctl daemon-reload# systemctl restart network</code></pre><p>在开发机测试ssh连接，连接该服务器。</p><pre><code>ssh root@192.168.164.104  // 密码为原机器的密码</code></pre><h4 id="4-重新引导，选择开机启动的内核信息"><a href="#4-重新引导，选择开机启动的内核信息" class="headerlink" title="4. 重新引导，选择开机启动的内核信息"></a>4. 重新引导，选择开机启动的内核信息</h4><p>查看内核启动信息如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cat</span> /boot/grub2/grub.cfg<span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># DO NOT EDIT THIS FILE</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># It is automatically generated by grub2-mkconfig using templates</span><span class="token comment" spellcheck="true"># from /etc/grub.d and settings from /etc/default/grub</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/00_header ###</span><span class="token keyword">set</span> pager<span class="token operator">=</span>1<span class="token keyword">if</span> <span class="token punctuation">[</span> -s <span class="token variable">$prefix</span>/grubenv <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  load_env<span class="token keyword">fi</span><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">"<span class="token variable">${next_entry}</span>"</span> <span class="token punctuation">]</span> <span class="token punctuation">;</span> <span class="token keyword">then</span>   <span class="token keyword">set</span> default<span class="token operator">=</span><span class="token string">"<span class="token variable">${next_entry}</span>"</span>   <span class="token keyword">set</span> next_entry<span class="token operator">=</span>   save_env next_entry   <span class="token keyword">set</span> boot_once<span class="token operator">=</span>true<span class="token keyword">else</span>   <span class="token keyword">set</span> default<span class="token operator">=</span><span class="token string">"<span class="token variable">${saved_entry}</span>"</span><span class="token keyword">fi</span><span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token string">"<span class="token variable">${feature_menuentry_id}</span>"</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  menuentry_id_option<span class="token operator">=</span><span class="token string">"--id"</span><span class="token keyword">else</span>  menuentry_id_option<span class="token operator">=</span><span class="token string">""</span><span class="token keyword">fi</span><span class="token function">export</span> menuentry_id_option<span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">"<span class="token variable">${prev_saved_entry}</span>"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  <span class="token keyword">set</span> saved_entry<span class="token operator">=</span><span class="token string">"<span class="token variable">${prev_saved_entry}</span>"</span>  save_env saved_entry  <span class="token keyword">set</span> prev_saved_entry<span class="token operator">=</span>  save_env prev_saved_entry  <span class="token keyword">set</span> boot_once<span class="token operator">=</span>true<span class="token keyword">fi</span><span class="token keyword">function</span> savedefault <span class="token punctuation">{</span>  <span class="token keyword">if</span> <span class="token punctuation">[</span> -z <span class="token string">"<span class="token variable">${boot_once}</span>"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>    saved_entry<span class="token operator">=</span><span class="token string">"<span class="token variable">${chosen}</span>"</span>    save_env saved_entry  <span class="token keyword">fi</span><span class="token punctuation">}</span><span class="token keyword">function</span> load_video <span class="token punctuation">{</span>  <span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token variable">$feature_all_video_module</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>    insmod all_video  <span class="token keyword">else</span>    insmod efi_gop    insmod efi_uga    insmod ieee1275_fb    insmod vbe    insmod vga    insmod video_bochs    insmod video_cirrus  <span class="token keyword">fi</span><span class="token punctuation">}</span>terminal_output console<span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token variable">$feature_timeout_style</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span> <span class="token punctuation">;</span> <span class="token keyword">then</span>  <span class="token keyword">set</span> timeout_style<span class="token operator">=</span>menu  <span class="token keyword">set</span> timeout<span class="token operator">=</span>5<span class="token comment" spellcheck="true"># Fallback normal timeout code in case the timeout_style feature is</span><span class="token comment" spellcheck="true"># unavailable.</span><span class="token keyword">else</span>  <span class="token keyword">set</span> timeout<span class="token operator">=</span>5<span class="token keyword">fi</span><span class="token comment" spellcheck="true">### END /etc/grub.d/00_header ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/00_tuned ###</span><span class="token keyword">set</span> tuned_params<span class="token operator">=</span><span class="token string">""</span><span class="token keyword">set</span> tuned_initrd<span class="token operator">=</span><span class="token string">""</span><span class="token comment" spellcheck="true">### END /etc/grub.d/00_tuned ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/01_users ###</span><span class="token keyword">if</span> <span class="token punctuation">[</span> -f <span class="token variable">${prefix}</span>/user.cfg <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  <span class="token function">source</span> <span class="token variable">${prefix}</span>/user.cfg  <span class="token keyword">if</span> <span class="token punctuation">[</span> -n <span class="token string">"<span class="token variable">${GRUB2_PASSWORD}</span>"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>    <span class="token keyword">set</span> superusers<span class="token operator">=</span><span class="token string">"root"</span>    <span class="token function">export</span> superusers    password_pbkdf2 root <span class="token variable">${GRUB2_PASSWORD}</span>  <span class="token keyword">fi</span><span class="token keyword">fi</span><span class="token comment" spellcheck="true">### END /etc/grub.d/01_users ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/10_linux ###</span>menuentry <span class="token string">'CentOS Linux (3.10.0-1127.el7.x86_64) 7 (Core)'</span> --class centos --class gnu-linux --class gnu --class os --unrestricted <span class="token variable">$menuentry_id_option</span> <span class="token string">'gnulinux-3.10.0-1127.el7.x86_64-advanced-5fadecf2-0b26-4140-a666-227c360d322d'</span> <span class="token punctuation">{</span>        load_video        <span class="token keyword">set</span> gfxpayload<span class="token operator">=</span>keep        insmod gzio        insmod part_msdos        insmod xfs        <span class="token keyword">set</span> root<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>        <span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token variable">$feature_platform_search_hint</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root --hint-bios<span class="token operator">=</span>hd0,msdos1 --hint-efi<span class="token operator">=</span>hd0,msdos1 --hint-baremetal<span class="token operator">=</span>ahci0,msdos1 --hint<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>  2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">else</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root 2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">fi</span>        linux16 /vmlinuz-3.10.0-1127.el7.x86_64 root<span class="token operator">=</span>/dev/mapper/centos-root ro crashkernel<span class="token operator">=</span>auto rd.lvm.lv<span class="token operator">=</span>centos/root rd.lvm.lv<span class="token operator">=</span>centos/swap rhgb quiet        initrd16 /initramfs-3.10.0-1127.el7.x86_64.img<span class="token punctuation">}</span>menuentry <span class="token string">'CentOS Linux (3.10.0-957.27.2.el7.x86_64) 7 (Core)'</span> --class centos --class gnu-linux --class gnu --class os --unrestricted <span class="token variable">$menuentry_id_option</span> <span class="token string">'gnulinux-3.10.0-957.27.2.el7.x86_64-advanced-5fadecf2-0b26-4140-a666-227c360d322d'</span> <span class="token punctuation">{</span>        load_video        <span class="token keyword">set</span> gfxpayload<span class="token operator">=</span>keep        insmod gzio        insmod part_msdos        insmod xfs        <span class="token keyword">set</span> root<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>        <span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token variable">$feature_platform_search_hint</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root --hint-bios<span class="token operator">=</span>hd0,msdos1 --hint-efi<span class="token operator">=</span>hd0,msdos1 --hint-baremetal<span class="token operator">=</span>ahci0,msdos1 --hint<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>  2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">else</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root 2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">fi</span>        linux16 /vmlinuz-3.10.0-957.27.2.el7.x86_64 root<span class="token operator">=</span>/dev/mapper/centos-root ro crashkernel<span class="token operator">=</span>auto rd.lvm.lv<span class="token operator">=</span>centos/root rd.lvm.lv<span class="token operator">=</span>centos/swap rhgb quiet        initrd16 /initramfs-3.10.0-957.27.2.el7.x86_64.img<span class="token punctuation">}</span>menuentry <span class="token string">'CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)'</span> --class centos --class gnu-linux --class gnu --class os --unrestricted <span class="token variable">$menuentry_id_option</span> <span class="token string">'gnulinux-3.10.0-957.el7.x86_64-advanced-5fadecf2-0b26-4140-a666-227c360d322d'</span> <span class="token punctuation">{</span>        load_video        <span class="token keyword">set</span> gfxpayload<span class="token operator">=</span>keep        insmod gzio        insmod part_msdos        insmod xfs        <span class="token keyword">set</span> root<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>        <span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token variable">$feature_platform_search_hint</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root --hint-bios<span class="token operator">=</span>hd0,msdos1 --hint-efi<span class="token operator">=</span>hd0,msdos1 --hint-baremetal<span class="token operator">=</span>ahci0,msdos1 --hint<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>  2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">else</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root 2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">fi</span>        linux16 /vmlinuz-3.10.0-957.el7.x86_64 root<span class="token operator">=</span>/dev/mapper/centos-root ro crashkernel<span class="token operator">=</span>auto rd.lvm.lv<span class="token operator">=</span>centos/root rd.lvm.lv<span class="token operator">=</span>centos/swap rhgb quiet        initrd16 /initramfs-3.10.0-957.el7.x86_64.img<span class="token punctuation">}</span>menuentry <span class="token string">'CentOS Linux (0-rescue-8872cb4bf724478ebd933ed18a1cf4a6) 7 (Core)'</span> --class centos --class gnu-linux --class gnu --class os --unrestricted <span class="token variable">$menuentry_id_option</span> <span class="token string">'gnulinux-0-rescue-8872cb4bf724478ebd933ed18a1cf4a6-advanced-5fadecf2-0b26-4140-a666-227c360d322d'</span> <span class="token punctuation">{</span>        load_video        insmod gzio        insmod part_msdos        insmod xfs        <span class="token keyword">set</span> root<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>        <span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token variable">$feature_platform_search_hint</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root --hint-bios<span class="token operator">=</span>hd0,msdos1 --hint-efi<span class="token operator">=</span>hd0,msdos1 --hint-baremetal<span class="token operator">=</span>ahci0,msdos1 --hint<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>  2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">else</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root 2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">fi</span>        linux16 /vmlinuz-0-rescue-8872cb4bf724478ebd933ed18a1cf4a6 root<span class="token operator">=</span>/dev/mapper/centos-root ro crashkernel<span class="token operator">=</span>auto rd.lvm.lv<span class="token operator">=</span>centos/root rd.lvm.lv<span class="token operator">=</span>centos/swap rhgb quiet        initrd16 /initramfs-0-rescue-8872cb4bf724478ebd933ed18a1cf4a6.img<span class="token punctuation">}</span>menuentry <span class="token string">'CentOS Linux (0-rescue-c8c3bfe51b21485288f6ad1fe1cbe3c2) 7 (Core)'</span> --class centos --class gnu-linux --class gnu --class os --unrestricted <span class="token variable">$menuentry_id_option</span> <span class="token string">'gnulinux-0-rescue-c8c3bfe51b21485288f6ad1fe1cbe3c2-advanced-5fadecf2-0b26-4140-a666-227c360d322d'</span> <span class="token punctuation">{</span>        load_video        insmod gzio        insmod part_msdos        insmod xfs        <span class="token keyword">set</span> root<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>        <span class="token keyword">if</span> <span class="token punctuation">[</span> x<span class="token variable">$feature_platform_search_hint</span> <span class="token operator">=</span> xy <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root --hint-bios<span class="token operator">=</span>hd0,msdos1 --hint-efi<span class="token operator">=</span>hd0,msdos1 --hint-baremetal<span class="token operator">=</span>ahci0,msdos1 --hint<span class="token operator">=</span><span class="token string">'hd0,msdos1'</span>  2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">else</span>          search --no-floppy --fs-uuid --set<span class="token operator">=</span>root 2f65063a-f7d7-438e-be8d-e1b86568b225        <span class="token keyword">fi</span>        linux16 /vmlinuz-0-rescue-c8c3bfe51b21485288f6ad1fe1cbe3c2 root<span class="token operator">=</span>/dev/mapper/centos-root ro crashkernel<span class="token operator">=</span>auto rd.lvm.lv<span class="token operator">=</span>centos/root rd.lvm.lv<span class="token operator">=</span>centos/swap rhgb quiet        initrd16 /initramfs-0-rescue-c8c3bfe51b21485288f6ad1fe1cbe3c2.img<span class="token punctuation">}</span><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">"x<span class="token variable">$default</span>"</span> <span class="token operator">=</span> <span class="token string">'CentOS Linux (3.10.0-957.27.2.el7.x86_64) 7 (Core)'</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span> default<span class="token operator">=</span><span class="token string">'Advanced options for CentOS Linux>CentOS Linux (3.10.0-957.27.2.el7.x86_64) 7 (Core)'</span><span class="token punctuation">;</span> <span class="token keyword">fi</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">### END /etc/grub.d/10_linux ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/20_linux_xen ###</span><span class="token comment" spellcheck="true">### END /etc/grub.d/20_linux_xen ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/20_ppc_terminfo ###</span><span class="token comment" spellcheck="true">### END /etc/grub.d/20_ppc_terminfo ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/30_os-prober ###</span><span class="token comment" spellcheck="true">### END /etc/grub.d/30_os-prober ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/40_custom ###</span><span class="token comment" spellcheck="true"># This file provides an easy way to add custom menu entries.  Simply type the</span><span class="token comment" spellcheck="true"># menu entries you want to add after this comment.  Be careful not to change</span><span class="token comment" spellcheck="true"># the 'exec tail' line above.</span><span class="token comment" spellcheck="true">### END /etc/grub.d/40_custom ###</span><span class="token comment" spellcheck="true">### BEGIN /etc/grub.d/41_custom ###</span><span class="token keyword">if</span> <span class="token punctuation">[</span> -f  <span class="token variable">${config_directory}</span>/custom.cfg <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  <span class="token function">source</span> <span class="token variable">${config_directory}</span>/custom.cfg<span class="token keyword">elif</span> <span class="token punctuation">[</span> -z <span class="token string">"<span class="token variable">${config_directory}</span>"</span> -a -f  <span class="token variable">$prefix</span>/custom.cfg <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>  <span class="token function">source</span> <span class="token variable">$prefix</span>/custom.cfg<span class="token punctuation">;</span><span class="token keyword">fi</span><span class="token comment" spellcheck="true">### END /etc/grub.d/41_custom ###</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查找menuentry相关的信息，<em>CentOS Linux (3.10.0-957.27.2.el7.x86_64) 7 (Core)</em> 选择这个信息进行启动，无视其它的启动信息，操作如下：</p><pre><code># grub2-set-default &#39;CentOS Linux (3.10.0-957.27.2.el7.x86_64) 7 (Core)&#39;# grub2-mkconfig -o /boot/grub/grub.cfg</code></pre><p>重启服务器，反复重启进行测试，容易进入grub rescue模式，导致无法启动的情况。在rescue模式下，执行以下命令，让系统回到正常的启动界面：</p><pre><code>grub rescue&gt; lsgrub rescue&gt; ls (hd0,msdos2)/grub rescue&gt; set root=(hd0,msdos2)/grub2grub rescue&gt; set prefix=(hd0,msdos2)/grub2grub rescue&gt; insmod normalgrub rescue&gt; normal</code></pre><p>这样执行完成后，就重新进入了前边启动时选择启动选项的页面，可以启动系统。</p><p>最后，在登录Linux系统后，重新生成以下grub.cfg文件</p><pre><code># cd /boot/grub2/ &amp;&amp; mv grub.cfg grub.cfg.bak# grub2-mkconfig &gt; /boot/grub2/grub.cfg# grub2-install --boot-directory=/boot /dev/sda</code></pre><p>生成完毕后，重启，恢复所有正常启动信息。</p><ol start="5"><li>（可选）执行恢复命令之前请再确认一下你所键入的命令是不是你想要的，执行恢复命令可能需要一段不短的时间。 恢复命令结束时，你的工作还没完成，别忘了重新创建那些在备份时被排除在外的目录：</li></ol><pre><code>mkdir procmkdir lost+foundmkdir mntmkdir sys</code></pre><h2 id="后续处理"><a href="#后续处理" class="headerlink" title="后续处理"></a>后续处理</h2><ul><li>问题一：docker run问题</li></ul><p>错误信息：</p><pre class="line-numbers language-txt"><code class="language-txt">docker: Error response from daemon: driver failed programming external connectivity on endpoint quizzical_thompson (c2b238f6b003b1f789c989db0d789b4bf3284ff61152ba40dacd0e01bd984653):  (iptables failed: iptables --wait -t filter -A DOCKER ! -i docker0 -o docker0 -p tcp -d 172.17.0.3 --dport 24224 -j ACCEPT: iptables: No chain/target/match by that name.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>解决方式，注意以管理员方式运行下面的命令：</p><pre><code>// 1. kill掉docker所有进程# pkill docker // 2.清空nat表的所有链# iptables -t nat -F// 3.停止docker默认网桥docker0# ifconfig docker0 down// 4.删除docker0网桥# brctl delbr docker0// 5. 重启docker服务# systemctl restart docker</code></pre><p>如果找不到brctl命令，用下面的方式安装依赖：</p><pre><code>$ sudo yum install -y bridge-utils</code></pre><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://www.cnblogs.com/szy2018/p/13845947.html" target="_blank" rel="noopener">https://www.cnblogs.com/szy2018/p/13845947.html</a></li><li><a href="https://www.cnblogs.com/niyeshiyoumo/p/7676097.html" target="_blank" rel="noopener">https://www.cnblogs.com/niyeshiyoumo/p/7676097.html</a></li><li><a href="https://blog.csdn.net/hutao1101175783/article/details/106362340" target="_blank" rel="noopener">https://blog.csdn.net/hutao1101175783/article/details/106362340</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于多module下代码结构的gitlab构建问题解决</title>
      <link href="/2021/01/14/guan-yu-duo-module-xia-dai-ma-jie-gou-de-gitlab-gou-jian-wen-ti-jie-jue/"/>
      <url>/2021/01/14/guan-yu-duo-module-xia-dai-ma-jie-gou-de-gitlab-gou-jian-wen-ti-jie-jue/</url>
      
        <content type="html"><![CDATA[<h1 id="关于多module代码结构的gitlab构建问题解决"><a href="#关于多module代码结构的gitlab构建问题解决" class="headerlink" title="关于多module代码结构的gitlab构建问题解决"></a>关于多module代码结构的gitlab构建问题解决</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前项目结构中，存在一个项目多个module的情况，也就是一个git项目中，多个微服务代码放在一个文件夹下的场景，而多个微服务对应在jenkins中创建了多个构建任务。这种场景下带来的问题是，当该文件夹下某一个微服务的代码修改后，会触发该文件夹下所有微服务进行构建。所以需要避免这个问题。</p><h2 id="面临问题"><a href="#面临问题" class="headerlink" title="面临问题"></a>面临问题</h2><p>如何提取git记录信息，该记录信息能够明确表示提交目录，用来判断是否输入该目录下的文件修改。</p><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><p>在gitlab中配置了每个项目的webhook链接用以触发构建，于是从webhook下手，利用webhook中的请求信息进行区分。下面是一次webhook发送时的请求体信息：</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>  <span class="token property">"object_kind"</span><span class="token operator">:</span> <span class="token string">"push"</span><span class="token punctuation">,</span>  <span class="token property">"event_name"</span><span class="token operator">:</span> <span class="token string">"push"</span><span class="token punctuation">,</span>  <span class="token property">"before"</span><span class="token operator">:</span> <span class="token string">"5c28233547357212cde8f51ca3608a87439c274a"</span><span class="token punctuation">,</span>  <span class="token property">"after"</span><span class="token operator">:</span> <span class="token string">"e6e94eaff904bd377448a7b795b9a7b79687d42f"</span><span class="token punctuation">,</span>  <span class="token property">"ref"</span><span class="token operator">:</span> <span class="token string">"refs/heads/master"</span><span class="token punctuation">,</span>  .....  <span class="token property">"project"</span><span class="token operator">:</span> <span class="token punctuation">{</span>   ......  <span class="token punctuation">}</span><span class="token punctuation">,</span>  <span class="token property">"commits"</span><span class="token operator">:</span> <span class="token punctuation">[</span>    <span class="token punctuation">{</span>      <span class="token property">"id"</span><span class="token operator">:</span> <span class="token string">"5c28233547357212cde8f51ca3608a87439c274a"</span><span class="token punctuation">,</span>      <span class="token property">"message"</span><span class="token operator">:</span> <span class="token string">"build(project): 修改忽略文件\n\n1. 修改忽略文件\n"</span><span class="token punctuation">,</span>      <span class="token property">"timestamp"</span><span class="token operator">:</span> <span class="token string">"2021-01-08T02:24:51Z"</span><span class="token punctuation">,</span>      <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"/commit/5c28233547357212cde8f51ca3608a87439c274a"</span><span class="token punctuation">,</span>      .......      <span class="token property">"added"</span><span class="token operator">:</span> <span class="token punctuation">[</span>      <span class="token punctuation">]</span><span class="token punctuation">,</span>      <span class="token property">"modified"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token string">"test-micro-service-bda-dataanalysis-management/.gitignore"</span><span class="token punctuation">,</span>        <span class="token string">"test-micro-service-bda-dataset-management/.gitignore"</span><span class="token punctuation">,</span>        <span class="token string">"test-micro-service-bda-datasource-management/.gitignore"</span><span class="token punctuation">,</span>        <span class="token string">"test-micro-service-bda-datavisualization-management/.gitignore"</span><span class="token punctuation">,</span>        <span class="token string">"test-micro-service-psl-equipment-management/.gitignore"</span>      <span class="token punctuation">]</span><span class="token punctuation">,</span>      <span class="token property">"removed"</span><span class="token operator">:</span> <span class="token punctuation">[</span>      <span class="token punctuation">]</span>    <span class="token punctuation">}</span>  <span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token property">"total_commits_count"</span><span class="token operator">:</span> <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token property">"push_options"</span><span class="token operator">:</span> <span class="token punctuation">{</span>  <span class="token punctuation">}</span><span class="token punctuation">,</span>  <span class="token property">"repository"</span><span class="token operator">:</span> <span class="token punctuation">{</span>    ......  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如下可以看到在<em>commit</em>节点中，主要的提交信息分为added、modified、removed这三部分，其内分为其它不同的文件，需要匹配其修改的文件所在路径。</p><p>通过gitlab中的Generic Webhook Trigger插件实现添加两个信息，一个是原有的判断分支的参数<em>ref</em>，另一个是需要进行匹配内容的范围，也就是<em>commit</em>节点中的修改内容。通过正则表达式先获取这两个内容，在最后的<em>Optional filter</em>这个部分进行最终判断。最后jenkins任务示例设置如下：</p><p><img src="%E5%A4%9A%E5%8F%82%E6%95%B0%E5%88%A4%E6%96%AD%E6%9D%A1%E4%BB%B6%E8%AE%BE%E7%BD%AE.png" alt></p><p>首先在<em>Post content parameters</em>添加两个参数，如下表</p><table><thead><tr><th>Variable</th><th>Expression（type）</th><th>value filter</th></tr></thead><tbody><tr><td>ref</td><td>$.ref（JSONPath）</td><td>^(refs/heads/</td></tr><tr><td>changed_files</td><td>$.commits[<em>].[‘modified’,’added’,’removed’][</em>] （JSONPath）</td><td></td></tr></tbody></table><p>然后添加触发的token信息，最后在Optional filter部分添加匹配的内容，如下：</p><ul><li>匹配文本（Text）</li></ul><pre class="line-numbers language-R"><code class="language-R">$ref $changed_files，<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>中间有空格分隔，表示两个参数信息，获取的内容组合为一个字符串</p><ul><li>正则表达式（Expression）</li></ul><pre class="line-numbers language-R"><code class="language-R">develop\s(.*"(test-micro-service-app-mng/)[^"]+?"){1,}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>匹配特定分支，加上文件夹路径信息，和上面的匹配文本进行对比。如果满足则启动构建，否则不启动。</p><p>这样就能区分在特定分支下提交的内容能够匹配并触发构建。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python文件路径问题</title>
      <link href="/2020/12/31/python-wen-jian-lu-jing-wen-ti/"/>
      <url>/2020/12/31/python-wen-jian-lu-jing-wen-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="python路径问题"><a href="#python路径问题" class="headerlink" title="python路径问题"></a>python路径问题</h1><h2 id="路径描述和解决方案"><a href="#路径描述和解决方案" class="headerlink" title="路径描述和解决方案"></a>路径描述和解决方案</h2><p>之前在小同志编写的项目中，出现了下面的一段代码:</p><pre class="line-numbers language-python"><code class="language-python">X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">,</span> scaler <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span>r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>调用的load_dataset方法如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">load_dataset</span><span class="token punctuation">(</span>DATASET_PATH<span class="token punctuation">)</span><span class="token punctuation">:</span>    seq_length <span class="token operator">=</span> <span class="token number">100</span>    split <span class="token operator">=</span> <span class="token number">0.8</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>DATASET_PATH<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>DATASET_PATH<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>DATASET_PATH<span class="token punctuation">)</span><span class="token punctuation">:</span>        df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>DATASET_PATH<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> FileNotFoundError<span class="token punctuation">(</span><span class="token string">'File %s not found!'</span> <span class="token operator">%</span> DATASET_PATH<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在python程序运行的过程中，诡异的事情发生了，具体体现在：明明文件就在相对路径的位置放着，通过shell执行命令可以切换到对应的文件夹，也能正常通过cat命令输出文件信息，但是在python程序中，业务逻辑始终停在自定义异常上。</p><p>环境信息如下，程序运行的根目录为<em>/home/python/app</em>，该目录下有两个文件夹，一个为src，是代码所在的位置，另一个是data，也就是test.csv文件所在位置。</p><p>下面开始尝试排查问题，在调用load_dataset()方法之前,故意输出多个路径信息对比，代码如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"1:"</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 添加这一句是为了更快的输出到docker日志信息中</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出信息如下：</p><pre><code>1:/home/python/data/test.csv</code></pre><p>在输出的内容中竟然少了<strong>app</strong>这个文件夹，这个运行结果出现在docker容器中，容器运行的系统是ubuntu。但是对比在windows下，同样的路径是可以访问到文件的。至于为何那么诡异，丢失了路径，目前暂时无法解释。</p><p>下一步进行了更多的测试，如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>basedir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"2:"</span><span class="token punctuation">,</span> basedir<span class="token punctuation">)</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"1:"</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"3："</span><span class="token punctuation">,</span> basedir <span class="token operator">+</span> r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出信息如下：</p><pre><code>2: /home/python/app/src1: /home/python/data/test.csv3： /home/python/app/src../data/test.csv</code></pre><p>但是还是发现不能解决这个问题，尝试进一步测试，如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>basedir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"2:"</span><span class="token punctuation">,</span> basedir<span class="token punctuation">)</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"1:"</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"3："</span><span class="token punctuation">,</span> basedir <span class="token operator">+</span> r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"4: "</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>basedir <span class="token operator">+</span> r<span class="token string">'/'</span><span class="token operator">+</span> r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出信息如下：</p><pre><code>2: /home/python/app/src1: /home/python/data/test.csv3： /home/python/app/src../data/test.csv4:  /home/python/app/src/data/test.csv</code></pre><p>在使用第四种方式的时候，正常可以定位到文档所在位置。</p><p>于是最终解决方式如下：</p><pre class="line-numbers language-python"><code class="language-python">X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">,</span> scaler <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>basedir <span class="token operator">+</span> r<span class="token string">'/'</span><span class="token operator">+</span> r<span class="token string">'../data/test.csv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><ul><li><a href="https://blog.csdn.net/qq_36711420/article/details/79631141" target="_blank" rel="noopener">https://blog.csdn.net/qq_36711420/article/details/79631141</a></li><li><a href="https://blog.csdn.net/qq_34814495/article/details/109682792" target="_blank" rel="noopener">https://blog.csdn.net/qq_34814495/article/details/109682792</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Springboot拆分依赖构建微服务方案</title>
      <link href="/2020/12/28/springboot-chai-fen-yi-lai-gou-jian-wei-fu-wu-fang-an/"/>
      <url>/2020/12/28/springboot-chai-fen-yi-lai-gou-jian-wei-fu-wu-fang-an/</url>
      
        <content type="html"><![CDATA[<h1 id="Springboot拆分打包并进行构建的调整（调整图片）"><a href="#Springboot拆分打包并进行构建的调整（调整图片）" class="headerlink" title="Springboot拆分打包并进行构建的调整（调整图片）"></a>Springboot拆分打包并进行构建的调整（调整图片）</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前在Springboot微服务中，在构建时进行打包的流程中，具体的产物为fat jar，也就是将所有的依赖信息以及业务代码全部打包为一个jar包，这样不仅在构建速度上拖慢，而且每次都要生成新的jar包。有没有可能将公共的依赖包剥离出来，只对业务代码进行打包，待运行时在将二者放在一起运行？</p><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>Docker 为了节约存储空间，所以采用了分层存储概念。共享数据会对镜像和容器进行分层，不同镜像可以共享相同数据，并且在镜像上为容器分配一个 RW 层来加快容器的启动顺序。</p><p>在构建镜像的过程中 Docker 将按照 Dockerfile 中指定的顺序逐步执行 Dockerfile 中的指令。随着每条指令的检查，Docker 将在其缓存中查找可重用的现有镜像，而不是创建一个新的（重复）镜像。</p><p>Dockerfile 的每一行命令都创建新的一层，包含了这一行命令执行前后文件系统的变化。为了优化这个过程，Docker 使用了一种缓存机制：只要这一行命令不变，那么结果和上一次是一样的，直接使用上一次的结果即可。</p><p>为了充分利用层级缓存，我们必须要理解 Dockerfile 中的命令行是如何工作的，尤其是RUN，ADD和COPY这几个命令。一个好的docker 镜像 应该是合理分层 减少变动次数 从而加快打包的速度和分发的速度。</p><p>我们在优化的时候也是借鉴这个特性，同时参考之前前端构建的特性，将不怎么变动的依赖jar包集合抽取出来，作为一个不怎么变更的层，然后将业务代码打为一个jar包，在运行时手动指定业务jar包依赖的jar包集合，从而实现构建提速。</p><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>目前具体的实现有两种方式：</p><ol><li>依托Springboot 2.3.0的版本特性支持，使用自带的分层打包机制进行。由于我们使用的还是Springboot 2.2.6，因此先搁置这一方案。</li><li>利用maven插件分离依赖jar包集合和业务jar包。这是目前所采用的。</li></ol><h2 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h2><p>这里使用业务建模的微服务来进行测试，服务名为：test-visualmodel，<a href="http://192.168.123.202:8181/test/business/test-vm/tree/feature.build.docker_depart" target="_blank" rel="noopener">项目所在</a>分支为：feature.build.docker_depart。</p><p>首先看一下整个项目的目录结构：</p><p><img src="%E9%A1%B9%E7%9B%AE%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.png" alt></p><p>需要注意的是，修改时要在module的内部的pom文件中进行修改，不能在最外部修改，否则容易造成生成的jar包集合路径不正确，导致找不到依赖，无法启动业务jar包。</p><p>在修改之前，需要确定的是，修改maven打包的部分，也就是<plugin>标签对应的内容。</plugin></p><p>下面对module内的pom文件进行修改如下：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!-- 增加以下信息 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>build</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugins</span><span class="token punctuation">></span></span>            <span class="token comment" spellcheck="true">&lt;!-- 设置 SpringBoot 打包插件只包含业务代码相关的部分，不包含外部的依赖jar包 --></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.springframework.boot<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>spring-boot-maven-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>layout</span><span class="token punctuation">></span></span>ZIP<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>layout</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>includes</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>include</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>${project.groupId}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>${project.artifactId}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>include</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>includes</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>            <span class="token comment" spellcheck="true">&lt;!--设置将 lib 拷贝到应用 Jar 外面，而且和生成的业务jar包平级 --></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>plugin</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.maven.plugins<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>maven-dependency-plugin<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>executions</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>execution</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>copy-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>                        <span class="token comment" spellcheck="true">&lt;!-- 这里表示在打包之前执行下面的操作 --></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>phase</span><span class="token punctuation">></span></span>prepare-package<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>phase</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goals</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>goal</span><span class="token punctuation">></span></span>copy-dependencies<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goal</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>goals</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>type</span><span class="token punctuation">></span></span>jar<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>type</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>includeTypes</span><span class="token punctuation">></span></span>jar<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>includeTypes</span><span class="token punctuation">></span></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>includeScope</span><span class="token punctuation">></span></span>runtime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>includeScope</span><span class="token punctuation">></span></span>                            <span class="token comment" spellcheck="true">&lt;!-- 这里解释为何在module内部进行配置， project.build.directory直接定位到module内部生成的target目录下 --></span>                            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>outputDirectory</span><span class="token punctuation">></span></span>${project.build.directory}/libs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>outputDirectory</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span>                    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>execution</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>executions</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugin</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>plugins</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>build</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样基本上就可以在本地进行测试了，在命令行中使用<em>mvn clean install</em>命令执行，打包后，target目录下的结构如下：</p><p><img src="target%E7%9B%AE%E5%BD%95%E4%BF%A1%E6%81%AF.png" alt></p><p>这样libs就是剥离出的jar包依赖集合，目录中的test开头的jar包自然就是我们的业务jar包。这样拆分后该jar包从以前的70多MB大小，降到300多kb大小，就加速了构建时copy操作的速度。</p><p>最后需要进行测试，相比之前启动命令发生了变化，如下：</p><pre><code>java  -Dloader.path=&quot;libs/&quot; -jar test-opt-visualmodel-1.0.0-SNAPSHOT.jar</code></pre><p>添加了*<em>-Dloader.path=”libs/“</em>参数，指定了依赖jar包集合所在的位置。即可正常启动。</p><p>进入构建流水线之前，最后还要调整一下Dockerfile的编写，如下：</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile">### 各微服务运行时所构建的镜像### 用该文件替换项目中的Dockerfile# 以基础镜像为底，执行FROM 192.168.123.202:5000/base_backend:0.0.1# 构建变更# 将依赖包单独剥离出来，转到lib目录下# 复制libs依赖信息到对应目录下COPY ./target/libs/ /app/libs/# 复制jar包所在地址 到 目标地址  --将jar包拷贝到镜像系统的相应地址COPY ./target/test-opt-visualmodel-1.0.0-SNAPSHOT.jar /app# 设置运行的加入点ENTRYPOINT ["sh", "-c", "java -Dloader.path=\"libs/\" -XX:MaxRAMPercentage=75.0 -XX:MaxRAM=1000m -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -javaagent:/app/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.namespace=${SKYWALKING_NAMESPACE} -Dskywalking.agent.service_name=${SKYWALKING_TARGET_SERVICE_NAME} -Dskywalking.collector.backend_service=${SKYWALKING_IP_PORT}  -Dspring.profiles.active=${CHANNEL} -Dspring.cloud.client.ip-address=${IP_ADDR} -Dnacos_ip=${NACOS_IP} -Dnacos_namespace=${NACOS_NAMESPACE} -jar test-opt-visualmodel-1.0.0-SNAPSHOT.jar"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要是修改了两点，一是将依赖jar包集合所在的文件夹copy到对应目录，另一个就是启动命令指定依赖jar包的路径信息。</p><h2 id="调整"><a href="#调整" class="headerlink" title="调整"></a>调整</h2><ol><li>ide中的调整启动方式</li></ol><p>需要在intellij idea编辑器中，添加启动参数，如下图：</p><p><img src="idea%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4.png" alt></p><p>主要是将<strong>-Dloader.path=”libs/“</strong>添加到启动时的VM Options一行中。</p><ol start="2"><li>异常信息：Error running “Application”:Command line is too long.Shorten command line for or also for Spring Boot configuration.</li></ol><p>解决方式：在项目所在的<em>.idea</em>文件夹中，找到<strong>workspace.xml</strong>文件，然后在文件中搜索<em>PropertiesComponent</em>，在该标签的内容中插入：</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>dynamic.classpath<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>最终修改后如下：</p><pre class="line-numbers language-xml"><code class="language-xml"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>component</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>PropertiesComponent<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ExpandSpringBootJavaOptionsPanel<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>RunOnceActivity.ShowReadmeOnStart<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>WebServerToolWindowFactoryState<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>false<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>aspect.path.notification.shown<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>last_opened_file_path<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>$PROJECT_DIR$/pom.xml<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>nodejs_interpreter_path.stuck_in_default_project<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>undefined stuck path<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>nodejs_npm_path_reset_for_default_project<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>nodejs_package_manager_path<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>npm<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>settings.editor.selected.configurable<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>MavenSettings<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token comment" spellcheck="true">&lt;!-- 新增一行解决异常信息 --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>dynamic.classpath<span class="token punctuation">"</span></span> <span class="token attr-name">value</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>component</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>需要调整好多东西，参考文章中的内容需要辩证看待，另外，也需要自己去实践，将方案做出来！</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul><li><a href="https://blog.csdn.net/ttzommed/article/details/106759670" target="_blank" rel="noopener">Springboot2.3.0分层打包</a></li><li><a href="https://www.jianshu.com/p/32456eea0488" target="_blank" rel="noopener">利用maven插件分离依赖jar包集合和业务jar包</a></li><li><a href="https://blog.csdn.net/weixin_44460333/article/details/104624236" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44460333/article/details/104624236</a></li><li><a href="https://juejin.cn/post/6844904119338008583" target="_blank" rel="noopener">https://juejin.cn/post/6844904119338008583</a></li><li><a href="https://blog.csdn.net/weixin_44460333/article/details/103020487" target="_blank" rel="noopener">Docker参考</a></li><li><a href="https://stackoverflow.com/questions/47926382/how-to-configure-shorten-command-line-method-for-whole-project-in-intellij" target="_blank" rel="noopener">运行异常解决</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>前端私服搭建及验证</title>
      <link href="/2020/12/28/qian-duan-si-fu-da-jian-ji-yan-zheng/"/>
      <url>/2020/12/28/qian-duan-si-fu-da-jian-ji-yan-zheng/</url>
      
        <content type="html"><![CDATA[<h1 id="前端私服搭建"><a href="#前端私服搭建" class="headerlink" title="前端私服搭建"></a>前端私服搭建</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>环境准备：</p><ol><li>镜像仓库：nexus 3.11</li><li>nodejs版本：10.16.0</li><li>yarn版本：1.22.4</li><li>npm版本：6.9.0</li></ol><h2 id="nexus3-仓库设置和用户设置"><a href="#nexus3-仓库设置和用户设置" class="headerlink" title="nexus3 仓库设置和用户设置"></a>nexus3 仓库设置和用户设置</h2><h3 id="仓库设置"><a href="#仓库设置" class="headerlink" title="仓库设置"></a>仓库设置</h3><p>前端仓库创建如下：</p><ol><li>npm-hosted: 私有的仓库，存储自己开发的自定义组件。</li></ol><p><img src="npm-hosted.png" alt></p><ol start="2"><li>npm-proxy：公共仓库代理，这里代理的是淘宝的<a href="http://registry.npm.taobao.org" target="_blank" rel="noopener">镜像仓库</a>。</li></ol><p><img src="npm-proxy.png" alt></p><ol start="3"><li>npm-group：整合上述两个仓库信息，作为一个仓库对外使用。</li></ol><p><img src="npm-group.png" alt></p><p>使用管理员账户进行添加。添加完成后，下面需要添加角色和用户，并设置权限。</p><h3 id="用户设置"><a href="#用户设置" class="headerlink" title="用户设置"></a>用户设置</h3><p>首先，设置一个前端专属角色，添加npm的相关授权，基本信息如下：</p><p><img src="npm%E8%A7%92%E8%89%B2%E8%AE%BE%E7%BD%AE.png" alt></p><p>然后添加用户npm-user，在用户层面添加npm和nx的相关角色，保证推送数据的时候可以正常上传，基本信息如下：</p><p><img src="npm%E7%94%A8%E6%88%B7%E8%AE%BE%E7%BD%AE.png" alt></p><p>其次，在Realms选项中，激活<em>npm Bearer Token Realm</em>，如下图：</p><p><img src="npm%E9%A2%86%E5%9F%9F%E7%99%BB%E5%BD%95.png" alt></p><p>最后，在匿名登录<em>Anonymous</em>部分配置，勾选<em>Allow anonymous users to access the server<br>*，然后username选择我们创建的npm-user用户，Realm选择</em>npm Bearer Token Realm*，最后保存即可。如下图：</p><p><img src="npm%E5%8C%BF%E5%90%8D%E7%99%BB%E5%BD%95.png" alt></p><p>全部设置完成后，基本信息如下：</p><ul><li>前端私服：<a href="http://192.168.123.202:8081/repository/npm-group/" target="_blank" rel="noopener">http://192.168.123.202:8081/repository/npm-group/</a></li><li>前端推送私服：<a href="http://192.168.123.202:8081/repository/npm-hosted/" target="_blank" rel="noopener">http://192.168.123.202:8081/repository/npm-hosted/</a></li><li>用户名密码：npm-user hoteam@2019</li><li>用户对应邮箱信息：<a href="mailto:lisongyang@hoteamsoft.com" target="_blank" rel="noopener">lisongyang@hoteamsoft.com</a></li></ul><h2 id="前端配置"><a href="#前端配置" class="headerlink" title="前端配置"></a>前端配置</h2><p>选择<a href="https://github.com/GoldSubmarine/workflow-bpmn-modeler.git" target="_blank" rel="noopener">workflow-bpmn-modeler</a>这个项目进行配置，先从github上拉取到本地，然后对package.json进行修改，如下：</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>  <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"ht-workflow-bpmn-modeler"</span><span class="token punctuation">,</span>  <span class="token property">"version"</span><span class="token operator">:</span> <span class="token string">"0.2.52"</span><span class="token punctuation">,</span>  ....  <span class="token property">"publishConfig"</span><span class="token operator">:</span> <span class="token punctuation">{</span>    <span class="token property">"registry"</span><span class="token operator">:</span> <span class="token string">"http://192.168.123.202:8081/repository/npm-hosted/"</span>  <span class="token punctuation">}</span><span class="token punctuation">,</span>  ...<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里主要是修改组件名称，修改发布时的仓库地址。</p><h3 id="npm设置"><a href="#npm设置" class="headerlink" title="npm设置"></a>npm设置</h3><p>首先设置yarn的远程仓库，变更为我们之前配置的私有仓库，如下：</p><pre class="line-numbers language-shell"><code class="language-shell">// 如果存在已经设置的registry的参数，需要先进行删除$ npm config delete registry$ npm config set registry http://192.168.123.202:8081/repository/npm-group/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这时候在对应的位置上（window中C:\Users\ht路径下）生成了<strong>.npmrc</strong>文件，内容如下：</p><pre class="line-numbers language-txt"><code class="language-txt">registry=http://192.168.123.202:8081/repository/npm-group/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后登陆这个私服的信息，操作如下：</p><pre class="line-numbers language-shell"><code class="language-shell">$ npm loginUsername: npm-userPassword: (输入密码，不显示)Email: (this IS public) lisongyang@hoteamsoft.com<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>操作完成后，<strong>.npmrc</strong>会产生变更，内容如下：</p><pre class="line-numbers language-shell"><code class="language-shell">registry=http://192.168.123.202:8081/repository/npm-group///192.168.123.202:8081/repository/npm-group/:_authToken=NpmToken.fdf0237e-72e9-3e99-91fa-9f513958689b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>最后需要对<strong>.npmrc</strong>文件配置，添加两个选项，最终结果如下：</p><pre class="line-numbers language-doc"><code class="language-doc">registry=http://192.168.123.202:8081/repository/npm-group///192.168.123.202:8081/repository/npm-group/:_authToken=NpmToken.fdf0237e-72e9-3e99-91fa-9f513958689balways-auth=truestrict-ssl=false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>或者在命令行中进入如下设置：</p><pre class="line-numbers language-shell"><code class="language-shell">npm config set always-auth truenpm config set strict-ssl false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>设置完成后，进入npm测试。</p><h3 id="npm推送测试"><a href="#npm推送测试" class="headerlink" title="npm推送测试"></a>npm推送测试</h3><pre class="line-numbers language-shell"><code class="language-shell">// 添加依赖信息$ npm install // 测试前端运行$ npm run serve// 测试推送$ npm publish<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="npm拉取测试"><a href="#npm拉取测试" class="headerlink" title="npm拉取测试"></a>npm拉取测试</h3><pre class="line-numbers language-shell"><code class="language-shell">$ npm install ht-workflow-bpmn-modeler@0.2.52 --verbose<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="npm问题"><a href="#npm问题" class="headerlink" title="npm问题"></a>npm问题</h3><ol><li>错误信息：</li></ol><pre class="line-numbers language-log"><code class="language-log">npm ERR! code E401npm ERR! Unable to authenticate, need: BASIC realm="Sonatype Nexus Repository Manager"npm ERR! A complete log of this run can be found in:npm ERR!     D:\dev\FrontEnd\nodejs\node_cache\_logs\2020-12-01T08_09_04_980Z-debug.log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解决方式：临时未确定，先使用yarn进行推送。</p><ol start="2"><li>错误信息：</li></ol><pre><code>npm ERR! code ECONNRESETnpm ERR! errno ECONNRESETnpm ERR! network request to http://192.168.123.202:8081/repository/npm-hosted/ht-workflow-bpmn-modeler failed, reason: read ECONNRESETnpm ERR! network This is a problem related to network connectivity.npm ERR! network In most cases you are behind a proxy or have bad network settings.npm ERR! networknpm ERR! network If you are behind a proxy, please make sure that thenpm ERR! network &#39;proxy&#39; config is set properly.  See: &#39;npm help config&#39;npm ERR! A complete log of this run can be found in:npm ERR!     D:\dev\FrontEnd\nodejs\node_cache\_logs\2020-12-01T08_27_31_872Z-debug.log</code></pre><p>解决方式：临时未确定，先使用yarn进行推送。</p><h3 id="yarn设置"><a href="#yarn设置" class="headerlink" title="yarn设置"></a>yarn设置</h3><p>在yarn设置以前必须先对npm进行设置，否则会出现拉取不到的情况。</p><p>首先设置yarn的远程仓库，然后通过yarn登录镜像仓库，最后进行测试。</p><pre class="line-numbers language-shell"><code class="language-shell">// 设置远程仓库$ yarn config set registry http://192.168.123.202:8081/repository/npm-group/// 登录远程仓库$ yarn login yarn login v1.22.4info npm username: npm-userinfo npm email: lisongyang@hoteamsoft.comDone in 0.06s.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>登录后，会在对应的位置上（window中C:\Users\ht路径下）生成了<strong>.yarnrc</strong>文件，内容如下：</p><pre class="line-numbers language-txt"><code class="language-txt">registry "http://192.168.123.202:8081/repository/npm-group/"email lisongyang@hoteamsoft.comusername npm-user<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>生成了以后。可以对项目进行测试。</p><h3 id="yarn推送测试"><a href="#yarn推送测试" class="headerlink" title="yarn推送测试"></a>yarn推送测试</h3><pre class="line-numbers language-shell"><code class="language-shell">// 添加依赖信息$ yarn install // 测试前端运行$ yarn run serve// 推送$ yarn publish <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>运行结果如下：</p><pre class="line-numbers language-shell"><code class="language-shell">$ yarn publishyarn publish v1.22.4[1/4] Bumping version...info Current version: 0.2.54question New version: 0.2.54[2/4] Logging in...info npm username: npm-userinfo npm email: lisongyang@hoteamsoft.comquestion npm password:success Logged in.[3/4] Publishing...$ yarn buildyarn run v1.22.4$ cross-env NODE_ENV=build vue-cli-service build --target lib --inline-vue --entry package/index.js\  Building for production as library (commonjs,umd,umd-min)... DONE  Compiled successfully in 11895ms                                                                                                                                                                          4:39:33 PM DONE  Compiled successfully in 11922ms                                                                                                                                                                          4:39:33 PM DONE  Compiled successfully in 11955ms                                                                                                                                                                          4:39:33 PM  File                                        Size                                                                                 Gzipped  dist\ht-workflow-bpmn-modeler.umd.min.js    3012.16 KiB                                                                          525.86 KiB  dist\ht-workflow-bpmn-modeler.umd.js        3012.16 KiB                                                                          525.86 KiB  dist\ht-workflow-bpmn-modeler.common.js     3011.74 KiB                                                                          525.74 KiB  Images and other types of assets omitted.Done in 15.04s.$ yarn lib && yarn cdn && yarn demoyarn run v1.22.4$ cross-env NODE_ENV=lib vue-cli-service build-  Building for production... DONE  Compiled successfully in 7864ms                                                                                                                                                                           4:39:45 PM  File                                     Size                                                                                   Gzipped  docs\lib\0.2.54\js\chunk-libs.js         2825.71 KiB                                                                            525.03 KiB  docs\lib\0.2.54\js\chunk-elementUI.js    1883.20 KiB                                                                            319.34 KiB  docs\lib\0.2.54\js\app.js                403.72 KiB                                                                             33.41 KiB  docs\lib\0.2.54\js\chunk-vue.js          240.43 KiB                                                                             65.09 KiB  docs\lib\0.2.54\js\runtime.js            6.12 KiB                                                                               1.64 KiB  Images and other types of assets omitted. DONE  Build complete. The docs\lib\0.2.54 directory is ready to be deployed. INFO  Check out deployment instructions at https://cli.vuejs.org/guide/deployment.htmlDone in 10.95s.yarn run v1.22.4$ cross-env NODE_ENV=cdn vue-cli-service build/  Building for production... DONE  Compiled successfully in 7697ms                                                                                                                                                                           4:39:56 PM  File                                     Size                                                                                   Gzipped  docs\cdn\0.2.54\js\chunk-libs.js         2825.71 KiB                                                                            525.03 KiB  docs\cdn\0.2.54\js\chunk-elementUI.js    1883.20 KiB                                                                            319.34 KiB  docs\cdn\0.2.54\js\app.js                403.72 KiB                                                                             33.41 KiB  docs\cdn\0.2.54\js\chunk-vue.js          240.43 KiB                                                                             65.09 KiB  docs\cdn\0.2.54\js\runtime.js            6.20 KiB                                                                               1.70 KiB  Images and other types of assets omitted. DONE  Build complete. The docs\cdn\0.2.54 directory is ready to be deployed. INFO  Check out deployment instructions at https://cli.vuejs.org/guide/deployment.htmlDone in 10.78s.yarn run v1.22.4$ cross-env NODE_ENV=demo vue-cli-service build/  Building for production... DONE  Compiled successfully in 7510ms                                                                                                                                                                           4:40:07 PM  File                               Size                                                                                      Gzipped  docs\demo\js\chunk-libs.js         2825.71 KiB                                                                               525.03 KiB  docs\demo\js\chunk-elementUI.js    1883.20 KiB                                                                               319.34 KiB  docs\demo\js\app.js                404.26 KiB                                                                                33.55 KiB  docs\demo\js\chunk-vue.js          240.44 KiB                                                                                65.09 KiB  docs\demo\js\runtime.js            6.14 KiB                                                                                  1.66 KiB  Images and other types of assets omitted. DONE  Build complete. The docs\demo directory is ready to be deployed. INFO  Check out deployment instructions at https://cli.vuejs.org/guide/deployment.htmlDone in 10.83s.success Published.[4/4] Revoking token...success Revoked login token.Done in 57.41s.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="yarn拉取测试"><a href="#yarn拉取测试" class="headerlink" title="yarn拉取测试"></a>yarn拉取测试</h3><p>在目标项目中，使用yarn进行拉取。在操作之前必须删除yarn.lock或者package-lock.json再进行操作：</p><pre class="line-numbers language-shell"><code class="language-shell">$ yarn add ht-workflow-bpmn-modeler@0.2.54yarn add v1.22.4[1/4] Resolving packages...[2/4] Fetching packages...info fsevents@1.2.13: The platform "win32" is incompatible with this module.info "fsevents@1.2.13" is an optional dependency and failed compatibility check. Excluding it from installation.info fsevents@2.1.3: The platform "win32" is incompatible with this module.info "fsevents@2.1.3" is an optional dependency and failed compatibility check. Excluding it from installation.[3/4] Linking dependencies...warning " > babel-core@7.0.0-bridge.0" has unmet peer dependency "@babel/core@^7.0.0-0".warning " > ant-design-vue@1.6.2" has unmet peer dependency "vue@>=2.6.0".warning "ant-design-vue > @ant-design/icons-vue@2.0.0" has unmet peer dependency "vue@>=2.5.0".warning " > cache-loader@4.1.0" has unmet peer dependency "webpack@^4.0.0".warning " > element-ui@2.13.2" has unmet peer dependency "vue@^2.5.17".warning " > viser-vue@2.4.8" has unmet peer dependency "vue@>=1".warning " > vue-svg-component-runtime@1.0.1" has unmet peer dependency "vue@>= 2.5.0".warning " > vuex@3.4.0" has unmet peer dependency "vue@^2.0.0".warning " > terser-webpack-plugin@1.4.3" has unmet peer dependency "webpack@^4.0.0".warning "@vue/cli-plugin-unit-jest > vue-jest@3.0.5" has unmet peer dependency "vue@^2.x".warning " > vue-loader@15.9.2" has unmet peer dependency "webpack@^3.0.0 || ^4.1.0 || ^5.0.0-0".warning " > @vue/test-utils@1.0.3" has unmet peer dependency "vue@2.x".warning " > babel-loader@7.1.5" has incorrect peer dependency "babel-core@6".warning " > babel-loader@7.1.5" has unmet peer dependency "webpack@2 || 3 || 4".warning " > eslint-loader@1.9.0" has incorrect peer dependency "eslint@>=1.6.0 <5.0.0".warning " > extract-text-webpack-plugin@3.0.2" has unmet peer dependency "webpack@^3.1.0".warning " > babel-loader@7.1.5" has incorrect peependency "webpack@^2.0.0 || ^3.0.0 || ^4.0.0".warning " > babel-loader@7.1.5" has unmet peer d" has unmet peer dependency "webpack@^2.0.0 || ^3.0.0 || ^4.0.0"warning " > eslint-loader@1.9.0" has incorrect pt peer dependency "webpack@1 || ^2 || ^2.1.0-beta || ^2.2.0-rc |warning " > extract-text-webpack-plugin@3.0.2" hpendency "webpack@^2.0.0 || ^3.0.0 || ^4.0.0".warning " > file-loader@1.1.11" has unmet peer dwarning " > friendly-errors-webpack-plugin@1.7.0warning " > html-webpack-plugin@2.30.1" has unmewarning " > less-loader@4.1.0" has unmet peer dewarning " > rollup-plugin-babel@3.0.7" has incorwarning " > rollup-plugin-vue@3.0.0" has unmet pwarning "rollup-plugin-vue > vue-template-validawarning " > sass-loader@7.3.1" has unmet peer dewarning " > uglifyjs-webpack-plugin@1.3.0" has uwarning " > vue-svg-icon-loader@2.1.1" has unmetwarning "vue-svg-icon-loader > vue-svg-componentwarning " > webpack-dev-server@2.11.5" has unmetwarning "webpack-dev-server > webpack-dev-middle[4/4] Building fresh packages...success Saved 1 new dependency.info Direct dependencies└─ ht-workflow-bpmn-modeler@0.2.53info All dependencies└─ ht-workflow-bpmn-modeler@0.2.53$ opencollective-postinstallThank you for using vue-antd-pro!If you rely on this package, please consider sup> https://opencollective.com/ant-design-pro-vue/Done in 12.40s.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主要还是使用yarn进行拉取和推送，npm publish时存在无法推送的问题！</p><p>如果执行yarn publish的时候，还是出现无法推送的情况，包括出现在yarn install时出现401错误的时候，尝试删除”.npmrc”文件中的<em>_authToken</em>所在行，重新执行“npm login”命令。</p><h2 id="番外篇：关于Dockerfile的重写"><a href="#番外篇：关于Dockerfile的重写" class="headerlink" title="番外篇：关于Dockerfile的重写"></a>番外篇：关于Dockerfile的重写</h2><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile"># 第一层面构建，打包前端代码#### 1. 指定node镜像版本FROM node:10.16.0 AS builder# 添加日期信息，如果需要更新所有缓存层，更新该处日期信息即可ENV REFRESH_DATE 2020-12-02_11:11# 2. 指定编译的工作空间WORKDIR /home/node/app# 3. 设置授权信息，本地开发机登录后获取私有镜像的authToken信息，放入.npmrc# NpmToken一行可以通过本地的机器使用npm login命令进行获取RUN touch /home/node/app/.npmrc && mkdir /home/node/app/node_global && mkdir /home/node/app/node_cache && echo "registry=http://192.168.123.202:8081/repository/npm-group/ \n\ //192.168.123.202:8081/repository/npm-group/:_authToken=NpmToken.a36e083f-d4b8-3e73-a98e-9dc1e5f61ccd \n\sass_binary_site=https://npm.taobao.org/mirrors/node-sass \n\always-auth=true \n\strict-ssl=false" >> /home/node/app/.npmrcRUN touch /home/node/app/.yarnrc && echo "registry \"http://192.168.123.202:8081/repository/npm-group/\" \n\email lisongyang@hoteamsoft.com \n\lastUpdateCheck 1606979147039 \n\\"registry=http://192.168.123.202:8081/repository/npm-group/\" true \n\sass_binary_site \"http://cdn.npm.taobao.org/dist/node-sass\" \n\username npm-user" >> /home/node/app/.yarnrc# 4. 安装打包需要的yarn工具RUN npm install -g yarn# 对yarn设置淘宝镜像#RUN yarn config set registry http://192.168.123.202:8081/repository/npm-group/#RUN yarn config set disturl https://npm.taobao.org/dist# 5. 添加package.jsonCOPY package.json /home/node/app/# 6. 安装依赖信息RUN yarn install --verbose# 7. 添加剩余代码到工作空间COPY . /home/node/app# 8. 编译代码RUN yarn run build# 第二层面构建#### 1.拉取自定义镜像名称FROM 192.168.123.202:5000/base_frontend:0.0.1# 2.将打包后的代码复制到运行位置COPY --from=builder /home/node/app/dist /var/www# 3.启动nginxENTRYPOINT ["nginx","-g","daemon off;"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其重点在于，.npmrc文件需要添加</p><pre><code>always-auth=true strict-ssl=false</code></pre><p>yarn在执行install命令的时候才能正常使用。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>12月份济南买车指南</title>
      <link href="/2020/12/27/12-yue-fen-ji-nan-mai-che-zhi-nan/"/>
      <url>/2020/12/27/12-yue-fen-ji-nan-mai-che-zhi-nan/</url>
      
        <content type="html"><![CDATA[<h1 id="12月份济南买车指南"><a href="#12月份济南买车指南" class="headerlink" title="12月份济南买车指南"></a>12月份济南买车指南</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今年因为疫情的原因，感觉到12月份的时候，车市进行了好几拨车展以及优惠活动，以此为契机做了个小型调查。主要参与的是我的两位同学，zh先生和lin先生。以12月初的舜耕车展加上经十东路、匡山一带、北园大街一带的4s店为主要范围进行调查。</p><h2 id="价格范围和备选"><a href="#价格范围和备选" class="headerlink" title="价格范围和备选"></a>价格范围和备选</h2><p>价格初步确定在10万到15万左右的A级或者B级车，这个价格是裸车价。落地价格范围选在12万到18万之间的车型，最主要的比较是轿车这个范围内的，混合有瑞虎8、观致7、荣威RX5 max这三款SUV。</p><p>我个人倾向于手动档，而zh先生倾向于自动挡中的AT版本而且倾向于买旅行车，lin先生就没啥意见，只要换挡好使就行。</p><p>然后备选车型确定如下：</p><ul><li><p>马自达</p><ul><li>次世代昂克塞拉， 1.5 手动质美</li><li>cx-4 2.0 中配，2.5低配两款</li><li>阿特兹2.5最低配</li></ul></li><li><p>领克</p><ul><li>03 1.5t高配， 2.0t低配两款车型</li></ul></li><li><p>本田</p><ul><li>雅阁 1.5t 最低配</li><li>思域 1.5t手动</li></ul></li><li><p>标致</p><ul><li>508L 1.6t 最低配</li></ul></li><li><p>现代</p><ul><li>索纳塔 1.5t最低配</li></ul></li><li><p>福特</p><ul><li>福克斯 2020款st-line</li><li>福克斯Active 2021款</li></ul></li><li><p>观致</p><ul><li>观致7 1.6t顶配</li></ul></li><li><p>奇瑞</p><ul><li>瑞虎8 1.6t 5座精英版</li></ul></li><li><p>雪弗兰</p><ul><li>迈锐宝xl 2.0t中配</li></ul></li><li><p>上汽名爵</p><ul><li>名爵6 1.5t手动豪华版</li></ul></li><li><p>上汽荣威</p><ul><li>荣威RX5 max 1.5t最低配手动版本</li></ul></li><li><p>斯柯达</p><ul><li>明锐1.5自动顶配</li><li>明锐旅行最低配</li></ul></li></ul><p>整个的车型列表就是这样。从我本人角度来说，10万左右选择昂克塞拉、名爵6、思域对比，只对比手动版本，15万左右选择508l和迈锐宝xl。在zh先生角度，领克03、福克斯Active、明锐旅行，这三款他比较喜欢。lin先生就跟着打酱油了（狗头）。</p><h2 id="车展和探店"><a href="#车展和探店" class="headerlink" title="车展和探店"></a>车展和探店</h2><p>这里没有很详细的报价单，好多是不允许拍照或者出具的，目前只能凭借记忆去写一下，然后是只能给出个大概的价格，仅供参考，毕竟这玩意一个时期一个价格。主要的价格信息包括，裸车价、落地价、贷款政策以及月供。</p><h3 id="1-领克"><a href="#1-领克" class="headerlink" title="1.领克"></a>1.领克</h3><p>我们在车展的时候了解的的是1.5t版本的高配，报价裸车是14.68万元，优惠1.1万元，贷款是两年6.6万元免息，月供大概在2800元左右。顺理成章的我们就去探店了，zh先生还是对3缸没信心，就直接看了2.0版本的低配，这台车报价是裸车优惠完成是138800，最高给到的优惠是17000元（这个优惠价格1.5t、2.0t相同），落地价格为164000元。免息政策同上，开走价格需要98000元，而且只能是现场订车，当日交定金。另外，以开走价格80000元计算，24期贷款月供需要3566元，需要1800元利息。</p><p>综合来说，领克2.0t的购置税较高，而且保险给定在了8500左右，贷款手续费和gps费用加起来差不多在9000，是收费最高的一个品牌，其它的品牌贷款手续费和gps费用大概在7000元左右。领克的宣传话术，就是什么玩意都往沃尔沃上靠（三大件+安全性）。zh先生带了驾照，有幸进行了1.5t高配版本的试驾，直观感受是车辆机械素质不错，配置齐全，变速箱反应灵敏，但是风噪不是很低，胎噪路噪非常重，后排空间不错，但是中间无头枕，过弯指向性优秀。</p><h3 id="2-本田"><a href="#2-本田" class="headerlink" title="2. 本田"></a>2. 本田</h3><p>车展上展出的是雅阁顶配版本的，我咨询了低配版本的价格，1.5t最低配，裸车报价16.99万元（优惠1万元后）。贷款政策为6万元18期免息，并未咨询落地价格，因为雅阁太贵了，而且1.5t感觉就是思域的1.5t刷了个一阶动力程序，真就是买发动机送车呗。探店时经十东路店聊到的是思域，这时候就开始不爽了，不管是1.5t最低配三厢手动，还是两厢版本次顶配，都没车，而且都告诉我订不到车，到车时间未知。在北园大街店是能订到车，目前1.5t手动三厢优惠1万元，贷款政策同上，落地价格在14.50左右。</p><p>综合来说，给人的感觉是本田很傲慢，对顾客态度有些差，但我没想到这是歧视我这样喜欢开手动档人士的开始！静态体验雅阁，感觉后排头部空间不足，</p><h3 id="3-标致"><a href="#3-标致" class="headerlink" title="3. 标致"></a>3. 标致</h3><p>车展看了508L，并未探店。508L静态体验比较好，没有那么反人类了，座椅舒适程度不错。咨询了1.6t最低配的价格，裸车价格在15.80万元，保险6000左右，带2980的金融服务费（无gps收费），24期免息可贷6.3万元，月供2500以内。未计算落地价格。</p><h3 id="4-现代"><a href="#4-现代" class="headerlink" title="4. 现代"></a>4. 现代</h3><p>车展主要看了新索纳塔，由于新车上市的原因，一分优惠没有，1.5t最低配16.18万元，36期免息可贷8.8万元，月供2500以内，3000元服务费，1980元gps费，公证费1120元。静态体验，乘坐一般，后排头部空间不足，中控感觉廉价了些。屌丝三宝之首得让给迈锐宝了。</p><h3 id="5-福特"><a href="#5-福特" class="headerlink" title="5. 福特"></a>5. 福特</h3><p>福特这边主要是受到福特性能部门的影响，心仪福克斯系列。车展主要看的是2020款福克斯st-line两厢，优惠后裸车价格在12万元，而且是清仓，还有两台红色两厢，三厢卖完，贷款是24期免息贷5万元，落地在14.5万元左右。探店时，看的是福克斯Active，旅行版本比两厢版本大不少，空间也好了很多，只是报价是一分钱不优惠，15.38万元，分期免息政策同上。</p><p>静态体验福克斯st-line，运动感营造较强，整车做工感觉还是差点，内饰缝隙不小，座椅有感觉，方向盘设计平底好评。</p><h3 id="6-观致"><a href="#6-观致" class="headerlink" title="6. 观致"></a>6. 观致</h3><p>车展主要看的是观致7 1.6t顶配版本，裸车13.68万元，贷款分期36期内任意金额，无免息，厂家贴息1.1万元，保险+购置税在1.5万元左右，落地14万元，提供所有颜色。</p><h3 id="7-奇瑞"><a href="#7-奇瑞" class="headerlink" title="7. 奇瑞"></a>7. 奇瑞</h3><p>车展主要看的是瑞虎8，咨询的是瑞虎8 1.6t精英版5座，裸车价格12.69万元，免息政策是24期5万元，手续费3000元，店内上保险4500元，上牌费120元。未探店。</p><p>静态体验了瑞虎8，确实空间非常好，做工感觉比观致7要好一些，但是科技程度不如观致7，毕竟和人顶配比，不太现实。</p><h3 id="8-雪弗兰"><a href="#8-雪弗兰" class="headerlink" title="8. 雪弗兰"></a>8. 雪弗兰</h3><p>车展上未看雪弗兰，直接进店看的车。展车是迈锐宝xl 2.0t低配版本，带机械式仪表的那种。中配报价15.69万元，贷款政策79000元，18期免息月供4388元。另一个是贷款10万元，39期，月供2791元，利息8867元。最终落地价格18.56万元。</p><p>静态体验，乘坐舒适，空间好，销售方面承诺有一个到明年4月份的保价协议，有降价或者其它4s有更低报价补差价的4倍赔偿。</p><h3 id="9-上汽名爵"><a href="#9-上汽名爵" class="headerlink" title="9. 上汽名爵"></a>9. 上汽名爵</h3><p>车展上未看，直接去4s看名爵6，配置为手动豪华版，裸车价格99800元。首付上路价格50451元，免息政策为24期贷款69860元，月供2910元，到车周期一周内，最快的三天。</p><p>名爵的保险是可以自己确定的，大概在4000左右，以出具的保单为准，而其它4s店都是一口价并且均在6000元以上，可能是因为车险改革的问题。</p><h3 id="10-上汽荣威"><a href="#10-上汽荣威" class="headerlink" title="10. 上汽荣威"></a>10. 上汽荣威</h3><p>直接探店，荣威RX5 max 1.5t最低配手动版本，直接告诉我不生产，也订不到车。</p><h3 id="11-斯柯达"><a href="#11-斯柯达" class="headerlink" title="11. 斯柯达"></a>11. 斯柯达</h3><p>直接探店，明锐只有自动挡版本了，手动挡一样订不到车，展车是1.5顶配的，咨询的是1.5自动最低配，报价9.38万元，后续未咨询。</p><h3 id="12-马自达"><a href="#12-马自达" class="headerlink" title="12. 马自达"></a>12. 马自达</h3><p>最后介绍马自达，马自达是看的最多的牌子，从阿特兹、cx-4到昂克塞拉。阿特兹2.5版本，裸车17.68万元，服务费4000+500，50%首付，24期无息。cx-4便宜7000元，金融政策相同。</p><p>而昂克塞拉，这个是我最早能找到订到车的4s店，1.5手动质美，裸车价格10.99万元，购置税+保险+4s服务费加上，第一次报价12.88万元，第二次阴差阳错搞出个4680的收费，而且落地价格一路奔着13.78万元去了。贷款厂家金融12期7万免息，低息24期7万，利息4000元。走银行可以贷9万元，36期，每年3000元利息，月供2800元，24期月供4000元。首付后开走的费用，从38880元，一路涨到50000元出头，这是最过山车的事儿。我还差点签了合同！</p><h2 id="其它信息"><a href="#其它信息" class="headerlink" title="其它信息"></a>其它信息</h2><ul><li><p>心得</p><ol><li>销售会咨询你，最早什么时候用车？也就是判断你会不会最近买，如果是，就使劲了；如果不是，就不太叼你。</li><li>最好在网上咨询，联系上某个销售后，再进店，否则你要的配置要么定不着，要么没啥优惠。</li></ol></li><li><p>购置税计算</p></li><li><p>保险报价问题</p></li></ul><p>大部分报价都在6000元以上，实际上自己单独买的话，只是三者100万+车损+交强在4000元左右，4s给上的基本上是全险，啥玩意玻璃险都有。但是我同事经历的事儿是，车玻璃被人砸了，但是玻璃险没给赔，因为找不到是谁干的。。。</p><ul><li>附加内容</li><li>临牌有效时间</li></ul><p>目前临牌有效时间在15天，可以打两次</p><ul><li><p>砍价策略</p></li><li><p>对比理由</p><ol><li>昂克塞拉、名爵6、思域三厢，这三个都有手动档</li><li>508L、迈锐宝xl 2.0t、阿特兹，这三个都是B级车</li><li>瑞虎8、观致7、cx-4，这三个都是suv</li></ol></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>并不是年底就真的便宜，车展时和探店时不一个优惠。</li><li>4s店各项收费太难受，尽量还是不要考虑贷款买车，否则会十分痛苦。（只针对我自己）</li><li>贵不一定好，需要多权衡，多看自己的用车场景。例如领克03，看着非常不错，但是试乘后隔音稀烂，还不如马自达3。</li><li>尽可能试驾，试驾后再对比！</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>从《演进式架构》思考我司云平台发展问题</title>
      <link href="/2020/11/20/cong-yan-jin-shi-jia-gou-si-kao-wo-si-yun-ping-tai-fa-zhan-wen-ti/"/>
      <url>/2020/11/20/cong-yan-jin-shi-jia-gou-si-kao-wo-si-yun-ping-tai-fa-zhan-wen-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="从《演进式架构》思考我司云平台发展问题"><a href="#从《演进式架构》思考我司云平台发展问题" class="headerlink" title="从《演进式架构》思考我司云平台发展问题"></a>从《演进式架构》思考我司云平台发展问题</h1><p>第一，肯定我们走微服务架构的路子，为了应对变化，分割为细小单元是正确的。但是，最大的问题在于组织机构建设不到位，人员培养不到位，逆康威定律影响的具体体现！</p><p>第二，支持微服务的方式，应该崇尚快速灵活、减少耦合，以领域驱动为核心进行服务构建。初期未能很好的做到这一点！</p><p>第三，肯定我们自身的演进能力，说明团队目前战斗力不弱。但是针对相悖的两个方向，例如SaaS模式和私有化部署，我们无法完全同时支持！而业务上需求量较少的情况下，还是使用当前微服务的一套进行支持，不符合客户的使用场景，造成资源浪费的情况。</p><p>总结一：架构是演进而来的，在不同时期支持不同的应用场景，并向前考虑一到两个阶段。针对私有化部署的，对于量不大的完全可以以单体模式进行部署，对于特殊需求的大客户，考虑使用微服务模式进行部署。</p><p>第四，在架构维度上，思考不足。可审计性、数据、安全、性能、合法性、伸缩性，这几点我们都略有涉及，却都没有很好的做到，需要我们认真反思。</p><p>第五，适应度函数，也就是对于各个架构维度的约束和衡量标准，目前我们没有明确的衡量标准，也没有明确的共识性质的约定。这个是做的不足的地方！</p><p>第六，对于自动化部分我们仅仅做到了DevOps流水线的构建，但是单元测试自动化的缺失、集成测试自动化的缺失，部署正式环境时未进行人工干预的审计。细节的缺失比较严重。</p><p>总结二：自动化是主流，应该构建从需求管理、变更提交、构建关联、自动测试、集成运行、人工合并、正式环境逐步替换的一条研发部署管理系统！</p><p>第七，当某个开发阶段完成时，要做好抛弃该阶段架构的准备。事实上我们已经这么做了！</p><p>第八，识别何处可变，坚持走决策可逆的道路，向既定方向前进并勇敢解决问题，构建错误隔离层（防腐层），构建可牺牲架构，应对外部开源依赖的变化（例如AntDesign万圣节事件），库与框架的按需更新，走持续交付的道路，服务内版本化并维持少数几个服务，最后以服务模板的方式集成公共代码或者功能业务的调用！</p><p>第九，控制权始终在我，以我为主。选择较为稳健的技术路线，始终以官方文档作为第一指导！</p><p>第十，一步迈出去，就不要回头看，不要去犹豫，而是向前走，遇山开山，遇水架桥。确定目标，就要在一定时间节点内坚持下去，在这个时间段内不允许动摇！</p><p>第十一，无论是需求还是技术路线，除前沿领域研究，严禁冒进行为，引入未经社区认证的第三方库需要严格审查代码！</p><p>第十二，团队建设，一职多能化，先注重T型人才的培养，才能谈Π型人才！</p><p>总结三：组织上到位，需求上合理，目标坚定，团队严肃活泼积极。以人为本！</p><p>最后，如果开发人员难以构建最简单的架构，那么转向更复杂的架构并不能解决问题，反而会事倍功半！</p><h1 id="未来开源方面的内容"><a href="#未来开源方面的内容" class="headerlink" title="未来开源方面的内容"></a>未来开源方面的内容</h1><ol><li>企业级网盘</li></ol><ul><li>权限、组织、人员、菜单</li><li>工作流审批</li><li>文件历史</li><li>文件树、文件夹结构和关系</li><li>文件访问权限</li><li>规则引擎</li></ul><ol start="2"><li>研发管理</li></ol><ul><li>DevOps</li><li>项目管理、关联gitlab或者分支提交，关联构建功能链条</li><li>最终部署运维层面、服务编排、监控</li><li>微服务化</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于微服务中Skywalking监控信息和ELK统一日志管理体系的对接</title>
      <link href="/2020/11/07/guan-yu-wei-fu-wu-zhong-skywalking-jian-kong-xin-xi-he-elk-tong-yi-ri-zhi-guan-li-ti-xi-de-dui-jie/"/>
      <url>/2020/11/07/guan-yu-wei-fu-wu-zhong-skywalking-jian-kong-xin-xi-he-elk-tong-yi-ri-zhi-guan-li-ti-xi-de-dui-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="当前场景"><a href="#当前场景" class="headerlink" title="当前场景"></a>当前场景</h2><p>以Skywalking+ELK体系构建当前针对微服务的“链路+日志监控”的整个监控手段，主要监控微服务的运行层面的问题。目前在公司的应用场景下存在的严重问题是，目前无法将二者进行联动，也就是说，在skywalking中看到了请求异常，无法马上转到ELK中查看对应链路上的微服务的所有日志，导致排查问题效率不足，影响问题解决。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>目前考虑的是，从Skywalking的前端页面进行抓取，获取存在问题的服务链路，获取各个微服务名称，在skywalking和ELK之外，再写一个微服务，这个微服务承载从Skywalking和ELK之间的连接，称之为MicroBridge。然后从Skywalking获取的服务信息，传入MicroBridge，最后从MicroBridge根据对应的时间信息和微服务名称，从ES中查询出对应的日志信息，进行展示。</p><h2 id="也许是更好的方案"><a href="#也许是更好的方案" class="headerlink" title="也许是更好的方案"></a>也许是更好的方案</h2><p><a href="https://cloud.tencent.com/developer/article/1696699" target="_blank" rel="noopener">参考文章</a></p><p>直接把traceId写入ES中，通过traceId进行日志搜索，更加高效。</p><p>但是这样对于业务微服务的侵入较大，是否对性能造成影响还有待考虑，毕竟频繁打印日志。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>jenkins批量复制job到新的view中的执行脚本</title>
      <link href="/2020/11/02/jenkins-pi-liang-fu-zhi-job-dao-xin-de-view-zhong-de-zhi-xing-jiao-ben/"/>
      <url>/2020/11/02/jenkins-pi-liang-fu-zhi-job-dao-xin-de-view-zhong-de-zhi-xing-jiao-ben/</url>
      
        <content type="html"><![CDATA[<h1 id="jenkins批量复制job到新的view中的groovy脚本"><a href="#jenkins批量复制job到新的view中的groovy脚本" class="headerlink" title="jenkins批量复制job到新的view中的groovy脚本"></a>jenkins批量复制job到新的view中的groovy脚本</h1><pre class="line-numbers language-groovy"><code class="language-groovy"><span class="token keyword">import</span> hudson<span class="token operator">.</span>model<span class="token operator">.</span><span class="token operator">*</span><span class="token keyword">def</span> str_view <span class="token operator">=</span> <span class="token string">"testcloud-develop"</span><span class="token keyword">def</span> str_new_view <span class="token operator">=</span> <span class="token string">"testcloud-212-develop-single"</span><span class="token keyword">def</span> str_search <span class="token operator">=</span> <span class="token string">"testcloud-"</span><span class="token keyword">def</span> str_replace <span class="token operator">=</span> <span class="token string">"testcloud-212-"</span><span class="token keyword">def</span> view <span class="token operator">=</span> Hudson<span class="token operator">.</span>instance<span class="token operator">.</span><span class="token function">getView</span><span class="token punctuation">(</span>str_view<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//copy all projects of a view</span><span class="token keyword">for</span><span class="token punctuation">(</span>item <span class="token keyword">in</span> view<span class="token operator">.</span><span class="token function">getItems</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>item<span class="token operator">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">.</span><span class="token function">contains</span><span class="token punctuation">(</span>str_search<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">// 说明文字，表示跳过未匹配到的job，可加可不加</span>      <span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"but $item.name ignore "</span><span class="token punctuation">)</span>      <span class="token keyword">continue</span>  <span class="token punctuation">}</span>  <span class="token comment" spellcheck="true">//create the new project name</span>  newName <span class="token operator">=</span> item<span class="token operator">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">.</span><span class="token function">replace</span><span class="token punctuation">(</span>str_search<span class="token punctuation">,</span> str_replace<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">// copy the job, disable and save it</span>  <span class="token keyword">def</span> job   <span class="token keyword">try</span> <span class="token punctuation">{</span>      job <span class="token operator">=</span> Hudson<span class="token operator">.</span>instance<span class="token operator">.</span><span class="token function">copy</span><span class="token punctuation">(</span>item<span class="token punctuation">,</span> newName<span class="token punctuation">)</span>  <span class="token punctuation">}</span><span class="token keyword">catch</span><span class="token punctuation">(</span>IllegalArgumentException e<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">// 重名的处理</span>      <span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"$newName job is exists"</span><span class="token punctuation">)</span>      <span class="token keyword">continue</span>  <span class="token punctuation">}</span> <span class="token keyword">catch</span><span class="token punctuation">(</span>Exception e<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">// 获取异常的处理</span>      <span class="token function">println</span><span class="token punctuation">(</span>e<span class="token operator">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token keyword">continue</span>  <span class="token punctuation">}</span>  job<span class="token operator">.</span>disabled <span class="token operator">=</span> <span class="token boolean">true</span>  job<span class="token operator">.</span><span class="token function">save</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">// update the workspace to avoid having two projects point to the same location</span>  <span class="token comment" spellcheck="true">//AbstractProject project = job</span>  <span class="token comment" spellcheck="true">//def new_workspace = project.getCustomWorkspace().replace(str_search, str_replace)</span>  <span class="token comment" spellcheck="true">//project.setCustomWorkspace(new_workspace)</span>  <span class="token comment" spellcheck="true">//project.save()</span>  Hudson<span class="token operator">.</span>instance<span class="token operator">.</span><span class="token function">getView</span><span class="token punctuation">(</span>str_new_view<span class="token punctuation">)</span><span class="token operator">.</span><span class="token function">add</span><span class="token punctuation">(</span>job<span class="token punctuation">)</span>  <span class="token function">println</span><span class="token punctuation">(</span><span class="token string">" $item.name copied as $newName"</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意，这里在使用之前，要先添加对应的View，添加时操作如下：</p><p><img src="jenkins%E6%B7%BB%E5%8A%A0View%E7%9A%84%E9%97%AE%E9%A2%98.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch索引的生命周期管理</title>
      <link href="/2020/10/14/elasticsearch-suo-yin-de-sheng-ming-zhou-qi-guan-li/"/>
      <url>/2020/10/14/elasticsearch-suo-yin-de-sheng-ming-zhou-qi-guan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="ElasticSearch索引的生命周期管理"><a href="#ElasticSearch索引的生命周期管理" class="headerlink" title="ElasticSearch索引的生命周期管理"></a>ElasticSearch索引的生命周期管理</h1><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>ElasticSearch 6.6.0版本后新增索引的生命周期管理。</p><p>参考易企秀的实现：</p><p>引入索引生命周期管理的一个最重要的目的就是对大量时序数据在es读写操作的性能优化，比如易企秀通过spark streaming读取Kafka中的日志实时写入es，这些日志高峰期每天10亿+，每分钟接近100w，在这么大数据量上进行操作是一件很麻烦的事，那么我们希望es能够对单分片超过50g或者30天前的索引进行归档，并能够自动删除90天前的索引，那么这个场景可以通过ILM进行策略配置来实现。</p><p>那么实际上我们可以参考易企秀的内容来制定我们自身的索引规则，尤其是目前在服务器资源紧张的情况下，而对于日志信息的记录要求保存的时间不高，因此借助索引的生命周期管理来优化ES的运行。</p><h2 id="生命周期简介"><a href="#生命周期简介" class="headerlink" title="生命周期简介"></a>生命周期简介</h2><p>ES索引生命周期管理分为4个阶段：hot、warm、cold、delete，其中hot主要负责对索引进行rollover操作，warm、cold、delete分别对rollover后的数据进一步处理（前提是配置了hot）。</p><table><thead><tr><th>phases</th><th>desc</th></tr></thead><tbody><tr><td>hot</td><td>主要处理时序数据的实时写入</td></tr><tr><td>warm</td><td>可以用来查询，但是不再写入</td></tr><tr><td>cold</td><td>索引不再有更新操作，并且查询也会很少</td></tr><tr><td>delete</td><td>数据将被删除</td></tr></tbody></table><p>需要注意的是上述4个阶段不是必须的，也就是说不是一定要从hot-&gt;warm-&gt;cold-&gt;delete，可以直接从hot-&gt;cold。</p><h2 id="设置规则"><a href="#设置规则" class="headerlink" title="设置规则"></a>设置规则</h2><p>参考易企秀的策略设置，考虑我们当前服务器负载以及硬盘空间，分别对目前系统中的两套ES进行规则设置，如下：</p><ul><li>ELK所在ES： 4G 3天 7天</li><li>Skywalking所在ES： 10G 7天 14天</li></ul><p>解释一下上述配置的含义，针对ELK所在的ES，对单分片超过<em>4G</em>或者<em>3天前</em>的索引进行归档，并能够自动删除<strong>7天前</strong>的索引；针对Skywalking所在的ES，对单分片超过<em>10G</em>或者<em>7天前</em>的索引进行归档，并能够自动删除<strong>14天前</strong>的索引。</p><h2 id="规则编写与落地"><a href="#规则编写与落地" class="headerlink" title="规则编写与落地"></a>规则编写与落地</h2><p>这里所有的操作，均借助<a href="https://github.com/lmenezes/cerebro" target="_blank" rel="noopener">Cerebro</a>对ES进行操作，以Skywalking所在ES为例子，进行设置。</p><p>首先查看一下系统中默认使用的规则，如下图：</p><p><img src="%E7%B3%BB%E7%BB%9F%E5%86%85%E9%BB%98%E8%AE%A4%E7%9A%84%E7%AD%96%E7%95%A5%E4%BF%A1%E6%81%AF.png" alt></p><p>然后看一下目前的索引状态，如下图：</p><p><img src="%E5%BD%93%E5%89%8D%E7%B4%A2%E5%BC%95%E6%9C%AA%E8%A2%AB%E7%AE%A1%E7%90%86.png" alt></p><p>这样目前所有的<strong>elastic-skywalking_</strong>开头的索引信息中<em>managed</em>状态均为false。那么久需要我们进行设置。</p><h3 id="1-定义策略"><a href="#1-定义策略" class="headerlink" title="1. 定义策略"></a>1. 定义策略</h3><p>针对Skywalking所在ES设置策略，将该策略应用到<strong>elastic-skywalking_</strong>开头的索引信息中。设置策略的json如下：</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>      <span class="token property">"policy"</span><span class="token operator">:</span> <span class="token punctuation">{</span>                           <span class="token property">"phases"</span><span class="token operator">:</span> <span class="token punctuation">{</span>      <span class="token property">"hot"</span><span class="token operator">:</span> <span class="token punctuation">{</span>                              <span class="token property">"actions"</span><span class="token operator">:</span> <span class="token punctuation">{</span>          <span class="token property">"rollover"</span><span class="token operator">:</span> <span class="token punctuation">{</span>                         <span class="token property">"max_size"</span><span class="token operator">:</span> <span class="token string">"10GB"</span><span class="token punctuation">,</span>            <span class="token property">"max_age"</span><span class="token operator">:</span> <span class="token string">"7d"</span>          <span class="token punctuation">}</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span><span class="token punctuation">,</span>      <span class="token property">"delete"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"min_age"</span><span class="token operator">:</span> <span class="token string">"14d"</span><span class="token punctuation">,</span>                   <span class="token property">"actions"</span><span class="token operator">:</span> <span class="token punctuation">{</span>          <span class="token property">"delete"</span><span class="token operator">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>                      <span class="token punctuation">}</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：rollover中配置归档策略，目前支持3种策略，分别是max_docs、max_size、max_age（请关注、具体后续内容介绍），其中的任何一个条件满足时都会触发索引的归档操作，并删除归档90天后的索引文件（其中delete属于phrase）。</p><p>然后操作如下：</p><p><img src="%E7%B4%A2%E5%BC%95%E4%BF%9D%E7%95%99%E8%A7%84%E5%88%99%E8%AE%BE%E7%BD%AE.png" alt></p><p>最后查看一下我们设置的规则，如下：</p><p><img src="%E7%B4%A2%E5%BC%95%E8%A7%84%E5%88%99%E8%AE%BE%E7%BD%AE%E7%BB%93%E6%9E%9C.png" alt></p><p>这样索引规则就设置完成了。</p><h3 id="2-使用策略"><a href="#2-使用策略" class="headerlink" title="2. 使用策略"></a>2. 使用策略</h3><p>在索引库上使用策略的方式有很多种，但我们的需求是对满足所有rollover条件的规则统统适用，所以需要配置一个索引模板，并指定对应的规则。策略应用的json如下：</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>  <span class="token property">"index_patterns"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"elastic-skywalking_*"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   <span class="token property">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{</span>    <span class="token property">"number_of_shards"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token property">"number_of_replicas"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token property">"index.lifecycle.name"</span><span class="token operator">:</span> <span class="token string">"skywalking-policy"</span><span class="token punctuation">,</span>          <span class="token property">"index.lifecycle.rollover_alias"</span><span class="token operator">:</span> <span class="token string">"datastream"</span>      <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>设置策略应用的结果如下：</p><p><img src="%E7%B4%A2%E5%BC%95%E7%AD%96%E7%95%A5%E5%BA%94%E7%94%A8.png" alt></p><p>其中 index.lifecycle.name 指定我们索引模板使用哪个策略进行管理，index.lifecycle.rollover_alias 配置该系列索引的别名，通过别名datastream可对elastic-skywalking_*相关的索引信息进行读写操作。</p><h3 id="3-别名添加"><a href="#3-别名添加" class="headerlink" title="3. 别名添加"></a>3. 别名添加</h3><p>在上面的步骤中添加了别名信息，但是查看所有的索引信息，发现并没有这个别名，如下：</p><p><img src="%E5%88%AB%E5%90%8D%E4%BF%A1%E6%81%AF%E6%9F%A5%E8%AF%A2.png" alt></p><p>这也就导致了设置的索引策略并不能生效，那么需要对各个索引信息添加别名，添加的json信息如下：</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>    <span class="token property">"actions"</span><span class="token operator">:</span> <span class="token punctuation">[</span>    <span class="token punctuation">{</span>      <span class="token property">"add"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"index"</span><span class="token operator">:</span> <span class="token string">"elastic-skywalking_*"</span><span class="token punctuation">,</span>        <span class="token property">"alias"</span><span class="token operator">:</span> <span class="token string">"datastream"</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">]</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>添加的结果如下：</p><p><img src="%E5%88%AB%E5%90%8D%E4%BF%A1%E6%81%AF%E6%B7%BB%E5%8A%A0.png" alt></p><p>添加完成后查看别名信息如下：</p><p><img src="%E5%88%AB%E5%90%8D%E4%BF%A1%E6%81%AF%E6%9F%A5%E7%9C%8B.png" alt></p><p><img src="%E5%88%AB%E5%90%8D%E4%BF%A1%E6%81%AF%E6%9F%A5%E7%9C%8B3.png" alt></p><p><img src="%E5%88%AB%E5%90%8D%E4%BF%A1%E6%81%AF%E6%9F%A5%E7%9C%8B2.png" alt></p><p>这样就完成了所有操作。可以看到最后一张图中在<em>phase_execution</em>下，出现了policy设置的相关信息。但是，设置完成后，只对于新生成的索引进行了管理，引入了我们生成的policy，并未对已有的索引进行管理，这也是这个方案的一个问题吧！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>根据Skywalking写入ES的特点，会在每天生成一个记录的索引信息，保存apm监控的链路数据。一天一滚动。目前临时不对索引的别名设置<strong>is_write_index = true</strong>这个选项(详细见<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-aliases.html" target="_blank" rel="noopener">参考连接</a>)。</p><p>设置完成后，需要在一个月内监控一下这个系统的运行情况！</p><!-- 由于别名操作索引时，同一时刻只能有一个索引被写，所以还需要设置 is_write_index = true 。curl -X PUT "localhost:9200/datastream-000001" -H 'Content-Type: application/json' -d'{  "aliases": {    "datastream": {      "is_write_index": true    }  }}' --> <h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><p><a href="https://www.jianshu.com/p/358fde8d8e27" target="_blank" rel="noopener">https://www.jianshu.com/p/358fde8d8e27</a></p></li><li><p><a href="https://www.jianshu.com/p/94e37a5b0878" target="_blank" rel="noopener">https://www.jianshu.com/p/94e37a5b0878</a></p></li><li><p><a href="https://www.cnblogs.com/Neeo/articles/10897280.html" target="_blank" rel="noopener">https://www.cnblogs.com/Neeo/articles/10897280.html</a></p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DDD领域驱动设计的学习</title>
      <link href="/2020/10/03/ddd-ling-yu-qu-dong-she-ji-de-xue-xi/"/>
      <url>/2020/10/03/ddd-ling-yu-qu-dong-she-ji-de-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="解决的实际问题"><a href="#解决的实际问题" class="headerlink" title="解决的实际问题"></a>解决的实际问题</h2><p>微服务怎么拆分才算合理，拆分到多小才是微服务？</p><h2 id="为什么是DDD"><a href="#为什么是DDD" class="headerlink" title="为什么是DDD"></a>为什么是DDD</h2><p>微服务的粒度应 该多大呀？微服务到底应该如何拆分和设计呢？微服务的边界应该在哪里？</p><p>问题：微服务拆分困境产生的根本原因就是不知道业务或者微服务的边界到底在什么地方</p><p>通过领域驱动设计方法定义领域模型，从而确定业务和应用边界，保证业务模型与代码模型的一致性。</p><p>方法论：两层设计、三步划定、两条边界</p><p>其核心思想就是将问 题域逐步分解，降低业务理解和系统实现的复杂度。</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="核心域、通用域和支撑域"><a href="#核心域、通用域和支撑域" class="headerlink" title="核心域、通用域和支撑域"></a>核心域、通用域和支撑域</h3><ul><li><p>领域：这个边界内要解决的业务问题域。领域可以进一步划分为子领域。我们把划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或更小的业务范围。</p></li><li><p>核心域、通用域和支撑域</p><ul><li><p>决定产品和公司核心竞争力的子域是核心域，它是业务成功的主要因素和公司的核心竞争力。</p></li><li><p>没有太多个性化的诉求，同时被多个子域使用的通用功能子域是通用域。</p></li><li><p>还有一种功能子域是必需的，但既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域， 它就是支撑域。</p></li></ul></li></ul><h3 id="限界上下文"><a href="#限界上下文" class="headerlink" title="限界上下文"></a>限界上下文</h3><ul><li>通用语言：在事件风暴过程中，通过团队交流达成共识的，能够简单、 清晰、准确描述业务涵义和规则的语言就是通用语言。通用语言包含术语和用例场景，并且能够直接反映在代码中。设计过程中我们可以用一些表格， 来记录事件风暴和微服务设计过程中产生的领域对象及其属性。</li></ul><p>通用语言定义上下文含义，限界上下文则定义领域边界</p><p>DDD 分析和设计过程中的每一个环节都需要保证限界上下文内 术语的统一，在代码模型设计的时侯就要建立领域对象和代码对象的一一映射，从而保证业 务模型和代码模型的一致，实现<strong>业务语言与代码语言的统一</strong>。</p><ul><li>限界就是领域的边界，而上下文则是 语义环境。</li></ul><p>领域边界就是通过限界上下文来定义的。</p><p>我们将限界上下文内的领域模型映射到微服务，就完成了从问题域到软件的解决方案。</p><h3 id="实体和值对象"><a href="#实体和值对象" class="headerlink" title="实体和值对象"></a>实体和值对象</h3><ul><li>实体：在 DDD 中有这样一类对象，它们拥有唯一标识符，且标识符在历经各种状态变更后仍能保 持一致。对这些对象而言，重要的不是其属性，而是其延续性和标识，对象的延续性和标识 会跨越甚至超出软件的生命周期。我们把这样的对象称为实体。</li></ul><p>业务形态、代码形态、运行形态、数据库形态</p><ul><li>实体和值对象是组成领域模型的基础单元。</li></ul><p>DDD 提倡从领域模型设计出发，而不是先设计数据模型。</p><ul><li><p>值对象：通过对象属性值来识别的对 象，它将多个相关属性组合为一个概念整体。值对象本质上就是一个集。</p></li><li><p>数据库设计原则：在领域建模时，我们可以将部分对象设计为值对象，保留对象的业务涵义，同时又减少了实体的数量；在数据建模时，我们可以将值对象嵌入实体，减少实体表的数量，简化数据库设计。</p></li></ul><p>是否要设计成值对象，你要看这个对象是否后续还会来回修改，会不会有生命周期。如 果不可修改，并且以后也不会专门针对它进行查询或者统计，你就可以把它设计成值对象。DDD强调领域模型而不是数据模型，所以在设计的时候不建议把数据模型放在很优先的位置，但 是如果你的数据需要经常修改，还是把它设计为实体吧。很多值对象来源于上一个业务流程或者外部第三方的数据，它在上游或者第三方是一个关键实体，甚至是聚合根。但在下游或者其他微服务内它是不可以修改的，要修改也只能从上游或者第三方修改后做整体替换，它只是一个值， 这类领域对象你是可以设计为值对象的。</p><h3 id="聚合和聚合根"><a href="#聚合和聚合根" class="headerlink" title="聚合和聚合根"></a>聚合和聚合根</h3><ul><li>聚合：领域模型内的实体和值对象就好比个体，而能让实体和值对象协同工作的组织就是聚合，它用来确保这些领域对象在实现共同的业务逻辑时，能保证数据的一致性。</li></ul><p>聚合包含聚合根和上下文边界。实体和值对象依附于聚合。</p><p>跨多个实体的业务逻辑通过领域服务来实现，跨多个聚合的业务逻辑通过应用服务来实现。</p><ul><li>聚合根：如果把聚合比作组织，那聚合根就是这个组织的负责人。聚合根也称为根实体，它不仅是实体，还是聚合的管理者。</li></ul><p>聚合根本质是实体，拥有自身的业务逻辑；它作为聚合的管理者，在聚合内部负责协调实体和值对象按照固定的业务规则协同完成共同的业务逻辑；在聚合之间，它还是聚合对外的接口人，以聚合根ID关联的方式接受外部任务和请求，在上下文内实现聚合之间的业务协同。</p><ul><li>如何设计：1. 在一致性边界内建模真正的不变条件；2. 设计小聚合；3. 通过唯一标识引用其它聚合；4. 在边界之外使用最终一致性；5. 通过应用层实现跨聚合的服务调用</li></ul><h3 id="一阶段总结"><a href="#一阶段总结" class="headerlink" title="一阶段总结"></a>一阶段总结</h3><ul><li>聚合的特点：高内聚、低耦合，它是领域模型中最底层的边界，可以作为拆分微服务的最小 单位，但我不建议你对微服务过度拆分。但在对性能有极致要求的场景中，聚合可以独立作 为一个微服务，以满足版本的高频发布和极致的弹性伸缩能力。</li></ul><p>一个微服务可以包含多个聚合，聚合之间的边界是微服务内天然的逻辑边界。有了这个逻辑 边界，在微服务架构演进时就可以以聚合为单位进行拆分和组合了，微服务的架构演进也就 不再是一件难事了。</p><ul><li><p>聚合根的特点：聚合根是实体，有实体的特点，具有全局唯一标识，有独立的生命周期。一 个聚合只有一个聚合根，聚合根在聚合内对实体和值对象采用直接对象引用的方式进行组织 和协调，聚合根与聚合根之间通过 ID 关联的方式实现聚合之间的协同。</p></li><li><p>实体的特点：有 ID 标识，通过 ID 判断相等性，ID 在聚合内唯一即可。状态可变，它依附 于聚合根，其生命周期由聚合根管理。实体一般会持久化，但与数据库持久化对象不一定是 一对一的关系。实体可以引用聚合内的聚合根、实体和值对象。</p></li><li><p>值对象的特点：无 ID，不可变，无生命周期，用完即扔。值对象之间通过属性值判断相等 性。它的核心本质是值，是一组概念完整的属性组成的集合，用于描述实体的状态和特征。 值对象尽量只引用值对象。</p></li><li><p>贫血模型：指使用的领域对象中只有setter和getter方法，所有的业务逻辑都不包含在领域对象中而是放在业务逻辑层。</p></li><li><p>充血模型将大多数业务逻辑放在领域实体中实现，实体本身包含了属性和它的业务行为，它在领域模型中就是一个具有业务行为和逻辑的基本业务单元。</p></li></ul><h3 id="领域事件"><a href="#领域事件" class="headerlink" title="领域事件"></a>领域事件</h3><ul><li>领域事件：这种事件发生后通常会导致进一步的业务操作，在DDD中这种事件被称为领域事件。强调最终一致性。</li></ul><p>领域事件驱动设计可以切断领域模型之间的强依赖关系，事件发布完成后，发布方不必关心 后续订阅方事件处理是否成功，这样可以实现领域模型的解耦，维护领域模型的独立性和数 据的一致性。在领域模型映射到微服务系统架构时，领域事件可以解耦微服务，微服务之间 的数据不必要求强一致性，而是基于事件的最终一致性。</p><p>微服务内的领域事件：一对一的，使用观察者模式；一对多的，一个事件如果同时更新多个聚合，按照DDD“一次事务只 更新一个聚合”的原则，考虑是否引入事件总线。</p><p>微服务间的领域事件：要总体考虑事件构建、发布和订阅、事件数据持久化、消息中间件，甚至事件数据持久化时还可能需要考虑引入分布式事务机制等。</p><p><strong>注意：</strong>分布式事务机制 会影响系统性能，增加微服务之间的耦合，所以我们还是要尽量避免使用分布式事务。</p><ul><li>领域事件总体架构：事件构建和发布、事件数据持久化、事件总线、消息中间件、事件 接收和处理等。</li></ul><p><strong>问题：</strong>如果采用了主流的消息队列（如rabbitmq，kafka，rocketmq），是否领域事件还需要持久化？</p><p><em>回复：</em>虽然这些消息队列自身有持久化的功能，但是中间过程，或者在订阅到数 据后，在处理之前出问题，需要进行数据对账，这样就没法找到发布时和处理后的数据版本。关键的业务数据我建议还是落库比较好。</p><h3 id="DDD分层架构"><a href="#DDD分层架构" class="headerlink" title="DDD分层架构"></a>DDD分层架构</h3><p>基础层、用户接口层、应用层、领域层</p><p><img src="%E5%88%86%E5%B1%82%E6%9E%B6%E6%9E%84%E7%BB%93%E6%9E%84.png" alt="补充图片"></p><p>DDD 分层架构有一个重要的原则：每层只能与位于其下方的层发生耦合。</p><p>微服务架构演进：</p><p>领域模型中对象的层次从内到外依次是：值对象、实体、聚 合和限界上下文。</p><p>微服务内服务的演进：</p><p>三层架构向DDD分层架构演进：</p><p><img src="%E4%B8%89%E5%B1%82%E6%9E%B6%E6%9E%84%E5%90%91DDD%E5%88%86%E5%B1%82%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B.png" alt="补充图片"></p><p>DDD 分层架构、整洁架构、六边形架构都是以领域模型 为核心，实行分层架构，内部核心业务逻辑与外部应用、资源隔离并解耦。</p><h3 id="中台和DDD的关系"><a href="#中台和DDD的关系" class="headerlink" title="中台和DDD的关系"></a>中台和DDD的关系</h3><p>当你去做领域划分的时候，请务必结合企业战略，这恰恰也体现了 DDD 领域建模的重要性。</p><p>中台：企业级的能力复用</p><h3 id="DDD实践"><a href="#DDD实践" class="headerlink" title="DDD实践"></a>DDD实践</h3><ul><li>如何构建中台业务模型？</li></ul><ol><li><p>自顶向下：适用于全新的应用系统建设，或旧系统推倒重建的情况。</p></li><li><p>自底向上：适用于遗留系统业务模型的演进式重构</p></li></ol><p>步骤：锁定系统所在业务域，构建领域模型；对齐业务域，构建中台业务模型；中台归类，根据领域模型设计微服务。</p><p>中台业务建模时，既要关注领域模型的完备性，也要关注不同渠道敏捷响应市场的要求。</p><p>构建多业务域的中台业务模型的过程，就是找出同一业务域内所有同类业务的领域模型，对 比分析域内领域模型和聚合的差异和共同点，打破原有的模型，完成新的中台业务模型重组或归并的过程。</p><p>核心要点：分域建模型，找准基准域，划定上下文，聚合重归类。</p><p>划分完成后，针对领域对象的调整：部分领域对象可能会根据新的业务要求，从原来的聚合中分离，重组到其它 聚合。新领域模型的领域对象，比如实体、领域服务等，在重组后可能还会根据新的业务场景和需求进行代码重构。</p><p>业务边界即微服务边界！</p><ul><li>如何用事件风暴构建领域模型？</li></ul><p>采用 DDD 方法建立的领域模型，可以清晰地划分微服务的逻辑边界和物理边界。</p><p>事件风暴正是 DDD 战略设计中经常使用的一种方法，它可以快速分析和分解复杂的业务领域，完成领域建模。</p><ol><li><p>准备部分：参与者、材料、场地、关注点</p></li><li><p>具体实施：</p><ul><li><p>产品愿景</p><p>参与角色：领域专家、业务需求方、产品经理、项目经理和开发经理。用户中台到底能够做什么？ 它的业务范围、目标用户、核心价值和愿景，与其它同类产品的差异和优势在哪里？<br>思考点：用户中台到底能够做什么？业务范围、目标用户、核心价值和愿景，与其它同类产品的差异和优势在哪里？</p></li><li><p>业务场景分析</p><p>参与角色：领域专家、产品经理、需求分析人员、架构师、项目经理、开发经理 和测试经理。<br>从用户视角出发的，根据业务流程或用户旅程，采用用例和场景分析，探索领域 中的典型场景，找出领域事件、实体和命令等领域对象，支撑领域建模。事件风暴参与者要 尽可能地遍历所有业务细节，充分发表意见，不要遗漏业务要点。<br>场景分析时会产生很多的命令和领域事件。</p></li><li><p>领域建模<br>参与角色：领域专家、产品经理、需求分析人员、架构师、项目经理、开发经理 和测试经理。<br>从命令和事件中提取产生这些行为的实体；根据聚合根的管理性质从实体中找出聚合根；划定限界上下文，根据上下文语义将聚合归类</p></li><li><p>微服务拆分与设计<br>参与的角色：领域专家、产品经理、需求分析人员、架构师、项目经理、开发经理和测试经理<br>在微服务拆分与设计时，我们不能简单地将领域模型作为拆分微服务的唯一标准，它只能作为微服务拆分的一个重要依据。</p></li></ul></li></ol><ul><li>代码模型</li></ul><p><img src="%E8%A1%A5%E5%85%85%EF%BC%9A%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BB%A3%E7%A0%81%E7%BB%84%E7%BB%87%E5%9B%BE.png" alt></p><p>微服务落地时首先要确定的就是微服务的代码结构</p><p>注意：聚合之间的代码边界一定要清晰；一定要有代码分层的概念。</p><p>DDD 强调先构建领域模型然后设计微服务，以保证领域模型和微服务的一体性，因此我们不能脱离领域模型来谈微服务的设计和落地。但在构建领域模型时，我们往往是站在业务视角的，并且有些领域对象还带着业务语言。我们还需要将领域模型作为微服务设计的输入，对领域对象进行设计和转换，让领域对象与代码对象建立映射关系。</p><p>领域层的领域对象：设计实体、找出聚合根、设计值对象、设计领域事件、设计领域服务、设计仓储。</p><p>应用层的领域对象：实体方法的封装、领域服务的组合和封装、应用服务的组合和编排。应用服务和事件的发布以及订阅。</p><p>建立领域对象与微服务代码对象的映射。</p><ul><li>边界</li></ul><p>微服务架构设计中的各种边界在架构演进中的作用。</p><p>逻辑边界：微服务内聚合之间的边界是逻辑边界。它是一个虚拟的边界，强调业务的内聚， 可根据需要变成物理边界，也就是说聚合也可以独立为微服务。</p><p>物理边界：微服务之间的边界是物理边界。它强调微服务部署和运行的隔离，关注微服务的 服务调用、容错和运行等。</p><p>代码边界：不同层或者聚合之间代码目录的边界是代码边界。它强调的是代码之间的隔离， 方便架构演进时代码的重组。</p><p>通过以上边界，我们可以让业务能力高内聚、代码松耦合，且清晰的边界，可以快速实现微 服务代码的拆分和组合，轻松实现微服务架构演进。但有一点一定要格外注意，边界清晰的 微服务，不是大单体向小单体的演进。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker问题汇总</title>
      <link href="/2020/10/02/docker-wen-ti-hui-zong/"/>
      <url>/2020/10/02/docker-wen-ti-hui-zong/</url>
      
        <content type="html"><![CDATA[<h2 id="关于Docker进程错误的问题解决"><a href="#关于Docker进程错误的问题解决" class="headerlink" title="关于Docker进程错误的问题解决"></a>关于Docker进程错误的问题解决</h2><p>错误日志如下：</p><pre><code>docker: Error response from daemon: driver failed programming external connectivity on endpoint server-register-k8s (c2d9369369d37d9ce02df690ae39787903fa3fc9b470ad1763c072575f8a8088):  (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 19013 -j DNAT --to-destination 172.17.0.2:19013 ! -i docker0: iptables: No chain/target/match by that name.18:37:04  (exit status 1)).</code></pre><p>解决方案：</p><pre><code>$ sudo iptables -F &amp;&amp; sudo iptables -t nat -F &amp;&amp; sudo iptables -t mangle -F &amp;&amp; sudo iptables -X$ sudo systemctl restart docker</code></pre><h2 id="解决docker因为polkit进程挂掉无法启动的问题"><a href="#解决docker因为polkit进程挂掉无法启动的问题" class="headerlink" title="解决docker因为polkit进程挂掉无法启动的问题"></a>解决docker因为polkit进程挂掉无法启动的问题</h2><pre><code>$ docker ps -aGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json?all=1: dial unix /var/run/docker.sock: connect: permission denied$ sudo systemctl restart dockerAuthorization not available. Check if polkit service is running or see debug message for more information.Failed to restart docker.service: Connection timed outSee system logs and &#39;systemctl status docker.service&#39; for details.$ sudo systemctl status polkit[sudo] password for centos:● polkit.service - Authorization Manager   Loaded: loaded (/usr/lib/systemd/system/polkit.service; static; vendor preset: enabled)   Active: failed (Result: timeout) since Wed 2020-07-22 00:16:04 CST; 1 months 6 days ago     Docs: man:polkit(8)  Process: 801 ExecStart=/usr/lib/polkit-1/polkitd --no-debug (code=killed, signal=TERM) Main PID: 801 (code=killed, signal=TERM)Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.$ sudo systemctl restart polkitAuthorization not available. Check if polkit service is running or see debug message for more information.Failed to restart polkit.service: Connection timed outSee system logs and &#39;systemctl status polkit.service&#39; for details.$ sudo yum install -y setroubleshoot setools$ sudo systemctl restart polkit$ sudo systemctl restart docker$ docker ps -aGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json?all=1: dial unix /var/run/docker.sock: connect: permission denied$ sudo gpasswd -a ${USER} docker$ newgrp docker$ docker ps -aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES222ca0539f7b        hello-world         &quot;/hello&quot;            11 months ago       Exited (0) 11 months ago                       recursing_nobel</code></pre><p>参考链接：<a href="https://www.cnblogs.com/dream397/p/13294875.html?utm_source=tuicool" target="_blank" rel="noopener">https://www.cnblogs.com/dream397/p/13294875.html?utm_source=tuicool</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Harbor安装以及docker registry的迁移</title>
      <link href="/2020/09/22/harbor-an-zhuang-yi-ji-docker-registry-de-qian-yi/"/>
      <url>/2020/09/22/harbor-an-zhuang-yi-ji-docker-registry-de-qian-yi/</url>
      
        <content type="html"><![CDATA[<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><ul><li>操作系统：CentOS 7.6 1804</li><li>Linux 内核版本：3.10.0-862.el7.x86_64</li><li>Docker版本：19.03.12</li><li>docker-compose版本：1.24.1</li><li>安装地址：10.0.93.105</li></ul><p>参考<a href>Linux系统初始化以及部署</a>文档进行配置。</p><h2 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h2><p>选择<a href="https://github.com/goharbor/harbor/releases/download/v1.10.3/harbor-offline-installer-v1.10.1.tgz" target="_blank" rel="noopener">harbor-offline-installer-v1.10.1</a>离线安装包，下载完成后，解压到/opt/harbor目录下。</p><h2 id="Harbor搭建以及配置"><a href="#Harbor搭建以及配置" class="headerlink" title="Harbor搭建以及配置"></a>Harbor搭建以及配置</h2><pre><code>$ cd /opt/harbor/$ cp harbor.yml.tmpl harbor.yml$ vim harbor.yml// 主要操作在于注释掉https相关设置以及添加hostname# Configuration file of Harbor# The IP address or hostname to access admin UI and registry service.# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.# hostname重新改写hostname: 10.0.93.105# http related confighttp:  # port for http, default is 80. If https enabled, this port will redirect to https port  port: 80# https related config# 注释掉https相关设置#https:  # https port for harbor, default is 443#  port: 443  # The path of cert and key files for nginx#  certificate: /your/certificate/path#  private_key: /your/private/key/path# Uncomment external_url if you want to enable external proxy# And when it enabled the hostname will no longer used# external_url: https://reg.mydomain.com:8433# The initial password of Harbor admin# It only works in first time to install harbor# Remember Change the admin password from UI after launching Harbor.harbor_admin_password: Harbor12345# Harbor DB configurationdatabase:  # The password for the root user of Harbor DB. Change this before any production use.  password: root123  # The maximum number of connections in the idle connection pool. If it &lt;=0, no idle connections are retained.  max_idle_conns: 50  # The maximum number of open connections to the database. If it &lt;= 0, then there is no limit on the number of open connections.  # Note: the default number of connections is 100 for postgres.  max_open_conns: 100# The default data volumedata_volume: /data# Harbor Storage settings by default is using /data dir on local filesystem# Uncomment storage_service setting If you want to using external storage# storage_service:#   # ca_bundle is the path to the custom root ca certificate, which will be injected into the truststore#   # of registry&#39;s and chart repository&#39;s containers.  This is usually needed when the user hosts a internal storage with self signed certificate.#   ca_bundle:#   # storage backend, default is filesystem, options include filesystem, azure, gcs, s3, swift and oss#   # for more info about this configuration please refer https://docs.docker.com/registry/configuration/#   filesystem:#     maxthreads: 100#   # set disable to true when you want to disable registry redirect#   redirect:#     disabled: false# Clair configurationclair:  # The interval of clair updaters, the unit is hour, set to 0 to disable the updaters.  updaters_interval: 12jobservice:  # Maximum number of job workers in job service  max_job_workers: 10notification:  # Maximum retry count for webhook job  webhook_job_max_retry: 10chart:  # Change the value of absolute_url to enabled can enable absolute url in chart  absolute_url: disabled# Log configurationslog:  # options are debug, info, warning, error, fatal  level: info  # configs for logs in local storage  local:    # Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.    rotate_count: 50    # Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.    # If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G    # are all valid.    rotate_size: 200M    # The directory on your host that store log    location: /var/log/harbor  # Uncomment following lines to enable external syslog endpoint.  # external_endpoint:  #   # protocol used to transmit log to external endpoint, options is tcp or udp  #   protocol: tcp  #   # The host of external endpoint  #   host: localhost  #   # Port of external endpoint  #   port: 5140#This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY!_version: 1.10.0# Uncomment external_database if using external database.# external_database:#   harbor:#     host: harbor_db_host#     port: harbor_db_port#     db_name: harbor_db_name#     username: harbor_db_username#     password: harbor_db_password#     ssl_mode: disable#     max_idle_conns: 2#     max_open_conns: 0#   clair:#     host: clair_db_host#     port: clair_db_port#     db_name: clair_db_name#     username: clair_db_username#     password: clair_db_password#     ssl_mode: disable#   notary_signer:#     host: notary_signer_db_host#     port: notary_signer_db_port#     db_name: notary_signer_db_name#     username: notary_signer_db_username#     password: notary_signer_db_password#     ssl_mode: disable#   notary_server:#     host: notary_server_db_host#     port: notary_server_db_port#     db_name: notary_server_db_name#     username: notary_server_db_username#     password: notary_server_db_password#     ssl_mode: disable# Uncomment external_redis if using external Redis server# external_redis:#   host: redis#   port: 6379#   password:#   # db_index 0 is for core, it&#39;s unchangeable#   registry_db_index: 1#   jobservice_db_index: 2#   chartmuseum_db_index: 3#   clair_db_index: 4# Uncomment uaa for trusting the certificate of uaa instance that is hosted via self-signed cert.# uaa:#   ca_file: /path/to/ca# Global proxy# Config http proxy for components, e.g. http://my.proxy.com:3128# Components doesn&#39;t need to connect to each others via http proxy.# Remove component from `components` array if want disable proxy# for it. If you want use proxy for replication, MUST enable proxy# for core and jobservice, and set `http_proxy` and `https_proxy`.# Add domain to the `no_proxy` field, when you want disable proxy# for some special registry.proxy:  http_proxy:  https_proxy:  # no_proxy endpoints will appended to 127.0.0.1,localhost,.local,.internal,log,db,redis,nginx,core,portal,postgresql,jobservice,registry,registryctl,clair,chartmuseum,notary-server  no_proxy:  components:    - core    - jobservice    - clair// :wq保存退出</code></pre><p>执行下面的命令，开始安装：</p><pre><code>$ sudo ./prepare.sh$ sudo ./install.sh</code></pre><p>在/opt/harbor目录下，会生成docker-compose.yml文件，如下：</p><pre><code>$ vim docker-compose.ymlversion: &#39;2.3&#39;services:  log:    image: goharbor/harbor-log:v1.10.1    container_name: harbor-log    restart: always    dns_search: .    cap_drop:      - ALL    cap_add:      - CHOWN      - DAC_OVERRIDE      - SETGID      - SETUID    volumes:      - /var/log/harbor/:/var/log/docker/:z      - ./common/config/log/logrotate.conf:/etc/logrotate.d/logrotate.conf:z      - ./common/config/log/rsyslog_docker.conf:/etc/rsyslog.d/rsyslog_docker.conf:z    ports:      - 127.0.0.1:1514:10514    networks:      - harbor  registry:    image: goharbor/registry-photon:v2.7.1-patch-2819-2553-v1.10.1    container_name: registry    restart: always    cap_drop:      - ALL    cap_add:      - CHOWN      - SETGID      - SETUID    volumes:      - /data/registry:/storage:z      - ./common/config/registry/:/etc/registry/:z      - type: bind        source: /data/secret/registry/root.crt        target: /etc/registry/root.crt    networks:      - harbor    dns_search: .    depends_on:      - log    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;registry&quot;  registryctl:    image: goharbor/harbor-registryctl:v1.10.1    container_name: registryctl    env_file:      - ./common/config/registryctl/env    restart: always    cap_drop:      - ALL    cap_add:      - CHOWN      - SETGID      - SETUID    volumes:      - /data/registry:/storage:z      - ./common/config/registry/:/etc/registry/:z      - type: bind        source: ./common/config/registryctl/config.yml        target: /etc/registryctl/config.yml    networks:      - harbor    dns_search: .    depends_on:      - log    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;registryctl&quot;  postgresql:    image: goharbor/harbor-db:v1.10.1    container_name: harbor-db    restart: always    cap_drop:      - ALL    cap_add:      - CHOWN      - DAC_OVERRIDE      - SETGID      - SETUID    volumes:      - /data/database:/var/lib/postgresql/data:z    networks:      harbor:    dns_search: .    env_file:      - ./common/config/db/env    depends_on:      - log    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;postgresql&quot;  core:    image: goharbor/harbor-core:v1.10.1    container_name: harbor-core    env_file:      - ./common/config/core/env    restart: always    cap_drop:      - ALL    cap_add:      - SETGID      - SETUID    volumes:      - /data/ca_download/:/etc/core/ca/:z      - /data/psc/:/etc/core/token/:z      - /data/:/data/:z      - ./common/config/core/certificates/:/etc/core/certificates/:z      - type: bind        source: ./common/config/core/app.conf        target: /etc/core/app.conf      - type: bind        source: /data/secret/core/private_key.pem        target: /etc/core/private_key.pem      - type: bind        source: /data/secret/keys/secretkey        target: /etc/core/key    networks:      harbor:    dns_search: .    depends_on:      - log      - registry      - redis      - postgresql    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;core&quot;  portal:    image: goharbor/harbor-portal:v1.10.1    container_name: harbor-portal    restart: always    cap_drop:      - ALL    cap_add:      - CHOWN      - SETGID      - SETUID      - NET_BIND_SERVICE    networks:      - harbor    dns_search: .    depends_on:      - log    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;portal&quot;  jobservice:    image: goharbor/harbor-jobservice:v1.10.1    container_name: harbor-jobservice    env_file:      - ./common/config/jobservice/env    restart: always    cap_drop:      - ALL    cap_add:      - CHOWN      - SETGID      - SETUID    volumes:      - /data/job_logs:/var/log/jobs:z      - type: bind        source: ./common/config/jobservice/config.yml        target: /etc/jobservice/config.yml    networks:      - harbor    dns_search: .    depends_on:      - core    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;jobservice&quot;  redis:    image: goharbor/redis-photon:v1.10.1    container_name: redis    restart: always    cap_drop:      - ALL    cap_add:      - CHOWN      - SETGID      - SETUID    volumes:      - /data/redis:/var/lib/redis    networks:      harbor:    dns_search: .    depends_on:      - log    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;redis&quot;  proxy:    image: goharbor/nginx-photon:v1.10.1    container_name: nginx    restart: always    cap_drop:      - ALL    cap_add:      - CHOWN      - SETGID      - SETUID      - NET_BIND_SERVICE    volumes:      - ./common/config/nginx:/etc/nginx:z    networks:      - harbor    dns_search: .    ports:      - 80:8080    depends_on:      - registry      - core      - portal      - log    logging:      driver: &quot;syslog&quot;      options:        syslog-address: &quot;tcp://127.0.0.1:1514&quot;        tag: &quot;proxy&quot;networks:  harbor:    external: false</code></pre><p>配置完成后，可以进行Harbor的启动，如下：</p><pre><code>$ docker-compose up -d</code></pre><p>最后直接访问<em><a href="http://10.0.93.105/" target="_blank" rel="noopener">http://10.0.93.105/</a></em>，用户名/密码为：admin/Harbor12345。</p><p>这样Harbor搭建完成！</p><h2 id="docker-registry迁移到Harbor"><a href="#docker-registry迁移到Harbor" class="headerlink" title="docker registry迁移到Harbor"></a>docker registry迁移到Harbor</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>docker registry 地址：192.168.163.202:5000，从这个地址迁移到10.0.93.105上的Harbor中。</p><p>下面的操作在10.0.93.107上完成，下面开始设置。</p><h3 id="迁移机器配置"><a href="#迁移机器配置" class="headerlink" title="迁移机器配置"></a>迁移机器配置</h3><p>设置docker所属的daemon.json文件，保证可以同时对两个docker镜像仓库进行拉取和推送，设置如下：</p><pre><code>$ sudo vim /etc/docker/daemon.json// 添加insecure-regsitry{    &quot;registry-mirrors&quot;: [        &quot;https://docker.mirrors.ustc.edu.cn&quot;,        &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,        &quot;https://registry.docker-cn.com&quot;    ],    &quot;insecure-registries&quot;: [        &quot;192.168.163.202:5000&quot;,        &quot;10.0.93.105&quot;    ],    &quot;live-restore&quot;: true,    &quot;log-driver&quot;: &quot;json-file&quot;,    &quot;log-opts&quot;: {        &quot;max-size&quot;: &quot;50m&quot;,        &quot;max-file&quot;: &quot;3&quot;    }}// :wq保存退出$ sudo systemctl daemon-reload$ sudo systemctl restart docker</code></pre><p>docker设置完成后，需要登录到harbor所在的镜像仓库中，操作如下：</p><pre><code>$ docker login -u admin -p Harbor12345 10.0.93.105</code></pre><p>登录后测试一下，能否向Harbor中推送镜像。</p><pre><code>$ docker imagesREPOSITORY                       TAG                 IMAGE ID            CREATED             SIZEprom/node-exporter               v1.0.1              0e0218889c33        3 months ago        26.4MB$ docker tag 0e0218889c33 10.0.93.105/library/prom/node-exporter:v1.0.11$ docker push 10.0.93.105/library/prom/node-exporter:v1.0.11The push refers to repository [10.0.93.105/library/prom/node-exporter]53fe7fa9c07c: Pushedbb208cc3e926: Pushedbef00f7ac5a9: Pushedv1.0.11: digest: sha256:f5b6df287cc3a87e8feb00e3dbbfc630eb411ca6dc3f61abfefe623022fa6927 size: 949</code></pre><p>Harbor默认是使用的library仓库，tag操作的时候注意下路径。推送完成，说明当前Harbor可用。</p><h3 id="迁移脚本编写"><a href="#迁移脚本编写" class="headerlink" title="迁移脚本编写"></a>迁移脚本编写</h3><p>先在Harbor中创建新项目，如图：</p><p><img src="%E5%88%9B%E5%BB%BAHarbor%E9%A1%B9%E7%9B%AE.png" alt></p><p>然后在107所在的机器上编写迁移脚本如下：</p><pre><code>$ vim to_harbor.sh#!/bin/bash# docker registry 迁移脚本images=`curl -s 192.168.163.202:5000/v2/_catalog | jq .repositories[] | tr -d &#39;&quot;&#39;`#测试部分#list=&quot;base_backend base_frontend&quot;#for image in $list;for image in $images;do    tags=`curl -s 192.168.163.202:5000/v2/$image/tags/list | jq .tags[] | tr -d &#39;&quot;&#39;`    for tag in $tags;    do        docker pull 192.168.163.202:5000/$image:$tag        docker tag 192.168.163.202:5000/$image:$tag 10.0.93.105/test/$image:$tag        docker push 10.0.93.105/test/$image:$tag        if [ $? -eq 0 ];then            echo &quot;###############################$image:$tag pull complete!###############################&quot;        else            echo &quot;$image:$tag pull failure!&quot;        fi    donedoneecho &quot;最后删除所有无用的镜像信息&quot;docker rmi -f $(docker images | grep -v &quot;prom&quot; |  awk &#39;{print $3}&#39;)// :wq保存并退出</code></pre><p>我们现存的镜像比较多，迁移非常耗时，而且迁移涉及大量的磁盘io，会对当前服务器造成影响。因此使用定时任务执行，如下：</p><pre><code>$ sudo touch /tmp/tmp.log$ sudo chown centos:centos /tmp/tmp.log$ crontab -e// 输入以下内容，配置九月23日晚上九点以后执行0 0 21 23 9 ? sh +x to_harbor.sh &gt; /tmp/tmp.log 2&gt;&amp;1 // :wq保存退出</code></pre><h3 id="测试和执行"><a href="#测试和执行" class="headerlink" title="测试和执行"></a>测试和执行</h3><p>测试一下脚本是否能正常执行，然后在脚本中修改以下内容：</p><pre><code>list=&quot;base_backend base_frontend&quot;for image in $list;#for image in $images;</code></pre><p>执行脚本测试拉取基础镜像的内容，如下：</p><pre><code>$ sh +x to_harbor.sh</code></pre><p>查看Harbor仓库中已经迁移了基础镜像仓库了。</p><p>最后恢复下文件，等待定时任务启动。</p><p>由于第二天休息，第三天回公司看结果，发现在昨天早上7点多完成的迁移，这样在Harbor中已经看到所有的镜像了。</p><h2 id="CI-CD流程中的变更"><a href="#CI-CD流程中的变更" class="headerlink" title="CI/CD流程中的变更"></a>CI/CD流程中的变更</h2><h3 id="主机docker服务配置调整"><a href="#主机docker服务配置调整" class="headerlink" title="主机docker服务配置调整"></a>主机docker服务配置调整</h3><p>这里使用spug运维平台批量执行脚本，脚本信息如下：</p><pre><code>echo &#39;&#39; | sudo -S sed -i &#39;s/192.168.163.202:5000/10.0.93.105\/test\//g&#39; /etc/docker/daemon.jsonecho &#39;&#39; | sudo systemctl daemon-reloadecho &#39;&#39; | sudo systemctl restart docker</code></pre><p>在echo后的引号内，写入密码信息，批量修改完成后，需要对docker镜像进行重启即可。</p><h3 id="jenkins调整"><a href="#jenkins调整" class="headerlink" title="jenkins调整"></a>jenkins调整</h3><p>对jenkins的调整主要涉及两个方面：</p><ol><li>构建任务中镜像名称中的url的替换</li><li>docker registry的地址重新配置为harbor的地址，并且需要对harbor添加用户名密码信息。 </li></ol><p>这样需要批量修改构建任务中对应的config.xml配置文件，使用sed命令进行替换：</p><pre><code>#!/bin/bashcd jobs/list=`find ./*/ -maxdepth 1 -type f -name config.xml`for configFile in $list;do    sed -i &#39;s/192.168.163.202:5000/10.0.93.105\/test/g&#39; $configFile    echo $configFile &#39;changed&#39;    echo &#39;==================================================&#39;done</code></pre><p>但是修改完成后，需要对每个构建任务添加认证信息，也就是添加Harbor的用户名密码作为登录信息，相当于在jenkins中进行了<em>docker login</em>操作。</p><h3 id="Dockerfile配置修改"><a href="#Dockerfile配置修改" class="headerlink" title="Dockerfile配置修改"></a>Dockerfile配置修改</h3><p>对Dockerfile的修改主要涉及到基础镜像的地址需要进行变更，但是由于jenkins所在的机器中，本地docker镜像有缓存，应该是jenkins所在机器上已经留存了所有的基础镜像，目前临时不需要进行修改。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul><li>迁移参考：<a href="https://soulchild.cn/691.html" target="_blank" rel="noopener">https://soulchild.cn/691.html</a></li><li><a href="https://www.cnblogs.com/iXiAo9/p/13665547.html" target="_blank" rel="noopener">https://www.cnblogs.com/iXiAo9/p/13665547.html</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于filestore服务上传文件大小限制的问题</title>
      <link href="/2020/09/03/guan-yu-filestore-fu-wu-shang-chuan-wen-jian-da-xiao-xian-zhi-de-wen-ti/"/>
      <url>/2020/09/03/guan-yu-filestore-fu-wu-shang-chuan-wen-jian-da-xiao-xian-zhi-de-wen-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="关于filestore服务上传文件大小限制的问题"><a href="#关于filestore服务上传文件大小限制的问题" class="headerlink" title="关于filestore服务上传文件大小限制的问题"></a>关于filestore服务上传文件大小限制的问题</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>2020年9月2日晚上，在前端服务推送到k8s以后，开始测试设备台账功能的文件上传操作，发现出现了Nginx 413错误，也就是<em><em>413 Request Entity Too Large</em>错误。不足1MB的文件可以正常上传，超过1MB的文件不能正常上传。根据官网和StackOverflow的建议，修复错误的方法很简单，在<em>server</em>节点下添加”*client_max_body_size 200m;</em>“，如下:</p><pre><code>    server {        listen       80;        server_name  localhost;        // 添加下面一句话解决问题        client_max_body_size 200m;</code></pre><h2 id="所走弯路"><a href="#所走弯路" class="headerlink" title="所走弯路"></a>所走弯路</h2><p>当时所有小伙伴一致认为是fastdfs之前设定的nginx服务导致了这个错误，于是立刻开始修改该nginx的问题，测试了添加“<em>client_max_body_size 200m;</em>”配置信息的功能，但还是不成功。更换配置项所在位置也无法解决问题，例如将“<em>client_max_body_size 200m;</em>”放在<em>location</em>节点下，或者放在<em>http</em>节点下。根据fastdfs搭建文档中的描述，该nginx实际上承担的是获取文件链接，也就是文件下载的功能，文件上传走的还是fastdfs服务的22122端口，于是在这个位置排查并不正确。</p><h2 id="正确的排查方式"><a href="#正确的排查方式" class="headerlink" title="正确的排查方式"></a>正确的排查方式</h2><p>对于微服务场景下，应该从全链路的角度来进行排查，首先梳理上传文件时涉及的服务信息，然后排查链路上涉及到的服务信息，最后查看所包含的nginx服务的点有几个，哪个影响了目前出现的结果。整个服务调用的链路信息如下：</p><p>前端服务–&gt;gateway服务 –&gt; filestore服务 –&gt; fastdfs服务<br>                               |                  |<br>                               | <strong>____</strong>&lt;&lt;&lt;_____ |<br>                                      文件获取</p><p>在整个链路上首先怀疑的是filestore服务出现问题，导致文件上传不正确，于是将本地服务接入k8s中，进行调试，结果依旧是大于1MB的文件无法进行上传！</p><p>根据这个操作，排除对fastdfs前nginx服务的问题。</p><p>重新讨论这一部分的内容时，发现开发的同事在强调一个事情，开发环境和测试环境使用同一套fastdfs时，在开发环境运行正常。既然开发环境运行正常，也同样证明了fastdfs前nginx服务不存在问题。</p><p>那么后续就是，整条链路上到底有几个nginx服务？列举如下：</p><ul><li>前端服务，由nginx承载</li><li>fastdfs之前的nginx服务</li><li>k8s中ingress功能由nginx实现</li></ul><p>之前的调试证明了fastdfs之前的nginx服务不存在问题，下一步看k8s中的ingress，由于目前所有的服务均使用nodePort模式对外暴露访问地址，并未使用ingress服务对外暴露服务地址，所以也排除这个信息。</p><p>那最后只剩下前端服务所在的nginx了，尝试去k8s中找到对应的psl前端服务所在的容器，修改容器中的配置文件，但是修改完成并没找到重新加载的方式。因此，在最早的一台服务器上（192.168.66.200），使用之前搭建的nginx进行测试。修改nginx.conf配置文件，添加“<em>client_max_body_size 200m;</em>”配置信息，然后重新打包psl前端项目并上传到服务器，进行测试，发现15MB的一个PPT文件是可以传上去的！</p><p>那这里就是问题所在了，前端镜像存在问题，再准确点说前端基础镜像存在问题，于是这里修改了前端基础镜像，添加“<em>client_max_body_size 200m;</em>”配置信息，最后部署到k8s中，问题解决！</p><h2 id="经验与教训"><a href="#经验与教训" class="headerlink" title="经验与教训"></a>经验与教训</h2><ol><li>对于微服务体系下的问题排查，一定要做到全链路考虑，前后端结合</li><li>尽早排除链路上不相关的内容</li><li>对每一个服务的走向和链路信息要十分熟悉才能尽可能快的排查！</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>内网迁移-: 验证记录文档</title>
      <link href="/2020/08/21/nei-wang-qian-yi-yan-zheng-ji-lu-wen-dang/"/>
      <url>/2020/08/21/nei-wang-qian-yi-yan-zheng-ji-lu-wen-dang/</url>
      
        <content type="html"><![CDATA[<h2 id="内网环境地址准备"><a href="#内网环境地址准备" class="headerlink" title="内网环境地址准备"></a>内网环境地址准备</h2><ul><li><p>双网卡跳板机：192.168.81.90(Project-dev-01)、192.168.66.251（外网地址）、内外网交互</p></li><li><p>域名服务器：192.168.81.93(Project-dev-02)、内网</p></li><li><p>测试服务器：192.168.81.92(Project-dev-03)、内网</p></li><li><p>开发机：</p></li></ul><h2 id="搭建DNS域名解析服务器（使用代理的方式替代DNS服务器）"><a href="#搭建DNS域名解析服务器（使用代理的方式替代DNS服务器）" class="headerlink" title="搭建DNS域名解析服务器（使用代理的方式替代DNS服务器）"></a>搭建DNS域名解析服务器（使用代理的方式替代DNS服务器）</h2><ol><li><p>在双网卡机器上挂载repo</p></li><li><p>域名服务器配置仓库信息</p></li><li><p>安装bind服务</p></li><li><p>配置域名解析</p></li></ol><h2 id="原则上内网只能进不能出"><a href="#原则上内网只能进不能出" class="headerlink" title="原则上内网只能进不能出"></a>原则上内网只能进不能出</h2><h2 id="内网的时钟同步的问题"><a href="#内网的时钟同步的问题" class="headerlink" title="内网的时钟同步的问题"></a>内网的时钟同步的问题</h2><p>首先在跳板机上搭建时钟同步服务，如下：</p><pre><code>$ sudo yum install ntp$ sudo vim /etc/ntp.conf// 添加以下内容restrict 192.168.81.0 mask 255.255.255.0 nomodify notrapserver 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburstserver cn.pool.ntp.org       iburstserver ntp1.aliyun.com       iburstserver 127.127.1.0fudge 127.127.1.0 stratum 10// :wq保存退出// 启动服务$ sudo systemctl enable ntpd$ sudo systemctl start ntpd// 查看服务运行情况$ ntpq -p     remote           refid      st t when poll reach   delay   offset  jitter==============================================================================+139.199.214.202 100.122.36.196   2 u   24   64  377   37.808   -1.303   6.896xstratum2-1.ntp. 89.109.251.23    2 u   22   64  177  264.838  -69.270  24.058-time.cloudflare 10.12.3.34       3 u   39   64    7  215.246   -1.959  16.837+a.ams.pobot.net 17.253.34.253    2 u   18   64  377  341.399  -13.098  20.904-ntp8.flashdance 192.36.143.150   2 u   79   64  356  306.375   30.427   7.068*120.25.115.20   10.137.53.7      2 u   17   64  377   46.599    1.891   3.464 localhost.local .INIT.          16 u    -  128    0    0.000    0.000   0.000// 开启相应的端口信息$ sudo firewall-cmd --add-port=123/udp --zone=public --permanent$ sudo firewall-cmd --add-port=123/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><p>跳板机上的服务就已经搭建成功了。下面要去设置内网机器的时钟同步，连接到跳板机！</p><p>对其它的内网机器进行设置：</p><pre><code>$ sudo yum install ntp$ sudo systemctl enable ntpd$ sudo vim /etc/ntp.conf// 修改以下内容指向192.168.81.90restrict 192.168.81.90 nomodify notrap noquery# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburstserver 192.168.81.90 iburst// :wq保存退出$ sudo systemctl start ntpd// 测试$ sudo ntpdate -d 192.168.81.90 3 Aug 19:35:56 ntpdate[29774]: ntpdate 4.2.6p5@1.2349-o Tue Jun 23 15:38:19 UTC 2020 (1)Looking for host 192.168.81.90 and service ntphost found : 192.168.81.90transmit(192.168.81.90)receive(192.168.81.90)transmit(192.168.81.90)receive(192.168.81.90)transmit(192.168.81.90)receive(192.168.81.90)transmit(192.168.81.90)receive(192.168.81.90)server 192.168.81.90, port 123stratum 3, precision -24, leap 00, trust 000refid [192.168.81.90], delay 0.02583, dispersion 0.00006transmitted 4, in filter 4reference time:    e2d202b1.57312e3d  Mon, Aug  3 2020 11:27:45.340originate timestamp: e2d20482.89df2f5c  Mon, Aug  3 2020 11:35:30.538transmit timestamp:  e2d2752e.f2da3078  Mon, Aug  3 2020 19:36:14.948filter delay:  0.02649  0.02591  0.02594  0.02583         0.00000  0.00000  0.00000  0.00000filter offset: -28844.4 -28844.4 -28844.4 -28844.4         0.000000 0.000000 0.000000 0.000000delay 0.02583, dispersion 0.00006offset -28844.410413 3 Aug 19:36:14 ntpdate[29774]: step time server 192.168.81.90 offset -28844.410413 sec</code></pre><h2 id="跳板机新增网卡的问题"><a href="#跳板机新增网卡的问题" class="headerlink" title="跳板机新增网卡的问题"></a>跳板机新增网卡的问题</h2><h2 id="以代理-host配置的方式，从跳板机设置对外访问的信息"><a href="#以代理-host配置的方式，从跳板机设置对外访问的信息" class="headerlink" title="以代理+host配置的方式，从跳板机设置对外访问的信息"></a>以代理+host配置的方式，从跳板机设置对外访问的信息</h2><p>前提：已经保证跳板机配置完成双网卡设置。</p><p>在跳板机，安装tinyproxy，如下：</p><pre><code>$ sudo yum install tinyproxy</code></pre><p>安装完成后进行配置，分为tinyproxy的配置以及要配置的出站域名信息，如下：</p><pre><code>$ sudo vim /etc/tinyproxy/tinyproxy.conf// 内容如下：User tinyproxyGroup tinyproxyPort 8888// 注释掉bind部分#Bind 192.168.0.1Timeout 600LogFile &quot;/var/log/tinyproxy/tinyproxy.log&quot;LogLevel InfoPidFile &quot;/var/run/tinyproxy/tinyproxy.pid&quot;// 注释掉Allow部分，允许所有代理设置后的服务器访问，不限制网段#Allow 192.168.81.0/24// 设置筛选的信息Filter &quot;/etc/tinyproxy/filter&quot;FilterURLs On// 默认为Yes，如果是未指定URL信息，则不允许对外访问FilterDefaultDeny Yes// :wq保存退出// 下面开始设置筛选URL的filter文件$ sudo vim /etc/tinyproxy/filter// 添加以下内容aliyun.comyum-idc.combit.edu.cn163.comdocker.comustc.edu.cntsinghua.edu.comlzu.edu.comhuaweicloud.combfsu.edu.cnneusoft.edu.cncqu.edu.cnfedoraproject.orgcentos.orgpool.ntp.org// :wq保存退出</code></pre><p>配置完成后，启动tinyproxy。如下：</p><pre><code>$ sudo systemctl enable tinyproxy$ sudo systemctl start tinyproxy// 开放端口信息$ sudo firewall-cmd --add-port=8888/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><p>跳板机配置完成后，转到内网机器上，配置代理信息，如下：</p><pre><code>$ sudo vim /etc/profile// 添加以下信息，注意：填写内网地址信息export http_proxy=http://192.168.81.90:8888/export FTP_PROXY=http://192.168.81.90:8888/export ftp_proxy=http://192.168.81.90:8888/export all_proxy=socks://192.168.81.90:8888/export ALL_PROXY=socks://192.168.81.90:8888/export HTTPS_PROXY=http://192.168.81.90:8888/export https_proxy=http://192.168.81.90:8888/export HTTP_PROXY=http://192.168.81.90:8888/export no_proxy=localhost,127.0.0.1// :wq保存退出// 下面开始测试，用curl访问测试$ curl --proxy 192.168.81.90:8888 docker.com// 第二项，使用yum命令测试$ sudo yum install epel-release// 执行后输出大量错误日志http://mirrors.163.com/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirrors.163.com; Unknown error&quot;Trying other mirror.http://mirrors.ustc.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirrors.ustc.edu.cn; Unknown error&quot;Trying other mirror.http://mirror.lzu.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirror.lzu.edu.cn; Unknown error&quot;Trying other mirror.http://mirror.bit.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirror.bit.edu.cn; Unknown error&quot;Trying other mirror.http://mirrors.nju.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirrors.nju.edu.cn; Unknown error&quot;Trying other mirror.http://mirrors.neusoft.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirrors.neusoft.edu.cn; Unknown error&quot;Trying other mirror.http://mirrors.bfsu.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirrors.bfsu.edu.cn; Unknown error&quot;Trying other mirror.http://ftp.sjtu.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: ftp.sjtu.edu.cn; Unknown error&quot;Trying other mirror.http://mirrors.tuna.tsinghua.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirrors.tuna.tsinghua.edu.cn; Unknown error&quot;Trying other mirror.http://mirrors.cqu.edu.cn/CentOS/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: mirrors.cqu.edu.cn; Unknown error&quot;Trying other mirror.https://download.docker.com/linux/centos/7/x86_64/stable/repodata/repomd.xml: [Errno 14] curl#6 - &quot;Could not resolve host: download.docker.com; Unknown error&quot;Trying other mirror.// ctrl+c终止// 修改yum代理信息$ sudo vim /etc/yum.conf// 最后进行追加proxy=http://192.168.81.90:8888// :wq保存退出// 重新进行测试$ sudo yum install epel-release</code></pre><p>测试后上述两项都能正常返回数据，即为配置成功！</p><p>目前初步验证完成Linux下内网和外网通信的问题，使用tinyproxy在跳板机搭建代理程序，并配置对外访问的URL信息，在内网机器中设置代理信息，初步验证yum安装依赖、下载外网文件成功。但是这个存在比较大的风险：</p><ol><li>参考华为对内网管理的条例，要求文件只许进不许出，目前存在文件可以从内网拷贝出来的问题 ，需要了解咱公司对于内网的管理规定并商定解决方案</li><li>目前对于跳板机的管理，配置可以对外访问的URL从跳板机上进行设定，而跳板机目前未进行权限管理，未限定登录人员，存在较大的风险。</li></ol><p>希望就以上两个风险点，得到各位的建议和观点。目前处于验证阶段，只能优先抛出问题</p><h2 id="maven部分搭建"><a href="#maven部分搭建" class="headerlink" title="maven部分搭建"></a>maven部分搭建</h2><h3 id="maven构建与nexus仓库的导入导出"><a href="#maven构建与nexus仓库的导入导出" class="headerlink" title="maven构建与nexus仓库的导入导出"></a>maven构建与nexus仓库的导入导出</h3><p>搭建nexus3依赖仓库。第一步，创建新的存储；第二步，创建新的镜像仓库，主要是针对hosted、proxy、group三种类型创建，其中proxy提供对外部的依赖仓库的访问，需要配置多个，hosted主要是存放本地开发的已打包的依赖信息，group是将上述两类镜像仓库合并到一起，统一由该仓库对外访问。第三步，创建新的角色信息，根据前面创建的镜像仓库，分配相应的访问权限到角色下，对应的权限中涉及的仓库信息，在下面进行设置。最后创建用户信息，分配前面对应的角色信息，即可完成对nexus3依赖仓库的设置。</p><p>在跳板机端配置代理域名，保证这些域名能够出站。切记，使用curl命令进行测试，不要使用ping命令测试，实践证明ping命令无法ping通域名，但是curl命令可以访问到各个出站地址的页面。</p><p>在本地开发机上，配置maven的settings.xml配置文件，执行示例程序的构建工作，设置如下：</p><p>settings.xml</p><pre class="line-numbers language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?></span><span class="token comment" spellcheck="true">&lt;!--Licensed to the Apache Software Foundation (ASF) under oneor more contributor license agreements.  See the NOTICE filedistributed with this work for additional informationregarding copyright ownership.  The ASF licenses this fileto you under the Apache License, Version 2.0 (the"License"); you may not use this file except in compliancewith the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing,software distributed under the License is distributed on an"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANYKIND, either express or implied.  See the License for thespecific language governing permissions and limitationsunder the License.--></span><span class="token comment" spellcheck="true">&lt;!-- | This is the configuration file for Maven. It can be specified at two levels: | |  1. User Level. This settings.xml file provides configuration for a single user, |                 and is normally provided in ${user.home}/.m2/settings.xml. | |                 NOTE: This location can be overridden with the CLI option: | |                 -s /path/to/user/settings.xml | |  2. Global Level. This settings.xml file provides configuration for all Maven |                 users on a machine (assuming they're all using the same Maven |                 installation). It's normally provided in |                 ${maven.conf}/settings.xml. | |                 NOTE: This location can be overridden with the CLI option: | |                 -gs /path/to/global/settings.xml | | The sections in this sample file are intended to give you a running start at | getting the most out of your Maven installation. Where appropriate, the default | values (values used when the setting is not specified) are provided. | |--></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>settings</span> <span class="token attr-name">xmlns</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/SETTINGS/1.0.0<span class="token punctuation">"</span></span>          <span class="token attr-name"><span class="token namespace">xmlns:</span>xsi</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://www.w3.org/2001/XMLSchema-instance<span class="token punctuation">"</span></span>          <span class="token attr-name"><span class="token namespace">xsi:</span>schemaLocation</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!-- localRepository   | The path to the local repository maven will use to store artifacts.   |   | Default: ${user.home}/.m2/repository  &lt;localRepository>/path/to/local/repo&lt;/localRepository>  --></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>localRepository</span><span class="token punctuation">></span></span>/home/centos/apache-maven-3.6.2/maven-repository<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>localRepository</span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!-- interactiveMode   | This will determine whether maven prompts you when it needs input. If set to false,   | maven will use a sensible default value, perhaps based on some other setting, for   | the parameter in question.   |   | Default: true  &lt;interactiveMode>true&lt;/interactiveMode>  --></span>  <span class="token comment" spellcheck="true">&lt;!-- offline   | Determines whether maven should attempt to connect to the network when executing a build.   | This will have an effect on artifact downloads, artifact deployment, and others.   |   | Default: false  &lt;offline>false&lt;/offline>  --></span>  <span class="token comment" spellcheck="true">&lt;!-- pluginGroups   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.   | when invoking a command line like "mvn prefix:goal". Maven will automatically add the group identifiers   | "org.apache.maven.plugins" and "org.codehaus.mojo" if these are not already contained in the list.   |--></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginGroups</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- pluginGroup     | Specifies a further group identifier to use for plugin lookup.    &lt;pluginGroup>com.your.plugins&lt;/pluginGroup>    --></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginGroups</span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!-- proxies   | This is a list of proxies which can be used on this machine to connect to the network.   | Unless otherwise specified (by system property or command-line switch), the first proxy   | specification in this list marked as active will be used.   |--></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>proxies</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- proxy     | Specification for one proxy, to be used in connecting to the network.     |    &lt;proxy>      &lt;id>optional&lt;/id>      &lt;active>true&lt;/active>      &lt;protocol>http&lt;/protocol>      &lt;username>proxyuser&lt;/username>      &lt;password>proxypass&lt;/password>      &lt;host>proxy.host.net&lt;/host>      &lt;port>80&lt;/port>      &lt;nonProxyHosts>local.net|some.host.com&lt;/nonProxyHosts>    &lt;/proxy>    --></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>proxies</span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!-- servers   | This is a list of authentication profiles, keyed by the server-id used within the system.   | Authentication profiles can be used whenever maven must make a connection to a remote server.   |--></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>servers</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- server     | Specifies the authentication information to use when connecting to a particular server, identified by     | a unique name within the system (referred to by the 'id' attribute below).     |     | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are     |       used together.     |    &lt;server>      &lt;id>deploymentRepo&lt;/id>      &lt;username>repouser&lt;/username>      &lt;password>repopwd&lt;/password>    &lt;/server>    --></span>    <span class="token comment" spellcheck="true">&lt;!-- Another sample, using keys to authenticate.    &lt;server>      &lt;id>siteServer&lt;/id>      &lt;privateKey>/path/to/private/key&lt;/privateKey>      &lt;passphrase>optional; leave empty if not used.&lt;/passphrase>    &lt;/server>    --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>server</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>nexus-osc<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>username</span><span class="token punctuation">></span></span>developer2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>username</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>password</span><span class="token punctuation">></span></span>test2019<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>password</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>server</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>servers</span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!-- mirrors   | This is a list of mirrors to be used in downloading artifacts from remote repositories.   |   | It works like this: a POM may declare a repository to use in resolving certain artifacts.   | However, this repository may have problems with heavy traffic at times, so people have mirrored   | it to several places.   |   | That repository definition will have a unique id, so we can create a mirror reference for that   | repository, to be used as an alternate download site. The mirror site will be the preferred   | server for that repository.   |--></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mirrors</span><span class="token punctuation">></span></span>    <span class="token comment" spellcheck="true">&lt;!-- mirror     | Specifies a repository mirror site to use instead of a given repository. The repository that     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.     |    &lt;mirror>      &lt;id>mirrorId&lt;/id>      &lt;mirrorOf>repositoryId&lt;/mirrorOf>      &lt;name>Human Readable Name for this Mirror.&lt;/name>      &lt;url>http://my.repository.com/repo/path&lt;/url>    &lt;/mirror>     --></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mirror</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>nexus-osc<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mirrorOf</span><span class="token punctuation">></span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mirrorOf</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>Nexus OSC<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://192.168.81.94:8081/repository/maven-test-group/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mirror</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mirrors</span><span class="token punctuation">></span></span>  <span class="token comment" spellcheck="true">&lt;!-- profiles   | This is a list of profiles which can be activated in a variety of ways, and which can modify   | the build process. Profiles provided in the settings.xml are intended to provide local machine-   | specific paths and repository locations which allow the build to work in the local environment.   |   | For example, if you have an integration testing plugin - like cactus - that needs to know where   | your Tomcat instance is installed, you can provide a variable here such that the variable is   | dereferenced during the build process to configure the cactus plugin.   |   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles   | section of this document (settings.xml) - will be discussed later. Another way essentially   | relies on the detection of a system property, either matching a particular value for the property,   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a   | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'.   | Finally, the list of active profiles can be specified directly from the command line.   |   | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact   |       repositories, plugin repositories, and free-form properties to be used as configuration   |       variables for plugins in the POM.   |   |--></span><span class="token comment" spellcheck="true">&lt;!--  &lt;profiles>  --></span>    <span class="token comment" spellcheck="true">&lt;!-- profile     | Specifies a set of introductions to the build process, to be activated using one or more of the     | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/>     | or the command line, profiles have to have an ID that is unique.     |     | An encouraged best practice for profile identification is to use a consistent naming convention     | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc.     | This will make it more intuitive to understand what the set of introduced profiles is attempting     | to accomplish, particularly when you only have a list of profile id's for debug.     |     | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo.    &lt;profile>      &lt;id>jdk-1.4&lt;/id>      &lt;activation>        &lt;jdk>1.4&lt;/jdk>      &lt;/activation>      &lt;repositories>        &lt;repository>          &lt;id>jdk14&lt;/id>          &lt;name>Repository for JDK 1.4 builds&lt;/name>          &lt;url>http://www.myhost.com/maven/jdk14&lt;/url>          &lt;layout>default&lt;/layout>          &lt;snapshotPolicy>always&lt;/snapshotPolicy>        &lt;/repository>      &lt;/repositories>    &lt;/profile>    --></span>    <span class="token comment" spellcheck="true">&lt;!--     | Here is another profile, activated by the system property 'target-env' with a value of 'dev',     | which provides a specific path to the Tomcat instance. To use this, your plugin configuration     | might hypothetically look like:     |     | ...     | &lt;plugin>     |   &lt;groupId>org.myco.myplugins&lt;/groupId>     |   &lt;artifactId>myplugin&lt;/artifactId>     |     |   &lt;configuration>     |     &lt;tomcatLocation>${tomcatPath}&lt;/tomcatLocation>     |   &lt;/configuration>     | &lt;/plugin>     | ...     |     | NOTE: If you just wanted to inject this configuration whenever someone set 'target-env' to     |       anything, you could just leave off the &lt;value/> inside the activation-property.     |    &lt;profile>      &lt;id>env-dev&lt;/id>      &lt;activation>        &lt;property>          &lt;name>target-env&lt;/name>          &lt;value>dev&lt;/value>        &lt;/property>      &lt;/activation>      &lt;properties>        &lt;tomcatPath>/path/to/tomcat/instance&lt;/tomcatPath>      &lt;/properties>    &lt;/profile>    --></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>activeProfiles</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>activeProfile</span><span class="token punctuation">></span></span>nexus-osc<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>activeProfile</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>activeProfiles</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>profiles</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>profile</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>nexus-osc<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>repository</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>central<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://192.168.81.94:8081/repository/maven-test-group/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repository</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>repositories</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepositories</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pluginRepository</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">></span></span>central<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>url</span><span class="token punctuation">></span></span>http://192.168.81.94:8081/repository/maven-test-group/<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>url</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>releases</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>releases</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>snapshots</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>enabled</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>enabled</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>snapshots</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepository</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pluginRepositories</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>profile</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>profiles</span><span class="token punctuation">></span></span><span class="token comment" spellcheck="true">&lt;!--  &lt;/profiles> --></span>  <span class="token comment" spellcheck="true">&lt;!-- activeProfiles   | List of profiles that are active for all builds.   |  &lt;activeProfiles>    &lt;activeProfile>alwaysActiveProfile&lt;/activeProfile>    &lt;activeProfile>anotherAlwaysActiveProfile&lt;/activeProfile>  &lt;/activeProfiles>  --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>settings</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试项目构建，执行命令如下：</p><pre><code>$ mvn clean install -U</code></pre><p>初次构建时，会从nexus3中进行拉取相关的依赖信息，缓存在本地。如果当前依赖仓库中没有任何远程仓库的jar包，则会从配置的远程仓库中进行拉取操作。拉取完成后再缓存到本地，在后续构建中，就不会再去重复进行拉取了。</p><p>测试构建完成后，一定要测试一下构建和推送，执行命令如下:</p><pre><code>$ mvn clean package deploy</code></pre><p>这时候该构建的示例程序应该被推送到hosted类型的仓库中，前提是该仓库已经被设置为<strong>allow redeploy</strong>。</p><h2 id="关于Linux内核的问题"><a href="#关于Linux内核的问题" class="headerlink" title="关于Linux内核的问题"></a>关于Linux内核的问题</h2><p>kernel:BUG: soft lockup - CPU#2 stuck for 23s! [ksoftirqd/2:274]</p><p>内核软锁死，请检查磁盘使用率以及内存占用，估计内存也快跑满了</p><p>先尝试一下 </p><pre><code># echo 30 &gt; /proc/sys/kernel/watchdog_thresh </code></pre><p>修改内核参数，让机器临时可以继续运行。</p><p>通过监控，看到后台cpu占用基本上吃满了。整体的实体机上cpu已经被吃满，导致其他虚拟机上同样出现了运行卡顿的情况！通过停止部分机器、释放资源的方式进行解决！</p><h2 id="jenkins设置"><a href="#jenkins设置" class="headerlink" title="jenkins设置"></a>jenkins设置</h2><p>导入镜像并启动，同时把之前备份的稳定运行的镜像中/var/jenkins_home目录下的所有文件打包，命名为jenkins_home.tar.gz，传到目标机器上。</p><p>启动jenkins镜像，然后将jenkins_home.tar.gz拷贝到运行的容器中，删除容器下/var/jenkins_home目录下的所有文件，然后将jenkins_home.tar.gz解压，覆盖到/var/jenkins_home目录中。</p><p>最后重启jenkins容器，即可进行访问！</p><p>这样做最大限度保证了插件版本对应相应的jenkins版本，不会出现重装插件时导致jenkins中构建不可用的情况。</p><h2 id="gitlab设置"><a href="#gitlab设置" class="headerlink" title="gitlab设置"></a>gitlab设置</h2><p>安装完成，添加用户，分组，创建项目即可</p><h2 id="EXSI系统中虚拟机假死，强制停机的操作"><a href="#EXSI系统中虚拟机假死，强制停机的操作" class="headerlink" title="EXSI系统中虚拟机假死，强制停机的操作"></a>EXSI系统中虚拟机假死，强制停机的操作</h2><p>首先使用ssh连接到exsi中，操作如下：</p><pre><code>$  ssh root@192.168.66.1Password:// 列出所有运行着的虚拟机信息// 找到要关闭的机器信息# esxcli vm process list.........Project-dev-03   World ID: 2206934   Process ID: 0   VMX Cartel ID: 2206872   UUID: 56 4d f1 af b7 b8 66 4f-02 29 3e 42 b1 d9 54 e8   Display Name: Project-dev-03   Config File: /vmfs/volumes/5d7280cc-085d5a77-2918-6c92bfdcfeba/Project-dev-03/Project-dev-03.vmx// 记下上面的World ID信息// 执行下面的命令进行关闭// 注意：有三种关闭虚拟机的方法，Soft 程度最低，hard 为立即执行，如果依然不能关闭，则可以使用force 模式。# esxcli vm process kill -t hard -w 2206934// 重新执行前面的命令查看该机器是否还在运行# esxcli vm process list</code></pre><p><strong>注意：</strong>一定要谨慎执行该命令！记得最后通过vsphere client查看VM状态。</p><h2 id="简单解决因为升级内核导致的服务器启动问题"><a href="#简单解决因为升级内核导致的服务器启动问题" class="headerlink" title="简单解决因为升级内核导致的服务器启动问题"></a>简单解决因为升级内核导致的服务器启动问题</h2><p>服务器因为升级内核导致启动时进入了emergency模式，通过journalctl查看日志。</p><p>错误日志信息：</p><pre><code>xfs (dm-0) metadata i/o error in xfs_trans_read_buf_map at</code></pre><p>错误解决，</p><pre><code>ls -l /dev/mapper/lrwxrwxrwx 1 root root 7 May 22 11:21 centos-root -&gt; ../dm-0lrwxrwxrwx 1 root root 7 May 22 11:21 centos-swap -&gt; ../dm-1$ xfs_repair -L /dev/mapper/centos-root</code></pre><p>自动修复即可！修复后需要卸载之前安装的有问题的内核信息如下：</p><pre><code>$ sudo yum remove kernel-3.10.0-1160.11.1.el7.x86_64</code></pre><h2 id="为Docker挂载数据盘，解决存储冲突问题"><a href="#为Docker挂载数据盘，解决存储冲突问题" class="headerlink" title="为Docker挂载数据盘，解决存储冲突问题"></a>为Docker挂载数据盘，解决存储冲突问题</h2><p>目前遇到了xfs同docker中overlay2存储出现冲突的问题，错误如下：</p><pre><code>Aug 01 17:35:28 localhost.localdomain systemd[1]: Received SIGRTMIN+20 from PID 288 (plymouthd).Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Metadata CRC error detected at xfs_allocbt_read_verify+0x1a/0xd0 [xfs], xfs_allocbt blockAug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Unmount and run xfs_repairAug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): First 128 bytes of corrupted metadata buffer:Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926030: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926040: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926050: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926060: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926070: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): metadata I/O error in &quot;xfs_trans_read_buf_map&quot; at daddr 0x2a22f8 len 8 error 74Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): xfs_do_force_shutdown(0x1) called from line 316 of file fs/xfs/xfs_trans_buf.c.  Return adAug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): I/O Error Detected. Shutting down filesystemAug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Please umount the filesystem and rectify the problem(s)Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Failed to recover intentsAug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): log mount finish failedAug 01 17:35:28 localhost.localdomain mount[529]: mount: mount /dev/mapper/centos-root on /sysroot failed: Structure needs cleaningAug 01 17:35:28 localhost.localdomain systemd[1]: sysroot.mount mount process exited, code=exited status=32Aug 01 17:35:28 localhost.localdomain systemd[1]: Failed to mount /sysroot.Aug 01 17:35:28 localhost.localdomain systemd[1]: Dependency failed for Initrd Root File System.Aug 01 17:35:28 localhost.localdomain systemd[1]: Dependency failed for Reload Configuration from the Real Root.Aug 01 17:35:28 localhost.localdomain systemd[1]: Job initrd-parse-etc.service/start failed with result &#39;dependency&#39;.Aug 01 17:35:28 localhost.localdomain systemd[1]: Triggering OnFailure= dependencies of initrd-parse-etc.service.Aug 01 17:35:28 localhost.localdomain systemd[1]: Job initrd-root-fs.target/start failed with result &#39;dependency&#39;.Aug 01 17:35:28 localhost.localdomain systemd[1]: Triggering OnFailure= dependencies of initrd-root-fs.target.Aug 01 17:35:28 localhost.localdomain systemd[1]: Unit sysroot.mount entered failed state.Aug 01 17:35:28 localhost.localdomain systemd[1]: Stopped dracut pre-udev hook.Aug 01 17:35:28 localhost.localdomain systemd[1]: Stopped dracut cmdline hook.</code></pre><p>按照<a href="https://www.jianshu.com/p/00ffd8df6010" target="_blank" rel="noopener">参考文章</a>中描述的内容，进行排查。但是当前机器已经不能正常开机，一旦开机只能进入紧急模式。所以需要考虑对硬盘进行修复。</p><p>再根据<a href="https://blog.csdn.net/runming56/article/details/81016404" target="_blank" rel="noopener">参考文章</a>进行修复，由于我们只有一个根目录存在的区，而且根目录所在区不能被umount。如果不能进行umount，是不能进行硬盘分区修复的。</p><p>尝试按照该<a href="https://blog.csdn.net/jycjyc/article/details/103073869" target="_blank" rel="noopener">参考文章</a>的方式进行修复，同样无效。</p><p>由于上述方式均无效，考虑对docker服务的存储单独挂载的手段解决这个问题。之所以这样考虑是因为目前的只有根目录的情况下，docker中的overlay2存储和centos本体的xfs存储存在冲突。那么单独对docker进行存储分区，使用ext4的方式进行存储，将可能出现的问题进行隔离！</p><h3 id="1-创建存储盘并挂载"><a href="#1-创建存储盘并挂载" class="headerlink" title="1. 创建存储盘并挂载"></a>1. 创建存储盘并挂载</h3><pre><code>// 查看当前硬盘情况$ sudo fdisk -lDisk /dev/sda: 128.8 GB, 128849018880 bytes, 251658240 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000322d6   Device Boot      Start         End      Blocks   Id  System/dev/sda1   *        2048     2099199     1048576   83  Linux/dev/sda2         2099200    41943039    19921920   8e  Linux LVMDisk /dev/mapper/centos-root: 16.3 GB, 16299065344 bytes, 31834112 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 65536 bytes / 65536 bytes// 开始分区$ sudo fdisk /dev/sdaWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.// 创建新分区Command (m for help): nPartition type:   p   primary (2 primary, 0 extended, 2 free)   e   extendedSelect (default p): pPartition number (3,4, default 3): 3Partition number (3,4, default 3): 3First sector (41943040-251658239, default 41943040):Using default value 41943040// 确认新分区大小在50GLast sector, +sectors or +size{K,M,G} (41943040-251658239, default 251658239): +50GPartition 3 of type Linux and of size 50 GiB is set// 保存设定Command (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy.The kernel still uses the old table. The new table will be used atthe next reboot or after you run partprobe(8) or kpartx(8)Syncing disks.// 确认新的分区信息$ sudo fdisk -lDisk /dev/sda: 128.8 GB, 128849018880 bytes, 251658240 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000322d6   Device Boot      Start         End      Blocks   Id  System/dev/sda1   *        2048     2099199     1048576   83  Linux/dev/sda2         2099200    41943039    19921920   8e  Linux LVM/dev/sda3        41943040   146800639    52428800   83  LinuxDisk /dev/mapper/centos-root: 16.3 GB, 16299065344 bytes, 31834112 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 65536 bytes / 65536 bytes// 不重启机器使分区信息生效$ sudo partprobe /dev/sda// 创建ext4存储格式的分区$ sudo mkfs.ext4 /dev/sda3mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks3276800 inodes, 13107200 blocks655360 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=2162163712400 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks:        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,        4096000, 7962624, 11239424Allocating group tables: doneWriting inode tables: doneCreating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done$ cd /mnt &amp;&amp; sudo mkdir sda-docker// 挂载新分区$ sudo mount /dev/sda3 /mnt/sda-docker/// 查看分区生效$ df -hFilesystem               Size  Used Avail Use% Mounted ondevtmpfs                 7.8G     0  7.8G   0% /devtmpfs                    7.8G     0  7.8G   0% /dev/shmtmpfs                    7.8G  9.0M  7.8G   1% /runtmpfs                    7.8G     0  7.8G   0% /sys/fs/cgroup/dev/mapper/centos-root   16G  3.5G   12G  23% //dev/sda1               1014M  192M  823M  19% /boottmpfs                    1.6G     0  1.6G   0% /run/user/1000/dev/sda3                 50G   53M   47G   1% /mnt/sda-docker</code></pre><h3 id="2-备份docker的存储信息"><a href="#2-备份docker的存储信息" class="headerlink" title="2. 备份docker的存储信息"></a>2. 备份docker的存储信息</h3><pre><code>$ sudo systemctl stop docker$ sudo mv /var/lib/docker /var/lib/docker_data$ sudo mkdir /var/lib/docker// 将该文件夹/var/lib/docker的存储信息，放到/dev/sda3存储上$ sudo mount /dev/sda3 /var/lib/docker$ sudo mv /var/lib/docker_data/* /var/lib/docker/$ sudo systemctl restart docker// 验证一下是否真的存储到存储挂载的位置上了$ cd /mnt/sda-docker/$ sudo su# ls builder  buildkit  containers  image  lost+found  network  overlay2  plugins  runtimes  swarm  tmp  trust  volumes// 最后需要添加到开机后即进行挂载的情况$ sudo vim /etc/rc.d/rc.local// 在文件最后进行追加信息echo &quot;/dev/sda3 /var/lib/docker ext4 defaults 0 0&quot; &gt;&gt;/etc/fstabmount -a// :wq保存退出// 注意对该文件设定可执行权限，重启后才能生效$ sudo chmod +x /etc/rc.d/rc.local</code></pre><p>上述设置完成后，可以进行一次机器的重启，使所有内容生效！</p><p>需要查看docker中的相关文件时，可以先从/var/lib/docker路径下查看，也可以重新挂载/dev/sda3分区，如下：</p><pre><code>$ sudo mount /dev/sda3 /mnt/sda-docker/$ cd /mnt/sda-docker &amp;&amp; ls -al</code></pre><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>参考地址：</p><ul><li><a href="https://www.cnblogs.com/sangmu/p/6629594.html" target="_blank" rel="noopener">https://www.cnblogs.com/sangmu/p/6629594.html</a></li><li><a href="https://www.jianshu.com/p/960535ba1318" target="_blank" rel="noopener">https://www.jianshu.com/p/960535ba1318</a></li><li><a href="https://www.cnblogs.com/walter-xh/p/10590723.html" target="_blank" rel="noopener">https://www.cnblogs.com/walter-xh/p/10590723.html</a></li></ul><h2 id="关于nexus3作为npm私有仓库的问题"><a href="#关于nexus3作为npm私有仓库的问题" class="headerlink" title="关于nexus3作为npm私有仓库的问题"></a>关于nexus3作为npm私有仓库的问题</h2><ol><li><p>无法拉取代理的npm依赖包的问题</p></li><li><p>关于现有使用docker构建时，无法使用yarn的情况</p></li></ol><p>尝试分析nodejs的基础镜像，找到.npmrc文件，将nexus3的登录验证信息写入。以此重新构建基础镜像！</p><p>或者使用npm优先安装yarn，找到.yarnrc文件，将远程仓库的地址写入。</p><p>达成多阶段构建要素！</p><ol start="3"><li>nexus3配置匿名访问的注意事项</li></ol><p><img src="%E5%8C%BF%E5%90%8D%E8%AE%BF%E9%97%AE.png" alt></p><h2 id="内网环境下对于npm和yarn的设定"><a href="#内网环境下对于npm和yarn的设定" class="headerlink" title="内网环境下对于npm和yarn的设定"></a>内网环境下对于npm和yarn的设定</h2><p>在服务器中全局代理设置完成的背景下，设置npm工具和yarn的配置。</p><p>首先是npm的配置文件*<em>.npmrc</em>文件：</p><pre class="line-numbers language-.npmrc"><code class="language-.npmrc">registry=http://192.168.81.94:8081/repository/npm-test-group-all/email=npm-test@testsoft.comalways-auth=truestrict-ssl=false//192.168.81.94:8081/repository/npm-test-group-all/:_authToken=NpmToken.5a59b901-9df2-3489-8c6b-78d0f5ac3346sass-binary-site=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>具体设置方式:</p><pre><code>$ npm config set registry http://192.168.81.94:8081/repository/npm-test-group-all/$ npm config set sass-binary-site http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/$ npm config set always-auth true$ npm config set strict-ssl=false$ npm loginusername: npm-testpassword:Email: npm-test@testsoft.com</code></pre><p>然后是*<em>.yarnrc</em>文件：</p><pre class="line-numbers language-.yarnrc"><code class="language-.yarnrc"># THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.# yarn lockfile v1registry "http://192.168.81.94:8081/repository/npm-test-group-all/"email npm-test@testsoft.comlastUpdateCheck 1596709896121strict-ssl falseusername npm-usersass_binary_site http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>具体设置方式：</p><pre><code>$ yarn config set registry http://192.168.81.94:8081/repository/npm-test-group-all/$ yarn config set sass_binary_site http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/$ yarn config set always-auth true$ yarn config set strict-ssl=false</code></pre><h2 id="关于前端构建的问题"><a href="#关于前端构建的问题" class="headerlink" title="关于前端构建的问题"></a>关于前端构建的问题</h2><ol><li>node-sass安装问题</li></ol><p>利用离线的方式安装node-sass，区分在Linux下的构建和在windows构建的不同！</p><p>对package.json改造，修改preinstall选项，以及添加指定nodeSass的配置项，主要是binding-node的选项！</p><pre><code>npm install node-sass --sass_binary_path=D:\\node_modules\\win32-x64-64_binding.node</code></pre><p>貌似上述安装方式并不能对yarn起作用！</p><p>只能是学习Linux中的安装方式，安装python2，尝试自己编译node-sass需要的依赖信息！</p><p>最终解决方式：</p><p>利用nexus3代理仓库，配置对node-sass的代理，将淘宝镜像’<a href="https://npm.taobao.org/mirrors/node-sass/&#39;地址进行代理，如下：" target="_blank" rel="noopener">https://npm.taobao.org/mirrors/node-sass/&#39;地址进行代理，如下：</a></p><p><img src="node-sass%E4%BB%A3%E7%90%86.jpg" alt></p><p>然后针对node-sass在windows侧本地的安装，配置node-sass在nexus3中的代理地址。在<strong>.npmrc*或者</strong>.yarnrc*中设置，如下：</p><pre><code>// 追加下面的配置sass-binary-site=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/</code></pre><p>或者在命令行中配置</p><pre><code>（Linux） export SASS_BINARY_SITE=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/（windows） set SASS_BINARY_SITE=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/</code></pre><p>如果SASS_BINARY_SITE无法访问，需要自行下载binding.node文件，然后将该文件所在路径设置为SASS_BINARY_PATH，如下：</p><pre><code>（Linux） export SASS_BINARY_PATH=~/node_modules/win32-x64-64.binding.node（windows） set SASS_BINARY_PATH=D:\node_modules\win32-x64-64.binding.node</code></pre><p>最后，安装node-sass。</p><pre><code>$ npm install -g node-sass -D --verbose</code></pre><p><strong>针对yarn的设置</strong>，经常出现下载不到binding.node文件，下载不到node-headers.tar.gz的问题</p><p>经过长期验证，得出以下的步骤：</p><p>1.1 找到yarn的缓存位置</p><pre><code>$ yarn cache dir/home/centos/.cache/yarn/v6（Linux路径）D:\node_modules\cache\Yarn\Cache\v6</code></pre><p>1.2 找到node-sass所在的路径，这里使用Everything查找yarn的路径，找到名称为：<strong>npm-node-sass-4.14.1-xxxxxxxxxxxx-integrity</strong>文件夹。要注意该文件夹可能有多个，需要定位到上面找到的yarn缓存路径！</p><p>1.3 查看你需要的binding.node文件版本，需要对package.json进行改造，如下：</p><pre><code>// 在scripts下添加preinstall相关的执行命令&quot;scripts&quot;: {        &quot;preinstall&quot;: &quot;node -p \&quot;[process.platform, process.arch, process.versions.modules].join(&#39;-&#39;)\&quot; &quot;,        ......    },</code></pre><p>添加后再执行</p><pre><code>$ npm install// 输出下面的信息win32-x64-64</code></pre><p>1.3 进入到上面找到的node-sass缓存路径，到<a href="https://github.com/sass/node-sass/releases" target="_blank" rel="noopener">github地址</a>对应你系统版本的binding.node文件，找到<strong>win32-x64-64_binding.node</strong>，下载下来。</p><p>1.4 将上面下载的文件copy到上面的yarn缓存路径下的<strong>npm-node-sass-4.14.1-xxxxxxxxxxxx-integrity</strong>文件夹中。在该文件夹中创建:</p><pre><code>$ mkdir -p vendor/win32-x64-64</code></pre><p>然后将<strong>win32-x64-64_binding.node</strong>改名为<strong>binding.node</strong>，放到刚才创建的文件夹中，放到win32-x64-64之中！</p><p>1.5 最后执行</p><pre><code>$ yarn install --network-timeout=1000000 --verbose</code></pre><p>参考文章：</p><ul><li><p><a href="https://www.jianshu.com/p/8aab08de6243" target="_blank" rel="noopener">https://www.jianshu.com/p/8aab08de6243</a></p></li><li><p><a href="https://www.jianshu.com/p/947d050f98f2" target="_blank" rel="noopener">https://www.jianshu.com/p/947d050f98f2</a></p></li><li><p><a href="https://www.jianshu.com/p/24d2b118a3ce" target="_blank" rel="noopener">https://www.jianshu.com/p/24d2b118a3ce</a></p></li></ul><ol start="2"><li>代理仓库nexus3中依赖拉取的问题</li></ol><p>检查Linux服务器上关于代理转发的问题，配置/etc/profile中的代理信息，前文已经阐述了。</p><p>剩下的可以直接在命令行中设置。</p><ol start="3"><li>关于windows端代理的设置</li></ol><p>使用proxifier进行全局代理的设置，需要注意的是，启动该软件时必须使用管理员权限。否则无法进行代理操作！</p><p>除了全局代理之外，剩下的可以在命令行中使用set命令设置，如下：</p><pre><code>$ set http_proxy=http://192.168.81.90:8888$ set https_proxy=http://192.168.81.90:8888</code></pre><ol start="4"><li>在jenkins中进行前端构建，解决node-sass无法下载的问题</li></ol><p>在前端构建过程中，由于全程使用docker进行构建，导致我们无法像在windows开发机上那样，手动拷贝binding.node文件到缓存文件夹下的路径中。这样就必须更换其他方式。</p><p>之前在nexus3设置中，使用了maven类型的仓库来代理淘宝的node-sass中的dist镜像，后来发现这样做不正确，重新使用raw类型的仓库来代替，创建如下图的仓库信息：</p><p><img src="raw%E7%B1%BB%E5%9E%8B%E7%9A%84dist%E4%BB%A3%E7%90%86%E4%BB%93%E5%BA%93.jpg" alt></p><p>仓库创建完成后，下一步需要进行配置，在Dockerfile中配置node-sass依赖信息，在<strong>yarn install</strong>指令执行之前，添加node-sass本地仓库信息，如下：</p><pre><code>...// 添加下面的内容yarn install --verbose --network-timeout=1000000 --sass_binary_site=http://192.168.81.94:8081/repository/raw-proxy-taobao-dist/node-sass...</code></pre><p>这样设置完成后，触发jenkins的构建即可，后续看到可以正常拉取binding.node文件，这样就可以正常进行构建了！</p><p>构建完成后，是可以看到nexus3中已经拉取了相应的依赖信息，如下图：</p><p><img src="%E8%8E%B7%E5%8F%96%E7%9A%84node-sass%E7%BC%93%E5%AD%98%E5%88%B0nexus3%E4%B8%AD.jpg" alt></p><p>最后，附上完整的Dockerfile文件，如下：</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile"># 第一层面构建，打包前端代码#### 1. 指定node镜像版本FROM 192.168.81.94:5000/node:10.16.0 AS builder# 添加日期信息，如果需要更新缓存层，更新该处日期信息即可ENV REFRESH_DATE 2020-08-20_10:11# 2. 指定编译的工作空间WORKDIR /home/node/app# 3. 设置授权信息，本地开发机登录后获取私有镜像的authToken信息，放入.npmrcRUN touch ~/.npmrc && mkdir ~/node_global && mkdir ~/node_cache && echo prefix=~/node_global \n\cache=~/node_cache \n\registry=http://192.168.81.94:8081/repository/npm-test-group-all/ \n\//192.168.81.94:8081/repository/npm-test-group-all/:_authToken=NpmToken.5a59b901-9df2-3489-8c6b-78d0f5ac3346# 4. 构建之前清理缓存# RUN npm cache clean --force# COPY .npmrc /home/node/app# COPY .yarnrc /home/node/app# 4. 安装打包需要的yarn工具RUN npm --registry http://192.168.81.94:8081/repository/npm-test-group-all/ install -g yarn# 对yarn设置淘宝镜像RUN yarn config set registry http://192.168.81.94:8081/repository/npm-test-group-all/#RUN yarn config set sass_binary_site http://192.168.81.94:8081/repository/raw-proxy-taobao-dist/node-sassRUN mkdir .cache && yarn config set cache-folder /home/node/app/.cache/# 4. 添加package.jsonCOPY package.json /home/node/app/#COPY ./vendor/ /home/node/app/#ENV SASS_BINARTY_PATH /home/node/app/vendor/linux-x64-64_binding.node# 安装依赖信息# 注意这句，在后面跟上sass_binary_site的标记，指定从本地代理拉取binding.node文件！RUN yarn install --verbose --network-timeout=1000000 --sass_binary_site=http://192.168.81.94:8081/repository/raw-proxy-taobao-dist/node-sass# 6. 添加剩余代码到工作空间COPY . /home/node/app# 7. 编译代码RUN yarn run build# 第二层面构建#### 1.拉取自定义镜像名称FROM 192.168.81.94:5000/base_frontend:0.0.1# 2.将打包后的代码复制到运行位置COPY --from=builder /home/node/app/dist /var/www# 3.启动nginxENTRYPOINT ["nginx","-g","daemon off;"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参考地址：</p><ul><li><a href="https://blog.csdn.net/a910196454/article/details/106220132" target="_blank" rel="noopener">https://blog.csdn.net/a910196454/article/details/106220132</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>内网迁移-：方案与规划</title>
      <link href="/2020/08/21/nei-wang-qian-yi-fang-an-yu-gui-hua/"/>
      <url>/2020/08/21/nei-wang-qian-yi-fang-an-yu-gui-hua/</url>
      
        <content type="html"><![CDATA[<h2 id="操作的范围"><a href="#操作的范围" class="headerlink" title="操作的范围"></a>操作的范围</h2><ul><li><p>内外网互通的部分</p><ol><li>yum安装源</li><li>maven仓库，需要从外网拉取的部分</li><li>docker镜像仓库，需要从外网拉取的部分</li><li>npm镜像仓库，需要从外网拉取的部分</li><li>jenkins插件仓库、sonarqube插件仓库</li><li>各类工具的联网激活</li></ol></li></ul><p>解释：</p><ol><li><p>yum安装源：服务器的更新升级、必要工具如curl等日常运维工具的安装</p></li><li><p>maven仓库：本地jar包发布，从外网拉取依赖的jar包保存在本地</p></li><li><p>docker镜像仓库：构建产物发布，从外网拉取必要的镜像信息，提供k8s构建工具链</p></li><li><p>npm镜像仓库：本地组件包发布，从外网拉取的组件信息</p></li><li><p>jenkins插件仓库</p></li></ol><ul><li><p>需要迁移到内网的部分</p><ol start="0"><li><p>机器部分<br>各个服务器的迁移，需要区分迁移的范围</p></li><li><p>基础设施<br>gitlab、sonarqube、jenkins、docker registry、nexus</p></li><li><p>服务部分<br>k8s、k8s外部服务、已运行的服务信息</p></li></ol></li><li><p>跳板机部分、VPN</p></li></ul><p>新增一台高性能服务器做跳板机，需要创建新的可靠的VPN接入。</p><ul><li><p>需要注意的是</p><ol><li>尽量不变更现有虚拟机的ip地址信息，允许将ip段地址迁移到内网</li><li>配置存储工具信息，尽量将存储挂载到服务器上以提升性能</li></ol></li></ul><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><ol start="0"><li>操作系统安装，初始化，系统更新，k8s安装</li><li>统一maven仓库为例，验证内网拉取新的依赖信息是否成功</li><li>统一各个docker仓库，统一从该仓库拉取所有镜像，并且对仓库拉取外部的docker镜像进行缓存并进行安全检查</li><li>统一npm镜像仓库，迁移到内网中</li><li>备份当前机器。备份完成后开始迁移工作，要求ip不做变更</li><li>基础设施迁移，gitlab、jenkins、sonarqube、docker registry（或者Harbor）、nexus，不再使用docker镜像，全部更换为安装版运行</li><li>全面迁移运行环境，全面迁移开发机进入内网</li><li>内网与外部云服务的连通</li></ol><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><ol><li>梳理服务器涉及的所有外网地址，域名ip信息</li><li>梳理服务器需要进行的配置变更部分，例如docker的daemon.json、hosts文件</li><li>梳理基础服务部分需要进行的配置变更，例如jenkins拉取地址的变化</li><li>梳理各个服务需要进行的配置变更</li><li>当前环境的备份</li></ol><h2 id="潜在问题"><a href="#潜在问题" class="headerlink" title="潜在问题"></a>潜在问题</h2><ol><li>部分工具的激活，例如开发工具idea的激活，这部分工具在哪个位置存储</li><li>入站审计，例如某些破解软件是否可以放到内网环境使用，工具链使用范围</li><li>docker镜像的安全审计，代理的docker镜像源中拉取的镜像审计</li><li>个人外网无法连接问题，是否配置个人电脑连接外网，陈君、庞德镇需要配置台式机接入内网环境</li><li>迁移时，所有人工作停滞的问题，如何协调</li><li>如何安装新的工具到内网运行环境中</li><li>开发教学资源如何配置，API手册、javadoc、日常开发的需要搜索引擎资源查询时，如何解决</li><li>内网安全规则如何培训？原则是什么？</li><li>尚有其他部门的代码目前在我们的代码库中管理，对于他们的如何进行处理？</li><li>是否有代码审计工具保证内网运行稳定？例如使用FCA </li></ol><h2 id="验证阶段"><a href="#验证阶段" class="headerlink" title="验证阶段"></a>验证阶段</h2><ul><li><ol><li>内外网交互验证</li></ol></li></ul><p>以yum包管理工具和maven、npm依赖管理工具，验证内外网互通问题，以正常拉取安装工具为准</p><ul><li><ol start="2"><li>基本开发验证</li></ol></li></ul><p>以简单的前端和后端代码为例子，在内网环境配置并执行构建，部署到测试服务器上，以接口调用互通为准</p><ul><li><ol start="3"><li>基础设施流水线运行验证</li></ol></li></ul><p>将CI/CD流水线接入，验证自动化构建工具运行，排查对外访问的URL不受影响；开发机接入，CI/CD流水线运行验证，自动化构建推送到运行环境中，以所有服务连通为准</p><ul><li><ol start="4"><li>kubernetes集群环境验证</li></ol></li></ul><p>验证k8s集群安装的工作，验证集群运行，镜像拉取</p><ul><li><ol start="5"><li>真实开发案例验证</li></ol></li></ul><p>验证目前的某只业务功能，在内网的开发流程，以及能否顺利上线完成功能需求</p><h2 id="验证过程"><a href="#验证过程" class="headerlink" title="验证过程"></a>验证过程</h2><ol start="0"><li>资源需求（需要外部协助）</li></ol><p>一台域名服务器、一台带有双网卡的跳板机、两台内网运行环境验证机器（可以是虚拟机）、一台内网开发机、三台内网k8s运行环境验证机器（可以是虚拟机，第四阶段需要）</p><p>其中运行环境验证机器配置要求：4核心8G内存100GB硬盘千兆网卡，该配置适用于所有运行环境验证机器。开发机配置：4核心8G内存100GB硬盘百兆网卡。</p><p>域名服务器和跳板机目前无法给出配置，需要外部协助配置。</p><ol><li><p>第一阶段（需要9.5天）</p><ul><li><p>准备所需资源 —- 2天</p></li><li><p>安装操作系统（需要外部协助），域名服务器配置，跳板机配置（需要外部协助） —- 3天</p></li><li><p>Centos7配置yum源，初始化配置内网运行环境 —- 1天</p></li><li><p>复制maven源到运行环境，复制npm源到内网运行环境，nexus安装  —- 2天</p></li><li><p>在跳板机配置外网转发规则（需要外部协助）— 0.5天</p></li><li><p>测试yum安装常用工具、maven/npm拉取常用依赖包信息（需要外部协助）  —- 1天</p></li></ul></li><li><p>第二阶段（需要2.5天）</p><ul><li><p>编写单个后端项目，本地运行   —- 0.5天</p></li><li><p>编写本地项目，生成jar包，构建打包推送到内网运行环境，测试服务是否能通过开发机访问  — 0.5天</p></li><li><p>编写前端项目，打包生成运行文件，部署到内网运行环境 —- 1天</p></li><li><p>前后端联调，通过即为验证完成   —- 0.5天</p></li></ul></li><li><p>第三阶段（需要4天）</p><ul><li><p>基础设施进入内网环境进行部署，迁移数据到内网环境，部署完成后验证是否可用（3天）</p><ul><li>gitlab：能够拉取和提交代码</li><li>jenkins：能够触发构建，能够从gitlab中拉取代码，能够将构建产物推送到docker镜像仓库</li><li>docker镜像仓库：能够从中推送和拉取镜像</li><li>nexus: 能够拉取依赖包，能够推送生成的jar包</li></ul></li><li><p>基础设施联调，CI/CD流水线构建全部走通（1天）</p></li></ul></li><li><p>第四阶段（需要6天）</p><ul><li>添加三台内网k8s运行的验证机器（2天）</li><li>k8s原生安装验证，不再使用rancher（4天）</li></ul></li><li><p>第五阶段（需要2天）</p><ul><li>拿一支真实的开发案例来实验，从内网走通整个开发流程（2天）</li></ul></li></ol><h2 id="输出内容"><a href="#输出内容" class="headerlink" title="输出内容"></a>输出内容</h2><ul><li><p>验证问题文档，记录验证过程中出现的内容</p></li><li><p>安装包列表，需要准备的工具合集</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>前端微服务在jenkins下的构建改造</title>
      <link href="/2020/08/03/qian-duan-wei-fu-wu-zai-jenkins-xia-de-gou-jian-gai-zao/"/>
      <url>/2020/08/03/qian-duan-wei-fu-wu-zai-jenkins-xia-de-gou-jian-gai-zao/</url>
      
        <content type="html"><![CDATA[<h2 id="前端微服务–微前端"><a href="#前端微服务–微前端" class="headerlink" title="前端微服务–微前端"></a>前端微服务–微前端</h2><p>微前端架构是一种类似于微服务的架构，它将微服务的理念应用于浏览器端，即将 Web 应用由单一的单体应用转变为多个小型前端应用聚合为一的应用。由此带来的变化是，这些前端应用可以独立运行、独立开发、独立部署。以及，它们应该可以在共享组件的同时进行并行开发。</p><p>根据目前的发展情况，有两种实施方式</p><ul><li><p>前端模块各自运行调试，最终统一打包，继续以单体模式运行</p></li><li><p>前端模块各自以运行调试，分别打包部署，使用以下方式进行组织：</p><ol><li>使用 HTTP 服务器的路由来重定向多个应用</li><li>在不同的框架之上设计通讯、加载机制，诸如 Mooa 和 Single-SPA</li><li>通过组合多个独立应用、组件来构建一个单体应用</li><li>iFrame。使用 iFrame 及自定义消息传递机制</li><li>使用纯 Web Components 构建应用</li><li>结合 Web Components 构建</li></ol></li></ul><p>根据我们目前项目中，对于前端微服务采用第一种方式进行组织，这样更快一些，实现更简单，只能说实现各自的开发和运行，但是并不是真正的前端微服务范畴，只能叫前端组件化。</p><h2 id="jenkins的改造"><a href="#jenkins的改造" class="headerlink" title="jenkins的改造"></a>jenkins的改造</h2><h3 id="0-项目改造案例"><a href="#0-项目改造案例" class="headerlink" title="0. 项目改造案例"></a>0. 项目改造案例</h3><p>以一个项目切分不同的分支，最后再将开发的文件夹合并的方式进行改造。</p><p>这里的示例项目是test1.0-ui，然后涉及到的三个分支分别是：</p><ul><li>微前端-dev-base（基础构建分支）</li><li>微前端-dev-roleManagement（角色功能分支）</li><li>微前端-dev-menuManagement（菜单功能分支）</li></ul><p>分别拉取各个分支，首先将<em>微前端-dev-roleManagement</em>分支下的src/views/operationManagementCenter/roleManagement文件夹拷贝到<em>微前端-dev-base</em>分支的src/views/operationManagementCenter文件夹下。</p><p>然后同样的将<em>微前端-dev-roleManagement</em>分支下src/views/operationManagementCenter/menuManagement文件夹拷贝到<em>微前端-dev-base</em>分支的src/views/operationManagementCenter文件夹下。</p><p>最后在<em>微前端-dev-base</em>分支进行打包操作。</p><h3 id="1-插件支持和脚本编写"><a href="#1-插件支持和脚本编写" class="headerlink" title="1. 插件支持和脚本编写"></a>1. 插件支持和脚本编写</h3><p>基于之前的前端构建项目进行改造，使用pipeline的方式组织该项目的构建，利用docker镜像进行打包。</p><p>涉及的插件如下：</p><ul><li>Git</li><li>Git client 上面两个是拉取项目使用的</li><li>Pipeline  构建的核心工具</li><li>Kubernetes CLI</li><li>Kubernetes</li><li>Kubernetes Continuous Deploy Plugin k8s中的部署工具</li><li>Qy Wechat Notification  通知使用工具</li><li>SSH Pipeline Steps      pipeline中ssh访问工具</li><li>Webhook Step Plugin    触发webhook的工具</li></ul><p>下面开始编写脚本，如下：</p><pre class="line-numbers language-groovy"><code class="language-groovy">pipeline <span class="token punctuation">{</span>    agent any    stages<span class="token punctuation">{</span>        <span class="token function">stage</span><span class="token punctuation">(</span><span class="token string">"checkout and multiple"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            steps<span class="token punctuation">{</span>                <span class="token function">dir</span><span class="token punctuation">(</span>path<span class="token punctuation">:</span> <span class="token string">"./MainFrontEnd"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                    <span class="token function">git</span><span class="token punctuation">(</span>                        branch<span class="token punctuation">:</span> <span class="token string">'微前端-dev-base'</span><span class="token punctuation">,</span>                         credentialsId<span class="token punctuation">:</span> <span class="token string">'lisongyang-Test-CICD'</span><span class="token punctuation">,</span>                         url<span class="token punctuation">:</span> <span class="token string">'http://192.168.128.202:8181/test/frontend/c.git'</span><span class="token punctuation">,</span>                        changelog<span class="token punctuation">:</span> <span class="token boolean">true</span>                    <span class="token punctuation">)</span>                <span class="token punctuation">}</span>                <span class="token function">dir</span><span class="token punctuation">(</span>path<span class="token punctuation">:</span> <span class="token string">"./MicroService1"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                    <span class="token function">git</span><span class="token punctuation">(</span>                        branch<span class="token punctuation">:</span> <span class="token string">'微前端-dev-roleManagement'</span><span class="token punctuation">,</span>                         credentialsId<span class="token punctuation">:</span> <span class="token string">'lisongyang-Test-CICD'</span><span class="token punctuation">,</span>                         url<span class="token punctuation">:</span> <span class="token string">'http://192.168.128.202:8181/test/frontend/test1.0-ui.git'</span><span class="token punctuation">,</span>                        changelog<span class="token punctuation">:</span> <span class="token boolean">true</span>                    <span class="token punctuation">)</span>                <span class="token punctuation">}</span>                <span class="token function">dir</span><span class="token punctuation">(</span>path<span class="token punctuation">:</span> <span class="token string">"./MicroService2"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                    <span class="token function">git</span><span class="token punctuation">(</span>                        branch<span class="token punctuation">:</span> <span class="token string">'微前端-dev-menuManagement'</span><span class="token punctuation">,</span>                         credentialsId<span class="token punctuation">:</span> <span class="token string">'lisongyang-Test-CICD'</span><span class="token punctuation">,</span>                         url<span class="token punctuation">:</span> <span class="token string">'http://192.168.128.202:8181/test/frontend/test1.0-ui.git'</span><span class="token punctuation">,</span>                        changelog<span class="token punctuation">:</span> <span class="token boolean">true</span>                    <span class="token punctuation">)</span>                <span class="token punctuation">}</span>                sh  <span class="token string">"""                    cp -r ./MicroService1/src/views/operationManagementCenter/roleManagement ./MainFrontEnd/src/views/operationManagementCenter/                    cp -r ./MicroService2/src/views/operationManagementCenter/menuManagement ./MainFrontEnd/src/views/operationManagementCenter/                    """</span>            <span class="token punctuation">}</span>            post<span class="token punctuation">{</span>                success<span class="token punctuation">{</span>                    echo <span class="token string">"build success - stage 1"</span>                <span class="token punctuation">}</span>                failure<span class="token punctuation">{</span>                    echo <span class="token string">"build falure!!!! - stage 1"</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token function">stage</span><span class="token punctuation">(</span><span class="token string">"build all"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            steps<span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">//def imageName = "192.168.128.202:5000/test/test-build-frontend:${env.BUILD_NUMBER}"</span>                <span class="token function">dir</span><span class="token punctuation">(</span>path<span class="token punctuation">:</span> <span class="token string">"./MainFrontEnd"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                    <span class="token function">withDockerServer</span><span class="token punctuation">(</span><span class="token punctuation">[</span>uri<span class="token punctuation">:</span> <span class="token string">'tcp://192.168.128.202:2375'</span><span class="token punctuation">,</span> credentialsId<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                        <span class="token function">withDockerRegistry</span><span class="token punctuation">(</span>url<span class="token punctuation">:</span> <span class="token string">'192.168.128.202:5000'</span><span class="token punctuation">,</span> credentialsId<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                            <span class="token comment" spellcheck="true">// sh """</span>                            <span class="token comment" spellcheck="true">//     cd ${WORKSPACE}/MainFrontEnd</span>                            <span class="token comment" spellcheck="true">// """</span>                            script<span class="token punctuation">{</span>                                docker<span class="token operator">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token string">'192.168.128.202:5000/test/test-build-frontend:${BUILD_NUMBER}'</span><span class="token punctuation">,</span> <span class="token string">'-f ./Dockerfile-opt .'</span><span class="token punctuation">)</span><span class="token operator">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token punctuation">)</span>                            <span class="token punctuation">}</span>                        <span class="token punctuation">}</span>                    <span class="token punctuation">}</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            post<span class="token punctuation">{</span>                success<span class="token punctuation">{</span>                    echo <span class="token string">"build success - stage 2"</span>                 <span class="token punctuation">}</span>                failure<span class="token punctuation">{</span>                    echo <span class="token string">"build falure!!!! - stage 2"</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// stage("deploy from ssh"){</span>        <span class="token comment" spellcheck="true">//     steps{</span>        <span class="token comment" spellcheck="true">//     }</span>        <span class="token comment" spellcheck="true">//     post{</span>        <span class="token comment" spellcheck="true">//         success{</span>        <span class="token comment" spellcheck="true">//             echo "build success - stage 3"</span>        <span class="token comment" spellcheck="true">//         }</span>        <span class="token comment" spellcheck="true">//         failure{</span>        <span class="token comment" spellcheck="true">//             echo "build falure!!!! - stage 3"</span>        <span class="token comment" spellcheck="true">//         }</span>        <span class="token comment" spellcheck="true">//     }</span>        <span class="token comment" spellcheck="true">// }</span>        <span class="token comment" spellcheck="true">// 部署到k8s中的配置，看下面的内容</span>        <span class="token function">stage</span><span class="token punctuation">(</span><span class="token string">"deploy to kubernetes"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            steps<span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">//def imageName = "192.168.128.202:5000/test/test-build-frontend:${env.BUILD_NUMBER}"</span>                <span class="token function">dir</span><span class="token punctuation">(</span>path<span class="token punctuation">:</span> <span class="token string">"./MainFrontEnd"</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                    kubernetesDeploy configs<span class="token punctuation">:</span> <span class="token string">'frontend-opt-k8s.yaml'</span><span class="token punctuation">,</span> kubeConfig<span class="token punctuation">:</span> <span class="token punctuation">[</span>path<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kubeconfigId<span class="token punctuation">:</span> <span class="token string">'66-220-k8s-config'</span><span class="token punctuation">,</span> secretName<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">,</span> ssh<span class="token punctuation">:</span> <span class="token punctuation">[</span>sshCredentialsId<span class="token punctuation">:</span> <span class="token string">'*'</span><span class="token punctuation">,</span> sshServer<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">,</span> textCredentials<span class="token punctuation">:</span> <span class="token punctuation">[</span>certificateAuthorityData<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">,</span> clientCertificateData<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">,</span> clientKeyData<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">,</span> serverUrl<span class="token punctuation">:</span> <span class="token string">'https://'</span><span class="token punctuation">]</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            post<span class="token punctuation">{</span>                success<span class="token punctuation">{</span>                    echo <span class="token string">"build success - stage 3"</span>                <span class="token punctuation">}</span>                failure<span class="token punctuation">{</span>                    echo <span class="token string">"build falure!!!! - stage 3"</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>编写完成后，需要修改Dockerfile-opt（专门为docker内构建前端项目创建）以及k8s文件，最终如下。</p><ol><li>Dockerfile-opt文件：</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 第一层面构建，打包前端代码</span><span class="token comment" spellcheck="true">#### 1. 指定node镜像版本</span>FROM node:10.16.0 AS builder<span class="token comment" spellcheck="true"># 添加日期信息，如果需要更新缓存层，更新该处日期信息即可</span>ENV REFRESH_DATE 2020-05-20_11:11<span class="token comment" spellcheck="true"># 2. 指定编译的工作空间</span>WORKDIR /home/node/app<span class="token comment" spellcheck="true"># 3. 设置授权信息，本地开发机登录后获取私有镜像的authToken信息，放入.npmrc</span><span class="token comment" spellcheck="true"># RUN touch ~/.npmrc &amp;&amp; mkdir ~/node_global &amp;&amp; mkdir ~/node_cache &amp;&amp; echo prefix=~/node_global \n\ </span><span class="token comment" spellcheck="true"># cache=~/node_cache \n\ </span><span class="token comment" spellcheck="true"># registry=http://10.0.88.159:8081/repository/npm-group/ \n\ </span><span class="token comment" spellcheck="true"># //10.0.88.159:8081/repository/npm-group/:_authToken=NpmToken.161b4deb-11f8-3d95-ac27-30510480ae42 >> ~/.npmrc</span><span class="token comment" spellcheck="true"># 4. 构建之前清理缓存</span><span class="token comment" spellcheck="true"># RUN npm cache clean --force</span><span class="token comment" spellcheck="true"># 4. 安装打包需要的yarn工具</span>RUN <span class="token function">npm</span> --registry https://registry.npm.taobao.org <span class="token function">install</span> -g yarn<span class="token comment" spellcheck="true"># 对yarn设置淘宝镜像</span>RUN yarn config <span class="token keyword">set</span> registry https://registry.npm.taobao.orgRUN yarn config <span class="token keyword">set</span> disturl https://npm.taobao.org/dist<span class="token comment" spellcheck="true"># 设置不使用ssl，解决401保存问题</span><span class="token comment" spellcheck="true">#RUN npm config set strict-ssl false</span><span class="token comment" spellcheck="true"># 解决登录问题</span><span class="token comment" spellcheck="true">#RUN npm install -g npm-cli-adduser --registry https://registry.npm.taobao.org</span><span class="token comment" spellcheck="true"># 解决node-sass安装失败的问题</span><span class="token comment" spellcheck="true"># RUN yarn config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ -g</span><span class="token comment" spellcheck="true"># 解决错误信息info fsevents@2.1.3: The platform "linux" is incompatible with this module.</span><span class="token comment" spellcheck="true"># info "fsevents@2.1.3" is an optional dependency and failed compatibility check. Excluding it from installation.</span><span class="token comment" spellcheck="true">#RUN yarn config set ignore-engines true</span><span class="token comment" spellcheck="true"># 4. 添加package.json</span>COPY package.json /home/node/app/<span class="token comment" spellcheck="true"># 5. 安装依赖，如果package.json未变更，将会沿用之前的镜像缓存</span><span class="token comment" spellcheck="true">#RUN NPM_USER=npm-user NPM_PASS=test@2019 NPM_EMAIL=lisongyang@testsoft.com NPM_REGISTRY=http://10.0.88.159:8081/repository/npm-group/ npm-cli-adduser </span><span class="token comment" spellcheck="true"># 查看自身信息</span><span class="token comment" spellcheck="true"># RUN npm whoami</span><span class="token comment" spellcheck="true"># 安装依赖信息</span>RUN yarn <span class="token function">install</span> --registry https://registry.npm.taobao.org --network-timeout 1000000<span class="token comment" spellcheck="true"># 6. 添加剩余代码到工作空间</span>COPY <span class="token keyword">.</span> /home/node/app<span class="token comment" spellcheck="true"># 7. 编译代码</span>RUN yarn run build<span class="token comment" spellcheck="true"># 第二层面构建</span><span class="token comment" spellcheck="true">#### 1.拉取自定义镜像名称</span>FROM 10.0.88.159:5000/base_frontend:0.0.1<span class="token comment" spellcheck="true"># 2.将打包后的代码复制到运行位置</span>COPY --from<span class="token operator">=</span>builder /home/node/app/dist /var/www<span class="token comment" spellcheck="true"># 3.启动nginx</span>ENTRYPOINT <span class="token punctuation">[</span><span class="token string">"nginx"</span>,<span class="token string">"-g"</span>,<span class="token string">"daemon off;"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>k8s部署文件：</li></ol><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>frontend  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>frontend<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> tcp      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">31080</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>frontend<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>frontend  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>frontend  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>frontend    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>        <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>          <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>                <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname                <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                  <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                    <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app                      <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                      <span class="token key atrule">values</span><span class="token punctuation">:</span>                        <span class="token punctuation">-</span> app<span class="token punctuation">-</span>test<span class="token punctuation">-</span>frontend              <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">1</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>frontend          <span class="token key atrule">image</span><span class="token punctuation">:</span> 192.168.128.202<span class="token punctuation">:</span>5000/test/test<span class="token punctuation">-</span>build<span class="token punctuation">-</span>frontend<span class="token punctuation">:</span>$BUILD_NUMBER          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">resources</span><span class="token punctuation">:</span>            <span class="token key atrule">requests</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1024Mi            <span class="token key atrule">limits</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1.5Gi          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> GATEWAY_HOST              <span class="token key atrule">value</span><span class="token punctuation">:</span> 192.168.128.221<span class="token punctuation">:</span><span class="token number">31333</span>              <span class="token comment" spellcheck="true">#value: gateway.test-basic-gateway:19020 </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要是调整了镜像名称以及网关地址。</p><h3 id="2-问题：关于docker的安装"><a href="#2-问题：关于docker的安装" class="headerlink" title="2. 问题：关于docker的安装"></a>2. 问题：关于docker的安装</h3><p>在构建前端服务时，出现了docker服务找不到的情况，这时候有两种解决方式：</p><ul><li>重建jenkins容器，关联本地的docker服务</li></ul><p>在该方法中，使用下面的命令重建jenkins容器：</p><pre class="line-numbers language-shell"><code class="language-shell">docker run  -d  -u root  -v /usr/bin/docker:/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7 jenkins/jenkins:2.208<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>启动后，通过thinbackup工具备份之前的jenkins构建信息，然后在新的jenkins服务中进行恢复即可。</p><p>这样虽然可以使用宿主机的docker服务，但是由于宿主机中，docker内部有些功能需要使用到root权限，挂载后容器内的命令会影响到host，会导致安全风险！不建议使用。</p><ul><li>在容器内安装docker服务</li></ul><p>在容器内安装，只可以在容器内使用，对外进行隔离，不影响外部的docker运行。</p><p>下面开始容器内安装docker服务：</p><ol><li>安装apt-transport-https模块</li></ol><p>这里初步使用apt update进行更新时，卡在”[0%] Working”的位置，不往下走。这时候先验证无网络问题，”ping baidu.com”操作一下。</p><p>首先离线安装apt-transport-https，从<a href="http://archive.ubuntu.com/ubuntu/pool/main/a/apt/apt-transport-https_1.2.32ubuntu0.1_amd64.deb" target="_blank" rel="noopener">改地址</a>下载离线安装包，传输到docker容器内。</p><pre><code>$ docker cp apt-transport-https_1.2.32ubuntu0.1_amd64.deb jenkins_official:/var/jenkins_home/$ $ docker exec -it -u root jenkins_official /bin/bashroot@cbbc3907c1bc: cd /var/jenkins_home/ &amp;&amp; dpkg -i apt-transport-https_1.2.32ubuntu0.1_amd64.deb</code></pre><p>排除网络问题后，说明外网可以联通，这时候再去尝试修改源文件信息，参考<a href="Linux系统初始化以及部署.md">Linux系统初始化以及部署</a>文档，修改源信息。由于这里Dockerfile编写的时候使用的是ubuntu，更改会稍有不同。操作如下：</p><pre class="line-numbers language-shell"><code class="language-shell">// 离线安装apt-transport-https// 外部编写sources.list文件$ vim sources.list// 输入以下内容​deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse// :wq保存退出// 拷贝新的源文件到镜像中$ docker cp sources.list jenkins_official:/var/jenkins_home/// 以root用户进行镜像$ docker exec -it -u root jenkins_official /bin/bashroot@cbbc3907c1bc: cd /etc/apt/root@cbbc3907c1bc: mv sources.list sources.list.bak// 把新的文件保存到这个位置root@cbbc3907c1bc:/etc/apt# cp /var/jenkins_home/sources.list ./// 开始更新root@cbbc3907c1bc:/etc/apt# apt-get update// 出现了错误信息如下W: GPG error: http://mirrors.aliyun.com/ubuntu xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 40976EAF437D05B5 NO_PUBKEY 3B4FE6ACC0B21F32W: The repository 'http://mirrors.aliyun.com/ubuntu xenial InRelease' is not signed.// 更新下密钥信息root@cbbc3907c1bc:/etc/apt# gpg --keyserver keyserver.ubuntu.com --recv 40976EAF437D05B5root@cbbc3907c1bc:/etc/apt# gpg --export --armor 40976EAF437D05B5 | apt-key add -root@cbbc3907c1bc:/etc/apt# gpg --keyserver keyserver.ubuntu.com --recv 3B4FE6ACC0B21F32root@cbbc3907c1bc:/etc/apt# gpg --export --armor 3B4FE6ACC0B21F32 | apt-key add -root@cbbc3907c1bc:/etc/apt# apt-get updateGet:1 http://mirrors.aliyun.com/ubuntu xenial InRelease [247 kB]Get:2 http://mirrors.aliyun.com/ubuntu xenial-security InRelease [109 kB]Get:3 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease [109 kB]Get:4 http://mirrors.aliyun.com/ubuntu xenial-proposed InRelease [260 kB]Get:5 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB]Fetched 832 kB in 1s (529 kB/s)Reading package lists... Done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后安装docker，遵循以下步骤进行：</p><pre><code>root@cbbc3907c1bc:/etc/apt#  apt-get -y install apt-transport-https \     ca-certificates \     curl \     gnupg2 \     software-properties-common root@cbbc3907c1bc:/etc/apt# curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo &quot;$ID&quot;)/gpg &gt; /tmp/dkey; apt-key add /tmp/dkeyroot@cbbc3907c1bc:/etc/apt# add-apt-repository \   &quot;deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo &quot;$ID&quot;) \   $(lsb_release -cs) \   stable&quot;root@cbbc3907c1bc:/etc/apt# apt-get updateroot@cbbc3907c1bc:/etc/apt# apt-get -y install docker-ce</code></pre><p>安装完成后可用，这时候在jenkins中可以正常使用docker程序了。</p><p>注意：这时候systemctl并不可用，而且docker也不能配置为服务，但是不影响docker在pipeline中的使用。</p><h3 id="3-问题：关于yarn安装依赖卡在-3-4-Linking-dependencies…"><a href="#3-问题：关于yarn安装依赖卡在-3-4-Linking-dependencies…" class="headerlink" title="3. 问题：关于yarn安装依赖卡在[3/4] Linking dependencies…"></a>3. 问题：关于yarn安装依赖卡在[3/4] Linking dependencies…</h3><p>参考链接：<a href="https://stackoverflow.com/questions/50683248/what-does-linking-dependencies-during-npm-yarn-install-really-do" target="_blank" rel="noopener">https://stackoverflow.com/questions/50683248/what-does-linking-dependencies-during-npm-yarn-install-really-do</a></p><p>解决方式：尝试添加本地的缓存地址，缓存依赖包信息。</p><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><p>关于docker安装的参考地址：</p><ul><li><a href="https://blog.csdn.net/qq_40460909/article/details/83307369" target="_blank" rel="noopener">https://blog.csdn.net/qq_40460909/article/details/83307369</a></li><li><a href="https://www.jianshu.com/p/88b5f436ffb0" target="_blank" rel="noopener">https://www.jianshu.com/p/88b5f436ffb0</a></li></ul><h2 id="重建jenkins镜像适应前端的构建行为"><a href="#重建jenkins镜像适应前端的构建行为" class="headerlink" title="重建jenkins镜像适应前端的构建行为"></a>重建jenkins镜像适应前端的构建行为</h2><p>从github拉取jenkins的镜像的构建源文件，然后做出以下的修改：</p><ul><li>切换sources.list，更换为国内源</li><li>安装maven并设置环境变量</li><li>安装nodejs以及必要的依赖包</li><li>安装docker程序包</li><li>插件预装</li></ul><p>Dockerfile的编写</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>jenkins的备份和迁移</title>
      <link href="/2020/08/02/jenkins-de-bei-fen-he-qian-yi/"/>
      <url>/2020/08/02/jenkins-de-bei-fen-he-qian-yi/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="jenkins导出镜像"><a href="#jenkins导出镜像" class="headerlink" title="jenkins导出镜像"></a>jenkins导出镜像</h2><pre><code>$ docker commit jenkins_official jenkins/jenkins:2.208$ docker export jenkins_official &gt; jenkins_2.208.img$ scp -P 15555 jenkins_2.208.img centos@192.168.88.180:~/</code></pre><p>用上述方式导出后，发送到新的机器上时，然后尝试启动，发现不成功，如下：</p><pre><code>// 在另外一台机器上执行$ docker import - jenkins/jenkins:2.208 &lt; jenkins_2.208.img$ docker run -d --name jenkins_official -v /home/centos/docker/jenkins_home:/jenkins_home -v /home/centos/maven/apache-maven-3.6.2:/data/maven3.6.2 -p 50001:50000 -p 9090:8080 --restart=always jenkins/jenkins:2.208docker: Error response from daemon: No command specified.See &#39;docker run --help&#39;.</code></pre><p>出现上述问题后，查看整个镜像的描述信息，如下:</p><pre><code>$ docker inspect b437[    {        &quot;Id&quot;: &quot;sha256:b43728bd35394ed1fd5a982fbaad893293cb926f2db00e395ff970a3f40a7653&quot;,        &quot;RepoTags&quot;: [            &quot;jenkins/jenkins:2.208&quot;        ],        &quot;RepoDigests&quot;: [],        &quot;Parent&quot;: &quot;&quot;,        &quot;Comment&quot;: &quot;Imported from -&quot;,        &quot;Created&quot;: &quot;2020-05-28T02:00:19.9158622Z&quot;,        &quot;Container&quot;: &quot;&quot;,        &quot;ContainerConfig&quot;: {            &quot;Hostname&quot;: &quot;&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;Tty&quot;: false,            &quot;OpenStdin&quot;: false,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: null,            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;&quot;,            &quot;Volumes&quot;: null,            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: null,            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: null        },        &quot;DockerVersion&quot;: &quot;19.03.5&quot;,        &quot;Author&quot;: &quot;&quot;,        &quot;Config&quot;: {            &quot;Hostname&quot;: &quot;&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;Tty&quot;: false,            &quot;OpenStdin&quot;: false,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: null,            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;&quot;,            &quot;Volumes&quot;: null,            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: null,            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: null        },        &quot;Architecture&quot;: &quot;amd64&quot;,        &quot;Os&quot;: &quot;linux&quot;,        &quot;Size&quot;: 742766101,        &quot;VirtualSize&quot;: 742766101,        &quot;GraphDriver&quot;: {            &quot;Data&quot;: {                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/d890ad5267d6e09d4c4e6d9264b85a282ffdf619b0f399de94ae685c17f9ba10/merged&quot;,                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/d890ad5267d6e09d4c4e6d9264b85a282ffdf619b0f399de94ae685c17f9ba10/diff&quot;,                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/d890ad5267d6e09d4c4e6d9264b85a282ffdf619b0f399de94ae685c17f9ba10/work&quot;            },            &quot;Name&quot;: &quot;overlay2&quot;        },        &quot;RootFS&quot;: {            &quot;Type&quot;: &quot;layers&quot;,            &quot;Layers&quot;: [                &quot;sha256:613397f3accc61821ab378b654c7f09f84dad36d603bd121a355a2d53c569335&quot;            ]        },        &quot;Metadata&quot;: {            &quot;LastTagTime&quot;: &quot;2020-05-28T10:00:20.275661145+08:00&quot;        }    }]</code></pre><p>可以看到镜像的cmd部分，命令为null，En导致启动失败。因此需要更换镜像的导出方式。</p><p>在执行docker commit命令后，使用其它的方式导出信息。</p><pre><code>// 在之前的机器上执行$ docker save c1746bb41658 &gt; jenkins.tar$ scp -P 15555 jenkins.tar centos@192.168.88.180:~/</code></pre><p>传输完成后，在后续机器上导入</p><pre><code>// 在另一台机器上执行$ docker load &lt; jenkins.tare4b20fcc48f4: Loading layer [==================================================&gt;]  105.6MB/105.6MB91ecdd7165d3: Loading layer [==================================================&gt;]   24.1MB/24.1MB73bfa217d66f: Loading layer [==================================================&gt;]  8.005MB/8.005MB5f3a5adb8e97: Loading layer [==================================================&gt;]  146.4MB/146.4MB9a11244a7e74: Loading layer [==================================================&gt;]   10.1MB/10.1MBb18043518924: Loading layer [==================================================&gt;]  3.584kB/3.584kB2ee490fbc316: Loading layer [==================================================&gt;]  205.6MB/205.6MB60fe639d301d: Loading layer [==================================================&gt;]  338.9kB/338.9kB167c0ea1e843: Loading layer [==================================================&gt;]  3.584kB/3.584kB37e1fe95171e: Loading layer [==================================================&gt;]  9.728kB/9.728kB65c4c6a6feb6: Loading layer [==================================================&gt;]  868.9kB/868.9kB811d818689d8: Loading layer [==================================================&gt;]  62.76MB/62.76MB06e095115ef5: Loading layer [==================================================&gt;]  3.584kB/3.584kBed68bc14f08b: Loading layer [==================================================&gt;]  9.728kB/9.728kB581bcef05992: Loading layer [==================================================&gt;]   5.12kB/5.12kB94dc46c85541: Loading layer [==================================================&gt;]  3.072kB/3.072kB87e68b9bebe3: Loading layer [==================================================&gt;]  7.168kB/7.168kB557f852b13c6: Loading layer [==================================================&gt;]  13.82kB/13.82kBd9caa2edb930: Loading layer [==================================================&gt;]  199.9MB/199.9MBLoaded image ID: sha256:c1746bb416581b8a9b81fee02ebb355a681752ffe9d38ba7a5adba02b89d3fc3$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE&lt;none&gt;              &lt;none&gt;              c1746bb41658        24 hours ago        750MBgitlab/gitlab-ce    12.2.5_101          3925ebbdc4d6        25 hours ago        1.75GBgitlab/gitlab-ce    12.2.5              a574a1869b14        4 months ago        1.75GB$ docker tag  c1746bb41658 jenkins/jenkins:2.208$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEjenkins/jenkins     2.208               c1746bb41658        24 hours ago        750MBgitlab/gitlab-ce    12.2.5_101          3925ebbdc4d6        25 hours ago        1.75GBgitlab/gitlab-ce    12.2.5              a574a1869b14        4 months ago        1.75GB$ docker inspect c1746bb41658[    {        &quot;Id&quot;: &quot;sha256:c1746bb416581b8a9b81fee02ebb355a681752ffe9d38ba7a5adba02b89d3fc3&quot;,        &quot;RepoTags&quot;: [            &quot;jenkins/jenkins:2.208&quot;        ],        &quot;RepoDigests&quot;: [],        &quot;Parent&quot;: &quot;&quot;,        &quot;Comment&quot;: &quot;&quot;,        &quot;Created&quot;: &quot;2020-05-27T02:18:31.310227817Z&quot;,        &quot;Container&quot;: &quot;cbbc3907c1bcbd577722d0dbed4459a9af5b5360e3bedbc03094528b3a0d2841&quot;,        &quot;ContainerConfig&quot;: {            &quot;Hostname&quot;: &quot;cbbc3907c1bc&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;jenkins&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;ExposedPorts&quot;: {                &quot;50000/tcp&quot;: {},                &quot;8080/tcp&quot;: {}            },            &quot;Tty&quot;: true,            &quot;OpenStdin&quot;: true,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: [                &quot;PATH=/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,                &quot;LANG=C.UTF-8&quot;,                &quot;JAVA_HOME=/usr/local/openjdk-8&quot;,                &quot;JAVA_VERSION=8u232&quot;,                &quot;JAVA_BASE_URL=https://github.com/AdoptOpenJDK/openjdk8-upstream-binaries/releases/download/jdk8u232-b09/OpenJDK8U-jdk_&quot;,                &quot;JAVA_URL_VERSION=8u232b09&quot;,                &quot;JENKINS_HOME=/var/jenkins_home&quot;,                &quot;JENKINS_SLAVE_AGENT_PORT=50000&quot;,                &quot;REF=/usr/share/jenkins/ref&quot;,                &quot;JENKINS_VERSION=2.208&quot;,                &quot;JENKINS_UC=https://updates.jenkins.io&quot;,                &quot;JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental&quot;,                &quot;JENKINS_INCREMENTALS_REPO_MIRROR=https://repo.jenkins-ci.org/incrementals&quot;,                &quot;COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log&quot;            ],            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;jenkins/jenkins&quot;,            &quot;Volumes&quot;: {                &quot;/var/jenkins_home&quot;: {}            },            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: [                &quot;/sbin/tini&quot;,                &quot;--&quot;,                &quot;/usr/local/bin/jenkins.sh&quot;            ],            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: {}        },        &quot;DockerVersion&quot;: &quot;19.03.2&quot;,        &quot;Author&quot;: &quot;&quot;,        &quot;Config&quot;: {            &quot;Hostname&quot;: &quot;cbbc3907c1bc&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;jenkins&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;ExposedPorts&quot;: {                &quot;50000/tcp&quot;: {},                &quot;8080/tcp&quot;: {}            },            &quot;Tty&quot;: true,            &quot;OpenStdin&quot;: true,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: [                &quot;PATH=/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,                &quot;LANG=C.UTF-8&quot;,                &quot;JAVA_HOME=/usr/local/openjdk-8&quot;,                &quot;JAVA_VERSION=8u232&quot;,                &quot;JAVA_BASE_URL=https://github.com/AdoptOpenJDK/openjdk8-upstream-binaries/releases/download/jdk8u232-b09/OpenJDK8U-jdk_&quot;,                &quot;JAVA_URL_VERSION=8u232b09&quot;,                &quot;JENKINS_HOME=/var/jenkins_home&quot;,                &quot;JENKINS_SLAVE_AGENT_PORT=50000&quot;,                &quot;REF=/usr/share/jenkins/ref&quot;,                &quot;JENKINS_VERSION=2.208&quot;,                &quot;JENKINS_UC=https://updates.jenkins.io&quot;,                &quot;JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental&quot;,                &quot;JENKINS_INCREMENTALS_REPO_MIRROR=https://repo.jenkins-ci.org/incrementals&quot;,                &quot;COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log&quot;            ],            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;jenkins/jenkins&quot;,            &quot;Volumes&quot;: {                &quot;/var/jenkins_home&quot;: {}            },            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: [                &quot;/sbin/tini&quot;,                &quot;--&quot;,                &quot;/usr/local/bin/jenkins.sh&quot;            ],            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: {}        },        &quot;Architecture&quot;: &quot;amd64&quot;,        &quot;Os&quot;: &quot;linux&quot;,        &quot;Size&quot;: 749645095,        &quot;VirtualSize&quot;: 749645095,        &quot;GraphDriver&quot;: {            &quot;Data&quot;: {                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/14e74494b7d5e6f6edade6abcd59118440346fa48d151642f4d6ca84a2763899/diff:/var/lib/docker/overlay2/160ca4b02111a4e91dd7cb72c8d903da556aa85b79121994c71d0f25dafecb6b/diff:/var/lib/docker/overlay2/768dac08d447e865a9456ea862b6fe8242e15bfc127e645e6136665b6e1e50ab/diff:/var/lib/docker/overlay2/5e63415ab060f1df0a4436a609cc277a68fb582ee04ac45479e45502703dbd1f/diff:/var/lib/docker/overlay2/1b3ddec07d1159647ab55af7dc1f413d314a7dfb140ab3cb3a10e73e6547707a/diff:/var/lib/docker/overlay2/d6d40054166e07dc1b81b01caeb373e27ccb2fabd1c438172a026f0b2fd14cee/diff:/var/lib/docker/overlay2/c8a667802affe38898c86cee24c17c911698536de7690cd6c00ba94d6b5a0e32/diff:/var/lib/docker/overlay2/46de4645b03414a8345932dac971c155077749f159bd95121bd94a9c61dbc332/diff:/var/lib/docker/overlay2/8820f1a5bd612cb450024dd06838d53e90bfc6b3905835333c5d086ae4718152/diff:/var/lib/docker/overlay2/1c45197624d037e0c5304be33a89b15f399f8aef5427f0052bd98c8f4e846e4a/diff:/var/lib/docker/overlay2/58108e3d56074702831819adc017cd76bf48c3e381c1926b5506c72d724d5619/diff:/var/lib/docker/overlay2/5aae50ad403940eb6665853bacc95f3b26fd107de435418460dbe5e7aeb2314a/diff:/var/lib/docker/overlay2/cc94aec27d7388468a38efc7d6a9fae4979e4fb9e8d095a4abe24d0b9545366b/diff:/var/lib/docker/overlay2/52e8cfa0b28387eb79384858333e4889fe7fe652c9c0ad11b81840aa9078057d/diff:/var/lib/docker/overlay2/538fe5e1f97fd5db5730c092d7c92cb5e7985d0cceb11fe5c488b935dfc3ec6d/diff:/var/lib/docker/overlay2/3cdfcac471d7bb04d6f1936f704d8c034ce36643f8da440791c1b940c7c0e639/diff:/var/lib/docker/overlay2/81604d5d12d8b8f266f6ec5e4fe17bb09c4be1b8f182737d80b6490b2bbbe602/diff:/var/lib/docker/overlay2/e38dbbc004152e1f7e6a3c325193ae1e9fe3388c853eecf46df8e26c730b6cec/diff&quot;,                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/7a8f041bc7472f86d7a15a6faf72dd565b89535f93551470e3ad1084941724e4/merged&quot;,                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/7a8f041bc7472f86d7a15a6faf72dd565b89535f93551470e3ad1084941724e4/diff&quot;,                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/7a8f041bc7472f86d7a15a6faf72dd565b89535f93551470e3ad1084941724e4/work&quot;            },            &quot;Name&quot;: &quot;overlay2&quot;        },        &quot;RootFS&quot;: {            &quot;Type&quot;: &quot;layers&quot;,            &quot;Layers&quot;: [                &quot;sha256:e4b20fcc48f4a225fd29ce3b3686cc51042ce1f076d88195b3705b5bb2f38c3d&quot;,                &quot;sha256:91ecdd7165d37f7ac4fb6051632936b95da73973f0261785e162b555458f4592&quot;,                &quot;sha256:73bfa217d66f35520cce481c1fbde51cbdba48113f549c41b68b33eca7bc1f0c&quot;,                &quot;sha256:5f3a5adb8e97eab0570fd2d83a112f270cb2d0b6e32cbb6b6770ebe7a3e9678e&quot;,                &quot;sha256:9a11244a7e74c1be586f2029f6ca6d9359afd5657403eec6fe6cf6658907427d&quot;,                &quot;sha256:b18043518924251be33af630922e6357383dc7e723864c79ba2e347197e5ce95&quot;,                &quot;sha256:2ee490fbc316cec390e0e2409b26d698f4825d673e14cf41055412accd902f1f&quot;,                &quot;sha256:60fe639d301de3b394e6adb800378832c86091e72ac712a5a4415a2bc1fff884&quot;,                &quot;sha256:167c0ea1e843e48675d88bd4d7ead1bf07c859e02c5b81384af4bad31ebcbd35&quot;,                &quot;sha256:37e1fe95171e3eb1b41b50ce512552635aed8879562bb55569ef10a494712cf0&quot;,                &quot;sha256:65c4c6a6feb60e4277a6d46ec7217ca5e89e68eeaa4bf20be2b4a38c58f441d6&quot;,                &quot;sha256:811d818689d836d0d7f247ee56d26518b000738a49685f87b39b104991dc2b05&quot;,                &quot;sha256:06e095115ef52045c10ad6d99d735f941eea9987fc01cd695da9589e66f93985&quot;,                &quot;sha256:ed68bc14f08ba0dc6c4b318819b10ca57145c63ac25cc9d95864bd0b3a9952bb&quot;,                &quot;sha256:581bcef0599240aa369b87716e81be3f70487b08476da5b1850e5fd303c955a4&quot;,                &quot;sha256:94dc46c85541c011448c5871401d83bec2589ebf5a4d28f6e00c70bea18d7af0&quot;,                &quot;sha256:87e68b9bebe3ec7ca616656bdba5c24b17ea1ac4795a86c5bf64e4001ed24e96&quot;,                &quot;sha256:557f852b13c6f82a43724259d13b6c36dff30f29b1b9493f586be1a061820ac2&quot;,                &quot;sha256:d9caa2edb9309e9e78275238d80c17ccce4d0a74dc9c2cef623f3f8d57945e7e&quot;            ]        },        &quot;Metadata&quot;: {            &quot;LastTagTime&quot;: &quot;2020-05-28T10:18:14.372683313+08:00&quot;        }    }]</code></pre><p>这样导出再导入的时候，Entrypoint就不为null了，重新启动jenkins尝试，如下：</p><pre><code>$ docker run -d --name jenkins_official -v /home/centos/docker/jenkins_home:/var/jenkins_home -v /home/centos/maven/apache-maven-3.6.2:/data/maven3.6.2 -p 50001:50000 -p 9090:8080 --restart=always jenkins/jenkins:2.208</code></pre><h2 id="查看jenkins容器的启动命令"><a href="#查看jenkins容器的启动命令" class="headerlink" title="查看jenkins容器的启动命令"></a>查看jenkins容器的启动命令</h2><p>使用插件查看</p><p>启动命令<br>docker run -d –name jenkins_official -v /home/centos/docker/jenkins_home:/var/jenkins_home -v /home/centos/maven/apache-maven-3.6.2:/data/maven3.6.2 -p 50001:50000/tcp -p 9090:8080/tcp –restart always -e ‘LANG=C.UTF-8’ jenkins/jenkins</p><h2 id="创建运行的文件夹"><a href="#创建运行的文件夹" class="headerlink" title="创建运行的文件夹"></a>创建运行的文件夹</h2><p>maven组件、创建文件夹，拷贝</p><h2 id="导入镜像到新机器中并启动"><a href="#导入镜像到新机器中并启动" class="headerlink" title="导入镜像到新机器中并启动"></a>导入镜像到新机器中并启动</h2><h2 id="初始化设置"><a href="#初始化设置" class="headerlink" title="初始化设置"></a>初始化设置</h2><p>按照之前的jenkins设置为一致，不安装插件，直接跳过。</p><p>配置插件安装地址</p><p>安装thinbackup插件</p><p>初始化设置thinbackup</p><h2 id="备份原有镜像内容"><a href="#备份原有镜像内容" class="headerlink" title="备份原有镜像内容"></a>备份原有镜像内容</h2><p>使用thinbackup的备份方式，点击BackupNow，将备份后的文件夹进行压缩，传输到目标机器中。</p><p><strong>注意：</strong> 在jenkins恢复时，在网页上无法判断是否恢复完毕，甚至存在浏览器不停转圈的情况。</p><p>面对这种情况，可以直接从docker镜像的日志中查看恢复进度，如下：</p><pre><code>$ docker logs -f --tail=100 jenkins_official2020-06-29 01:25:13.774+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;ssh-credentials&#39;.2020-06-29 01:25:13.779+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;workflow-job&#39;.2020-06-29 01:25:13.784+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;workflow-basic-steps&#39;.2020-06-29 01:25:13.788+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;jsch&#39;.2020-06-29 01:25:13.789+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;github-api&#39;.2020-06-29 01:25:13.790+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;gitlab-merge-request-jenkins&#39;.2020-06-29 01:25:13.795+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;workflow-cps&#39;.2020-06-29 01:25:13.796+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: R.................................2020-06-29 01:29:25.522+0000 [id=76]    INFO    h.model.UpdateCenter$DownloadJob#run: Starting the installation of pubsub-light on behalf of admin2020-06-29 01:29:26.737+0000 [id=76]    INFO    h.m.UpdateCenter$UpdateCenterConfiguration#download: Downloading pubsub-light2020-06-29 01:29:27.046+0000 [id=71]    INFO    o.j.h.p.t.ThinBackupMgmtLink#doRestore: Restore finished.</code></pre><p>直到日志中出现<em>Restore finished.</em>即为恢复完成！</p><h2 id="重新覆盖新机器中的镜像信息"><a href="#重新覆盖新机器中的镜像信息" class="headerlink" title="重新覆盖新机器中的镜像信息"></a>重新覆盖新机器中的镜像信息</h2><p>通过thinbackup进行恢复，拷贝文件到文件夹中</p><p>文件解压，回到页面上点击restore</p><h2 id="迁移后的剩余问题"><a href="#迁移后的剩余问题" class="headerlink" title="迁移后的剩余问题"></a>迁移后的剩余问题</h2><ol><li>jenkins的URL地址进行变更，变更为目标服务器的</li></ol><p>重新设置jenkins的URL地址</p><ol start="2"><li>账号凭据等失效的问题</li></ol><p>重建账号信息</p><ol start="3"><li>本地maven工具的失效的问题</li></ol><p>重新拷贝maven工具到容器中，然后在jenkins中重新配置</p><ol start="4"><li>pom文件执行错误的问题:构建时，使用了项目根目录下的pom.xml文件，并不是构建的对应目录下的文件</li></ol><p>报错信息如下：</p><pre><code>09:51:02 Parsing POMs09:51:02 using settings config with name Maven Local Settings09:51:02 Replacing all maven server entries not found in credentials list is true09:51:02 ERROR: Failed to parse POMs09:51:02 org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:09:51:02 [ERROR] Non-resolvable import POM: Failure to transfer org.springframework.cloud:spring-cloud-dependencies:pom:Hoxton.SR1 from https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced. Original error: Could not transfer artifact org.springframework.cloud:spring-cloud-dependencies:pom:Hoxton.SR1 from/to central (https://repo.maven.apache.org/maven2): Connect to repo.maven.apache.org:443 [repo.maven.apache.org/151.101.40.215] failed: Connection timed out (Connection timed out) @ com.testsoft:test-utils:1.0.0-SNAPSHOT, /var/jenkins_home/workspace/test-dict-develop/pom.xml, line 82, column 2509:51:02 [ERROR] Non-resolvable import POM: Failure to transfer com.alibaba.cloud:spring-cloud-alibaba-dependencies:pom:2.2.1.RELEASE from https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced. Original error: Could not transfer artifact com.alibaba.cloud:spring-cloud-alibaba-dependencies:pom:2.2.1.RELEASE from/to central (https://repo.maven.apache.org/maven2): Connect to repo.maven.apache.org:443 [repo.maven.apache.org/151.101.40.215] failed: Connection timed out (Connection timed out) @ com.testsoft:test-utils:1.0.0-SNAPSHOT, /var/jenkins_home/workspace/test-dict-develop/pom.xml, line 89, column 2509:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for org.springframework.cloud:spring-cloud-commons:jar is missing. @ com.testsoft:test-data-dictionary-server:[unknown-version], /var/jenkins_home/workspace/test-dict-develop/test-data-dictionary-server/pom.xml, line 29, column 2109:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for org.springframework.cloud:spring-cloud-starter-netflix-eureka-client:jar is missing. @ com.testsoft:test-data-dictionary-server:[unknown-version], /var/jenkins_home/workspace/test-dict-develop/test-data-dictionary-server/pom.xml, line 81, column 2109:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-config:jar is missing. @ com.testsoft:test-data-dictionary-server:[unknown-version], /var/jenkins_home/workspace/test-dict-develop/test-data-dictionary-server/pom.xml, line 85, column 2109:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for org.springframework.cloud:spring-cloud-starter-sleuth:jar is missing. @ com.testsoft:test-utils:1.0.0-SNAPSHOT, /var/jenkins_home/workspace/test-dict-develop/pom.xml, line 74, column 2109:51:02 09:51:02     at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:383)09:51:02     at hudson.maven.MavenEmbedder.buildProjects(MavenEmbedder.java:370)09:51:02     at hudson.maven.MavenEmbedder.readProjects(MavenEmbedder.java:340)09:51:02     at hudson.maven.MavenModuleSetBuild$PomParser.invoke(MavenModuleSetBuild.java:1329)09:51:02     at hudson.maven.MavenModuleSetBuild$PomParser.invoke(MavenModuleSetBuild.java:1126)09:51:02     at hudson.FilePath.act(FilePath.java:1075)09:51:02     at hudson.FilePath.act(FilePath.java:1058)09:51:02     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.parsePoms(MavenModuleSetBuild.java:987)09:51:02     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.doRun(MavenModuleSetBuild.java:691)09:51:02     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:504)09:51:02     at hudson.model.Run.execute(Run.java:1853)09:51:02     at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:543)09:51:02     at hudson.model.ResourceController.execute(ResourceController.java:97)09:51:02     at hudson.model.Executor.run(Executor.java:427)</code></pre><p>处理方式：在maven命令部分，添加*<em>-U</em>参数，确保拉取时拉取最新的jar包</p><ol start="5"><li>迁移以后，jenkins中的ssh连接失效</li></ol><p>报错信息：</p><pre><code>com.jcraft.jsch.JSchException: SSH_MSG_DISCONNECT: 2 Too many authentication failures 13:18:37     at com.jcraft.jsch.Session.read(Session.java:1004)13:18:37     at com.jcraft.jsch.UserAuthPassword.start(UserAuthPassword.java:91)13:18:37     at com.jcraft.jsch.Session.connect(Session.java:470)13:18:37     at org.jvnet.hudson.plugins.CredentialsSSHSite.createSession(CredentialsSSHSite.java:132)13:18:37     at org.jvnet.hudson.plugins.CredentialsSSHSite.executeCommand(CredentialsSSHSite.java:208)13:18:37     at org.jvnet.hudson.plugins.SSHBuilder.perform(SSHBuilder.java:104)13:18:37     at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20)13:18:37     at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:741)13:18:37     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.build(MavenModuleSetBuild.java:946)13:18:37     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.doRun(MavenModuleSetBuild.java:896)13:18:37     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:504)13:18:37     at hudson.model.Run.execute(Run.java:1853)13:18:37     at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:543)13:18:37     at hudson.model.ResourceController.execute(ResourceController.java:97)13:18:37     at hudson.model.Executor.run(Executor.java:427)13:18:37 Build step &#39;Execute shell script on remote host using ssh&#39; marked build as failure</code></pre><p>处理方式：SSH密钥过期的问题，重新添加密钥即可</p><h2 id="关于jenkins中容器过大的问题调整"><a href="#关于jenkins中容器过大的问题调整" class="headerlink" title="关于jenkins中容器过大的问题调整"></a>关于jenkins中容器过大的问题调整</h2><p>backup内容过大的问题–&gt;删除过多的备份调整备份策略    31G  –&gt; 1.1G</p><ol><li>修改定时任务</li></ol><p>0 23 * * 1</p><p>每周一23点执行一次备份</p><ol start="2"><li>删除多余的备份</li></ol><p>jobs–&gt;产出物的积累问题，产出物jar包              7.9G   –&gt; 7.9G</p><ol><li><p>产出物jar包占用大小，后续构建完成后推送到nexus3镜像，并且删除本地的jar包</p></li><li><p>构建物的保留策略，需要重新制定，<a href="https://blog.csdn.net/liliwang90/article/details/104690491" target="_blank" rel="noopener">参考链接</a></p></li></ol><p>workspace–&gt; 排序，查看最大与最小的内容           14G    –&gt; 9.6G</p><ol><li><p>项目拆分，存在重复的情况</p></li><li><p>前端依赖问题，先备份前端依赖，再逐步删除依赖信息</p></li><li><p>快速批量删除Jenkins构建清理磁盘空间并按参数保留最近构建</p></li></ol><h2 id="jenkins升级"><a href="#jenkins升级" class="headerlink" title="jenkins升级"></a>jenkins升级</h2><p>这里从2.204版本升级到2.263.4_LTS版本。</p><p>直接从<a href="https://updates.jenkins-ci.org/download/war/" target="_blank" rel="noopener">官网</a>下载2.263.4版本的war包。</p><p>首先备份之前的war包，通过下面的命令进行：</p><pre><code>// 全局查找war包信息# find / -name jenkins.war/usr/lib/jenkins/jenkins.war// 停止jenkins运行# systemctl stop jenkins// 一般默认路径为/usr/lib/jenkins/# cd /usr/lib/jenkins# mv jenkins.war /root/jenkins_backup</code></pre><p>备份完成后，下载war包，也可以选择在客户机下载后再拷贝进去。</p><pre><code>// 下载war包信息# wget https://updates.jenkins-ci.org/download/war/2.274/jenkins.war</code></pre><p>下载完成war包后，再重新启动jenkins服务。</p><pre><code># systemctl restart jenkins// 查看端口号# sudo netstat -nlp | grep 9090</code></pre><p>这样就完成了jenkins的升级。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>jenkins备份相对不那么好做，由于历史的原因，一些配置的设定上并没有那么合理，需要在迁移完成后重新进行设置，并且一定构建一支工程进行测试，确保测试无误后，投入使用。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://blog.csdn.net/liliwang90/article/details/104690491" target="_blank" rel="noopener">快速批量删除Jenkins构建清理磁盘空间并按参数保留最近构建</a></li></ul><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li><p>时间更新</p></li><li><p>统一构建账户的调整</p></li><li><p>全局变量的参数化配置</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于Eureka无法相互注册且服务只是注册到单一Eureka的问题总结</title>
      <link href="/2020/07/30/guan-yu-eureka-wu-fa-xiang-hu-zhu-ce-qie-fu-wu-zhi-shi-zhu-ce-dao-dan-yi-eureka-de-wen-ti-zong-jie/"/>
      <url>/2020/07/30/guan-yu-eureka-wu-fa-xiang-hu-zhu-ce-qie-fu-wu-zhi-shi-zhu-ce-dao-dan-yi-eureka-de-wen-ti-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="关于Eureka无法相互注册且服务只是注册到单一Eureka的问题总结"><a href="#关于Eureka无法相互注册且服务只是注册到单一Eureka的问题总结" class="headerlink" title="关于Eureka无法相互注册且服务只是注册到单一Eureka的问题总结"></a>关于Eureka无法相互注册且服务只是注册到单一Eureka的问题总结</h1><h2 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h2><p>我们有三个Eureka服务，以206、207、208表征。</p><p>在昨天的问题排查中，出现了以下的场景：</p><ol><li>Eureka不能相互注册成为集群</li><li>有两个Eureka相互注册成功，也就是206、208相互注册成功，207游离</li><li>线上环境，新启动的业务服务注册到了游离的Eureka服务上，导致网关侧以及远程Restful调用的时候找不到业务服务，报错</li><li>线下启动业务服务，可以注册到三个Eureka上</li></ol><h2 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h2><p>通过ELK日志统一收集系统，查看错误日志信息，我们目前只对206服务进行了日志采集，先来看206的日志信息：</p><pre><code>2020-07-23 17:31:49.164 ERROR [test-register,,,] 12962 --- [_192.168.128.207-13] c.n.e.cluster.ReplicationTaskProcessor   : Network level connection to peer 192.168.128.207; retrying after delaycom.sun.jersey.api.client.ClientHandlerException: java.net.ConnectException: Connection refused (Connection refused)        at com.sun.jersey.client.apache4.ApacheHttpClient4Handler.handle(ApacheHttpClient4Handler.java:187) ~[jersey-apache-client4-1.19.1.jar!/:1.19.1]        at com.netflix.eureka.cluster.DynamicGZIPContentEncodingFilter.handle(DynamicGZIPContentEncodingFilter.java:48) ~[eureka-core-1.9.13.jar!/:1.9.13]        at com.netflix.discovery.EurekaIdentityHeaderFilter.handle(EurekaIdentityHeaderFilter.java:27) ~[eureka-client-1.9.13.jar!/:1.9.13]        at com.sun.jersey.api.client.Client.handle(Client.java:652) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.sun.jersey.api.client.WebResource.handle(WebResource.java:682) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.sun.jersey.api.client.WebResource$Builder.post(WebResource.java:570) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.netflix.eureka.transport.JerseyReplicationClient.submitBatchUpdates(JerseyReplicationClient.java:117) ~[eureka-core-1.9.13.jar!/:1.9.13]        at com.netflix.eureka.cluster.ReplicationTaskProcessor.process(ReplicationTaskProcessor.java:80) ~[eureka-core-1.9.13.jar!/:1.9.13]        at com.netflix.eureka.util.batcher.TaskExecutors$BatchWorkerRunnable.run(TaskExecutors.java:193) [eureka-core-1.9.13.jar!/:1.9.13]        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_202]Caused by: java.net.ConnectException: Connection refused (Connection refused)        at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_202]        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_202]        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_202]        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_202]        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_202]        at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_202]        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:121) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:180) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:144) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:134) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:605) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.DefaultRequestDirector.execute$original$wN9FcpoY(DefaultRequestDirector.java:440) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.DefaultRequestDirector.execute$original$wN9FcpoY$accessor$3nRtAICB(DefaultRequestDirector.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.DefaultRequestDirector$auxiliary$TWCaq941.call(Unknown Source) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:93) ~[skywalking-agent.jar:6.4.0]        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.AbstractHttpClient.doExecute$original$X5K2K7NE(AbstractHttpClient.java:835) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.AbstractHttpClient.doExecute$original$X5K2K7NE$accessor$UW83IYMZ(AbstractHttpClient.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.AbstractHttpClient$auxiliary$prYdOZrp.call(Unknown Source) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:93) ~[skywalking-agent.jar:6.4.0]        at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:118) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56) ~[httpclient-4.5.12.jar!/:4.5.12]        at com.sun.jersey.client.apache4.ApacheHttpClient4Handler.handle(ApacheHttpClient4Handler.java:173) ~[jersey-apache-client4-1.19.1.jar!/:1.19.1]        ... 10 common frames omitted2020-07-23 17:31:58.629 ERROR [test-register,,,] 12962 --- [_192.168.128.207-19] c.n.e.cluster.ReplicationTaskProcessor   : It seems to be a socket read timeout exception, it will retry later. if it continues to happen and some eureka node occupied all the cpu time, you should set property &#39;eureka.server.peer-node-read-timeout-ms&#39; to a bigger valuecom.sun.jersey.api.client.ClientHandlerException: java.net.SocketTimeoutException: Read timed out        at com.sun.jersey.client.apache4.ApacheHttpClient4Handler.handle(ApacheHttpClient4Handler.java:187) ~[jersey-apache-client4-1.19.1.jar!/:1.19.1]        at com.netflix.eureka.cluster.DynamicGZIPContentEncodingFilter.handle(DynamicGZIPContentEncodingFilter.java:48) ~[eureka-core-1.9.13.jar!/:1.9.13]        at com.netflix.discovery.EurekaIdentityHeaderFilter.handle(EurekaIdentityHeaderFilter.java:27) ~[eureka-client-1.9.13.jar!/:1.9.13]        at com.sun.jersey.api.client.Client.handle(Client.java:652) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.sun.jersey.api.client.WebResource.handle(WebResource.java:682) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.sun.jersey.api.client.WebResource$Builder.post(WebResource.java:570) ~[jersey-client-1.19.1.jar!/:1.19.1]        at com.netflix.eureka.transport.JerseyReplicationClient.submitBatchUpdates(JerseyReplicationClient.java:117) ~[eureka-core-1.9.13.jar!/:1.9.13]        at com.netflix.eureka.cluster.ReplicationTaskProcessor.process(ReplicationTaskProcessor.java:80) ~[eureka-core-1.9.13.jar!/:1.9.13]        at com.netflix.eureka.util.batcher.TaskExecutors$BatchWorkerRunnable.run(TaskExecutors.java:193) [eureka-core-1.9.13.jar!/:1.9.13]        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_202]Caused by: java.net.SocketTimeoutException: Read timed out        at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_202]        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_202]        at java.net.SocketInputStream.read(SocketInputStream.java:171) ~[na:1.8.0_202]        at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_202]        at org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:161) ~[httpcore-4.4.13.jar!/:4.4.13]        at org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:82) ~[httpcore-4.4.13.jar!/:4.4.13]        at org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:276) ~[httpcore-4.4.13.jar!/:4.4.13]        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:138) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259) ~[httpcore-4.4.13.jar!/:4.4.13]        at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:294) ~[httpcore-4.4.13.jar!/:4.4.13]        at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:257) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.conn.AbstractClientConnAdapter.receiveResponseHeader(AbstractClientConnAdapter.java:230) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273) ~[httpcore-4.4.13.jar!/:4.4.13]        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125) ~[httpcore-4.4.13.jar!/:4.4.13]        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:679) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.DefaultRequestDirector.execute$original$wN9FcpoY(DefaultRequestDirector.java:481) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.DefaultRequestDirector.execute$original$wN9FcpoY$accessor$3nRtAICB(DefaultRequestDirector.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.DefaultRequestDirector$auxiliary$TWCaq941.call(Unknown Source) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:93) ~[skywalking-agent.jar:6.4.0]        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.AbstractHttpClient.doExecute$original$X5K2K7NE(AbstractHttpClient.java:835) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.AbstractHttpClient.doExecute$original$X5K2K7NE$accessor$UW83IYMZ(AbstractHttpClient.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.AbstractHttpClient$auxiliary$prYdOZrp.call(Unknown Source) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:93) ~[skywalking-agent.jar:6.4.0]        at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:118) ~[httpclient-4.5.12.jar!/:4.5.12]        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56) ~[httpclient-4.5.12.jar!/:4.5.12]        at com.sun.jersey.client.apache4.ApacheHttpClient4Handler.handle(ApacheHttpClient4Handler.java:173) ~[jersey-apache-client4-1.19.1.jar!/:1.19.1]        ... 10 common frames omitted</code></pre><p>这里发现，访问207服务超时，说明207服务存在问题，但是查看207服务的启动日志并未发现异常信息。如下：</p><pre><code>DEBUG 2020-07-23 17:31:07:093 main AgentPackagePath : The beacon class location is jar:file:/home/centos/skywalking-bin/apache-skywalking-apm-bin/agent/skywalking-agent.jar!/org/apache/skywalking/apm/agent/core/boot/AgentPackagePath.class.INFO 2020-07-23 17:31:07:097 main SnifferConfigInitializer : Config file found in /home/centos/skywalking-bin/apache-skywalking-apm-bin/agent/config/agent.config.    ______________        ________                __   /  _/ ____/ __ \      / ____/ /___  __  ______/ /   / // /   / /_/ /_____/ /   / / __ \/ / / / __  / _/ // /___/ ____/_____/ /___/ / /_/ / /_/ / /_/ //___/\____/_/          \____/_/\____/\__,_/\__,_/============ * powered by testsoft * ============2020-07-23 17:31:26.529  INFO [test-register,,,] 14661 --- [           main] c.a.n.c.c.impl.LocalConfigInfoProcessor  : LOCAL_SNAPSHOT_PATH:/home/centos/nacos/config2020-07-23 17:31:26.694  INFO [test-register,,,] 14661 --- [           main] c.a.nacos.client.config.impl.Limiter     : limitTime:5.02020-07-23 17:31:27.040  WARN [test-register,,,] 14661 --- [           main] c.a.c.n.c.NacosPropertySourceBuilder     : Ignore the empty nacos configuration and get it based on dataId[test-register] &amp; group[DEFAULT_GROUP]2020-07-23 17:31:27.047  INFO [test-register,,,] 14661 --- [           main] c.a.nacos.client.config.utils.JVMUtil    : isMultiInstance:false2020-07-23 17:31:27.084  INFO [test-register,,,] 14661 --- [           main] b.c.PropertySourceBootstrapConfiguration : Located property source: [BootstrapPropertySource {name=&#39;bootstrapProperties-test-register-peer1.yml,DEFAULT_GROUP&#39;}, BootstrapPropertySource {name=&#39;bootstrapProperties-test-register.yml,DEFAULT_GROUP&#39;}, BootstrapPropertySource {name=&#39;bootstrapProperties-test-register,DEFAULT_GROUP&#39;}]2020-07-23 17:31:27.209  INFO [test-register,,,] 14661 --- [           main] c.h.c.s.ServerRegisterApplication        : The following profiles are active: peer12020-07-23 17:31:30.192  WARN [test-register,,,] 14661 --- [           main] o.s.boot.actuate.endpoint.EndpointId     : Endpoint ID &#39;nacos-config&#39; contains invalid characters, please migrate to a valid format.2020-07-23 17:31:31.499  WARN [test-register,,,] 14661 --- [           main] o.s.boot.actuate.endpoint.EndpointId     : Endpoint ID &#39;service-registry&#39; contains invalid characters, please migrate to a valid format.2020-07-23 17:31:32.833  INFO [test-register,,,] 14661 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=0a3a956c-cd1a-35c8-ae9f-743a8cc478b82020-07-23 17:31:38.139  INFO [test-register,,,] 14661 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 19012 (http)2020-07-23 17:31:38.162  INFO [test-register,,,] 14661 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]2020-07-23 17:31:38.162  INFO [test-register,,,] 14661 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.33]2020-07-23 17:31:38.611  INFO [test-register,,,] 14661 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext2020-07-23 17:31:38.611  INFO [test-register,,,] 14661 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 11320 ms2020-07-23 17:31:39.731  WARN [test-register,,,] 14661 --- [           main] c.n.c.sources.URLConfigurationSource     : No URLs will be polled as dynamic configuration sources.2020-07-23 17:31:39.732  INFO [test-register,,,] 14661 --- [           main] c.n.c.sources.URLConfigurationSource     : To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath.2020-07-23 17:31:39.802  INFO [test-register,,,] 14661 --- [           main] c.netflix.config.DynamicPropertyFactory  : DynamicPropertyFactory is initialized with configuration sources: com.netflix.config.ConcurrentCompositeConfiguration@31a2a9fa2020-07-23 17:31:42.077  INFO [test-register,,,] 14661 --- [           main] o.s.cloud.commons.util.InetUtils         : Cannot determine local hostname2020-07-23 17:31:43.217  INFO [test-register,,,] 14661 --- [           main] c.s.j.s.i.a.WebApplicationImpl           : Initiating Jersey application, version &#39;Jersey: 1.19.1 03/11/2016 02:08 PM&#39;2020-07-23 17:31:43.513  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON encoding codec LegacyJacksonJson2020-07-23 17:31:43.513  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON decoding codec LegacyJacksonJson2020-07-23 17:31:44.523  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML encoding codec XStreamXml2020-07-23 17:31:44.523  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML decoding codec XStreamXml2020-07-23 17:31:47.543  INFO [test-register,,,] 14661 --- [           main] o.s.s.web.DefaultSecurityFilterChain     : Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@37314843, org.springframework.security.web.context.SecurityContextPersistenceFilter@70f148dc, org.springframework.security.web.header.HeaderWriterFilter@4d69d288, org.springframework.security.web.authentication.logout.LogoutFilter@4dd2ef54, org.springframework.security.web.authentication.www.BasicAuthenticationFilter@45d4421d, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@77aea, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@3a3883c4, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@50122012, org.springframework.security.web.session.SessionManagementFilter@787178b1, org.springframework.security.web.access.ExceptionTranslationFilter@4b28a7bf, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@74e497ae]2020-07-23 17:31:47.589  WARN [test-register,,,] 14661 --- [           main] c.n.c.sources.URLConfigurationSource     : No URLs will be polled as dynamic configuration sources.2020-07-23 17:31:47.590  INFO [test-register,,,] 14661 --- [           main] c.n.c.sources.URLConfigurationSource     : To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath.2020-07-23 17:31:50.199  INFO [test-register,,,] 14661 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService &#39;applicationTaskExecutor&#39;2020-07-23 17:31:52.842  INFO [test-register,,,] 14661 --- [           main] o.s.cloud.commons.util.InetUtils         : Cannot determine local hostname2020-07-23 17:31:53.861  INFO [test-register,,,] 14661 --- [           main] o.s.cloud.commons.util.InetUtils         : Cannot determine local hostname2020-07-23 17:31:53.995  WARN [test-register,,,] 14661 --- [           main] ockingLoadBalancerClientRibbonWarnLogger : You already have RibbonLoadBalancerClient on your classpath. It will be used by default. As Spring Cloud Ribbon is in maintenance mode. We recommend switching to BlockingLoadBalancerClient instead. In order to use it, set the value of `spring.cloud.loadbalancer.ribbon.enabled` to `false` or remove spring-cloud-starter-netflix-ribbon from your project.2020-07-23 17:31:54.274  INFO [test-register,,,] 14661 --- [           main] o.s.c.n.eureka.InstanceInfoFactory       : Setting initial instance status as: STARTING2020-07-23 17:31:54.848  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Initializing Eureka in region us-east-12020-07-23 17:31:55.892  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON encoding codec LegacyJacksonJson2020-07-23 17:31:55.892  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON decoding codec LegacyJacksonJson2020-07-23 17:31:55.892  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML encoding codec XStreamXml2020-07-23 17:31:55.892  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML decoding codec XStreamXml2020-07-23 17:31:56.242  INFO [test-register,,,] 14661 --- [           main] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration2020-07-23 17:31:56.347  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Disable delta property : false2020-07-23 17:31:56.348  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Single vip registry refresh property : null2020-07-23 17:31:56.348  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Force full registry fetch : false2020-07-23 17:31:56.348  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Application is null : false2020-07-23 17:31:56.348  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Registered Applications size is zero : true2020-07-23 17:31:56.348  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Application version is -1: true2020-07-23 17:31:56.348  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Getting all instance registry info from the eureka server2020-07-23 17:31:57.524  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : The response status is 2002020-07-23 17:31:57.535  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Starting heartbeat executor: renew interval is: 302020-07-23 17:31:57.540  INFO [test-register,,,] 14661 --- [           main] c.n.discovery.InstanceInfoReplicator     : InstanceInfoReplicator onDemand update allowed rate per min is 42020-07-23 17:31:57.562  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Discovery Client initialized at timestamp 1595496717546 with initial instances count: 172020-07-23 17:31:57.671  INFO [test-register,,,] 14661 --- [           main] c.n.eureka.DefaultEurekaServerContext    : Initializing ...2020-07-23 17:31:57.676  INFO [test-register,,,] 14661 --- [           main] c.n.eureka.cluster.PeerEurekaNodes       : Adding new peer nodes [http://test:test2019@192.168.128.208:19013/eureka/, http://test:test2019@192.168.128.206:19011/eureka/]2020-07-23 17:31:57.686  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON encoding codec LegacyJacksonJson2020-07-23 17:31:57.687  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON decoding codec LegacyJacksonJson2020-07-23 17:31:57.687  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML encoding codec XStreamXml2020-07-23 17:31:57.687  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML decoding codec XStreamXml2020-07-23 17:31:57.855  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON encoding codec LegacyJacksonJson2020-07-23 17:31:57.856  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON decoding codec LegacyJacksonJson2020-07-23 17:31:57.856  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML encoding codec XStreamXml2020-07-23 17:31:57.856  INFO [test-register,,,] 14661 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML decoding codec XStreamXml2020-07-23 17:31:57.953  INFO [test-register,,,] 14661 --- [           main] c.n.eureka.cluster.PeerEurekaNodes       : Replica node URL:  http://test:test2019@192.168.128.208:19013/eureka/2020-07-23 17:31:57.953  INFO [test-register,,,] 14661 --- [           main] c.n.eureka.cluster.PeerEurekaNodes       : Replica node URL:  http://test:test2019@192.168.128.206:19011/eureka/2020-07-23 17:31:57.975  INFO [test-register,,,] 14661 --- [           main] c.n.e.registry.AbstractInstanceRegistry  : Finished initializing remote region registries. All known remote regions: []2020-07-23 17:31:57.976  INFO [test-register,,,] 14661 --- [           main] c.n.eureka.DefaultEurekaServerContext    : Initialized2020-07-23 17:31:58.184  INFO [test-register,,,] 14661 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 18 endpoint(s) beneath base path &#39;/actuator&#39;2020-07-23 17:31:58.267  INFO [test-register,,,] 14661 --- [           main] o.s.c.n.e.s.EurekaServiceRegistry        : Registering application test-REGISTER with eureka with status UP2020-07-23 17:31:58.268  INFO [test-register,,,] 14661 --- [           main] com.netflix.discovery.DiscoveryClient    : Saw local status change event StatusChangeEvent [timestamp=1595496718268, current=UP, previous=STARTING]2020-07-23 17:31:58.303  INFO [test-register,,,] 14661 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_test-REGISTER/test-register@192.168.128.207:19012: registering service...2020-07-23 17:31:58.318  INFO [test-register,,,] 14661 --- [      Thread-14] o.s.c.n.e.server.EurekaServerBootstrap   : Setting the eureka configuration..2020-07-23 17:31:58.328  INFO [test-register,,,] 14661 --- [      Thread-14] o.s.c.n.e.server.EurekaServerBootstrap   : Eureka data center value eureka.datacenter is not set, defaulting to default2020-07-23 17:31:58.329  INFO [test-register,,,] 14661 --- [      Thread-14] o.s.c.n.e.server.EurekaServerBootstrap   : Eureka environment value eureka.environment is not set, defaulting to test2020-07-23 17:31:58.410  INFO [test-register,,,] 14661 --- [      Thread-14] o.s.c.n.e.server.EurekaServerBootstrap   : isAws returned false2020-07-23 17:31:58.410  INFO [test-register,,,] 14661 --- [      Thread-14] o.s.c.n.e.server.EurekaServerBootstrap   : Initialized server context2020-07-23 17:31:58.465  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-FILESTORE/test-filestore@192.168.128.199:20060 with status UP (replication=true)2020-07-23 17:31:58.466  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-GATEWAY/test-gateway@192.168.128.209:19020 with status UP (replication=true)2020-07-23 17:31:58.466  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-BASE-PSL/test-base-psl@192.168.128.199:24020 with status UP (replication=true)2020-07-23 17:31:58.467  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-APP-INF/test-app-inf@192.168.128.199:20140 with status UP (replication=true)2020-07-23 17:31:58.468  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-REGISTER/test-register@192.168.128.206:19011 with status UP (replication=true)2020-07-23 17:31:58.468  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-REGISTER/test-register@192.168.128.208:19013 with status UP (replication=true)2020-07-23 17:31:58.469  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-BASE-RDP/test-base-rdp@192.168.128.199:20151 with status UP (replication=true)2020-07-23 17:31:58.470  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-ADMIN-SERVER/test-admin-server@192.168.128.199:19760 with status UP (replication=true)2020-07-23 17:31:58.475  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-UAA/test-uaa@192.168.128.199:19060 with status UP (replication=true)2020-07-23 17:31:58.476  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-FORM/test-form@192.168.128.199:20040 with status UP (replication=true)2020-07-23 17:31:58.476  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-ID-GENERATOR/test-id-generator@192.168.128.199:8080 with status UP (replication=true)2020-07-23 17:31:58.477  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-PSL-EQUIPMENT-MANAGEMENT/test-psl-equipment-management@192.168.128.199:24010 with status UP (replication=true)2020-07-23 17:31:58.478  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-UMC/test-umc@192.168.128.199:19070 with status UP (replication=true)2020-07-23 17:31:58.478  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-DICT/test-dict@10.0.93.99:19090 with status UP (replication=true)2020-07-23 17:31:58.479  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-WORKFLOW/test-workflow@192.168.128.199:20050 with status UP (replication=true)2020-07-23 17:31:58.480  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-BASE/test-base@192.168.128.199:20130 with status UP (replication=true)2020-07-23 17:31:58.480  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-VISUALMODEL/test-visualmodel@10.0.93.99:20020 with status UP (replication=true)2020-07-23 17:31:58.480  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.r.PeerAwareInstanceRegistryImpl    : Got 17 instances from neighboring DS node2020-07-23 17:31:58.480  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.r.PeerAwareInstanceRegistryImpl    : Renew threshold is: 282020-07-23 17:31:58.481  INFO [test-register,,,] 14661 --- [      Thread-14] c.n.e.r.PeerAwareInstanceRegistryImpl    : Changing status to UP2020-07-23 17:31:58.499  INFO [test-register,,,] 14661 --- [      Thread-14] e.s.EurekaServerInitializerConfiguration : Started Eureka Server2020-07-23 17:31:58.511  INFO [test-register,,,] 14661 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_test-REGISTER/test-register@192.168.128.207:19012 - registration status: 2042020-07-23 17:31:58.519  INFO [test-register,,,] 14661 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 19012 (http) with context path &#39;&#39;2020-07-23 17:31:58.521  INFO [test-register,,,] 14661 --- [           main] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 190122020-07-23 17:31:58.699  INFO [test-register,,,] 14661 --- [io-19012-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet &#39;dispatcherServlet&#39;2020-07-23 17:31:58.700  INFO [test-register,,,] 14661 --- [io-19012-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet &#39;dispatcherServlet&#39;2020-07-23 17:31:58.731  INFO [test-register,,,] 14661 --- [io-19012-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 31 ms2020-07-23 17:31:59.308  INFO [test-register,9481285215d134b2,9481285215d134b2,false] 14661 --- [io-19012-exec-2] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-REGISTER/test-register@192.168.128.207:19012 with status UP (replication=true)2020-07-23 17:31:59.522  INFO [test-register,,,] 14661 --- [           main] o.s.cloud.commons.util.InetUtils         : Cannot determine local hostname2020-07-23 17:31:59.526  INFO [test-register,,,] 14661 --- [           main] c.h.c.s.ServerRegisterApplication        : Started ServerRegisterApplication in 43.119 seconds (JVM running for 54.87)2020-07-23 17:31:59.543  INFO [test-register,,,] 14661 --- [           main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.128.206_18848-858b37b1-35be-4564-a24e-dc2c322d5784] [subscribe] test-register.yml+DEFAULT_GROUP+858b37b1-35be-4564-a24e-dc2c322d57842020-07-23 17:31:59.547  INFO [test-register,,,] 14661 --- [           main] c.a.nacos.client.config.impl.CacheData   : [fixed-192.168.128.206_18848-858b37b1-35be-4564-a24e-dc2c322d5784] [add-listener] ok, tenant=858b37b1-35be-4564-a24e-dc2c322d5784, dataId=test-register.yml, group=DEFAULT_GROUP, cnt=12020-07-23 17:31:59.547  INFO [test-register,,,] 14661 --- [           main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.128.206_18848-858b37b1-35be-4564-a24e-dc2c322d5784] [subscribe] test-register+DEFAULT_GROUP+858b37b1-35be-4564-a24e-dc2c322d57842020-07-23 17:31:59.547  INFO [test-register,,,] 14661 --- [           main] c.a.nacos.client.config.impl.CacheData   : [fixed-192.168.128.206_18848-858b37b1-35be-4564-a24e-dc2c322d5784] [add-listener] ok, tenant=858b37b1-35be-4564-a24e-dc2c322d5784, dataId=test-register, group=DEFAULT_GROUP, cnt=12020-07-23 17:31:59.548  INFO [test-register,,,] 14661 --- [           main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.128.206_18848-858b37b1-35be-4564-a24e-dc2c322d5784] [subscribe] test-register-peer1.yml+DEFAULT_GROUP+858b37b1-35be-4564-a24e-dc2c322d57842020-07-23 17:31:59.548  INFO [test-register,,,] 14661 --- [           main] c.a.nacos.client.config.impl.CacheData   : [fixed-192.168.128.206_18848-858b37b1-35be-4564-a24e-dc2c322d5784] [add-listener] ok, tenant=858b37b1-35be-4564-a24e-dc2c322d5784, dataId=test-register-peer1.yml, group=DEFAULT_GROUP, cnt=12020-07-23 17:31:59.563  INFO [test-register,,,] 14661 --- [4e-dc2c322d5784] c.a.n.client.config.impl.ClientWorker    : get changedGroupKeys:[]2020-07-23 17:31:59.903  INFO [test-register,78c19310aaaa187c,78c19310aaaa187c,false] 14661 --- [io-19012-exec-4] c.n.e.registry.AbstractInstanceRegistry  : Registered instance test-REGISTER/test-register@192.168.128.207:19012 with status UP (replication=true)2020-07-23 17:32:29.268  INFO [test-register,,,] 14661 --- [4e-dc2c322d5784] c.a.n.client.config.impl.ClientWorker    : get changedGroupKeys:[]2020-07-23 17:32:58.483  INFO [test-register,,,] 14661 --- [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task </code></pre><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><ol><li><p>eureka server之间相互成为peer node，如果有一台eureka server挂了，则eureka server之间的replication都会受影响，人工接入更改eureka server的serverUrl信息，则可以主动剔除掉挂掉的peerNode，未能剔除，则会报错。报上述错误！</p></li><li><p>通过重启存在错误的eureka服务，解决该问题。</p></li><li><p>关于Eureka配置文件中的配置信息，如下：</p></li></ol><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">eureka</span><span class="token punctuation">:</span>  <span class="token key atrule">client</span><span class="token punctuation">:</span>    <span class="token key atrule">register-with-eureka</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">fetch-registry</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>register-with-eureka表示：是否注册自身到eureka服务器。</li><li>fetch-registry表示：是否从eureka上获取注册信息</li></ul><p>如果存在上述的配置项，注释掉这两个选项也是可以解决这个问题的。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2020/07/25/kafka-jiao-xue/"/>
      <url>/2020/07/25/kafka-jiao-xue/</url>
      
        <content type="html"><![CDATA[<h1 id="kafka教学"><a href="#kafka教学" class="headerlink" title="kafka教学"></a>kafka教学</h1><p>———————-zookeeper———————–</p><p>分布式协调服务，一个通用的无单点问题的分布式协调框架</p><p>多机多进程</p><p>网络不可靠、通信不同步、关键节点可能不可用</p><p>客户端：curactor</p><p>Observer（只接收请求）、Leader（所有写）、Follower（可成为Leader）</p><p>尝试添加follower角色</p><p>过半数方法</p><p>ZAB协议</p><p>客户端长连接</p><p>zk的节点：有序无序、持久临时</p><p>应用：分布式id，雪花算法、利用zk生成滚动序号，保证递增</p><p>watch机制</p><p>临时性节点+watch机制</p><p>kafka：0.10.2.1  + zookeeper：3.4.6版本对应</p><p>2181对外、2881节点内部通信、3881Leader选举。</p><p>注意配置端口号对应</p><p>clientPort=21810<br>server.1=192.168.229.201:28880:38880<br>server.2=192.168.229.206:28880:38880<br>server.3=192.168.229.209:28880:38880</p><p>实践：对zookeeper配置服务，对kafka配置服务，同时关联kafka manager</p><p>—————–kafka————————</p><p>日志信息</p><p>单机50w~60w的数据</p><p>Partition的副本机制，高可用</p><p>key Hash算法，消息有序性保证，导致partition之间的不均衡。</p><p>多个Consumer来读取一个Topic(理想情况下是一个Consumer读取Topic的一个Partition）,那么可以让这些Consumer属于同一个Consumer Group即可实现消息的多Consumer并行处理。</p><p>消息保留策略：</p><p>log.retention.hours：消息保留多长时间（默认168小时，即一周）<br>log.retention.bytes：消息保留最大占用空间</p><p>最先达到最先触发。</p><p>副本与ISR</p><p>ISR数量必须与分区副本数相同，否则会出现问题</p><p>Producer可靠性</p><p>水位：Leader成功将分区复制到follower中</p><p>问题：jdk的版本选择。</p><p>实践：添加kafka的broker，新增一个进入集群中。</p><p>———————kafka 运维——————————–</p><p>kafka Manager 二次启动时需要删除上一次运行所创建的RUNNING_PID已存在，需要删除该id</p><p>———————-实践部分———————————-</p><p>各类基础服务配置为Service，使用systemctl进行管理。</p><p>需要总结centos中配置服务的教程！</p><p>可以通过脚本的方式触发针对topic的重选举。</p><p>Telegraf+JMXTrans+influxDB+Grafana</p><p>######################################################################################################################</p><h1 id="Telegraf-grafana培训"><a href="#Telegraf-grafana培训" class="headerlink" title="Telegraf+grafana培训"></a>Telegraf+grafana培训</h1><p>TICK技术栈</p><p>Telegraf+InfluxDB+Chronograf+kapacitor（预警）</p><p>实验一完成。</p><p>注意：influxdb使用的是UTC时间</p><p>时间+值，多个序列</p><p>Retention Policies 对所有数据表起作用</p><p>对于InfluxDB初始化:</p><ol><li>禁用信息收集</li></ol><p>reporting-disabled=true</p><ol start="2"><li>禁用每个数据库的序列个数</li></ol><ol start="3"><li>禁用每个维度最多维度的个数</li></ol><p>influxDB配置信息</p><p>业务关系链路，图数据库</p><p>谁向你写入，数据来源</p><p>治理和审计</p><p>预警体系</p><p>自动运维层面</p><p>自有运营与其它产品开发</p><p>TDengine</p><p>Saiku</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>centos下不同jdk版本共存的问题</title>
      <link href="/2020/07/21/centos-xia-bu-tong-jdk-ban-ben-gong-cun-de-wen-ti/"/>
      <url>/2020/07/21/centos-xia-bu-tong-jdk-ban-ben-gong-cun-de-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="问题场景"><a href="#问题场景" class="headerlink" title="问题场景"></a>问题场景</h2><p>ElasticSearch7.6的安装需要java11支持，而目前服务器安装的是java8（jdk1.8），而且es安装在另一个用户下，独自运行。</p><p>而且es7.6在java8环境下无法进行启动！</p><p>这样需要根据不同用户切换不同的jdk版本。</p><h2 id="使用alternative管理"><a href="#使用alternative管理" class="headerlink" title="使用alternative管理"></a>使用alternative管理</h2><p>首先，需要安装jdk11，直接通过yum进行安装</p><pre><code>$  sudo yum list | grep java-11-openjdkjava-11-openjdk.x86_64                   1:11.0.7.10-4.el7_8            @updatesjava-11-openjdk-headless.x86_64          1:11.0.7.10-4.el7_8            @updatesjava-11-openjdk.i686                     1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-demo.i686                1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-demo.x86_64              1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-devel.i686               1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-devel.x86_64             1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-headless.i686            1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc.i686             1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc.x86_64           1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc-zip.i686         1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc-zip.x86_64       1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-jmods.i686               1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-jmods.x86_64             1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-src.i686                 1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-src.x86_64               1:11.0.7.10-4.el7_8            updates$ sudo yum install java-11-openjdk.x86_64 -y</code></pre><p>安装完成后，发现之前的java指向了java11，并非是原来的java8。</p><pre><code>// 安装前$ java -versionjava version &quot;1.8.0_202&quot;Java(TM) SE Runtime Environment (build 1.8.0_202-b08)Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)// 安装后$ java -versionopenjdk version &quot;11.0.7&quot; 2020-04-14 LTSOpenJDK Runtime Environment 18.9 (build 11.0.7+10-LTS)OpenJDK 64-Bit Server VM 18.9 (build 11.0.7+10-LTS, mixed mode, sharing)</code></pre><p>这时候alternative工具就出场了，通过该命令来查看目前已经生效的java运行程序，如下：</p><pre><code>$ sudo alternatives --config javaThere is 1 program that provides &#39;java&#39;.  Selection    Command-----------------------------------------------*+ 1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)Enter to keep the current selection[+], or type selection number:</code></pre><p>这时候新安装的jdk就生效了，也指向了其所在路径。这里可以添加多个jdk进行管理，将最开始安装的jdk导入查看，如下：</p><pre><code>$ sudo alternatives --install /usr/bin/java java /usr/java/jdk1.8.0_202/bin/ 2// 查看新增的jdk信息$ sudo alternatives --config javaThere are 2 programs which provide &#39;java&#39;.  Selection    Command-----------------------------------------------   1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)*+ 2           /usr/java/jdk1.8.0_202/bin/javaEnter to keep the current selection[+], or type selection number:</code></pre><p>这时候我们可以进行切换操作，如下：</p><pre><code>$ sudo alternatives --config javaThere are 2 programs which provide &#39;java&#39;.  Selection    Command-----------------------------------------------*+ 1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)   2           /usr/java/jdk1.8.0_202/bin/javaEnter to keep the current selection[+], or type selection number: 2</code></pre><p>输入2，选择新增的java执行信息，回车后即可生效。</p><pre><code>$ sudo alternatives --config javaThere are 2 programs which provide &#39;java&#39;.  Selection    Command-----------------------------------------------   1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)*+ 2           /usr/java/jdk1.8.0_202/bin/javaEnter to keep the current selection[+], or type selection number: // ctrl+c退出$ java -versionjava version &quot;1.8.0_202&quot;Java(TM) SE Runtime Environment (build 1.8.0_202-b08)Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)</code></pre><p>已经切换到java8的信息了，说明切换已经生效了。</p><p>但是alternative工具，只能全局生效，不管是普通用户还是root用户，都是同一个java环境，并不能进行java环境的共存。</p><h2 id="分而治之"><a href="#分而治之" class="headerlink" title="分而治之"></a>分而治之</h2><p>首先从alternative工具中删除所有java环境，如下：</p><pre><code>$ sudo alternatives --remove java /usr/java/jdk1.8.0_202/bin/java$ sudo alternatives --remove java /usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java$ sudo alternatives --config java// 执行后无输出信息，说明删除完成</code></pre><p>然后注释掉/etc/profile中java环境的配置，并使其生效：</p><pre><code>$ sudo vim /etc/profile// 拉到文档最后，注释掉配置的java环境信息# # JAVA env#JAVA_HOME=/usr/java/jdk1.8.0_202#JRE_HOME=$JAVA_HOME/jre#CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib#PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin#export JAVA_HOME JRE_HOME CLASS_PATH PATH// :wq保存退出// 使其生效$ source /etc/profile</code></pre><p>最后在不同的用户下，仿照/etc/profile中配置，配置不同的java环境</p><pre><code>// 切换到centos用户下操作$ su - centos// 输入密码信息[centos]$ vim ~/.bashrc// 在最后追加JAVA_HOME=/usr/java/jdk1.8.0_202JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH[centos]$ source ~/.bashrc[centos]$ java -versionjava version &quot;1.8.0_202&quot;Java(TM) SE Runtime Environment (build 1.8.0_202-b08)Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)// 切换到es用户（elasticsearch7.6.2部署所在的用户）$ su - es// 输入密码信息[es]$ vim ~/.bashrc// 在最后追加JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH[es]$ source ~/.bashrcopenjdk version &quot;11.0.7&quot; 2020-04-14 LTSOpenJDK Runtime Environment 18.9 (build 11.0.7+10-LTS)OpenJDK 64-Bit Server VM 18.9 (build 11.0.7+10-LTS, mixed mode, sharing)</code></pre><p>这样根据用户的不同可以使用不同版本的java环境，这样就能启动es了。</p><h2 id="关于skywalking启动的流程"><a href="#关于skywalking启动的流程" class="headerlink" title="关于skywalking启动的流程"></a>关于skywalking启动的流程</h2><ol><li><p>切换到es用户，执行*<em>./elasticsearch-7.6.2/bin/elasticsearch -d</em>命令，启动es</p></li><li><p>切换到centos用户，执行*<em>./apache-skywalking-apm-bin-es7/bin/statup.sh</em>，启动skywalking</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2020/07/19/kafka-xue-xi-2/"/>
      <url>/2020/07/19/kafka-xue-xi-2/</url>
      
        <content type="html"><![CDATA[<h1 id="kafka学习-2"><a href="#kafka学习-2" class="headerlink" title="kafka学习 2"></a>kafka学习 2</h1><h2 id="压缩策略"><a href="#压缩策略" class="headerlink" title="压缩策略"></a>压缩策略</h2><p>两种例外情况就可能让 Broker 重新压缩消息。</p><p>情况一：Broker 端指定了和 Producer 端不同的压缩算法。<br>情况二：Broker 端发生了消息格式转换</p><p>Kafka 会将启用了哪种压缩算法封装进消息集合中<br>Producer 端压缩、Broker 端保持、Consumer 端解压缩。</p><p>解压缩对 Broker 端性能 是有一定影响的，特别是对 CPU 的使用率而言。</p><p>四种压缩算法：GZIP、Snappy、LZ4、zstd</p><p>即在吞吐量方面：LZ4 &gt; Snappy &gt; zstd 和 GZIP；在压缩比方面，zstd &gt; LZ4 &gt; GZIP &gt; Snappy</p><p>何时启用压缩？</p><ol><li><p>Producer端完成压缩：Producer 程序运 行机器上的 CPU 资源要很充足</p></li><li><p>CPU 资源充足这一条件，如果你的环境中带宽资源有限，建议开启压缩，建议你开启 zstd 压缩，这样能极大地节省网络资源消耗</p></li></ol><p>解压缩问题：</p><p>有条件的话尽量保证 不要出现消息格式转换的情况</p><h2 id="无消息丢失"><a href="#无消息丢失" class="headerlink" title="无消息丢失"></a>无消息丢失</h2><p>原则：<br>Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。</p><p>已提交的消息：若干个 Broker 成 功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。</p><p>有限度的持久化保证：Kafka 不可能保证在任何情况下 都做到不丢失消息。</p><ul><li>Producer丢失消息</li></ul><p>Kafka 依然不认为这条消息属于已提交消息，故对它不做任何持久化保证。</p><p>执行完一个操作 后不去管它的结果是否成功。调用 producer.send(msg) 就属于典型的“fire and forget”</p><p>解决方式：Producer 永远要使用带有回调通知的发送 API，也就是说不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)</p><ul><li>消费者程序丢失数据</li></ul><p>Consumer 端的位移数据，容易造成数据丢失。没有真正地确认消息是否真的被消费就“盲目”地更新了位移。</p><p>解决方式：维持先消费消息（阅读），再更新位移（书签）的顺序。如果是多线程异步处理消费消息，Consumer 程序不要开 启自动提交位移，而是要应用程序手动提交位移。</p><p>总结：</p><ol><li>不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一 定要使用带有回调通知的 send 方法。</li><li>设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。 如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。 这是最高等级的“已提交”定义。</li><li>设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到 的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了retries &gt; 0 的 Producer 能够自动重试消息发送，避免消息丢失。</li><li>设置 unclean.leader.election.enable = false。这是 Broker 端的参数，它控制的是哪 些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么 它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false， 即不允许这种情况的发生。</li><li>设置 replication.factor &gt;= 3。这也是 Broker 端的参数。其实这里想表述的是，最好将 消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余。</li><li>设置 min.insync.replicas &gt; 1。这依然是 Broker 端参数，控制的是消息至少要被写入 到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千 万不要使用默认值 1。</li><li>确保 replication.factor &gt; min.insync.replicas。如果两者相等，那么只要有一个副本挂 机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要 在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1。</li><li>确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设 置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程 处理的场景而言是至关重要的。</li></ol><p>问题：</p><p>Kafka 还有一种特别隐秘的消息丢失场景：增加主题分区。当增加主题分区后，在某 段“不凑巧”的时间间隔后，Producer 先于 Consumer 感知到新增加的分区，而 Consumer 设置的是“从最新位移处”开始读取消息，因此在 Consumer 感知到新分区 前，Producer 发送的这些消息就全部“丢失”了，或者说 Consumer 无法读取到这些消 息。严格来说这是 Kafka 设计上的一个小缺陷，你有什么解决的办法吗？</p><p>解答：新增分区之后，producer先感知并发送数据，消费者后感知，消费者 的offset会定位到新分区的最后一条消息，如果你配置了auto.offset.reset=latest就会这样的</p><h2 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h2><p>preHandler、postHandler</p><p>多个拦截器可用ArrayList串联：<br>props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</p><p>org.apache.kafka.clients.producer.ProducerInterceptor  </p><p>onSend：该方法会在消息发送之前被调用<br>onAcknowledgement：该方法会在消息成功提交或发送失败之后被调用。（onAcknowledgement 的调用要早于 callback 的调用。值得注意的是，这个方法和 onSend 不是在同一个线程中被调用的， 因此如果你在这两个方法中调用了某个共享可变对象，一定要保证线程安全哦。还有一 点很重要，这个方法处在 Producer 发送的主路径中，所以最好别放一些太重的逻辑进 去，否则你会发现你的 Producer TPS 直线下降。）</p><p>org.apache.kafka.clients.consumer.ConsumerInterceptor</p><p>onConsume：该方法在消息返回给 Consumer 程序之前调用。<br>onCommit：Consumer 在提交位移之后调用该方法。</p><p>应用场景：Kafka 拦截器可以 应用于包括客户端监控、端到端系统性能检测、消息审计等多种功能在内的场景。</p><h2 id="生产者如何管理TCP连接"><a href="#生产者如何管理TCP连接" class="headerlink" title="生产者如何管理TCP连接"></a>生产者如何管理TCP连接</h2><p>Kafka 的所有通信都是基于 TCP 的</p><p>作为一个基于报文的协议，TCP 能够被用于多 路复用连接场景的前提是，上层的应用协议（比如 HTTP） 允许发送多条消息。</p><p>何时创建：在创建 KafkaProducer 实例时，生产者应用会 在后台创建并启动一个名为 Sender 的线程，该 Sender 线 程开始运行时首先会创建与 Broker 的连接。</p><p>TCP 连接是在创建 KafkaProducer 实例时建立的。TCP 连接还可能在两个地方被创建：一个是在更 新元数据后，另一个是在消息发送时。</p><p>何时关闭：一种是用户主动 关闭；一种是 Kafka 自动关闭（connections.max.idle.ms 的值有关）。</p><h2 id="消息交付可靠性保障"><a href="#消息交付可靠性保障" class="headerlink" title="消息交付可靠性保障"></a>消息交付可靠性保障</h2><ul><li>最多一次（at most once）：消息可能会丢失，但绝不会 被重复发送。</li><li>至少一次（at least once）：消息不会丢失，但有可能被 重复发送。</li><li>精确一次（exactly once）：消息不会丢失，也不会被重 复发送。</li></ul><p>Kafka 是怎么做到精确一次：幂等性（Idempotence）和事务 （Transaction）</p><p>幂等性：安全重试，反正它们也不会破坏我们的系统状态</p><p>在命令式编程语言（比如 C）中，若一个子程序是幂等 的，那它必然不能修改系统状态。这样不管运行这个子程 序多少次，与该子程序关联的那部分系统状态保持不变。 在函数式编程语言（比如 Scala 或 Haskell）中，很多纯 函数（pure function）天然就是幂等的，它们不执行任 何的 side effect。</p><p>开启：props.put(“enable.idempotence”, ture) 或者 props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CO NFIG，true)</p><p>缺陷：只能保证单分区上的幂等性</p><p>事务性：实现多分区以及多会话上的消息无重复</p><p>提供的安全性保障是经典的 ACID，即原子性（Atomicity）、一致性 (Consistency)、隔离性 (Isolation) 和持久性 (Durability)<br>隔离性表明并发执行的事务彼此相互隔离，互不影响</p><p>开启：开启 enable.idempotence = true。设置 Producer 端参数 transctional.id</p><p>幂等性Producer只能保证单分区、单会话上的消息幂等性；而事务能够保证跨分区、跨会话间的幂等性。</p><h2 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h2><p>Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制.</p><ol><li>Consumer Group 下可以有一个或多个 Consumer 实 例。这里的实例可以是一个单独的进程，也可以是同一进 程下的线程。在实际场景中，使用进程更为常见一些。</li><li>Group ID 是一个字符串，在一个 Kafka 集群中，它标识 唯一的一个 Consumer Group。</li><li>Consumer Group 下所有实例订阅的主题的单个分区， 只能分配给组内的某个 Consumer 实例消费。这个分区 当然也可以被其他的 Group 消费。</li></ol><p>点对点模型和发布 / 订阅模型</p><p>Consumer Group机制：如果所有实例都属于同一个 Group， 那么它实现的就是消息队列模型；如果所有实例分别属于不同的 Group，那么它实现的就是发布 / 订阅模型。</p><p>确定Consumer的实例数量：理想情况下，Consumer 实例的数量应该等于该 Group 订阅主题的分区总数。</p><p>位移管理：对于 Consumer Group 而言，它是一组 KV 对，Key 是分区，V 对应 Consumer 消费该分区的最新位移。如果用</p><p>新版本采用了将位移 保存在 Kafka 内部主题的方法。这个内部主题就是让人既爱又恨的 __consumer_offsets。</p><p>重平衡：Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区。比如某个 Group 下有 20 个 Consumer 实例，它订阅了一个具有 100 个分区的 Topic。正常情况下，Kafka 平均会为每个 Consumer 分配 5 个分区。这个分配的过程就叫 Rebalance。</p><p>发生条件：</p><ol><li>组成员数发生变更。比如有新的 Consumer 实例加入组 或者离开组，抑或是有 Consumer 实例崩溃被“踢 出”组。</li><li>订阅主题数发生变更。Consumer Group 可以使用正则 表达式的方式订阅主题，比如<br>consumer.subscribe(Pattern.compile(“t.*c”)) 就表 明该 Group 订阅所有以字母 t 开头、字母 c 结尾的主 题。在 Consumer Group 的运行过程中，你新创建了一 个满足这样条件的主题，那么该 Group 就会发生 Rebalance。</li><li>订阅主题的分区数发生变更。Kafka 当前只能允许增加一 个主题的分区数。当分区数增加时，就会触发订阅该主题 的所有 Group 开启 Rebalance。</li></ol><p>缺陷：效率低，STW僵死问题。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于ElasticSearch的数据清理以及日志清理</title>
      <link href="/2020/07/15/guan-yu-elasticsearch-de-shu-ju-qing-li-yi-ji-ri-zhi-qing-li/"/>
      <url>/2020/07/15/guan-yu-elasticsearch-de-shu-ju-qing-li-yi-ji-ri-zhi-qing-li/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在服务日常的运行中，ELK日志体系出现了大量的日志文件占据硬盘空间的问题，以及存在ES中数据过多，导致索引和查询的速度变慢的情况。急切需要定期清理的策略！</p><p>这里定期清理的时候，分为两个部分，一个是ES中存储的数据问题，数据量过大导致查询变慢；另一个是ES、Logstash、Kibana以及Skywalking这些工具的日常运行日志，这些日志生成过多，而且占据硬盘空间较大。</p><p>鉴于我们现在并未真正进入生产环境，这里在开发环境中，对运行日志设定存储策略，规则如下：</p><ul><li>Skywalking日志的定期清理，保留最近三天的运行日志信息</li><li>ES、Logstash、Kibana各自保留最近三天内的日志信息</li></ul><p>对ES的数据设定存储策略，规则如下：</p><ul><li>保留最近三天的数据信息</li></ul><h2 id="ES数据清理"><a href="#ES数据清理" class="headerlink" title="ES数据清理"></a>ES数据清理</h2><p>清理脚本如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span><span class="token comment" spellcheck="true">###################################</span><span class="token comment" spellcheck="true">#删除早于3天的ES集群的索引</span><span class="token comment" spellcheck="true">###################################</span><span class="token comment" spellcheck="true"># IP地址信息，ip:port</span>IP_ADDR<span class="token operator">=</span><span class="token string">"ip:port"</span><span class="token keyword">function</span> delete_indices<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    comp_date<span class="token operator">=</span>`date -d <span class="token string">"3 day ago"</span> +<span class="token string">"%Y-%m-%d"</span>`    date1<span class="token operator">=</span><span class="token string">"<span class="token variable">$1</span> 00:00:00"</span>    date2<span class="token operator">=</span><span class="token string">"<span class="token variable">$comp_date</span> 00:00:00"</span>    t1<span class="token operator">=</span>`date -d <span class="token string">"<span class="token variable">$date1</span>"</span> +%s<span class="token variable"><span class="token variable">`</span>    t2<span class="token operator">=</span><span class="token variable">`</span></span><span class="token function">date</span> -d <span class="token string">"<span class="token variable">$date2</span>"</span> +%s`     <span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$t1</span> -le <span class="token variable">$t2</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>         <span class="token keyword">echo</span> <span class="token string">"<span class="token variable">$1</span>时间早于<span class="token variable">$comp_date</span>，进行索引删除"</span>        <span class="token comment" spellcheck="true">#转换一下格式，将类似2017--01格式转化为2017.10.01</span>        format_date<span class="token operator">=</span>`echo $<span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">'s/-/\./g'</span>`        curl -XDELETE http://<span class="token variable">$IP_ADDR</span>/*<span class="token variable">$format_date</span>    <span class="token keyword">fi</span><span class="token punctuation">}</span>curl -XGET http://<span class="token variable">$IP_ADDR</span>/_cat/indices <span class="token operator">|</span> <span class="token function">awk</span> -F<span class="token string">" "</span> <span class="token string">'{print <span class="token variable">$3</span>}'</span> <span class="token operator">|</span> <span class="token function">awk</span> -F<span class="token string">"-"</span> <span class="token string">'{print <span class="token variable">$NF</span>}'</span> <span class="token operator">|</span> <span class="token function">egrep</span> <span class="token string">"[0-9]*\.[0-9]*\.[0-9]*"</span> <span class="token operator">|</span> <span class="token function">sort</span> <span class="token operator">|</span> <span class="token function">uniq</span>  <span class="token operator">|</span> <span class="token function">sed</span> <span class="token string">'s/\./-/g'</span> <span class="token operator">|</span> <span class="token keyword">while</span> <span class="token function">read</span> LINE<span class="token keyword">do</span>    <span class="token comment" spellcheck="true">#调用索引删除函数</span>    delete_indices <span class="token variable">$LINE</span><span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>利用date命令获取对应的日期信息，删除对应日期之前的索引数据。</p><p>后续修改主机的ip地址信息，配置crontab定时任务，执行即可。</p><p><strong>注意：</strong>当ES启用x-pack插件时，存在对ES的用户名和密码的验证，在curl执行时，一定注意要添加<em>*-u ES_USERNAME:ES_PASSWORD</em>!</p><h2 id="Skywalking日志清理"><a href="#Skywalking日志清理" class="headerlink" title="Skywalking日志清理"></a>Skywalking日志清理</h2><p>清理脚本如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span><span class="token comment" spellcheck="true">###################################</span><span class="token comment" spellcheck="true">#删除早于2天的ES集群的索引</span><span class="token comment" spellcheck="true">###################################</span><span class="token keyword">function</span> delete_indices<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    comp_date<span class="token operator">=</span>`date -d <span class="token string">"2 day ago"</span> +<span class="token string">"%Y-%m-%d"</span>`    date1<span class="token operator">=</span><span class="token string">"<span class="token variable">$1</span> 00:00:00"</span>    date2<span class="token operator">=</span><span class="token string">"<span class="token variable">$comp_date</span> 00:00:00"</span>    t1<span class="token operator">=</span>`date -d <span class="token string">"<span class="token variable">$date1</span>"</span> +%s<span class="token variable"><span class="token variable">`</span>    t2<span class="token operator">=</span><span class="token variable">`</span></span><span class="token function">date</span> -d <span class="token string">"<span class="token variable">$date2</span>"</span> +%s`    <span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$t1</span> -le <span class="token variable">$t2</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>        <span class="token keyword">echo</span> <span class="token string">"<span class="token variable">$1</span>时间早于<span class="token variable">$comp_date</span>，进行索引删除"</span>        <span class="token comment" spellcheck="true">#curl -XDELETE http://192.168.21.208:9200/*$format_date</span>        <span class="token function">rm</span> -rf skywalking-oap-server-<span class="token variable">$1*</span>.log    <span class="token keyword">fi</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># 筛选日志信息，对应路径下，筛选日志文件名称，提取日期信息，传入处理函数中</span><span class="token function">cd</span> /home/centos/apache-skywalking-apm-bin-es7/logs <span class="token operator">&amp;&amp;</span> <span class="token function">ls</span> -al <span class="token operator">|</span> <span class="token function">grep</span> skywalking-oap-server-20 <span class="token operator">|</span> <span class="token function">awk</span> <span class="token string">'{print <span class="token variable">$9</span>}'</span> <span class="token operator">|</span> <span class="token function">sort</span> <span class="token operator">|</span> <span class="token function">uniq</span> <span class="token operator">|</span> <span class="token function">grep</span> -P -o <span class="token string">"[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}"</span> <span class="token operator">|</span> <span class="token keyword">while</span> <span class="token function">read</span> LINE<span class="token keyword">do</span>    <span class="token comment" spellcheck="true">#调用索引删除函数</span>    delete_indices <span class="token variable">$LINE</span><span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将脚本置于对应的机器中，目前该脚本可以放置在192.168.21.208机器上，配置crontab定时任务，执行即可。</p><h2 id="ELK运行日志限制"><a href="#ELK运行日志限制" class="headerlink" title="ELK运行日志限制"></a>ELK运行日志限制</h2><h3 id="ElasticSearch日志配置"><a href="#ElasticSearch日志配置" class="headerlink" title="ElasticSearch日志配置"></a>ElasticSearch日志配置</h3><pre><code>$ sudo vim /opt/es/config/log4j2.properties######## Server JSON ############################appender.rolling.type = RollingFileappender.rolling.name = rollingappender.rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_server.jsonappender.rolling.layout.type = ESJsonLayoutappender.rolling.layout.type_name = serverappender.rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}-%d{yyyy-MM-dd}-%i.json.gzappender.rolling.policies.type = Policiesappender.rolling.policies.time.type = TimeBasedTriggeringPolicyappender.rolling.policies.time.interval = 1appender.rolling.policies.time.modulate = trueappender.rolling.policies.size.type = SizeBasedTriggeringPolicyappender.rolling.policies.size.size = 128MBappender.rolling.strategy.type = DefaultRolloverStrategyappender.rolling.strategy.fileIndex = nomaxappender.rolling.strategy.action.type = Deleteappender.rolling.strategy.action.basepath = ${sys:es.logs.base_path}appender.rolling.strategy.action.condition.type = IfFileNameappender.rolling.strategy.action.condition.glob = ${sys:es.logs.cluster_name}-*# 删除该处的内容#appender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize#appender.rolling.strategy.action.condition.nested_condition.exceeds = 2GB# 下面是替换的内容appender.rolling.strategy.action.condition.nested_condition.type = IfLastModifiedappender.rolling.strategy.action.condition.nested_condition.age = 3D</code></pre><p>配置信息解释：</p><pre><code>appender.rolling.type = RollingFile ：配置RollingFile输出源appender.rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}.log：日志到/var/log/elasticsearch/production.logappender.rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}-%d{yyyy-MM-dd}-%i.log.gz：滚动日志到/var/log/elasticsearch/production-yyyy-MM-dd-i.log，日志将被压缩在每个滚动上，并且i将被递增。appender.rolling.policies.time.type = TimeBasedTriggeringPolicy：使用基于时间的滚动策略appender.rolling.policies.time.interval = 1：每天滚动日志appender.rolling.policies.time.modulate = true：在一天的边界上对齐滚动条(而不是每24小时滚动一次)appender.rolling.policies.size.type = SizeBasedTriggeringPolicy：使用基于大小的滚动策略appender.rolling.policies.size.size = 256MB：在256MB后滚动日志appender.rolling.strategy.action.type = Delete：在滚动日志时使用删除操作appender.rolling.strategy.action.condition.type = IfFileName：只删除匹配文件模式的日志appender.rolling.strategy.action.condition.glob = ${sys:es.logs.cluster_name}-*：模式是只删除主日志appender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize：只有当我们积累了太多的压缩日志时才删除appender.rolling.strategy.action.condition.nested_condition.exceeds = 2GB：压缩日志的大小条件是2GB</code></pre><p>修改的内容如下：</p><ul><li>appender.rolling.strategy.action.condition.nested_condition.type= IfLastModified   # 要应用于与glob匹配的文件的嵌套条件</li><li>appender.rolling.strategy.action.condition.nested_condition.age= 3D                # 保留日志三天</li></ul><p>原因如下：</p><p>默认配置问题：ElasticSearch默认情况下会每天rolling一个文件，当到达2G的时候，才开始清除超出的部分，当一个文件只有几十K的时候，文件会一直累计下来，一直占据存储空间，并不清除！</p><p>修改完成后记得重启ElasticSearch！</p><h3 id="Logstash日志配置"><a href="#Logstash日志配置" class="headerlink" title="Logstash日志配置"></a>Logstash日志配置</h3><pre><code>$ sudo vim /opt/logstash/config/log4j2.properties// 从DefaultRolloverStrategy这个开始新增appender.rolling.strategy.type = DefaultRolloverStrategyappender.rolling.strategy.max = 30appender.rolling.strategy.action.type = Deleteappender.rolling.strategy.action.basepath = ${sys:es.logs.base_path}appender.rolling.strategy.action.condition.type = IfFileNameappender.rolling.strategy.action.condition.glob = ${sys:es.logs.cluster_name}-*appender.rolling.strategy.action.condition.nested_condition.type = IfLastModifiedappender.rolling.strategy.action.condition.nested_condition.exceeds = 3D</code></pre><p>修改原因：</p><p>默认配置问题:Logstash会一直增长gc文件和不停增多的rolling日志文件，并且不会删除。</p><p>这里配置基本上和ES的日志配置是一致的。</p><p>修改完成后记得重启Logstash。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>日志信息保留，三个月周期，设置时参考上述配置信息即可！</p><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><ul><li>清理脚本编写：<a href="https://blog.csdn.net/felix_yujing/article/details/78207667" target="_blank" rel="noopener">https://blog.csdn.net/felix_yujing/article/details/78207667</a></li><li>关于ELK的运行日志设置：<a href="https://blog.51cto.com/huanghai/2430038" target="_blank" rel="noopener">https://blog.51cto.com/huanghai/2430038</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CKA备考学习</title>
      <link href="/2020/07/12/cka-bei-kao-xue-xi/"/>
      <url>/2020/07/12/cka-bei-kao-xue-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="CKA备考学习"><a href="#CKA备考学习" class="headerlink" title="CKA备考学习"></a>CKA备考学习</h1><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><ul><li>source &lt;(kubectl completion bash) # 切换到任意节点，先执行命令补全，减少负担</li><li>kubernetes集群信息，1.18.1</li><li>关于<a href="https://kubernetes.io/docs/，请尽量使用科学上网访问，另外可能存在jquery找不到的情况，需要把jquery的地址添加到科学上网" target="_blank" rel="noopener">https://kubernetes.io/docs/，请尽量使用科学上网访问，另外可能存在jquery找不到的情况，需要把jquery的地址添加到科学上网</a></li></ul><h2 id="kubectl-get-命令使用"><a href="#kubectl-get-命令使用" class="headerlink" title="kubectl get 命令使用"></a>kubectl get 命令使用</h2><p>kubectl get pods &lt;pod名称&gt; -o yaml -n <namespace> &gt;<br>查看创建的参数信息</namespace></p><p>kubectl describe  查看运行情况以及事件信息</p><p>变量选择器：–field-selector</p><p>标签展示：-L –labels</p><p>输出格式选择：-o json/yaml。。。</p><p>kubectl get pod -A –show-labels 展示所有pod的标签信息</p><p>kubectl get pod -A -L k8s-app  展示所有pod，添加k8s-app标签列，包含改标签的则显示</p><p>kubectl get pod  -L k8s-app -n kube-system</p><p>kubectl get pod  -L k8s-app –no-headers -n kube-system 去掉标题头信息</p><p>kubectl label node worker type=nginx      对node打标签<br>node/worker labeled</p><p>kubectl get pod -n test-ns –watch  查看该命名空间下的pod状态变化<br>NAME                                READY   STATUS              RESTARTS   AGE<br>nginx-deployment-59db5dc5d5-l4x6x   0/1     ContainerCreating   0          18s<br>nginx-deployment-59db5dc5d5-l4x6x   1/1     Running             0          28s</p><p>-l, –selector 选择器方式<br>kubectl get pod  -l k8s-app=kube-proxy -n kube-system  筛选标签k8s-app的值为kube-proxy的pod</p><p>–sort-by   根据条件排序<br>kubectl  get pod -A –sort-by metadata.creationTimestamp -o wide  </p><p>—————————学习与做题的分割线—————————————————–</p><p>题目1：Set configuration context $kubectl config use-context k8s. List all PVs sorted by name, saving the full kubectl output to /opt/KUCC0010/my_volumes. Use kubectl own functionally for sorting the output, and do not manipulate it any further.</p><p>我的解答：</p><pre><code>$ kubectl config use-context k8s$ kubectl get pv -A --sort-by name &gt; /opt/KUCC0010/my_volumes</code></pre><p>标准解答：</p><pre><code>kubectl config use-context k8s # 切换集群环境 mkdir /opt/KUCC0010 # 创建对应文件夹kubectl get --help | grep &#39;sort&#39; # 查看排序命令写法 kubectl get pv --help # 查看--all-namespaces 的用法kubectl get pv -A -o yaml | grep name # 建议操作之前看一下name属性是否包含kubectl get pv --all-namespaces --sort-by={.metadata.name} &gt; /opt/KUCC0010/my_volumes</code></pre><h2 id="kubectl-logs-命令使用"><a href="#kubectl-logs-命令使用" class="headerlink" title="kubectl logs 命令使用"></a>kubectl logs 命令使用</h2><p>-c 指定查看pod下某个容器的日志</p><p>创建一个多container的pod，如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>multi<span class="token punctuation">-</span>container<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> redis        <span class="token key atrule">name</span><span class="token punctuation">:</span> redis      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> consul        <span class="token key atrule">name</span><span class="token punctuation">:</span> consul      <span class="token comment" spellcheck="true">#标签选择器</span>      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">type</span><span class="token punctuation">:</span> nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查看运行情况：</p><p>kubectl get pod -n test-ns –watch<br>NAME                                     READY   STATUS              RESTARTS   AGE<br>nginx-multi-container-6f7fbb7c59-j4sjc   0/3     ContainerCreating   0          10s<br>nginx-multi-container-6f7fbb7c59-j4sjc   3/3     Running             0          49s</p><p>查看日志信息：<br>kubectl logs nginx-multi-container-6f7fbb7c59-j4sjc -n test-ns<br>error: a container name must be specified for pod nginx-multi-container-6f7fbb7c59-j4sjc, choose one of: [nginx redis consul]</p><p>使用-c指定container，再来查看日志信息：<br>kubectl logs nginx-multi-container-6f7fbb7c59-j4sjc -c redis -n test-ns<br>1:C 12 Jul 2020 11:46:06.093 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo<br>1:C 12 Jul 2020 11:46:06.093 # Redis version=6.0.5, bits=64, commit=00000000, modified=0, pid=1, just started<br>1:C 12 Jul 2020 11:46:06.093 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf<br>1:M 12 Jul 2020 11:46:06.094 * Running mode=standalone, port=6379.<br>1:M 12 Jul 2020 11:46:06.094 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.<br>1:M 12 Jul 2020 11:46:06.094 # Server initialized<br>1:M 12 Jul 2020 11:46:06.094 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command ‘echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled’ as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.<br>1:M 12 Jul 2020 11:46:06.094 * Ready to accept connections</p><p>根据labels筛选要展示的pod日志信息<br> kubectl logs -l k8s-app=kube-dns -n kube-system<br>[INFO] plugin/ready: Still waiting on: “kubernetes”<br>// 第一个pod<br>.:53<br>[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7<br>CoreDNS-1.6.7<br>linux/amd64, go1.13.6, da7f65b<br>E0712 09:54:34.694215       1 reflector.go:153] pkg/mod/k8s.io/client-go@v0.17.2/tools/cache/reflector.go:105: Failed to list *v1.Endpoints: Get <a href="https://172.168.0.1:443/api/v1/endpoints?limit=500&amp;resourceVersion=0" target="_blank" rel="noopener">https://172.168.0.1:443/api/v1/endpoints?limit=500&amp;resourceVersion=0</a>: dial tcp 172.168.0.1:443: connect: connection timed out<br>E0712 09:54:34.694327       1 reflector.go:153] pkg/mod/k8s.io/client-go@v0.17.2/tools/cache/reflector.go:105: Failed to list *v1.Service: Get <a href="https://172.168.0.1:443/api/v1/services?limit=500&amp;resourceVersion=0" target="_blank" rel="noopener">https://172.168.0.1:443/api/v1/services?limit=500&amp;resourceVersion=0</a>: dial tcp 172.168.0.1:443: connect: connection timed out<br>E0712 09:54:34.694370       1 reflector.go:153] pkg/mod/k8s.io/client-go@v0.17.2/tools/cache/reflector.go:105: Failed to list *v1.Namespace: Get <a href="https://172.168.0.1:443/api/v1/namespaces?limit=500&amp;resourceVersion=0" target="_blank" rel="noopener">https://172.168.0.1:443/api/v1/namespaces?limit=500&amp;resourceVersion=0</a>: dial tcp 172.168.0.1:443: connect: connection timed out<br>// 第二个pod<br>.:53<br>[INFO] plugin/reload: Running configuration MD5 = 4e235fcc3696966e76816bcd9034ebc7<br>CoreDNS-1.6.7<br>linux/amd64, go1.13.6, da7f65b</p><p>根据时间范围选择要打印的日志信息，–since后添加1h,1m,30s这样的数值<br>kubectl logs etcd-master1  -n kube-system –since=1h</p><p>筛选某个时间点后的日志信息，<br>kubectl logs etcd-master1  -n kube-system –since-time=2020-07-12T12:37:26Z<br>2020-07-12 12:37:26.292580 I | mvcc: store.index: compact 281140<br>2020-07-12 12:37:26.309379 I | mvcc: finished scheduled compaction at 281140 (took 16.558415ms)<br>2020-07-12 12:42:26.297970 I | mvcc: store.index: compact 281968<br>2020-07-12 12:42:26.312766 I | mvcc: finished scheduled compaction at 281968 (took 14.279005ms)<br>2020-07-12 12:47:26.304332 I | mvcc: store.index: compact 282795<br>2020-07-12 12:47:26.319885 I | mvcc: finished scheduled compaction at 282795 (took 14.755136ms)<br>2020-07-12 12:52:26.309255 I | mvcc: store.index: compact 283624<br>2020-07-12 12:52:26.322046 I | mvcc: finished scheduled compaction at 283624 (took 12.421311ms)</p><p>查看kubelet系统级日志，对其它服务同样适用，可以加-n确定最近多少行的日志，可以加–no-pager让日志合理打印换行展示<br>journalctl -n kubelet</p><p>audit、event日志信息<br>event实际是一种系统资源，变动、升级、创建、删除等信息<br>kubectl get events</p><p>注意：1. 开启日志复用和滚动  2. docker升级版本，注意日志生成的路径信息</p><p>—————————学习与做题的分割线—————————————————–<br>题目2：Set configuration context $kubectl config use-context k8s. Monitor the logs of Pod foobar and Extract log lines corresponding to error unable-to-access-website . Write them to /opt/KULM00201/foobar.</p><p>解答1：</p><pre><code>$ kubectl config use-context k8s$ mkdir /opt/KULM00201/$ kubectl logs foobar | grep &#39;unable-to-access-website&#39; &gt; /opt/KULM00201/foobar</code></pre><h2 id="kubernetes中的pod和label"><a href="#kubernetes中的pod和label" class="headerlink" title="kubernetes中的pod和label"></a>kubernetes中的pod和label</h2><p>创建pod</p><p>kubectl run pod1 –image=nginx –generator=run-pod/v1<br>Flag –generator has been deprecated, has no effect and will be removed in the future.<br>pod/pod1 created</p><p>用配置文件的方式，创建pod</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> pod<span class="token punctuation">-</span><span class="token number">3</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.16.1    <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>kubectl apply -f nginx.yaml</p><p>给一个pod添加label<br>kubectl run pod1 –image=nginx –generator=run-pod/v1 –labels function=mantou</p><p>查看pod所含有的labels<br>kubectl get pod pod1 -o yaml | more<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  annotations:<br>    cni.projectcalico.org/podIP: 10.172.171.74/32<br>    cni.projectcalico.org/podIPs: 10.172.171.74/32<br>  creationTimestamp: “2020-07-12T15:04:13Z”<br>  labels:<br>    function: mantou<br>…….</p><p>或者是：</p><p> kubectl describe pod pod1 | grep Labels<br>Labels:       function=mantou</p><p>添加或修改pod已有的labels<br>kubectl edit pod pod1</p><p>apiVersion: v1<br>kind: Pod<br>metadata:<br>  annotations:<br>    cni.projectcalico.org/podIP: 10.172.171.74/32<br>    cni.projectcalico.org/podIPs: 10.172.171.74/32<br>  creationTimestamp: “2020-07-12T15:04:13Z”<br>  labels:<br>    function: mifan       # 修改馒头为米饭<br>    nengbunengchi: neng   # 新增一个label</p><p>记得保存退出</p><p>—————————学习与做题的分割线—————————————————–<br>题目3：创建一个Pod名称为nginx-app，镜像为nginx，添加label disk=ssd和label env=prod</p><p>解答1：</p><pre><code>$ kubectl run nginx-app --image=nginx --labels disk=ssd,env=prod</code></pre><p>解答2：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>app  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">disk</span><span class="token punctuation">:</span> ssd    <span class="token key atrule">env</span><span class="token punctuation">:</span> prod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>kubectl apply -f nginx.yaml</code></pre><h2 id="kubernetes中的多容器pod"><a href="#kubernetes中的多容器pod" class="headerlink" title="kubernetes中的多容器pod"></a>kubernetes中的多容器pod</h2><p>题目4：创建一个名字为kucc的Pod，其中内部运行着nginx+redis+memcached+consul 四个容器</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> kucc<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> redis    <span class="token key atrule">name</span><span class="token punctuation">:</span> redis  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> memcached    <span class="token key atrule">name</span><span class="token punctuation">:</span> memcached  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> consul    <span class="token key atrule">name</span><span class="token punctuation">:</span> consul<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>$ kubectl apply -f kucc.yaml</code></pre><p>———-学习与做题的分割线————-</p><p>多容器pod的定义方式：</p><ul><li>sidecar: 不起决定作用，多带点东西，例如Prometheus中node_export、fluentd日志采集</li><li>adapter: 修改入站和出站的数据，例如格式化容器输出的监控数据RESTFUL format，功能类似hdmi转vga的转接头</li><li>ambassador: (proxy) istio，spring cloud gateway，zuul，使用容器提供的代理模式，而不是使用k8s自身的代理模式</li></ul><h2 id="kubernetes中DaemonSet相关概念"><a href="#kubernetes中DaemonSet相关概念" class="headerlink" title="kubernetes中DaemonSet相关概念"></a>kubernetes中DaemonSet相关概念</h2><p>DaemonSet: 确保在全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。</p><p>使用范围：</p><ul><li>运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。</li><li>在每个 Node 上运行日志收集 daemon，例如fluentd、logstash。</li><li>在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。</li></ul><p>具体使用：kubernetes中log-pilot部署</p><p>taint与toleration：Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是相吸的。Tolerations作用于pod上,允许(但不是必须)pod被调度到有符合的污点(taint)的节点上。</p><p><a href="https://www.cnblogs.com/tylerzhou/p/11026364.html" target="_blank" rel="noopener">https://www.cnblogs.com/tylerzhou/p/11026364.html</a></p><p>——————————学习与做题的分割线———————————————</p><p>题目5：确保在 kubectl 集群的每个节点上运行一个 Nginx Pod。其中 Nginx Pod 必须使用 Nginx 镜像。<strong>不要覆盖当前环境中的任何 traints。</strong> 使用 Daemonset 来完成这个任务，Daemonset 的名字使用 ds。</p><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a></p><p>daemonset.yaml</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> ds  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>$ kubectl apply -f daemonset.yaml $ kubectl get daemonset -n test-nsNAME   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGEds     1         1         1       1            1           &lt;none&gt;          19s</code></pre><p>如果是<strong>需要覆盖taints</strong>，例如新增一个节点taint，</p><pre><code>kubectl taint nodes node1 key=value:NoSchedule或者查看taintkubectl describe nodes node1 |grep -E &#39;(Roles|Taints)&#39;</code></pre><p>然后允许nginx镜像调度到taint所在的节点，“key”值要与“effect”值对应，配置如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> ds  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>ns<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx          <span class="token comment" spellcheck="true"># 该位置与下面template中的labels中name的值对应</span>  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">kubernetes.io/hostname</span><span class="token punctuation">:</span> master1      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> <span class="token string">"key"</span>          <span class="token key atrule">operator</span><span class="token punctuation">:</span> <span class="token string">"Equal"</span>          <span class="token key atrule">value</span><span class="token punctuation">:</span> <span class="token string">"value"</span>          <span class="token key atrule">effect</span><span class="token punctuation">:</span> <span class="token string">"NoSchedule"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="kubernetes中initcontainer相关概念"><a href="#kubernetes中initcontainer相关概念" class="headerlink" title="kubernetes中initcontainer相关概念"></a>kubernetes中initcontainer相关概念</h2><p>一个pod里可以运行多个容器,它也可以运行一个或者多个初始容器,初始容器先于应用容器运行,除了以下两点外,初始容器和普通容器没有什么两样:</p><ul><li>它们总是run to completion</li><li>一个初始容器必须成功运行另一个才能运行</li></ul><p>初始容器不支持可用性探针(readiness probe),因为它在ready之前必须run to completion。如果在一个pod里指定了多个初始容器,则它们会依次启动起来(pod内的普通容器并行启动),并且只有上一个成功下一个才能启动.当所有的初始容器都启动了,kubernetes才开始启普通应用容器。</p><p>因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：</p><ul><li>Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。例如，没有必要仅为了在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具而去FROM 一个镜像来生成一个新的镜像。</li><li>Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。</li><li>应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。</li><li>Init 容器能以不同于Pod内应用容器的文件系统视图运行。因此，Init容器可具有访问 Secrets 的权限，而应用容器不能够访问。</li><li>由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。一旦前置条件满足，Pod内的所有的应用容器会并行启动。</li></ul><p>——————————学习与做题的分割线———————————————<br>题目6： 添加一个 initcontainer 到 lum(/etc/data)这个 initcontainer 应该创建一个名为/workdir/calm.txt 的空文件，如果/workdir/calm.txt 没有被检测到，这个 Pod 应该退出。</p><p>解答：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>pod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> lum    <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>      <span class="token key atrule">path</span><span class="token punctuation">:</span> /etc/data  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp<span class="token punctuation">-</span>container    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>                       <span class="token comment" spellcheck="true"># 如果这里不加volumeMounts的配置，无法启动</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> lum      <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /workdir/    <span class="token key atrule">initContainers</span><span class="token punctuation">:</span>   <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> init<span class="token punctuation">-</span>myservice    <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'sh'</span><span class="token punctuation">,</span> <span class="token string">'-c'</span><span class="token punctuation">,</span> <span class="token string">"touch /workdir/calm.txt"</span><span class="token punctuation">]</span>    <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> lum       <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /workdir/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>$ kubectl apply -f initcontainer.yaml pod/myapp-pod created$ kubectl get pod myapp-pod -wNAME        READY   STATUS     RESTARTS   AGEmyapp-pod   0/1     Init:0/1   0          2smyapp-pod   0/1     PodInitializing   0          13smyapp-pod   1/1     Running           0          22s</code></pre><h2 id="kubernetes中deployment相关概念"><a href="#kubernetes中deployment相关概念" class="headerlink" title="kubernetes中deployment相关概念"></a>kubernetes中deployment相关概念</h2><p>Deployment控制ReplicaSet的多个版本，ReplicaSet控制Pod个数。</p><p>Deployment实际上一个两层控制器，遵循一种滚动更新的方式来实升级现有的容器，这个能力的实现，依赖的就是ReplicaSet这个对象。</p><p>当我们修改了Deployment对象后，Deployment控制器会使用修改后的模板，创建一个新的ReplicaSet对象，这时候有两个RelicaSet对象，<br>Deployment通过控制ReplicaSet对象的pod数量来达到滚动升级的效果。</p><p>例如A和B，如果最终设置的pod数都是3，通过A-1，B+1这样的方式，直到A的pod数量变为0，最终 达到了滚动升级的目的。<br>同时，因为存在多个ReplicaSet，让回滚成为了可能。</p><p>Deployment 通过“控制器模式”，来操作 ReplicaSet 的个数和属性，进而实现“水平扩展 / 收缩”和“滚动更新”这两个编排动作。</p><p>ReplicaSet 多个相同的Pod运行集合，ReplicaSet由Deployment控制。</p><p>Deployment的伸缩：修改replicas的数量</p><p>——————————学习与做题的分割线———————————————<br>题目7：创建 deployment 名字为 nginx-app 容器采用 1.11.9 版本的 nginx  这个 deployment 包含 3 个副本,接下来通过滚动升级的方式更新镜像版本为 1.12.0，并记录这个更新，最后，回滚这个更新到之前的 1.11.9 版本</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>app  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx        <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.11.9<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>$ kubectl apply -f deployment.yaml deployment.apps/nginx-app created$ kubectl get deployNAME               READY   UP-TO-DATE   AVAILABLE   AGEnginx-app          3/3     3            3           58s$ kubectl get rsNAME                          DESIRED   CURRENT   READY   AGEnginx-app-7f5f57f857          3         3         0       6s$ kubectl rollout status deployment.v1.apps/nginx-appdeployment &quot;nginx-app&quot; successfully rolled out$ kubectl rollout history deployment.v1.apps/nginx-appdeployment.apps/nginx-app REVISION  CHANGE-CAUSE1         &lt;none&gt;$ kubectl set image deployment/nginx-app nginx=nginx:1.12.0 --record$  kubectl rollout status deployment.v1.apps/nginx-appWaiting for deployment &quot;nginx-app&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 1 old replicas are pending termination...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 1 old replicas are pending termination...deployment &quot;nginx-app&quot; successfully rolled out$  kubectl rollout history deployment.v1.apps/nginx-appdeployment.apps/nginx-app REVISION  CHANGE-CAUSE1         &lt;none&gt;2         kubectl set image deployment/nginx-app nginx=nginx:1.12.0 --record=true$ kubectl rollout undo deployment.v1.apps/nginx-app$  kubectl rollout status deployment.v1.apps/nginx-appWaiting for deployment &quot;nginx-app&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 1 old replicas are pending termination...Waiting for deployment &quot;nginx-app&quot; rollout to finish: 1 old replicas are pending termination...deployment &quot;nginx-app&quot; successfully rolled out$  kubectl rollout history deployment.v1.apps/nginx-appdeployment.apps/nginx-app REVISION  CHANGE-CAUSE2         kubectl set image deployment/nginx-app nginx=nginx:1.12.0 --record=true3         &lt;none&gt;// 标号最新为最近的状态</code></pre><h2 id="kubernetes中service相关概念"><a href="#kubernetes中service相关概念" class="headerlink" title="kubernetes中service相关概念"></a>kubernetes中service相关概念</h2><p>将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。</p><p>使用Kubernetes，您无需修改应用程序即可使用不熟悉的服务发现机制。 Kubernetes为Pods提供自己的IP地址和一组Pod的单个DNS名称，并且可以在它们之间进行负载平衡。</p><p>Service Pod的对外访问的暴露，loadBalance、nodePort、ClusterIP三种方式</p><p>——————————学习与做题的分割线———————————————<br>题目8：创建和配置 service，名字为 front-end-service。可以通过 NodePort/ClusterIp 开访问，并且路由到 front-end 的 Pod 上。</p><p>解答：</p><p>Pod:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>app  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>app<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx    <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>$ kubectl apply -f nginx.yaml$  kubectl get podNAME                                READY   STATUS              RESTARTS   AGEnginx-app                           1/1     Running   0          4s</code></pre><p>Service:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> front<span class="token punctuation">-</span>end<span class="token punctuation">-</span>service<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx<span class="token punctuation">-</span>app  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span>      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">80</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>$ kubectl apply -f service-nginx.yaml service/front-end-service created$ kubectl get svcNAME                TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)        AGEfront-end-service   NodePort    172.168.170.0     &lt;none&gt;        80:30676/TCP   5s$ curl localhost:30676</code></pre><p>或者操作:</p><pre><code>$ kubectl expose pod nginx-app --name=front-end --port=80  --type=NodePort</code></pre><h2 id="kubernetes中namespace相关概念"><a href="#kubernetes中namespace相关概念" class="headerlink" title="kubernetes中namespace相关概念"></a>kubernetes中namespace相关概念</h2><p>——————————学习与做题的分割线———————————————<br>题目9：创建一个 Pod，名字为 Jenkins，镜像使用 Jenkins。在新的 namespace website-frontend 上创建</p><p>解答：</p><pre><code>$ kubectl create ns website-frontend $ vim jenkins-pod.yaml// 输入以下内容apiVersion: v1kind: Podmetadata:  name: jenkins  namespace: website-frontendspec:  containers:  - name: jenkins    image: jenkins$ kubectl apply -f jenkins-pod.yaml</code></pre><h2 id="–dry-run概念"><a href="#–dry-run概念" class="headerlink" title="–dry-run概念"></a>–dry-run概念</h2><p>使用 –dry-run 参数预览要发送到集群的对象，而无需真正提交。</p><p>——————————学习与做题的分割线———————————————<br>题目10：创建 deployment 的 spec 文件: 使用 redis 镜像，7 个副本，label 为 app_enb_stage=dev deployment 名字为 kual00201 保存这个 spec 文件到/opt/KUAL00201/deploy_spec.yaml完成后，清理(删除)在此任务期间生成的任何新的 k8s API 对象</p><p>解答：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> kual00201  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app_enb_stage</span><span class="token punctuation">:</span> dev<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app_enb_stage</span><span class="token punctuation">:</span> dev  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app_enb_stage</span><span class="token punctuation">:</span> dev    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> redis        <span class="token key atrule">image</span><span class="token punctuation">:</span> redis<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>$ mkdir -p /opt/KUAL00201/$ kubectl apply -f redis.yaml --dry-run -o yaml &gt; /opt/KUAL00201/deploy_spec.yaml</code></pre><h2 id="格式化输出信息的问题"><a href="#格式化输出信息的问题" class="headerlink" title="格式化输出信息的问题"></a>格式化输出信息的问题</h2><ol><li><p>Service和Pod对应，通过labels进行选择。</p></li><li><p>格式化输出信息<em>*-o custom-columns=NAME:metadata.name</em></p></li></ol><p>——————————学习与做题的分割线———————————————<br>题目11：创建一个文件/opt/kucc.txt ，这个文件列出所有的 service 为 foo ,在 namespace 为 production 的 Pod这个文件的格式是每行一个 Pod的名字</p><p>解答：</p><pre><code>$ kubectl get svc -n production --show-labels | grep fooapp=test$ kubectl get pod -l app=test -o custom-columns=NAME:metadata.name &gt; /opt/kucc.txt</code></pre><h2 id="Secrets"><a href="#Secrets" class="headerlink" title="Secrets"></a>Secrets</h2><p>——————————学习与做题的分割线———————————————<br>题目12：创建一个secret,名字为super-secret包含用户名bob,创建pod1挂载该secret，路径为/secret，创建pod2，使用环境变量引用该secret，该变量的环境变量名为ABC。</p><p>centos设置代理时，no_proxy必须是使用具体的地址信息，否则不生效！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>log-pilot+kafka+ELK构建生产级日志管理系统</title>
      <link href="/2020/07/10/log-pilot-kafka-elk-gou-jian-sheng-chan-ji-ri-zhi-guan-li-xi-tong/"/>
      <url>/2020/07/10/log-pilot-kafka-elk-gou-jian-sheng-chan-ji-ri-zhi-guan-li-xi-tong/</url>
      
        <content type="html"><![CDATA[<h1 id="log-pilot-kafka-ELK构建生产级日志管理体系"><a href="#log-pilot-kafka-ELK构建生产级日志管理体系" class="headerlink" title="log-pilot+kafka+ELK构建生产级日志管理体系"></a>log-pilot+kafka+ELK构建生产级日志管理体系</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>log-pilot 是阿里云提供的日志收集镜像。我们可以在每台机器上部署一个 log-pilot 实例，就可以收集机器上所有 Docker 应用日志。</p><p>log-pilot具有以下特性：</p><ul><li>一个单独的 log 进程收集机器上所有容器的日志。不需要为每个容器启动一个 log 进程。</li><li>支持文件日志和 stdout。docker log dirver 亦或 logspout 只能处理 stdout，log-pilot 不仅支持收集 stdout 日志，还可以收集文件日志。</li><li>声明式配置。当您的容器有日志要收集，只要通过 label 声明要收集的日志文件的路径，无需改动其他任何配置，log-pilot 就会自动收集新容器的日志。</li><li>支持多种日志存储方式。无论是强大的阿里云日志服务，还是比较流行的 elasticsearch 组合，甚至是 graylog，log-pilot 都能把日志投递到正确的地点。</li><li>开源。log-pilot 完全开源，您可以从 Git项目地址 下载代码。如果现有的功能不能满足您的需要，欢迎提 issue。</li></ul><p>ELK是统一的日志管理工具，分为Logstash、ElasticSearch、Kibana。Logstash对日志进行收集和处理筛选，ElasticSearch负责日志的存储，Kibana进行日志的查询和展示。</p><p>Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。</p><p>利用以上工具链构建日志管理体系。</p><h2 id="架构图示"><a href="#架构图示" class="headerlink" title="架构图示"></a>架构图示</h2><p><img src="%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FELK.png" alt></p><p>运行流程如下：</p><ol><li>通过log-pilot收集服务所在容器的日志信息并将收集到的日志信息写入kafka</li><li>利用Logstash从kafka中，获取日志信息，并进行处理，写入后向的ElasticSearch</li><li>ElasticSearch提供日志存储和分片功能</li><li>利用kibana对日志进行统一展示和查询，并根据条件进行筛选</li></ol><h2 id="版本信息和机器信息"><a href="#版本信息和机器信息" class="headerlink" title="版本信息和机器信息"></a>版本信息和机器信息</h2><ol><li>版本信息：</li></ol><ul><li><p>ELK体系：7.6.2</p></li><li><p>kafka：2.11-2.4.1</p></li><li><p>log-pilot: 0.9.7-filebeat(使用filebeat版本进行收集)</p></li></ul><ol start="2"><li>机器信息</li></ol><table><thead><tr><th>IP地址</th><th>功能</th></tr></thead><tbody><tr><td>192.168.229.237</td><td>ELK服务所在机器</td></tr><tr><td>192.168.229.206</td><td>kafka</td></tr><tr><td>192.168.229.206</td><td>zookeeeper</td></tr><tr><td>192.168.229.209</td><td>zookeeper</td></tr><tr><td>192.168.229.201</td><td>zookeeper</td></tr></tbody></table><h2 id="ELK部署"><a href="#ELK部署" class="headerlink" title="ELK部署"></a>ELK部署</h2><h3 id="1-组件及对应端口"><a href="#1-组件及对应端口" class="headerlink" title="1. 组件及对应端口"></a>1. 组件及对应端口</h3><table><thead><tr><th>服务</th><th>端口</th></tr></thead><tbody><tr><td>Elasticsearch</td><td>9200（数据连接）、9300（集群通信）</td></tr><tr><td>Cerebro</td><td>9000</td></tr><tr><td>Kibana</td><td>5601</td></tr><tr><td>Logstash</td><td>6514（tcp/udp）、6515（tcp）</td></tr><tr><td>zookeeper</td><td>28880、38880、21810</td></tr></tbody></table><h3 id="2-ELK文件及目录"><a href="#2-ELK文件及目录" class="headerlink" title="2. ELK文件及目录"></a>2. ELK文件及目录</h3><table><thead><tr><th>文件或目录</th><th>说明</th></tr></thead><tbody><tr><td>/opt/es/config/elasticsearch.yml</td><td>es的主配置文件</td></tr><tr><td>/opt/es/config/jvm.options</td><td>es的jvm配置文件</td></tr><tr><td>/opt/es</td><td>es安装目录</td></tr><tr><td>/opt/es_logs/</td><td>es日志目录</td></tr><tr><td>/opt/es_data/</td><td>es数据目录</td></tr><tr><td>/opt/kibana/config/kibana.yml</td><td>kibana主配置文件</td></tr><tr><td>/opt/cerebro/conf/application.conf</td><td>cerebro主配置文件</td></tr><tr><td>/opt/logstash-conf/</td><td>logstash配置文件目录</td></tr></tbody></table><h3 id="3-安装ELK"><a href="#3-安装ELK" class="headerlink" title="3. 安装ELK"></a>3. 安装ELK</h3><h4 id="3-1-使用脚本进行安装"><a href="#3-1-使用脚本进行安装" class="headerlink" title="3.1 使用脚本进行安装"></a>3.1 使用脚本进行安装</h4><p>登录192.168.229.237服务器，执行下面的命令进行安装</p><pre><code># cd ~# wget -O elk762.sh http://www.bigops.com/bigops-install/elk762.sh# sh +x elk762.sh</code></pre><p>提示：如果脚本下载elk太慢，可以手动下载，把ELK下载文件和elk762.sh脚本放在同一个目录再运行安装脚本。</p><h4 id="3-2-启动服务"><a href="#3-2-启动服务" class="headerlink" title="3.2 启动服务"></a>3.2 启动服务</h4><h2 id="后续步骤一：设置ES密码"><a href="#后续步骤一：设置ES密码" class="headerlink" title="后续步骤一：设置ES密码"></a><strong>后续步骤一：设置ES密码</strong></h2><p>等待10秒后，查看ES是否启动，运行命令：</p><pre><code>$ sudo netstat -nptl|grep 9[2,3]00</code></pre><p>如果没有启动，请尝试手动启动：</p><pre><code>$ su - es(es) $ /opt/es/bin/elasticsearch</code></pre><p>或者使用下面的命令进行启动：</p><pre><code>$ sudo systemctl start es</code></pre><p>9200和9300端口启动后，运行下面命令设置密码</p><pre><code>$ sudo /opt/es/bin/elasticsearch-setup-passwords interactivePlease confirm that you would like to continue [y/N]，回答y</code></pre><p>要设置的密码比较多，都设置成ES连接密码，这里所有的密码均设置为<em>111111</em>。</p><h2 id="后续步骤二：启动kibana"><a href="#后续步骤二：启动kibana" class="headerlink" title="后续步骤二：启动kibana"></a><strong>后续步骤二：启动kibana</strong></h2><p>运行下面的命令启动kibana：</p><pre><code>$ sudo systemctl restart kibana</code></pre><p>等待10秒后，查看端口是否启动，运行命令</p><pre><code>$ sudo netstat -nplt|grep 5601</code></pre><p>5601端口启动后，使用浏览器访问：<a href="http://120.132.33.210:5601" target="_blank" rel="noopener">http://120.132.33.210:5601</a></p><p><strong>默认登录用户名：elastic</strong><br><strong>密码：111111</strong></p><p><em>安装时，ELK7.6.2自带x-pack插件进行认证。</em></p><h2 id="后续步骤三：启动logstash"><a href="#后续步骤三：启动logstash" class="headerlink" title="后续步骤三：启动logstash"></a><strong>后续步骤三：启动logstash</strong></h2><p>运行命令</p><pre><code>$ sudo systemctl restart logstash</code></pre><p>等待10秒后，查看端口是否启动，运行命令</p><pre><code>$ sudo netstat -npl|grep 6514</code></pre><h4 id="3-3-修改防火墙策略"><a href="#3-3-修改防火墙策略" class="headerlink" title="3.3 修改防火墙策略"></a>3.3 修改防火墙策略</h4><p>确认防火墙6514/(tcp/udp)、6515(tcp)等端口容许被访问。</p><pre><code>// 开启kibana端口号$ sudo firewall-cmd --add-port=6514/tcp --zone=public --permanent$ sudo firewall-cmd --add-port=6514/udp --zone=public --permanent$ sudo firewall-cmd --add-port=6515/tcp --zone=public --permanent// 开启ElasticSearch端口号$ sudo firewall-cmd --add-port=9200/tcp --zone=public --permanent$ sudo firewall-cmd --add-port=9300/tcp --zone=public --permanent// 开启kibana端口号$ sudo firewall-cmd --add-port=5601/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><p>如果使用了公有云，需要打开端口策略。</p><h4 id="3-4-测试Logstash端口连通性"><a href="#3-4-测试Logstash端口连通性" class="headerlink" title="3.4 测试Logstash端口连通性"></a>3.4 测试Logstash端口连通性</h4><p>登录一台日志客户端，运行命令确认6514、6515端口是open。</p><pre><code>nmap -sU -pU:6514 日志服务器IP</code></pre><p>等Bigops系统安装好后，登录kibana查看是否有数据进来。</p><h4 id="3-5-安装cerebro，用于图形化管理ES（选装）"><a href="#3-5-安装cerebro，用于图形化管理ES（选装）" class="headerlink" title="3.5 安装cerebro，用于图形化管理ES（选装）"></a>3.5 安装cerebro，用于图形化管理ES（选装）</h4><pre><code>$ cd ~$ wget -O cerebro.sh http://www.bigops.com/bigops-install/cerebro.sh$ sudo bash cerebro.sh</code></pre><p>安装完成后，开启cerebro对应的端口信息：</p><pre><code>$ sudo firewall-cmd --add-port=9000/tcp --zone=public --permanent$ sudo firewall-cmd --reload</code></pre><p>直接访问<a href="http://192.168.229.237:9000，登录时*输入用户名elastic，输入密码11111*，即可进入。" target="_blank" rel="noopener">http://192.168.229.237:9000，登录时*输入用户名elastic，输入密码11111*，即可进入。</a></p><h2 id="kafka部署"><a href="#kafka部署" class="headerlink" title="kafka部署"></a>kafka部署</h2><p>首先部署zookeeper，在201、206、209上部署zookeeper，参考<strong>Linux系统初始化以及部署</strong>文档进行安装。</p><p>然后安装kafka：</p><pre><code>$ cd  ~ &amp;&amp; mkdir kafka$ wget https://mirror.bit.edu.cn/apache/kafka/2.4.1/kafka_2.11-2.4.1.tgz$ tar -zxvf kafka_2.11-2.4.1.tgz$ cd kafka_2.11-2.4.1/$ vim config/server.properties// 修改下面几个选项// 设置监听器advertised.listeners=PLAINTEXT://192.168.229.206:9092// 设置zookeeper连接zookeeper.connect=192.168.229.206:21810,192.168.229.209:21810,192.168.229.201:21810// :wq保存退出</code></pre><p>最后启动kafka服务：</p><pre><code>$ nohup ./bin/kafka-server-start.sh config/server.properties &amp;$ ps -ef | grep kafka// 查看是否开启</code></pre><p>下面简单测试一下kafka是否能正常运行，创建topic的操作如下：</p><pre><code>$ ./bin/kafka-topics.sh --create --zookeeper 192.168.229.206:21810,192.168.229.209:21810,192.168.229.201:21810 --replication-factor 1 --partitions 1 --topic test// 查看已经创建的topic$ ./bin/kafka-topics.sh --list --zookeeper 192.168.229.206:21810test</code></pre><p>出现了test说明已经创建完成了。</p><p>稍后进行产生消息和消费消息的操作：</p><pre><code>// 产生消息$ ./bin/kafka-console-producer.sh --broker-list PLAINTEXT://192.168.229.206:9092 --topic test&gt;{&quot;key&quot;:&quot;value&quot;}// 消费消息$ ./bin/kafka-console-consumer.sh --bootstrap-server 192.168.229.206:9092 --topic test --from-beginning{&quot;key&quot;:&quot;value&quot;}</code></pre><p>这样消息就能进行消费了，kafka就能正常运行了。</p><h2 id="docker日志收集实践"><a href="#docker日志收集实践" class="headerlink" title="docker日志收集实践"></a>docker日志收集实践</h2><p>目前开发环境中，服务部署使用docker容器进行，需要对docker容器进行日志收集。</p><h3 id="1-log-pilot部署"><a href="#1-log-pilot部署" class="headerlink" title="1. log-pilot部署"></a>1. log-pilot部署</h3><p>登录开发环境部署机器192.168.229.199，拉取log-pilot官方镜像，并进行启动，以非交互式不保存任何数据的方式启动，便于我们调试。</p><pre><code>$ docker pull registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeat$ docker run --rm -it --name log-pilot --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /etc/localtime:/etc/localtime -v /:/host:ro --cap-add SYS_ADMIN -e &#39;LOGGING_OUTPUT=kafka&#39; -e &#39;KAFKA_BROKERS=192.168.229.206:9092&#39; -e &#39;KAFKA_DEFAULT_TOPIC=test&#39;  registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeat</code></pre><p>在运行log-pilot容器时，需要指定以下全局变量：</p><ul><li><p>LOGGING_OUTPUT=kafka<br>指定日志信息写入kafka</p></li><li><p>KAFKA_BROKERS=192.168.229.206:9092<br>指定kafka的服务器地址信息</p></li><li><p>KAFKA_DEFAULT_TOPIC=test<br>指定默认写入的topic为test，不强制要求</p></li></ul><p>下面解释一下docker run 时指定的参数信息：</p><table><thead><tr><th>Options</th><th>Mean</th></tr></thead><tbody><tr><td>-i</td><td>以交互模式运行容器，通常与 -t 同时使用；</td></tr><tr><td>-t</td><td>为容器重新分配一个伪输入终端，通常与 -i 同时使用；</td></tr><tr><td>-d</td><td>后台运行容器，并返回容器ID；</td></tr></tbody></table><p>这样，就可以启动了日志收集的工具。但是目前我们要停止该容器，ctrl+c就可以停止该程序，先将kafka写入logstash走通。</p><h3 id="2-kafka测试写入Logstash"><a href="#2-kafka测试写入Logstash" class="headerlink" title="2. kafka测试写入Logstash"></a>2. kafka测试写入Logstash</h3><p>登录到kafka所在机器，启动我们上述使用的命令，向<em>test</em>所在的topic中写入数据，如下：</p><pre><code>// 产生消息$ ./bin/kafka-console-producer.sh --broker-list PLAINTEXT://192.168.229.206:9092 --topic test&gt;{&quot;key&quot;:&quot;value&quot;}</code></pre><p>然后转到ELK安装的机器上，对Logstash先进行停用，配置kafka写入的内容，以命令行方式启动Logstash，如下：</p><pre><code>$ sudo systemctl stop logstash// 转换到logstash配置文件所在目录$ cd /opt/logstash-conf/$ sudo vim kafa.conf// 写入以下信息input{    kafka {        bootstrap_servers =&gt; &quot;192.168.229.206:9092&quot;        topics =&gt; [&quot;test&quot;]        codec =&gt; &quot;json&quot;    }}filter{    grok {        match =&gt; {            remove_field =&gt; [ &quot;beat&quot;, &quot;source&quot;, &quot;stream&quot;, &quot;prospector&quot;, &quot;offset&quot;, &quot;@version&quot;, &quot;@timestamp&quot;]        }    }}output {        stdout {            codec =&gt; rubydebug        }        #elasticsearch {        #        hosts =&gt; [&quot;192.168.229.237:9200&quot;]        #        user =&gt; &quot;elastic&quot;        #        password =&gt; &quot;111111&quot;        #        index =&gt; &quot;filebeat-%{+yyyy.MM.dd}&quot;        #}}// :wq保存退出// 启动Logstash，可打印调试日志$ sudo /opt/logstash/bin/logstash --path.settings /opt/logstash/config -f /opt/logstash-conf/kafka.conf --verbose --debug</code></pre><p>目前不需要设置写入到ES，直接在命令行中输出日志信息。这样命令行中会输出的信息如下：</p><pre><code>[2020-07-10T14:09:11,039][DEBUG][logstash.filters.grok    ][main] Running grok filter {:event=&gt;#&lt;LogStash::Event:0x4f54382f&gt;}[2020-07-10T14:09:11,039][DEBUG][org.apache.kafka.clients.consumer.internals.Fetcher][main] [Consumer clientId=logstash-0, groupId=logstash] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(test-0), toForget=(), implied=()) to broker 192.168.229.206:9092 (id: 0 rack: null)[2020-07-10T14:09:11,045][DEBUG][logstash.filters.grok    ][main] Event now:  {:event=&gt;#&lt;LogStash::Event:0x4f54382f&gt;}{    &quot;@timestamp&quot; =&gt; 2020-07-10T06:09:10.937Z,          &quot;tags&quot; =&gt; [        [0] &quot;_jsonparsefailure&quot;,        [1] &quot;_grokparsefailure&quot;    ],       &quot;message&quot; =&gt; &quot;{\&quot;\&quot;\e[Dkey\&quot;:\&quot;value\&quot;}&quot;,      &quot;@version&quot; =&gt; &quot;1&quot;}</code></pre><p>这样从kafka到Logstash写入信息就走通了。这时候使用ctrl+c停止运行的logstash，重新以服务方式启动，如下：</p><pre><code>$ sudo systemctl restart logstash</code></pre><h3 id="3-log-pilot写入kafka"><a href="#3-log-pilot写入kafka" class="headerlink" title="3. log-pilot写入kafka"></a>3. log-pilot写入kafka</h3><p>这里启动log-pilot镜像，更换写入的topic，来测试从log-pilot将日志写入kafka中。</p><p>在开发环境部署机器中，我们启动一个数据字典服务，利用log-pilot来采集该服务所在docker镜像的日志信息，启动方式如下：</p><pre><code>$ docker run -d -p 19090:19090 -e CHANNEL=&quot;standalone&quot; -e IP_ADDR=&quot;192.168.229.199&quot; -e NACOS_IP=&quot;192.168.229.206:18848&quot; -e NACOS_NAMESPACE=&quot;858b37b1-35be-4564-a24e-dc2c322d5784&quot;  -e SKYWALKING_NAMESPACE=&quot;test-dev&quot; -e SKYWALKING_TARGET_SERVICE_NAME=&quot;test-data-dict-develop&quot; -e SKYWALKING_IP_PORT=&quot;192.168.229.208:11800&quot; --label aliyun.logs.dict=stdout --label aliyun.logs.dict.tags=&quot;topic=test,env=dev,service=test-data-dict&quot;  --name test-dict 192.168.229.202:5000/test-data-dict-develop:$BUILD_NUMBER</code></pre><p>这里比起之前的启动命令，多出了两个参数：</p><ul><li><p>–label aliyun.logs.dict=stdout<br>指定日志所属的服务，stdout表示采集docker输出的日志信息</p></li><li><p>–label aliyun.logs.dict.tags=”topic=test,env=dev,service=test-data-dict”<br>指定服务对应的tags，其中<em>topic=test</em>指定了kafka里面已有的topic，<em>env=dev</em>指定来自开发环境的日志，<em>service=test-data-dict</em>指定服务的名称</p></li></ul><p>启动数据字典之后，用与上文同样的方式启动log-pilot镜像，这样就开启了收集日志的工作：</p><pre><code>$ docker run --rm -it --name log-pilot --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /etc/localtime:/etc/localtime -v /:/host:ro --cap-add SYS_ADMIN -e &#39;LOGGING_OUTPUT=kafka&#39; -e &#39;KAFKA_BROKERS=192.168.229.206:9092&#39; -e &#39;KAFKA_DEFAULT_TOPIC=test&#39;  registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeat</code></pre><p>如何验证数据字典服务的日志已经被写入到消息队列中？我们可以通过命令行中kafka-console-consumer.sh工具来实现</p><pre><code>$ ./bin/kafka-console-consumer.sh --bootstrap-server 192.168.229.206:9092 --topic test --from-beginning{&quot;key&quot;:&quot;value&quot;}</code></pre><p>另外也可以通过kafka的图形化客户端来查看，本人开发机使用win7 64位系统。下载<a href="https://www.kafkatool.com/download.html" target="_blank" rel="noopener">kafka Tool 2.0.7</a>，如下图：</p><p><img src="kafkatool%E4%B8%8B%E8%BD%BD.png" alt></p><p>安装后，连接目前运行的kafka服务，如下：</p><p><img src="kafkatool%E8%BF%9E%E6%8E%A5.png" alt></p><p>连接后查看topics下test所在的Partition 0中的信息，点击左上角的开始按钮，如下：</p><p><img src="kafkatool%E6%9F%A5%E7%9C%8B%E4%BF%A1%E6%81%AF.png" alt></p><p>如果展示不出来，或者展示的信息为乱码，需要设置topic的Content Types中，选择展示String类型的信息，如下：</p><p><img src="%E8%AE%BE%E7%BD%AE%E4%BD%BF%E7%94%A8String%E5%B1%95%E7%A4%BA.png" alt></p><p>这样就能看到，log-pilot收集的日志已经发送到kafka中了。</p><p><strong>注意</strong>：后续的服务都应该参考数据字典的设置，在构建时添加对应的</p><h3 id="4-服务联调"><a href="#4-服务联调" class="headerlink" title="4. 服务联调"></a>4. 服务联调</h3><p>前面的连通性测试都已经通过，这时候就要正式启动该套体系了，首先修改Logstash的设置，保证能够写入的ElasticSearch中，然后修改logs-pilot的启动方式，让它在后台自动运行。</p><h3 id="4-1-修改Logstash的配置信息"><a href="#4-1-修改Logstash的配置信息" class="headerlink" title="4.1 修改Logstash的配置信息"></a>4.1 修改Logstash的配置信息</h3><p>登录到ELK所在服务器上，停用Logstash服务，修改配置信息，如下：</p><pre><code>$ sudo systemctl stop logstash// 转换到logstash配置文件所在目录$ cd /opt/logstash-conf/$ sudo vim kafa.conf// 写入以下信息input{    kafka {        bootstrap_servers =&gt; &quot;192.168.229.206:9092&quot;        topics =&gt; [&quot;test&quot;]        codec =&gt; &quot;json&quot;    }}filter{    grok {        match =&gt; {            remove_field =&gt; [ &quot;beat&quot;, &quot;source&quot;, &quot;stream&quot;, &quot;prospector&quot;, &quot;offset&quot;, &quot;@version&quot;, &quot;@timestamp&quot;]        }    }}output {        #stdout {        #    codec =&gt; rubydebug        #}        elasticsearch {                hosts =&gt; [&quot;192.168.229.237:9200&quot;]                user =&gt; &quot;elastic&quot;                password =&gt; &quot;111111&quot;                index =&gt; &quot;filebeat-%{+yyyy.MM.dd}&quot;        }}// :wq保存退出// 重启生效$ sudo systemctl start logstash</code></pre><p>去掉之前测试时使用的命令行输出–stdout，解除对ElasticSearch写入的配置的注释，配置Logstash可以写入ES中。</p><p>要注意，这里的<strong>index =&gt; “filebeat-%{+yyyy.MM.dd}”</strong>，是我们后续再kibana中配置显示是所需要匹配的索引信息。</p><h3 id="4-2-修改log-pilot的启动方式"><a href="#4-2-修改log-pilot的启动方式" class="headerlink" title="4.2 修改log-pilot的启动方式"></a>4.2 修改log-pilot的启动方式</h3><p>登录开发环境机器，停止目前在命令行中运行的log-pilot容器信息，使用下面的命令正式启动：</p><pre><code>$ docker run -d --restart always --name log-pilot --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /etc/localtime:/etc/localtime -v /:/host:ro --cap-add SYS_ADMIN -e &#39;LOGGING_OUTPUT=kafka&#39; -e &#39;KAFKA_BROKERS=192.168.229.206:9092&#39; -e &#39;KAFKA_DEFAULT_TOPIC=test&#39;  registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeat</code></pre><p>这样log-pilot就可以在后台运行了。</p><h3 id="5-kibana设置和展示"><a href="#5-kibana设置和展示" class="headerlink" title="5. kibana设置和展示"></a>5. kibana设置和展示</h3><p>在前面我们的的ELK体系已经搭建完成了，这样就可以直接登录kibana系统了。</p><ul><li><p>地址：192.168.229.237:5601</p></li><li><p>用户名：elastic</p></li><li><p>密码：111111</p></li></ul><p>登录后如下图：</p><p><img src="Kibana%E9%A6%96%E9%A1%B5.png" alt></p><p>首先查看索引信息，点击Management（管理），选在kibana中的索引模式，如下：</p><p><img src="kibana%E7%B4%A2%E5%BC%95%E6%A8%A1%E5%BC%8F.png" alt></p><p>点击右上角<strong>创建索引模式</strong>，按照下图开始创建：</p><p><img src="kibana%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%A8%A1%E5%BC%8F.png" alt></p><p>输入<strong>filebeat-*</strong>，这样能匹配出下面的索引信息，自动提示可用。点击下一步，进入配置设置：</p><p><img src="kibana%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E7%AD%9B%E9%80%89%E8%AE%BE%E7%BD%AE.png" alt></p><p>按照图示设置完成，点击选择时间筛选字段名称后，右下方的<em>创建索引模式</em>按钮，变为可点击，点击后即可创建索引模式。</p><p>创建完成后回到索引模式页面，会展示出刚才创建的<strong>filebeat-*</strong>。</p><p>最后，查看一下我们在ES中的数据信息，如下图操作：</p><p><img src="kibana%E6%9F%A5%E7%9C%8B%E7%B4%A2%E5%BC%95%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF.png" alt></p><p>这样就能看到我们自己写入的日志信息了！</p><h2 id="日志收集设置"><a href="#日志收集设置" class="headerlink" title="日志收集设置"></a>日志收集设置</h2><p>这里发现一个问题，当日志收集时，面对一些错误日志，本应该一行显示的，变成了多行显示。如下图：</p><p><img src="%E6%97%A5%E5%BF%97%E6%96%AD%E5%BC%80.png" alt></p><p>尤其是对于异常日志，不能很好地合并到一块，导致查看错误日志的时候不直观。</p><p>对于上述问题，重写log-pilot镜像中的配置文件，并重新制作log-pilot的镜像信息。</p><p>首先重新编写filebeat.tpl文件：</p><pre><code>$ mkdir log-pilot &amp;&amp; vim filebeat.tpl// 输入以下信息{{range .configList}}- type: log  enabled: true  paths:      - {{ .HostDir }}/{{ .File }}  multiline.pattern: &#39;^\[|\{|([0-9]{4}-[0-9]{2}-[0-9]{2})|([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})|([0-9]{1,2}-[A-Za-z]{3}-[0-9]{4})&#39;  multiline.negate: true  multiline.match: after  multiline.max_lines: 10000  scan_frequency: 10s  fields_under_root: true  {{if .Stdout}}  docker-json: true  {{end}}  {{if eq .Format "json"}}  json.keys_under_root: true  {{end}}  fields:      {{range $key, $value := .Tags}}      {{ $key }}: {{ $value }}      {{end}}      {{range $key, $value := $.container}}      {{ $key }}: {{ $value }}      {{end}}  tail_files: false  close_inactive: 2h  close_eof: false  close_removed: true  clean_removed: true  close_renamed: false{{end}}// :wq保存退出</code></pre><p>主要修改的位置是<strong>multiline.pattern</strong>，这里的正则表达式主要是匹配了日志信息的开头，凡是以日期信息开头的都进行多行匹配，合并为同一条数据，直到遇到下一个日志信息开头停止。</p><p>这样就可以将分在多行中的数据，合并到同一条日志信息中。</p><p>然后制作新的log-pilot镜像，如下：</p><pre><code>$ vim Dockerfile// 填写以下信息FROM registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeatCOPY filebeat.tpl /pilot/// :wq保存退出// 制作docker镜像$ docker build -t log-pilot-self:0.9.7-filebeat .// 制作完成后打tag并推送到docker镜像仓库中$ docker images | grep log-pilot-selflog-pilot-self                                    0.9.7-filebeat      a0a464bd4dc1        31 hours ago        119MB$ docker tag log-pilot-self:0.9.7-filebeat 192.168.229.202:5000/log-pilot-self:0.9.7-filebeat$ docker push 192.168.229.202:5000/log-pilot-self:0.9.7-filebeat</code></pre><p>这样就操作完成了，最后需要替换一下之前运行的log-pilot镜像信息。</p><pre><code>$ docker run -d --name log-pilot --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /etc/localtime:/etc/localtime -v /:/host:ro --cap-add SYS_ADMIN -e &#39;LOGGING_OUTPUT=kafka&#39; -e &#39;KAFKA_BROKERS=192.168.229.206:9092&#39; -e &#39;KAFKA_DEFAULT_TOPIC=test&#39;  192.168.229.202:5000/log-pilot-self:0.9.7-filebeat</code></pre><p>这样再去查看日志信息时，就大大减少了断开的情况。</p><h2 id="k8s日志收集实践"><a href="#k8s日志收集实践" class="headerlink" title="k8s日志收集实践"></a>k8s日志收集实践</h2><h3 id="1-log-pilot部署-1"><a href="#1-log-pilot部署-1" class="headerlink" title="1. log-pilot部署"></a>1. log-pilot部署</h3><p>转到k8s管理机，登录到192.168.229.240上，首先编写部署的配置文件，如下：</p><pre><code>$ mkdir log-pilot &amp;&amp; cd log-pilot/$ vim log-pilot.yamlapiVersion: apps/v1kind: DaemonSetmetadata:  name: log-pilot  labels:    app: log-pilot  namespace: test-basic-log-pilotspec:  updateStrategy:    type: RollingUpdate  selector:    matchLabels:      app: log-pilot  template:    metadata:      labels:        app: log-pilot      annotations:        scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;    spec:      tolerations:      - key: node-role.kubernetes.io/master        effect: NoSchedule      containers:      - name: log-pilot        image: 192.168.229.202:5000/log-pilot-self:0.9.7-filebeat        resources:          limits:            memory: 500Mi          requests:            cpu: 200m            memory: 200Mi        env:          - name: &quot;NODE_NAME&quot;            valueFrom:              fieldRef:                fieldPath: spec.nodeName          - name: &quot;LOGGING_OUTPUT&quot;            value: &quot;kafka&quot;          - name: &quot;KAFKA_BROKERS&quot;            value: &quot;192.168.229.206:9092&quot;          - name: &quot;KAFKA_DEFAULT_TOPIC&quot;            value: &quot;test&quot;        volumeMounts:        - name: sock          mountPath: /var/run/docker.sock        - name: root          mountPath: /host          readOnly: true        - name: varlib          mountPath: /var/lib/filebeat        - name: varlog          mountPath: /var/log/filebeat        - name: localtime          mountPath: /etc/localtime          readOnly: true        livenessProbe:          failureThreshold: 3          exec:            command:            - /pilot/healthz          initialDelaySeconds: 10          periodSeconds: 10          successThreshold: 1          timeoutSeconds: 2        securityContext:          capabilities:            add:            - SYS_ADMIN      terminationGracePeriodSeconds: 30      volumes:      - name: sock        hostPath:          path: /var/run/docker.sock      - name: root        hostPath:          path: /      - name: varlib        hostPath:          path: /var/lib/filebeat          type: DirectoryOrCreate      - name: varlog        hostPath:          path: /var/log/filebeat          type: DirectoryOrCreate      - name: localtime        hostPath:          path: /etc/localtime// :wq保存退出// 部署$ kubectl create ns test-basic-log=pilot$ kubectl apply -f log-pilot.yaml -n test-basic-log=pilot</code></pre><p>部署完成后，查看已存在的pod信息，如下：</p><pre><code>$ kubectl get pod -n test-basic-log-pilotNAME              READY   STATUS    RESTARTS   AGElog-pilot-6rw6z   1/1     Running   0          22hlog-pilot-bkb9w   1/1     Running   1          22hlog-pilot-jcp2d   1/1     Running   0          22hlog-pilot-kd79n   1/1     Running   2          22hlog-pilot-lpbv7   1/1     Running   0          22hlog-pilot-nwgxn   1/1     Running   1          22h</code></pre><p>所有node机器上都会部署一个log-pilot的pod，用来采集该节点的相应服务的信息。</p><h3 id="2-服务联调"><a href="#2-服务联调" class="headerlink" title="2. 服务联调"></a>2. 服务联调</h3><p>对服务进行改造，还是以数据字典为例，对数据字典部署的k8s配置文件yaml进行配置：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dictionary  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dictionary<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>SERVER_PORT<span class="token punctuation">}</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> tcp      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>SERVER_PORT<span class="token punctuation">}</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dictionary<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dictionary  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dictionary  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dictionary    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>        <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>          <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>                <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname                <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                  <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                    <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app                      <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                      <span class="token key atrule">values</span><span class="token punctuation">:</span>                        <span class="token punctuation">-</span> app<span class="token punctuation">-</span>test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dictionary              <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">1</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data<span class="token punctuation">-</span>dictionary          <span class="token key atrule">image</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>DOCKER_HUB<span class="token punctuation">}</span>/data<span class="token punctuation">-</span>dictionary<span class="token punctuation">-</span>k8s<span class="token punctuation">:</span>$BUILD_NUMBER          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>            <span class="token key atrule">preStop</span><span class="token punctuation">:</span>              <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>                <span class="token key atrule">port</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>SERVER_PORT<span class="token punctuation">}</span>                <span class="token key atrule">path</span><span class="token punctuation">:</span> /spring/shutdown          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>SERVER_PORT<span class="token punctuation">}</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>SERVER_PORT<span class="token punctuation">}</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">resources</span><span class="token punctuation">:</span>            <span class="token key atrule">requests</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 500Mi            <span class="token key atrule">limits</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>SERVER_PORT<span class="token punctuation">}</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> IP_ADDR              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> status.podIP            <span class="token comment" spellcheck="true"># 配置环境变量，类似服务docker启动项中设置的那样</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> aliyun_logs_dict              <span class="token key atrule">value</span><span class="token punctuation">:</span> stdout            <span class="token comment" spellcheck="true"># 配置环境变量，配置tag</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> aliyun_logs_dict_tags              <span class="token key atrule">value</span><span class="token punctuation">:</span> topic=test<span class="token punctuation">,</span>env=test<span class="token punctuation">,</span>service=test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dict            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_IP              <span class="token key atrule">value</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>NACOS_IP_PORT<span class="token punctuation">}</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>NACOS_NAMESPACE<span class="token punctuation">}</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>k8s<span class="token punctuation">-</span>test            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_TARGET_SERVICE_NAME              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>data<span class="token punctuation">-</span>dic            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_IP_PORT              <span class="token key atrule">value</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>SKYWALKING_IP_PORT<span class="token punctuation">}</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CHANNEL              <span class="token key atrule">value</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>CHANNEL<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>配置完成后，利用CI/CD流水线进行构建，推送到k8s运行环境中。</p><p>注意配置的时候<strong>env=test</strong>，通过该tag确定日志信息来自k8s中的服务，区分于开发环境中的日志。</p><p>最后查看日志信息上报，如下图：</p><p><img src="kibana%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF%E4%B8%8A%E6%8A%A5-%E7%AD%9B%E9%80%89.png" alt></p><p><img src="kibana%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF%E4%B8%8A%E6%8A%A5-%E7%BB%93%E6%9E%9C.png" alt></p><h2 id="分词器hanlp插件安装"><a href="#分词器hanlp插件安装" class="headerlink" title="分词器hanlp插件安装"></a>分词器hanlp插件安装</h2><p>下载对应elasticSearch版本的hanlp插件，下载7.6.2版本的插件并且下载hanlp的1.7.5版本的数据包，并上传服务器！</p><p>插件安装包名称：elasticsearch-analysis-hanlp-7.6.2.zip<br>数据包名称：data-for-1.7.5.zip</p><p>开始安装插件：</p><pre class="line-numbers language-shell"><code class="language-shell">// 安装插件$ sudo /opt/es/bin/elasticsearch-plugin install file:///home/tempuser/elk/elasticsearch-analysis-hanlp-7.6.2.zip-> Installing file:///home/tempuser/elk/elasticsearch-analysis-hanlp-7.6.2.zip-> Downloading file:///home/tempuser/elk/elasticsearch-analysis-hanlp-7.6.2.zip[=================================================] 100%   @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@     WARNING: plugin requires additional permissions     @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.io.FilePermission plugins/analysis-hanlp/data/-#plus read,write,delete* java.io.FilePermission plugins/analysis-hanlp/hanlp.cache#plus read,write,delete* java.lang.RuntimePermission getClassLoader* java.lang.RuntimePermission setContextClassLoader* java.net.SocketPermission * connect,resolve* java.util.PropertyPermission * read,writeSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]// 输入y-> Installed analysis-hanlp// 添加数据包，默认位置是${ES_HOME}/plugins/analysis-hanlpsudo mv data-for-1.7.5.zip /opt/es/plugins/analysis-hanlp/cd /opt/es/plugins/analysis-hanlp/ && unzip data-for-1.7.5.zip// 解压时需要注意，会覆盖该目录下的data文件夹，所以建议先删除该data文件夹再进行解压，否则会出现要合并多个文件的情况// 例如  // replace data/dictionary/tc/s2t.txt.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: y//   inflating: data/dictionary/tc/s2t.txt.bin  //   inflating: data/dictionary/tc/s2tw.bin  // 重启sudo systemctl restart es<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>重启后遇到错误：</p><pre class="line-numbers language-log"><code class="language-log">[2021-07-13T16:48:57,314][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [10.3.61.242] Active license is now [BASIC]; Security is enabled[2021-07-13T16:48:57,341][INFO ][o.e.g.GatewayService     ] [10.3.61.242] recovered [5] indices into cluster_state[2021-07-13T16:48:57,338][ERROR][o.e.x.s.a.e.ReservedRealm] [10.3.61.242] failed to retrieve password hash for reserved user [elastic]org.elasticsearch.action.UnavailableShardsException: at least one primary shard for the index [.security-7] is unavailable        at org.elasticsearch.xpack.security.support.SecurityIndexManager.getUnavailableReason(SecurityIndexManager.java:182) ~[x-pack-security-7.6.2.jar:7.6.2]        at org.elasticsearch.xpack.security.authc.esnative.NativeUsersStore.getReservedUserInfo(NativeUsersStore.java:525) [x-pack-security-7.6.2.jar:7.6.2]        at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.getUserInfo(ReservedRealm.java:212) [x-pack-security-7.6.2.jar:7.6.2]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解决方式：重置密码信息。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h2 id="补充内容，同样脚本安装ELK，es无法启动"><a href="#补充内容，同样脚本安装ELK，es无法启动" class="headerlink" title="补充内容，同样脚本安装ELK，es无法启动"></a>补充内容，同样脚本安装ELK，es无法启动</h2><p>具体日志如下：</p><pre class="line-numbers language-log"><code class="language-log">[2021-06-16T15:36:48,503][INFO ][o.e.b.BootstrapChecks    ] [10.3.62.53] bound or publishing to a non-loopback address, enforcing bootstrap checks[2021-06-16T15:36:48,512][INFO ][o.e.c.c.ClusterBootstrapService] [10.3.62.53] skipping cluster bootstrapping as local node does not match bootstrap requirements: [10.3.62.53:9300][2021-06-16T15:36:48,644][WARN ][o.e.t.TcpTransport       ] [10.3.62.53] exception caught on transport layer [Netty4TcpChannel{localAddress=/10.3.62.53:9300, remoteAddress=/10.3.62.53:47356}], closing connectionio.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: No available authentication scheme        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:473) ~[netty-codec-4.1.43.Final.jar:4.1.43.Final]        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:281) ~[netty-codec-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1422) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:931) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:700) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:600) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:554) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:514) [netty-transport-4.1.43.Final.jar:4.1.43.Final]        at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1050) [netty-common-4.1.43.Final.jar:4.1.43.Final]        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.43.Final.jar:4.1.43.Final]        at java.lang.Thread.run(Thread.java:830) [?:?]    Caused by: javax.net.ssl.SSLHandshakeException: No available authentication scheme        at sun.security.ssl.Alert.createSSLException(Alert.java:131) ~[?:?]        at sun.security.ssl.Alert.createSSLException(Alert.java:117) ~[?:?]        at sun.security.ssl.TransportContext.fatal(TransportContext.java:311) ~[?:?]        at sun.security.ssl.TransportContext.fatal(TransportContext.java:267) ~[?:?]        at sun.security.ssl.TransportContext.fatal(TransportContext.java:258) ~[?:?]        at sun.security.ssl.CertificateMessage$T13CertificateProducer.onProduceCertificate(CertificateMessage.java:955) ~[?:?]        at sun.security.ssl.CertificateMessage$T13CertificateProducer.produce(CertificateMessage.java:944) ~[?:?]        at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:440) ~[?:?]        at sun.security.ssl.ClientHello$T13ClientHelloConsumer.goServerHello(ClientHello.java:1252) ~[?:?]        at sun.security.ssl.ClientHello$T13ClientHelloConsumer.consume(ClientHello.java:1188) ~[?:?]        at sun.security.ssl.ClientHello$ClientHelloConsumer.onClientHello(ClientHello.java:851) ~[?:?]        at sun.security.ssl.ClientHello$ClientHelloConsumer.consume(ClientHello.java:812) ~[?:?]        at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396) ~[?:?]        at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444) ~[?:?]        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1260) ~[?:?]        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1247) ~[?:?]        at java.security.AccessController.doPrivileged(AccessController.java:691) ~[?:?]        at sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:1192) ~[?:?]        at io.netty.handler.ssl.SslHandler.runAllDelegatedTasks(SslHandler.java:1502) ~[netty-handler-4.1.43.Final.jar:4.1.43.Final]        at io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1516) ~[netty-handler-4.1.43.Final.jar:4.1.43.Final]        at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1400) ~[netty-handler-4.1.43.Final.jar:4.1.43.Final]        at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1227) ~[netty-handler-4.1.43.Final.jar:4.1.43.Final]        at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1274) ~[netty-handler-4.1.43.Final.jar:4.1.43.Final]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而且在执行密码设置的时候日志信息如下：</p><pre class="line-numbers language-log"><code class="language-log">$ sudo /opt/es/bin/elasticsearch-setup-passwords interactive --verboseRunning with configuration path: /opt/es/configTesting if bootstrap password is valid for http://172.14.0.1:9200/_security/_authenticate?pretty<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>链接地址为我主机上安装的docker所在地址，很明显这个地址不正确。</p><p>解决方式：</p><p>在es下的config文件夹中添加以下内容：</p><pre class="line-numbers language-yml"><code class="language-yml">#network.publish_host: ${你的主机ip地址}network.publish_host: 192.168.229.10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这样就能防止docker下的地址干扰我们的elasticSearch运行！</p><p>参考地址：</p><ul><li><a href="https://elasticsearch.cn/question/8587" target="_blank" rel="noopener">https://elasticsearch.cn/question/8587</a></li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li>log-pilot运行说明：<a href="https://github.com/AliyunContainerService/log-pilot/blob/master/docs/filebeat/docs.md" target="_blank" rel="noopener">https://github.com/AliyunContainerService/log-pilot/blob/master/docs/filebeat/docs.md</a></li><li>ELK安装部署文档：<a href="http://docs.bigops.com/an-zhuang/an-zhuang-elk.html" target="_blank" rel="noopener">http://docs.bigops.com/an-zhuang/an-zhuang-elk.html</a></li><li><a href="https://blog.csdn.net/ltliyue/article/details/105121849" target="_blank" rel="noopener">https://blog.csdn.net/ltliyue/article/details/105121849</a></li><li>kafka安装测试：<a href="https://segmentfault.com/a/1190000012990954" target="_blank" rel="noopener">https://segmentfault.com/a/1190000012990954</a></li><li>kafka使用：<a href="https://blog.csdn.net/weixin_38004638/article/details/91975123" target="_blank" rel="noopener">https://blog.csdn.net/weixin_38004638/article/details/91975123</a></li><li>docker容器执行参数：<a href="https://blog.csdn.net/qq_19381989/article/details/102781663，https://blog.csdn.net/nzjdsds/article/details/81981732" target="_blank" rel="noopener">https://blog.csdn.net/qq_19381989/article/details/102781663，https://blog.csdn.net/nzjdsds/article/details/81981732</a></li><li>log-pilot日志收集：<a href="https://github.com/AliyunContainerService/log-pilot/issues/121#issuecomment-638047973" target="_blank" rel="noopener">https://github.com/AliyunContainerService/log-pilot/issues/121#issuecomment-638047973</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux下不同jdk版本共存问题</title>
      <link href="/2020/07/09/linux-xia-bu-tong-jdk-ban-ben-gong-cun-wen-ti/"/>
      <url>/2020/07/09/linux-xia-bu-tong-jdk-ban-ben-gong-cun-wen-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux下不同jdk版本共存问题"><a href="#Linux下不同jdk版本共存问题" class="headerlink" title="Linux下不同jdk版本共存问题"></a>Linux下不同jdk版本共存问题</h1><h2 id="问题场景"><a href="#问题场景" class="headerlink" title="问题场景"></a>问题场景</h2><p>ElasticSearch7.6.2的安装需要java11支持，而目前服务器安装的是java8（jdk1.8），而且es安装在另一个用户下，独自运行。</p><p>而且es7.6在java8环境下无法进行启动！</p><p>这样需要根据不同用户切换不同的jdk版本。</p><h2 id="使用alternative管理"><a href="#使用alternative管理" class="headerlink" title="使用alternative管理"></a>使用alternative管理</h2><p>首先，需要安装jdk11，直接通过yum进行安装</p><pre><code>$  sudo yum list | grep java-11-openjdkjava-11-openjdk.x86_64                   1:11.0.7.10-4.el7_8            @updatesjava-11-openjdk-headless.x86_64          1:11.0.7.10-4.el7_8            @updatesjava-11-openjdk.i686                     1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-demo.i686                1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-demo.x86_64              1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-devel.i686               1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-devel.x86_64             1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-headless.i686            1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc.i686             1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc.x86_64           1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc-zip.i686         1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-javadoc-zip.x86_64       1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-jmods.i686               1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-jmods.x86_64             1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-src.i686                 1:11.0.7.10-4.el7_8            updatesjava-11-openjdk-src.x86_64               1:11.0.7.10-4.el7_8            updates$ sudo yum install java-11-openjdk.x86_64 -y</code></pre><p>安装完成后，发现之前的java指向了java11，并非是原来的java8。</p><pre><code>// 安装前$ java -versionjava version &quot;1.8.0_202&quot;Java(TM) SE Runtime Environment (build 1.8.0_202-b08)Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)// 安装后$ java -versionopenjdk version &quot;11.0.7&quot; 2020-04-14 LTSOpenJDK Runtime Environment 18.9 (build 11.0.7+10-LTS)OpenJDK 64-Bit Server VM 18.9 (build 11.0.7+10-LTS, mixed mode, sharing)</code></pre><p>这时候alternative工具就出场了，通过该命令来查看目前已经生效的java运行程序，如下：</p><pre><code>$ sudo alternatives --config javaThere is 1 program that provides &#39;java&#39;.  Selection    Command-----------------------------------------------*+ 1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)Enter to keep the current selection[+], or type selection number:</code></pre><p>这时候新安装的jdk就生效了，也指向了其所在路径。这里可以添加多个jdk进行管理，将最开始安装的jdk导入查看，如下：</p><pre><code>$ sudo alternatives --install /usr/bin/java java /usr/java/jdk1.8.0_202/bin/ 2// 查看新增的jdk信息$ sudo alternatives --config javaThere are 2 programs which provide &#39;java&#39;.  Selection    Command-----------------------------------------------   1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)*+ 2           /usr/java/jdk1.8.0_202/bin/javaEnter to keep the current selection[+], or type selection number:</code></pre><p>这时候我们可以进行切换操作，如下：</p><pre><code>$ sudo alternatives --config javaThere are 2 programs which provide &#39;java&#39;.  Selection    Command-----------------------------------------------*+ 1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)   2           /usr/java/jdk1.8.0_202/bin/javaEnter to keep the current selection[+], or type selection number: 2</code></pre><p>输入2，选择新增的java执行信息，回车后即可生效。</p><pre><code>$ sudo alternatives --config javaThere are 2 programs which provide &#39;java&#39;.  Selection    Command-----------------------------------------------   1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java)*+ 2           /usr/java/jdk1.8.0_202/bin/javaEnter to keep the current selection[+], or type selection number: // ctrl+c退出$ java -versionjava version &quot;1.8.0_202&quot;Java(TM) SE Runtime Environment (build 1.8.0_202-b08)Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)</code></pre><p>已经切换到java8的信息了，说明切换已经生效了。</p><p>但是alternative工具，只能全局生效，不管是普通用户还是root用户，都是同一个java环境，并不能进行java环境的共存。</p><h2 id="分而治之"><a href="#分而治之" class="headerlink" title="分而治之"></a>分而治之</h2><p>首先从alternative工具中删除所有java环境，如下：</p><pre><code>$ sudo alternatives --remove java /usr/java/jdk1.8.0_202/bin/java$ sudo alternatives --remove java /usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java$ sudo alternatives --config java// 执行后无输出信息，说明删除完成</code></pre><p>然后注释掉/etc/profile中java环境的配置，并使其生效：</p><pre><code>$ sudo vim /etc/profile// 拉到文档最后，注释掉配置的java环境信息# # JAVA env#JAVA_HOME=/usr/java/jdk1.8.0_202#JRE_HOME=$JAVA_HOME/jre#CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib#PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin#export JAVA_HOME JRE_HOME CLASS_PATH PATH// :wq保存退出// 使其生效$ source /etc/profile</code></pre><p>最后在不同的用户下，仿照/etc/profile中配置，配置不同的java环境</p><pre><code>// 切换到centos用户下操作$ su - centos// 输入密码信息[centos]$ vim ~/.bashrc// 在最后追加JAVA_HOME=/usr/java/jdk1.8.0_202JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH[centos]$ source ~/.bashrc[centos]$ java -versionjava version &quot;1.8.0_202&quot;Java(TM) SE Runtime Environment (build 1.8.0_202-b08)Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)// 切换到es用户（elasticsearch7.6.2部署所在的用户）$ su - es// 输入密码信息[es]$ vim ~/.bashrc// 在最后追加JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH[es]$ source ~/.bashrcopenjdk version &quot;11.0.7&quot; 2020-04-14 LTSOpenJDK Runtime Environment 18.9 (build 11.0.7+10-LTS)OpenJDK 64-Bit Server VM 18.9 (build 11.0.7+10-LTS, mixed mode, sharing)</code></pre><p>这样根据用户的不同可以使用不同版本的java环境，这样就能启动es了。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s中pod运行的问题排查</title>
      <link href="/2020/07/08/k8s-zhong-pod-yun-xing-de-wen-ti-pai-cha/"/>
      <url>/2020/07/08/k8s-zhong-pod-yun-xing-de-wen-ti-pai-cha/</url>
      
        <content type="html"><![CDATA[<h2 id="关于pom文件版本号对于Dockerfile的影响"><a href="#关于pom文件版本号对于Dockerfile的影响" class="headerlink" title="关于pom文件版本号对于Dockerfile的影响"></a>关于pom文件版本号对于Dockerfile的影响</h2><p>我们之前的设定，对于pom文件中的版本感知不明显，导致我们在Dockerfile里面写死了jar包的版本号。</p><p>当pom文件中的版本号变更时，由于Dockerfile中未能及时变更，导致出现了构建运行镜像失败的问题。</p><p>临时解决方式：</p><p>在构建时添加sed命令，用来修改Dockerfile中的版本不正确的问题。如下图：</p><p><img src="./imgs/%E4%BF%AE%E6%94%B9%E7%89%88%E6%9C%AC.png" alt></p><h2 id="关于k8s中服务反复重启导致无法对外访问的问题排查"><a href="#关于k8s中服务反复重启导致无法对外访问的问题排查" class="headerlink" title="关于k8s中服务反复重启导致无法对外访问的问题排查"></a>关于k8s中服务反复重启导致无法对外访问的问题排查</h2><p>以test-form-design-rdp服务为例子，日志如下：</p><pre><code>DEBUG 2020-06-10 13:45:53:361 main AgentPackagePath : The beacon class location is jar:file:/app/skywalking-agent/skywalking-agent.jar!/org/apache/skywalking/apm/agent/core/boot/AgentPackagePath.class.INFO 2020-06-10 13:45:53:366 main SnifferConfigInitializer : Config file found in /app/skywalking-agent/config/agent.config.  .   ____          _            __ _ _ /\\ / ___&#39;_ __ _ _(_)_ __  __ _ \ \ \ \( ( )\___ | &#39;_ | &#39;_| | &#39;_ \/ _` | \ \ \ \ \\/  ___)| |_)| | | | | || (_| |  ) ) ) )  &#39;  |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot ::        (v2.2.6.RELEASE)2020-06-10 13:46:08.622  INFO [test-form-rdp,,,] 6 --- [           main] c.a.n.c.c.impl.LocalConfigInfoProcessor  : LOCAL_SNAPSHOT_PATH:/root/nacos/config2020-06-10 13:46:08.874  INFO [test-form-rdp,,,] 6 --- [           main] c.a.nacos.client.config.impl.Limiter     : limitTime:5.02020-06-10 13:46:09.042  WARN [test-form-rdp,,,] 6 --- [           main] c.a.c.n.c.NacosPropertySourceBuilder     : Ignore the empty nacos configuration and get it based on dataId[test-form-rdp] &amp; group[DEFAULT_GROUP]2020-06-10 13:46:09.052  INFO [test-form-rdp,,,] 6 --- [           main] c.a.nacos.client.config.utils.JVMUtil    : isMultiInstance:false2020-06-10 13:46:09.125  INFO [test-form-rdp,,,] 6 --- [           main] b.c.PropertySourceBootstrapConfiguration : Located property source: [BootstrapPropertySource {name=&#39;bootstrapProperties-test-form-rdp-standalone.yml,DEFAULT_GROUP&#39;}, BootstrapPropertySource {name=&#39;bootstrapProperties-test-form-rdp.yml,DEFAULT_GROUP&#39;}, BootstrapPropertySource {name=&#39;bootstrapProperties-test-form-rdp,DEFAULT_GROUP&#39;}]2020-06-10 13:46:09.235  INFO [test-form-rdp,,,] 6 --- [           main] c.h.i.form.IcpCloudFormApplication       : The following profiles are active: standalone2020-06-10 13:46:16.690  WARN [test-form-rdp,,,] 6 --- [           main] o.s.boot.actuate.endpoint.EndpointId     : Endpoint ID &#39;nacos-config&#39; contains invalid characters, please migrate to a valid format.2020-06-10 13:46:18.605  INFO [test-form-rdp,,,] 6 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Multiple Spring Data modules found, entering strict repository configuration mode!2020-06-10 13:46:18.612  INFO [test-form-rdp,,,] 6 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.2020-06-10 13:46:18.735  INFO [test-form-rdp,,,] 6 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 100ms. Found 0 MongoDB repository interfaces.2020-06-10 13:46:18.812  INFO [test-form-rdp,,,] 6 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Multiple Spring Data modules found, entering strict repository configuration mode!2020-06-10 13:46:18.819  INFO [test-form-rdp,,,] 6 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data Redis repositories in DEFAULT mode.2020-06-10 13:46:18.897  INFO [test-form-rdp,,,] 6 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 6ms. Found 0 Redis repository interfaces.2020-06-10 13:46:19.672  WARN [test-form-rdp,,,] 6 --- [           main] o.s.boot.actuate.endpoint.EndpointId     : Endpoint ID &#39;service-registry&#39; contains invalid characters, please migrate to a valid format.2020-06-10 13:46:21.712  INFO [test-form-rdp,,,] 6 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=738057ce-da16-3790-9a18-b67c26d5c0382020-06-10 13:46:26.888  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration&#39; of type [org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:26.906  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;objectPostProcessor&#39; of type [org.springframework.security.config.annotation.configuration.AutowireBeanFactoryObjectPostProcessor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:26.915  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;org.springframework.security.access.expression.method.DefaultMethodSecurityExpressionHandler@20a9f5fb&#39; of type [org.springframework.security.access.expression.method.DefaultMethodSecurityExpressionHandler] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:26.917  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration&#39; of type [org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:26.969  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;org.springframework.security.config.annotation.method.configuration.Jsr250MetadataSourceConfiguration&#39; of type [org.springframework.security.config.annotation.method.configuration.Jsr250MetadataSourceConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:26.974  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;jsr250MethodSecurityMetadataSource&#39; of type [org.springframework.security.access.annotation.Jsr250MethodSecurityMetadataSource] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:26.980  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;methodSecurityMetadataSource&#39; of type [org.springframework.security.access.method.DelegatingMethodSecurityMetadataSource] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:27.028  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;redisCacheConfig&#39; of type [com.testsoft.icpcloud.common.cache.config.RedisCacheConfig$$EnhancerBySpringCGLIB$$c422b7ae] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:27.390  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;org.springframework.cloud.sleuth.instrument.web.client.TraceWebClientAutoConfiguration$TraceOAuthConfiguration&#39; of type [org.springframework.cloud.sleuth.instrument.web.client.TraceWebClientAutoConfiguration$TraceOAuthConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:27.475  INFO [test-form-rdp,,,] 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#39;spring.sleuth.redis-org.springframework.cloud.sleuth.instrument.redis.TraceRedisProperties&#39; of type [org.springframework.cloud.sleuth.instrument.redis.TraceRedisProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-06-10 13:46:28.918  INFO [test-form-rdp,,,] 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 20041 (testtp)2020-06-10 13:46:28.947  INFO [test-form-rdp,,,] 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]2020-06-10 13:46:28.948  INFO [test-form-rdp,,,] 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.33]2020-06-10 13:46:29.219  INFO [test-form-rdp,,,] 6 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext2020-06-10 13:46:29.220  INFO [test-form-rdp,,,] 6 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 19798 ms2020-06-10 13:46:30.020  WARN [test-form-rdp,,,] 6 --- [           main] c.n.c.sources.URLConfigurationSource     : No URLs will be polled as dynamic configuration sources.2020-06-10 13:46:30.020  INFO [test-form-rdp,,,] 6 --- [           main] c.n.c.sources.URLConfigurationSource     : To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath.2020-06-10 13:46:30.085  INFO [test-form-rdp,,,] 6 --- [           main] c.netflix.config.DynamicPropertyFactory  : DynamicPropertyFactory is initialized with configuration sources: com.netflix.config.ConcurrentCompositeConfiguration@4dd4965a2020-06-10 13:46:30.566  INFO [test-form-rdp,,,] 6 --- [           main] c.a.d.s.b.a.DruidDataSourceAutoConfigure : Init DruidDataSourceLoading class `com.mysql.jdbc.Driver&#39;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#39;. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.2020-06-10 13:46:32.724  INFO [test-form-rdp,,,] 6 --- [           main] com.alibaba.druid.pool.DruidDataSource   : {dataSource-1} inited2020-06-10 13:46:34.160  INFO [test-form-rdp,,,] 6 --- [           main] org.mongodb.driver.cluster               : Cluster created with settings {hosts=[mongo-outer.test-basic-mongo-outer:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout=&#39;30000 ms&#39;, maxWaitQueueSize=500}2020-06-10 13:46:34.433  INFO [test-form-rdp,,,] 6 --- [           main] org.mongodb.driver.cluster               : Cluster description not yet available. Waiting for 30000 ms before timing out2020-06-10 13:46:34.603  INFO [test-form-rdp,,,] 6 --- [ngo-outer:27017] org.mongodb.driver.connection            : Opened connection [connectionId{localValue:1, serverValue:391}] to mongo-outer.test-basic-mongo-outer:270172020-06-10 13:46:34.640  INFO [test-form-rdp,,,] 6 --- [ngo-outer:27017] org.mongodb.driver.cluster               : Monitor thread successfully connected to server with description ServerDescription{address=mongo-outer.test-basic-mongo-outer:27017, type=STANDALONE, state=CONNECTED, ok=true, version=ServerVersion{versionList=[4, 2, 7]}, minWireVersion=0, maxWireVersion=8, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=19311961}2020-06-10 13:46:43.815  INFO [test-form-rdp,,,] 6 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 2 endpoint(s) beneath base path &#39;/actuator&#39;2020-06-10 13:46:46.083  WARN [test-form-rdp,,,] 6 --- [           main] c.n.c.sources.URLConfigurationSource     : No URLs will be polled as dynamic configuration sources.2020-06-10 13:46:46.083  INFO [test-form-rdp,,,] 6 --- [           main] c.n.c.sources.URLConfigurationSource     : To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath.2020-06-10 13:46:48.433  INFO [test-form-rdp,,,] 6 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService &#39;applicationTaskExecutor&#39; _ _   |_  _ _|_. ___ _ |    _| | |\/|_)(_| | |_\  |_)||_|_\     /               |                        3.3.12020-06-10 13:46:50.785  INFO [test-form-rdp,,,] 6 --- [           main] pertySourcedRequestMappingHandlerMapping : Mapped URL path [/v2/api-docs] onto method [springfox.documentation.swagger2.web.Swagger2Controller#getDocumentation(String, HttpServletRequest)]2020-06-10 13:46:52.471  INFO [test-form-rdp,,,] 6 --- [           main] .s.s.UserDetailsServiceAutoConfiguration :Using generated security password: 6595127a-b2c1-4947-9f73-c17b7a2c70022020-06-10 13:46:53.214  INFO [test-form-rdp,,,] 6 --- [           main] .r.c.IcpSecurityResourceServerConfigurer : IcpResourceServerAutoConfiguration is working!!!2020-06-10 13:46:53.511  INFO [test-form-rdp,,,] 6 --- [           main] o.s.s.web.DefaultSecurityFilterChain     : Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@2ce80c85, org.springframework.security.web.context.SecurityContextPersistenceFilter@2ea3f905, org.springframework.security.web.header.HeaderWriterFilter@9fe1e14, org.springframework.security.web.authentication.logout.LogoutFilter@1e8f20bf, org.springframework.security.oauth2.provider.authentication.OAuth2AuthenticationProcessingFilter@1c027237, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@194feb27, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@3df920, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@25dd9eb6, org.springframework.security.web.session.SessionManagementFilter@41d34a11, org.springframework.security.web.access.ExceptionTranslationFilter@327fd5c9, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@9a00385]2020-06-10 13:46:54.159  WARN [test-form-rdp,,,] 6 --- [           main] ockingLoadBalancerClientRibbonWarnLogger : You already have RibbonLoadBalancerClient on your classpath. It will be used by default. As Spring Cloud Ribbon is in maintenance mode. We recommend switching to BlockingLoadBalancerClient instead. In order to use it, set the value of `spring.cloud.loadbalancer.ribbon.enabled` to `false` or remove spring-cloud-starter-netflix-ribbon from your project.2020-06-10 13:46:54.918  INFO [test-form-rdp,,,] 6 --- [           main] o.s.c.n.eureka.InstanceInfoFactory       : Setting initial instance status as: STARTING2020-06-10 13:46:55.075  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Initializing Eureka in region us-east-12020-06-10 13:46:55.357  INFO [test-form-rdp,,,] 6 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON encoding codec LegacyJacksonJson2020-06-10 13:46:55.357  INFO [test-form-rdp,,,] 6 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON decoding codec LegacyJacksonJson2020-06-10 13:46:56.068  INFO [test-form-rdp,,,] 6 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML encoding codec XStreamXml2020-06-10 13:46:56.068  INFO [test-form-rdp,,,] 6 --- [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML decoding codec XStreamXml2020-06-10 13:46:56.968  INFO [test-form-rdp,,,] 6 --- [           main] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration2020-06-10 13:46:57.134  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Disable delta property : false2020-06-10 13:46:57.135  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Single vip registry refresh property : null2020-06-10 13:46:57.135  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Force full registry fetch : false2020-06-10 13:46:57.135  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Application is null : false2020-06-10 13:46:57.135  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Registered Applications size is zero : true2020-06-10 13:46:57.135  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Application version is -1: true2020-06-10 13:46:57.135  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Getting all instance registry info from the eureka server2020-06-10 13:46:58.190  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : The response status is 2002020-06-10 13:46:58.197  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Starting heartbeat executor: renew interval is: 302020-06-10 13:46:58.201  INFO [test-form-rdp,,,] 6 --- [           main] c.n.discovery.InstanceInfoReplicator     : InstanceInfoReplicator onDemand update allowed rate per min is 42020-06-10 13:46:58.215  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Discovery Client initialized at timestamp 1591768018207 with initial instances count: 172020-06-10 13:46:58.239  INFO [test-form-rdp,,,] 6 --- [           main] o.s.c.n.e.s.EurekaServiceRegistry        : Registering application test-FORM-RDP with eureka with status UP2020-06-10 13:46:58.242  INFO [test-form-rdp,,,] 6 --- [           main] com.netflix.discovery.DiscoveryClient    : Saw local status change event StatusChangeEvent [timestamp=1591768018242, current=UP, previous=STARTING]2020-06-10 13:46:58.247  INFO [test-form-rdp,,,] 6 --- [           main] d.s.w.p.DocumentationPluginsBootstrapper : Context refreshed2020-06-10 13:46:58.284  INFO [test-form-rdp,,,] 6 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_test-FORM-RDP/test-form-rdp@10.42.3.29:20041: registering service...2020-06-10 13:46:58.493  INFO [test-form-rdp,,,] 6 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_test-FORM-RDP/test-form-rdp@10.42.3.29:20041 - registration status: 2042020-06-10 13:46:58.593  INFO [test-form-rdp,,,] 6 --- [           main] d.s.w.p.DocumentationPluginsBootstrapper : Found 1 custom documentation plugin(s)2020-06-10 13:46:58.794  INFO [test-form-rdp,,,] 6 --- [           main] s.d.s.w.s.ApiListingReferenceScanner     : Scanning for api listing references2020-06-10 13:46:59.984  INFO [test-form-rdp,,,] 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 20041 (testtp) with context path &#39;&#39;2020-06-10 13:46:59.988  INFO [test-form-rdp,,,] 6 --- [           main] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 200412020-06-10 13:46:59.996  INFO [test-form-rdp,,,] 6 --- [           main] c.h.i.form.IcpCloudFormApplication       : Started IcpCloudFormApplication in 58.544 seconds (JVM running for 66.916)2020-06-10 13:47:00.017  INFO [test-form-rdp,,,] 6 --- [           main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.229.211_18848-d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e] [subscribe] test-form-rdp.yml+DEFAULT_GROUP+d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e2020-06-10 13:47:00.021  INFO [test-form-rdp,,,] 6 --- [           main] c.a.nacos.client.config.impl.CacheData   : [fixed-192.168.229.211_18848-d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e] [add-listener] ok, tenant=d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e, dataId=test-form-rdp.yml, group=DEFAULT_GROUP, cnt=12020-06-10 13:47:00.021  INFO [test-form-rdp,,,] 6 --- [           main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.229.211_18848-d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e] [subscribe] test-form-rdp+DEFAULT_GROUP+d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e2020-06-10 13:47:00.021  INFO [test-form-rdp,,,] 6 --- [           main] c.a.nacos.client.config.impl.CacheData   : [fixed-192.168.229.211_18848-d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e] [add-listener] ok, tenant=d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e, dataId=test-form-rdp, group=DEFAULT_GROUP, cnt=12020-06-10 13:47:00.022  INFO [test-form-rdp,,,] 6 --- [           main] c.a.n.client.config.impl.ClientWorker    : [fixed-192.168.229.211_18848-d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e] [subscribe] test-form-rdp-standalone.yml+DEFAULT_GROUP+d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e2020-06-10 13:47:00.022  INFO [test-form-rdp,,,] 6 --- [           main] c.a.nacos.client.config.impl.CacheData   : [fixed-192.168.229.211_18848-d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e] [add-listener] ok, tenant=d79ba2e9-da64-40e8-9dcd-92cc24bcaf4e, dataId=test-form-rdp-standalone.yml, group=DEFAULT_GROUP, cnt=12020-06-10 13:47:00.032  INFO [test-form-rdp,,,] 6 --- [cd-92cc24bcaf4e] c.a.n.client.config.impl.ClientWorker    : get changedGroupKeys:[]2020-06-10 13:47:29.743  INFO [test-form-rdp,,,] 6 --- [cd-92cc24bcaf4e] c.a.n.client.config.impl.ClientWorker    : get changedGroupKeys:[]2020-06-10 13:47:59.246  INFO [test-form-rdp,,,] 6 --- [cd-92cc24bcaf4e] c.a.n.client.config.impl.ClientWorker    : get changedGroupKeys:[]2020-06-10 13:48:28.750  INFO [test-form-rdp,,,] 6 --- [cd-92cc24bcaf4e] c.a.n.client.config.impl.ClientWorker    : get changedGroupKeys:[]2020-06-10 13:48:58.254  INFO [test-form-rdp,,,] 6 --- [cd-92cc24bcaf4e] c.a.n.client.config.impl.ClientWorker    : get changedGroupKeys:[]2020-06-10 13:49:15.018  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] o.s.c.n.e.s.EurekaServiceRegistry        : Unregistering application test-FORM-RDP with eureka with status DOWN2020-06-10 13:49:15.019  WARN [test-form-rdp,,,] 6 --- [extShutdownHook] com.netflix.discovery.DiscoveryClient    : Saw local status change event StatusChangeEvent [timestamp=1591768155018, current=DOWN, previous=UP]2020-06-10 13:49:15.020  INFO [test-form-rdp,,,] 6 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_test-FORM-RDP/test-form-rdp@10.42.3.29:20041: registering service...2020-06-10 13:49:15.027  INFO [test-form-rdp,,,] 6 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_test-FORM-RDP/test-form-rdp@10.42.3.29:20041 - registration status: 2042020-06-10 13:49:15.055  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] o.s.s.concurrent.ThreadPoolTaskExecutor  : Shutting down ExecutorService &#39;applicationTaskExecutor&#39;2020-06-10 13:49:15.198  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] com.alibaba.druid.pool.DruidDataSource   : {dataSource-1} closing ...2020-06-10 13:49:15.232  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] com.alibaba.druid.pool.DruidDataSource   : {dataSource-1} closed2020-06-10 13:49:15.239  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] com.netflix.discovery.DiscoveryClient    : Shutting down DiscoveryClient ...2020-06-10 13:49:18.241  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] com.netflix.discovery.DiscoveryClient    : Unregistering ...2020-06-10 13:49:18.253  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_test-FORM-RDP/test-form-rdp@10.42.3.29:20041 - deregister  status: 2002020-06-10 13:49:18.274  INFO [test-form-rdp,,,] 6 --- [extShutdownHook] com.netflix.discovery.DiscoveryClient    : Completed shut down of DiscoveryClient</code></pre><p>启动过程中，未发生任何错误，在启动完成后却出现了服务自动下线的情况，体现在Eureka中就是服务明明上线了，过一分钟左右就自动下线了。</p><p>目前怀疑是k8s的健康检查干掉了正在运行的服务，这时候需要排查配置文件和k8s部署文件的区别。</p><p>配置文件如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token key atrule">application</span><span class="token punctuation">:</span>    <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>rdp  <span class="token key atrule">data</span><span class="token punctuation">:</span>    <span class="token key atrule">mongodb</span><span class="token punctuation">:</span>      <span class="token key atrule">uri</span><span class="token punctuation">:</span> mongodb<span class="token punctuation">:</span>//form<span class="token punctuation">-</span>design<span class="token punctuation">:</span>123456@mongo<span class="token punctuation">-</span>outer.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>mongo<span class="token punctuation">-</span>outer<span class="token punctuation">:</span>27017/test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design      <span class="token comment" spellcheck="true"># uri: mongodb://127.0.0.1:27017/test-form</span>  <span class="token key atrule">datasource</span><span class="token punctuation">:</span>    <span class="token key atrule">username</span><span class="token punctuation">:</span> root    <span class="token key atrule">password</span><span class="token punctuation">:</span> testMySQL789    <span class="token key atrule">driver-class-name</span><span class="token punctuation">:</span> com.mysql.jdbc.Driver    <span class="token key atrule">url</span><span class="token punctuation">:</span> jdbc<span class="token punctuation">:</span>mysql<span class="token punctuation">:</span>//mysql<span class="token punctuation">-</span>outer.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>mysql<span class="token punctuation">-</span>outer<span class="token punctuation">:</span>3306/FORMDESIGN<span class="token punctuation">?</span>allowMultiQueries=true<span class="token important">&amp;serverTimezone</span>=Asia/Shanghai<span class="token important">&amp;characterEncoding</span>=utf8<span class="token important">&amp;useUnicode</span>=true<span class="token important">&amp;useSSL</span>=false    <span class="token key atrule">druid</span><span class="token punctuation">:</span>      <span class="token key atrule">initial-size</span><span class="token punctuation">:</span> <span class="token number">8</span>      <span class="token key atrule">min-idle</span><span class="token punctuation">:</span> <span class="token number">8</span>      <span class="token key atrule">max-active</span><span class="token punctuation">:</span> <span class="token number">50</span>      <span class="token key atrule">max-wait</span><span class="token punctuation">:</span> <span class="token number">60000</span>      <span class="token key atrule">time-between-eviction-runs-millis</span><span class="token punctuation">:</span> <span class="token number">60000</span>      <span class="token key atrule">min-evictable-idle-time-millis</span><span class="token punctuation">:</span> <span class="token number">300000</span>      <span class="token key atrule">validationQuery</span><span class="token punctuation">:</span> select 'x'      <span class="token key atrule">test-while-idle</span><span class="token punctuation">:</span> <span class="token boolean important">true  </span>      <span class="token key atrule">test-on-borrow</span><span class="token punctuation">:</span> <span class="token boolean important">false  </span>      <span class="token key atrule">test-on-return</span><span class="token punctuation">:</span> <span class="token boolean important">false  </span>      <span class="token key atrule">multi-statement-allow</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token key atrule">filters</span><span class="token punctuation">:</span> config<span class="token punctuation">,</span>stat        <span class="token key atrule">poolPreparedStatements</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token key atrule">maxPoolPreparedStatementPerConnectionSize</span><span class="token punctuation">:</span> <span class="token number">20</span>      <span class="token key atrule">maxOpenPreparedStatements</span><span class="token punctuation">:</span> <span class="token number">20</span>      <span class="token key atrule">web-stat-filter</span><span class="token punctuation">:</span>        <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token key atrule">url-pattern</span><span class="token punctuation">:</span> /*        <span class="token key atrule">exclusions</span><span class="token punctuation">:</span> /druid/*<span class="token punctuation">,</span>*.js<span class="token punctuation">,</span>*.gif<span class="token punctuation">,</span>*.jpg<span class="token punctuation">,</span>*.bmp<span class="token punctuation">,</span>*.png<span class="token punctuation">,</span>*.css<span class="token punctuation">,</span>*.ico        <span class="token key atrule">session-stat-enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token key atrule">session-stat-max-count</span><span class="token punctuation">:</span> <span class="token number">10</span>      <span class="token key atrule">stat-view-servlet</span><span class="token punctuation">:</span>        <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token key atrule">url-pattern</span><span class="token punctuation">:</span> /druid/*        <span class="token key atrule">reset-enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token key atrule">login-username</span><span class="token punctuation">:</span> root        <span class="token key atrule">login-password</span><span class="token punctuation">:</span> testMySQL789      <span class="token key atrule">filter</span><span class="token punctuation">:</span>        <span class="token key atrule">wall</span><span class="token punctuation">:</span>          <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true </span>          <span class="token key atrule">db-type</span><span class="token punctuation">:</span> mysql          <span class="token key atrule">config</span><span class="token punctuation">:</span>            <span class="token key atrule">alter-table-allow</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">truncate-allow</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token key atrule">drop-table-allow</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">multi-statement-allow</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">none-base-statement-allow</span><span class="token punctuation">:</span> <span class="token boolean important">false </span>            <span class="token key atrule">update-where-none-check</span><span class="token punctuation">:</span> <span class="token boolean important">true </span>            <span class="token key atrule">select-into-outfile-allow</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token key atrule">metadata-allow</span><span class="token punctuation">:</span> <span class="token boolean important">true </span>          <span class="token key atrule">log-violation</span><span class="token punctuation">:</span> <span class="token boolean important">true </span>          <span class="token key atrule">throw-exception</span><span class="token punctuation">:</span> <span class="token boolean important">true </span>  <span class="token key atrule">cache</span><span class="token punctuation">:</span>    <span class="token key atrule">caffeine</span><span class="token punctuation">:</span>      <span class="token key atrule">spec</span><span class="token punctuation">:</span> initialCapacity=50<span class="token punctuation">,</span>maximumSize=500<span class="token punctuation">,</span>expireAfterAccess=5s<span class="token punctuation">,</span>expireAfterWrite=10s<span class="token punctuation">,</span>refreshAfterWrite=5s  <span class="token key atrule">redis</span><span class="token punctuation">:</span>     <span class="token key atrule">host</span><span class="token punctuation">:</span> redis<span class="token punctuation">-</span>outer.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>redis<span class="token punctuation">-</span>outer    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">6379</span>    <span class="token key atrule">password</span><span class="token punctuation">:</span> test@redis160    <span class="token key atrule">timeout</span><span class="token punctuation">:</span> <span class="token number">60000</span>    <span class="token key atrule">database</span><span class="token punctuation">:</span> <span class="token number">2</span>    <span class="token key atrule">lettuce</span><span class="token punctuation">:</span>      <span class="token key atrule">pool</span><span class="token punctuation">:</span>        <span class="token key atrule">max-active</span><span class="token punctuation">:</span> <span class="token number">8</span>        <span class="token key atrule">max-wait</span><span class="token punctuation">:</span> <span class="token number">-1</span>        <span class="token key atrule">max-idle</span><span class="token punctuation">:</span> <span class="token number">8</span>        <span class="token key atrule">min-idle</span><span class="token punctuation">:</span> <span class="token number">0</span>    <span class="token key atrule">jedis</span><span class="token punctuation">:</span>      <span class="token key atrule">pool</span><span class="token punctuation">:</span>        <span class="token key atrule">max-active</span><span class="token punctuation">:</span> <span class="token number">8</span>        <span class="token key atrule">max-wait</span><span class="token punctuation">:</span> <span class="token number">-1</span>        <span class="token key atrule">max-idle</span><span class="token punctuation">:</span> <span class="token number">8</span>        <span class="token key atrule">min-idle</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token key atrule">eureka</span><span class="token punctuation">:</span>  <span class="token key atrule">instance</span><span class="token punctuation">:</span>    <span class="token key atrule">prefer-ip-address</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">instance-id</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>spring.application.name<span class="token punctuation">}</span>@$<span class="token punctuation">{</span>spring.cloud.client.ip<span class="token punctuation">-</span>address<span class="token punctuation">}</span><span class="token punctuation">:</span>$<span class="token punctuation">{</span>server.port<span class="token punctuation">}</span>    <span class="token key atrule">ipAddress</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>spring.cloud.client.ip<span class="token punctuation">-</span>address<span class="token punctuation">}</span>  <span class="token key atrule">client</span><span class="token punctuation">:</span>    <span class="token key atrule">service-url</span><span class="token punctuation">:</span>      <span class="token key atrule">defaultZone</span><span class="token punctuation">:</span> testtp<span class="token punctuation">:</span>//test<span class="token punctuation">:</span>test2019@eureka<span class="token punctuation">-</span>0.eureka.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>eureka.svc.cluster.local<span class="token punctuation">:</span>19011/eureka/<span class="token punctuation">,</span>testtp<span class="token punctuation">:</span>//test<span class="token punctuation">:</span>test2019@eureka<span class="token punctuation">-</span>1.eureka.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>eureka.svc.cluster.local<span class="token punctuation">:</span>19011/eureka/<span class="token punctuation">,</span>testtp<span class="token punctuation">:</span>//test<span class="token punctuation">:</span>test2019@eureka<span class="token punctuation">-</span>2.eureka.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>eureka.svc.cluster.local<span class="token punctuation">:</span>19011/eureka/      <span class="token comment" spellcheck="true">#defaultZone: testtp://test:test2019@127.0.0.1:19011/eureka/</span><span class="token key atrule">swagger</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token key atrule">auth-server</span><span class="token punctuation">:</span> testtp<span class="token punctuation">:</span>//test<span class="token punctuation">-</span>rdp<span class="token punctuation">-</span>uaa<span class="token key atrule">security</span><span class="token punctuation">:</span>  <span class="token key atrule">oauth2</span><span class="token punctuation">:</span>    <span class="token key atrule">client</span><span class="token punctuation">:</span>      <span class="token key atrule">client-id</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>rdp       <span class="token key atrule">client-secret</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>rdp@testdev       <span class="token key atrule">access-token-uri</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>auth<span class="token punctuation">-</span>server<span class="token punctuation">}</span>/oauth/token       <span class="token key atrule">user-authorization-uri</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>auth<span class="token punctuation">-</span>server<span class="token punctuation">}</span>/oauth/authorize     <span class="token key atrule">resource</span><span class="token punctuation">:</span>      <span class="token key atrule">token-info-uri</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>auth<span class="token punctuation">-</span>server<span class="token punctuation">}</span>/oauth/check_token <span class="token key atrule">server</span><span class="token punctuation">:</span>  <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">20041</span><span class="token comment" spellcheck="true"># mybatis-plus:</span><span class="token comment" spellcheck="true">#   mapper-locations: classpath:mybatis/mapper/*Mapper.xml</span><span class="token comment" spellcheck="true">#   configuration:</span><span class="token comment" spellcheck="true">#     cache-enabled: true</span><span class="token comment" spellcheck="true">#     call-setters-on-nulls: true</span><span class="token comment" spellcheck="true">#     log-impl: org.apache.ibatis.logging.stdout.StdOutImpl</span><span class="token key atrule">icpcloud</span><span class="token punctuation">:</span>  <span class="token key atrule">security</span><span class="token punctuation">:</span>    <span class="token key atrule">resource</span><span class="token punctuation">:</span>      <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>      <span class="token key atrule">permitAll</span><span class="token punctuation">:</span>        <span class="token key atrule">filterPath</span><span class="token punctuation">:</span> /emp/info/**/security<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里端口号配置的是20041，那么这时在看该服务的k8s部署文档：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">20040</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> tcp      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">20040</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>        <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>          <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>                <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname                <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                  <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                    <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app                      <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                      <span class="token key atrule">values</span><span class="token punctuation">:</span>                        <span class="token punctuation">-</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design                        <span class="token punctuation">-</span> test<span class="token punctuation">-</span>workflow                        <span class="token punctuation">-</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp              <span class="token key atrule">weigtest</span><span class="token punctuation">:</span> <span class="token number">1</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp          <span class="token key atrule">image</span><span class="token punctuation">:</span> 10.0.88.159<span class="token punctuation">:</span>5000/formdesign<span class="token punctuation">-</span>rdp<span class="token punctuation">-</span>develop<span class="token punctuation">:</span>$BUILD_NUMBER          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>            <span class="token key atrule">preStop</span><span class="token punctuation">:</span>              <span class="token key atrule">testtpGet</span><span class="token punctuation">:</span>                <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">20040</span>                <span class="token key atrule">path</span><span class="token punctuation">:</span> /spring/shutdown          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">testtpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">20040</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">testtpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">20040</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">resources</span><span class="token punctuation">:</span>            <span class="token key atrule">requests</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 350Mi            <span class="token key atrule">limits</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1.5Gi          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">20040</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> IP_ADDR              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> status.podIP            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_IP              <span class="token key atrule">value</span><span class="token punctuation">:</span> 192.168.229.211<span class="token punctuation">:</span><span class="token number">18848</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> d79ba2e9<span class="token punctuation">-</span>da64<span class="token punctuation">-</span>40e8<span class="token punctuation">-</span>9dcd<span class="token punctuation">-</span>92cc24bcaf4e            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>k8s<span class="token punctuation">-</span>develop            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_TARGET_SERVICE_NAME              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>form<span class="token punctuation">-</span>design<span class="token punctuation">-</span>rdp            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_IP_PORT              <span class="token key atrule">value</span><span class="token punctuation">:</span> 10.0.88.163<span class="token punctuation">:</span><span class="token number">11800</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CHANNEL              <span class="token key atrule">value</span><span class="token punctuation">:</span> standalone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此处端口号为20040，这时候两个位置并不统一，导致了访问出现问题，服务上线后又迅速被干掉了。</p><p>解决方式：固定端口号信息，以配置文件为准，进行修改即可！</p><h2 id="关于服务器内核自动升级问题的解决"><a href="#关于服务器内核自动升级问题的解决" class="headerlink" title="关于服务器内核自动升级问题的解决"></a>关于服务器内核自动升级问题的解决</h2><p>由于部分机器进行了</p><pre><code>sudo yum update</code></pre><p>操作，导致内核信息进行了升级，导致有一些机器在启动的时候出现了黑屏和无法启动的情况。</p><p>由于在启动时，默认选择了新升级的内核进行了启动，导致报错。如下：</p><p><img src="%E6%96%B0%E5%86%85%E6%A0%B8%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99.png" alt></p><p>目前可用的稳定版内核为：3.10.0-1062.12.1.el7.x86_64，或者低于该版本的内核信息。</p><p>以192.168.229.221机器为例进行查看和修改，首先登陆192.168.229.221，查看已安装的内核信息：</p><pre><code>$ sudo rpm -qa |grep kernelkernel-tools-libs-3.10.0-1127.10.1.el7.x86_64kernel-tools-3.10.0-1127.10.1.el7.x86_64abrt-addon-kerneloops-2.1.11-57.el7.centos.x86_64kernel-3.10.0-1127.10.1.el7.x86_64kernel-3.10.0-862.el7.x86_64kernel-3.10.0-1062.12.1.el7.x86_64$ sudo rpm -e kernel.x86_64error: &quot;kernel.x86_64&quot; specifies multiple packages:  kernel-3.10.0-862.el7.x86_64  kernel-3.10.0-1062.12.1.el7.x86_64  kernel-3.10.0-1127.10.1.el7.x86_64</code></pre><p>然后查看系统可用内核，查看启动时可选择的内核信息：</p><pre><code>$ sudo cat /boot/grub2/grub.cfg |grep menuentryif [ x&quot;${feature_menuentry_id}&quot; = xy ]; then  menuentry_id_option=&quot;--id&quot;  menuentry_id_option=&quot;&quot;export menuentry_id_optionmenuentry &#39;CentOS Linux (3.10.0-1127.10.1.el7.x86_64) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-3.10.0-862.el7.x86_64-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {menuentry &#39;CentOS Linux (3.10.0-1062.12.1.el7.x86_64) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-3.10.0-862.el7.x86_64-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {menuentry &#39;CentOS Linux (3.10.0-862.el7.x86_64) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-3.10.0-862.el7.x86_64-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {menuentry &#39;CentOS Linux (0-rescue-ffe45569e61149c0a79c77c7b38c8e33) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-0-rescue-ffe45569e61149c0a79c77c7b38c8e33-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {</code></pre><p>下一步查看当前内核信息：</p><pre><code>$ uname -aLinux dev-k8s-node01 3.10.0-1127.10.1.el7.x86_64 #1 SMP Wed Jun 3 14:28:03 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</code></pre><p>目前默认的就是这个3.10.0-1127的内核进行启动的，可能会存在问题，这时候需要修改开机时的默认使用的内核。</p><pre><code>$ sudo grub2-set-default  &#39;CentOS Linux (3.10.0-1062.12.1.el7.x86_64) 7 (Core)&#39;$ sudo grub2-editenv listsaved_entry=CentOS Linux (3.10.0-1062.12.1.el7.x86_64) 7 (Core)</code></pre><p>修改完成后，进行如下操作：</p><ol><li>删除存在启动问题内核</li></ol><pre><code>$ sudo rpm -qa | grep kernelkernel-tools-libs-3.10.0-1127.10.1.el7.x86_64kernel-tools-3.10.0-1127.10.1.el7.x86_64abrt-addon-kerneloops-2.1.11-57.el7.centos.x86_64kernel-3.10.0-1127.10.1.el7.x86_64kernel-3.10.0-862.el7.x86_64kernel-3.10.0-1062.12.1.el7.x86_64$ sudo yum remove kernel-3.10.0-1127.10.1.el7.x86_64Loaded plugins: fastestmirror, langpacks, versionlockSkipping the running kernel: kernel-3.10.0-1127.10.1.el7.x86_64No Packages marked for removal </code></pre><p>删除内核时出现上述错误，是因为当前内核正在使用，需要机器进行重启后才能删除。</p><ol start="2"><li>禁用内核自动更新</li></ol><pre><code>// 复制保留原来的配置文件$ sudo cp /etc/yum.conf /etc/yum.conf.bak$ sudo vim /etc/yum.conf// 在[main]的最后添加 exclude=kernel*exclude=kernel*// :wq保存退出</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>jenkins参数化构建实践--动态配置k8s部署文件</title>
      <link href="/2020/07/02/jenkins-can-shu-hua-gou-jian-shi-jian-dong-tai-pei-zhi-k8s-bu-shu-wen-jian/"/>
      <url>/2020/07/02/jenkins-can-shu-hua-gou-jian-shi-jian-dong-tai-pei-zhi-k8s-bu-shu-wen-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="jenkins参数化构建实践–动态配置k8s部署文件"><a href="#jenkins参数化构建实践–动态配置k8s部署文件" class="headerlink" title="jenkins参数化构建实践–动态配置k8s部署文件"></a>jenkins参数化构建实践–动态配置k8s部署文件</h1><h2 id="使用插件"><a href="#使用插件" class="headerlink" title="使用插件"></a>使用插件</h2><p>利用jenkins中的Build With Parameters Plugin插件进行构建。</p><h2 id="后端参数化构建设置"><a href="#后端参数化构建设置" class="headerlink" title="后端参数化构建设置"></a>后端参数化构建设置</h2><ul><li><ol><li>配置文件的变更</li></ol></li></ul><p>以test-instance-utils项目中的data-dictionary-k8s.yaml配置文件为例子：</p><pre class="line-numbers language-YAML"><code class="language-YAML">---apiVersion: v1kind: Servicemetadata:  name: test-instance-data-dictionary  namespace: test-instance-all-service  labels:    app: test-instance-data-dictionaryspec:  ports:    - port: ${SERVER_PORT}      name: tcp      targetPort: ${SERVER_PORT}  selector:    app: test-instance-data-dictionary---apiVersion: apps/v1kind: Deploymentmetadata:  name: test-instance-data-dictionary  namespace: test-instance-all-servicespec:  minReadySeconds: 10  revisionHistoryLimit: 10  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 0      maxSurge: 1  replicas: 1  selector:    matchLabels:      app: test-instance-data-dictionary  template:    metadata:      labels:        app: test-instance-data-dictionary    spec:      affinity:        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:            - podAffinityTerm:                topologyKey: kubernetes.io/hostname                labelSelector:                  matchExpressions:                    - key: app                      operator: In                      values:                        - app-test-instance-data-dictionary              weight: 1      containers:        - name: data-dictionary          image: ${DOCKER_HUB}/data-dictionary-k8s:$BUILD_NUMBER          imagePullPolicy: Always          lifecycle:            preStop:              httpGet:                port: ${SERVER_PORT}                path: /spring/shutdown          livenessProbe:            httpGet:              path: /actuator/health              port: ${SERVER_PORT}            initialDelaySeconds: 180            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          readinessProbe:            httpGet:              path: /actuator/health              port: ${SERVER_PORT}            initialDelaySeconds: 180            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          resources:            requests:              memory: 500Mi            limits:              memory: 1Gi          ports:            - containerPort: ${SERVER_PORT}          env:            - name: IP_ADDR              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: ${NACOS_IP_PORT}            - name: NACOS_NAMESPACE              value: ${NACOS_NAMESPACE}            - name: SKYWALKING_NAMESPACE              value: test-instance-k8s-test            - name: SKYWALKING_TARGET_SERVICE_NAME              value: test-instance-data-dic            - name: SKYWALKING_IP_PORT              value: ${SKYWALKING_IP_PORT}            - name: CHANNEL              value: ${CHANNEL}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要确定下面几个变量：</p><ul><li><p>${SERVER_PORT} —- 服务端口号，需要同配置文件中进行对应</p></li><li><p>${DOCKER_HUB}  —- 拉取镜像的地址</p></li><li><p>${NACOS_IP_PORT}  —- 配置中心地址</p></li><li><p>${NACOS_NAMESPACE} —- 配置中心的命名空间</p></li><li><p>${SKYWALKING_IP_PORT} —- 全链路监控skywalking的地址监控</p></li><li><p>${CHANNEL}  —- 配置文件后缀，服务名+后缀确定拉取的配置文件名称</p></li></ul><ul><li><ol start="2"><li>构建时配置的变更</li></ol></li></ul><p>先来看效果，点击Build with Parameters后，出现下图效果</p><p><img src="%E5%8F%82%E6%95%B0%E5%8C%96%E6%9E%84%E5%BB%BA.png" alt></p><p>借助<em>Extended Choice Parameter</em>插件，进行参数化构建的配置，最终配置如下：</p><p><img src="jenkins%E6%9E%84%E5%BB%BA%E6%95%B4%E4%BD%93%E9%85%8D%E7%BD%AE.png" alt></p><p>可以看到我们添加了上述几个变量的参数，并且设置了默认值。</p><p>这样就可以达到不变更提交代码，就可以自行配置构建参数的目的，简化操作。</p><h2 id="前端参数化构建的设置"><a href="#前端参数化构建的设置" class="headerlink" title="前端参数化构建的设置"></a>前端参数化构建的设置</h2><ul><li><ol><li>配置文件的变更</li></ol></li></ul><pre class="line-numbers language-YAML"><code class="language-YAML">---apiVersion: v1kind: Servicemetadata:  name: test-instance-frontend  namespace: test-instance-all-service  labels:    app: test-instance-frontendspec:  type: NodePort  ports:    - port: ${CONTAINER_PORT}      name: tcp      targetPort: ${CONTAINER_PORT}      nodePort: ${NODE_PORT}  selector:    app: test-instance-frontend---apiVersion: apps/v1kind: Deploymentmetadata:  name: test-instance-frontend  namespace: test-instance-all-servicespec:  minReadySeconds: 10  revisionHistoryLimit: 10  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 0      maxSurge: 1  replicas: 1  selector:    matchLabels:      app: test-instance-frontend  template:    metadata:      labels:        app: test-instance-frontend    spec:      affinity:        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:            - podAffinityTerm:                topologyKey: kubernetes.io/hostname                labelSelector:                  matchExpressions:                    - key: app                      operator: In                      values:                        - app-test-instance-frontend              weight: 1      containers:        - name: test-instance-frontend          image: ${DOCKER_HUB}/test-instance-frontend-k8s:$BUILD_NUMBER          imagePullPolicy: Always          resources:            requests:              memory: 1024Mi            limits:              memory: 1.5Gi          ports:            - containerPort: ${CONTAINER_PORT}          env:            - name: GATEWAY_HOST              value: ${GATEWAY_HOST}              #value: gateway.test-instance-basic-gateway:19020 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要确定下面几个变量：</p><ul><li>${CONTAINER_PORT} —- 镜像运行的端口信息</li><li>${NODE_PORT}      —- 对外服务的端口号</li><li>${DOCKER_HUB}     —- 拉取镜像的地址</li><li>${GATEWAY_HOST}   —- 网关配置</li></ul><ul><li><ol start="2"><li>构建时配置的变更</li></ol></li></ul><p>借助<em>Extended Choice Parameter</em>插件，进行参数化构建的配置，最终配置如下：</p><p><img src="%E5%89%8D%E7%AB%AF%E5%8F%82%E6%95%B0%E5%8C%96%E6%9E%84%E5%BB%BA.png" alt></p><p>这样前端参数化构建就完成了。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li>参数化构建和webhook触发构建是否冲突？</li></ol><p>通过提交测试，证明两部分不冲突，能够正常触发并且以默认值的方式进行构建。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>利用参数化构建，实现部分参数的剥离配置，在后续部署或者调整时，灵活性更大。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>FastDFS搭建记录</title>
      <link href="/2020/06/24/fastdfs-da-jian-ji-lu/"/>
      <url>/2020/06/24/fastdfs-da-jian-ji-lu/</url>
      
        <content type="html"><![CDATA[<h1 id="FastDFS搭建记录"><a href="#FastDFS搭建记录" class="headerlink" title="FastDFS搭建记录"></a>FastDFS搭建记录</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>fastDFS 是以C语言开发的一项开源轻量级分布式文件系统，他对文件进行管理，主要功能有：文件存储，文件同步，文件访问（文件上传/下载），特别适合以文件为载体的在线服务，如图片网站，视频网站等。</p><p>利用FastDFS搭建分布式文件系统，基于客户端/服务器的文件存储系统，其对等特性允许一些系统扮演客户端和服务器的双重角色，可供多个用户访问的服务器，比如，用户可以“发表”一个允许其他客户机访问的目录，一旦被访问，这个目录对客户机来说就像使用本地驱动器一样。</p><p>FastDFS由跟踪服务器(Tracker Server)、存储服务器(Storage Server)和客户端(Client)构成。</p><ul><li>Tracker server 追踪服务器</li></ul><p>追踪服务器负责接收客户端的请求，选择合适的组合storage server ，tracker server 与 storage server之间也会用心跳机制来检测对方是否活着。Tracker需要管理的信息也都放在内存中，并且里面所有的Tracker都是对等的（每个节点地位相等），易扩展。客户端访问集群的时候会随机分配一个Tracker来和客户端交互。</p><ul><li>Storage server 储存服务器</li></ul><p>实际存储数据，分成若干个组（group），实际traker就是管理的storage中的组，而组内机器中则存储数据，group可以隔离不同应用的数据，不同的应用的数据放在不同group里面，</p><ul><li>主从型分布式存储，存储空间方便拓展，有利于实现海量存储</li><li>fastDFS对文件内容做hash处理，避免出现重复文件</li><li>然后fastDFS结合Nginx集成, 提供网站效率</li></ul><ul><li>客户端Client</li></ul><p>主要是上传下载数据的服务器，也就是我们自己的项目所部署在的服务器。每个客户端服务器都需要安装Nginx，利用nginx访问文件数据。</p><h2 id="docker方式安装"><a href="#docker方式安装" class="headerlink" title="docker方式安装"></a>docker方式安装</h2><p>首先查找fastdfs的安装包，并进行拉取：</p><pre><code>$ docker search fastdfsNAME                         DESCRIPTION                                     STARS               OFFICIAL            AUTOMATEDseason/fastdfs               FastDFS                                         64luhuiguo/fastdfs             FastDFS is an open source high performance d…   25                                      [OK]ygqygq2/fastdfs-nginx        整合了nginx的fastdfs                                20                                      [OK]morunchang/fastdfs           A FastDFS image                                 19delron/fastdfs                                                               12moocu/fastdfs                fastdfs5.11                                     9qbanxiaoli/fastdfs           FastDFS+FastDHT单机版                              8                                       [OK]ecarpo/fastdfs-storage                                                       4ecarpo/fastdfs                                                               3lionheart/fastdfs-tracker    just have a try on autobuilded -_-#             3                                       [OK]imlzw/fastdfs-tracker        fastdfs的tracker服务                               3                                       [OK]imlzw/fastdfs-storage-dht    fastdfs的storage服务,并且集成了fastdht的服务…              2                                       [OK]manuku/fastdfs-fastdht       fastdfs fastdht                                 2                                       [OK]john123951/fastdfs_storage   fastdfs storage                                 1                                       [OK]manuku/fastdfs-tracker       fastdfs tracker                                 1                                       [OK]lionheart/fastdfs_tracker    fastdfs file system‘s tracker node              1basemall/fastdfs-nginx       fastdfs with nginx                              1                                       [OK]appcrash/fastdfs_nginx       fastdfs with nginx                              1leaon/fastdfs                fastdfs                                         1evan1120/fastdfs_tracker     The fastdfs tracker docker image, only conta…   1                                       [OK]evan1120/fastdfs_storage     The fastdfs storage image                       1                                       [OK]tsl0922/fastdfs              FastDFS is an open source high performance d…   0                                       [OK]lovetutu/fastdfs_fastdht     fastdfs_fastdht_docker                          0mypjb/fastdfs                this is a fastdfs docker project                0                                       [OK]manuku/fastdfs-storage-dht   fastdfs storage dht                             0                                       [OK]$ docker pull delron/fastdfs$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEhello-world         latest              bf756fb1ae65        6 months ago        13.3kBdelron/fastdfs      latest              8487e86fc6ee        2 years ago         464MB</code></pre><p>拉取完毕后开始进行分步启动，首先构建tracker容器并启动，然后构建storage容器并启动。</p><pre><code>$ docker run -d --network=host --name tracker -v /var/fdfs/tracker:/var/fdfs delron/fastdfs tracker$ docker run -d --network=host --name storage -e TRACKER_SERVER=192.168.122.236:22122 -v /var/fdfs/storage:/var/fdfs -e GROUP_NAME=group1 delron/fastdfs storage$ docker ps -aCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                   PORTS               NAMESc49cea16c916        delron/fastdfs      &quot;/usr/bin/start1.sh …&quot;   2 weeks ago         Up 21 hours                                  storage5d2c2c7eccda        delron/fastdfs      &quot;/usr/bin/start1.sh …&quot;   2 weeks ago         Up 21 hours                                  tracker</code></pre><p>下面主要是对两个镜像中的配置信息进行修改</p><pre><code>$ docker exec -it storage /bin/bashbash4.2-# cat /etc/fdfs/storage.conf# is this config file disabled# false for enabled# true for disableddisabled=false# the name of the group this storage server belongs to## comment or remove this item for fetching from tracker server,# in this case, use_storage_id must set to true in tracker.conf,# and storage_ids.conf must be configed correctly.group_name=group1# bind an address of this host# empty for bind all addresses of this hostbind_addr=# if bind an address of this host when connect to other servers# (this storage server as a client)# true for binding the address configed by above parameter: &quot;bind_addr&quot;# false for binding any address of this hostclient_bind=true# the storage server port# 注意第一个端口号，23000为storage服务所运行的端口号port=23000      # connect timeout in seconds# default value is 30sconnect_timeout=30# network timeout in seconds# default value is 30snetwork_timeout=60# heart beat interval in secondsheart_beat_interval=30# disk usage report interval in secondsstat_report_interval=60# the base path to store data and log filesbase_path=/var/fdfs# max concurrent connections the server supported# default value is 256# more max_connections means more memory will be usedmax_connections=256# the buff size to recv / send data# this parameter must more than 8KB# default value is 64KB# since V2.00buff_size = 256KB# accept thread count# default value is 1# since V4.07accept_threads=1# work thread count, should &lt;= max_connections# work thread deal network io# default value is 4# since V2.00work_threads=4# if disk read / write separated##  false for mixed read and write##  true for separated read and write# default value is true# since V2.00disk_rw_separated = true# disk reader thread count per store base path# for mixed read / write, this parameter can be 0# default value is 1# since V2.00disk_reader_threads = 1# disk writer thread count per store base path# for mixed read / write, this parameter can be 0# default value is 1# since V2.00disk_writer_threads = 1# when no entry to sync, try read binlog again after X milliseconds# must &gt; 0, default value is 200mssync_wait_msec=50# after sync a file, usleep milliseconds# 0 for sync successively (never call usleep)sync_interval=0# storage sync start time of a day, time format: Hour:Minute# Hour from 0 to 23, Minute from 0 to 59sync_start_time=00:00# storage sync end time of a day, time format: Hour:Minute# Hour from 0 to 23, Minute from 0 to 59sync_end_time=23:59# write to the mark file after sync N files# default value is 500write_mark_file_freq=500# path(disk or mount point) count, default value is 1store_path_count=1# store_path#, based 0, if store_path0 not exists, it&#39;s value is base_path# the paths must be existstore_path0=/var/fdfs#store_path1=/var/fdfs2# subdir_count  * subdir_count directories will be auto created under each# store_path (disk), value can be 1 to 256, default value is 256subdir_count_per_path=256# tracker_server can ocur more than once, and tracker_server format is#  &quot;host:port&quot;, host can be hostname or ip address# 注意第三个端口号，22122是tracker服务运行的端口tracker_server=192.168.209.121:22122#standard log level as syslog, case insensitive, value list:### emerg for emergency### alert### crit for critical### error### warn for warning### notice### info### debuglog_level=info#unix group name to run this program,#not set (empty) means run by the group of current userrun_by_group=#unix username to run this program,#not set (empty) means run by current userrun_by_user=# allow_hosts can ocur more than once, host can be hostname or ip address,# &quot;*&quot; (only one asterisk) means match all ip addresses# we can use CIDR ips like 192.168.5.64/26# and also use range like these: 192.168.1.[0-254] and host[01-08,20-25].domain.com# for example:# allow_hosts=192.168.1.[1-15,20]# allow_hosts=host[01-08,20-25].domain.com# allow_hosts=192.168.5.64/26allow_hosts=*# the mode of the files distributed to the data path# 0: round robin(default)# 1: random, distributted by hash codefile_distribute_path_mode=0# valid when file_distribute_to_path is set to 0 (round robin),# when the written file count reaches this number, then rotate to next path# default value is 100file_distribute_rotate_count=100# call fsync to disk when write big file# 0: never call fsync# other: call fsync when written bytes &gt;= this bytes# default value is 0 (never call fsync)fsync_after_written_bytes=0# sync log buff to disk every interval seconds# must &gt; 0, default value is 10 secondssync_log_buff_interval=10# sync binlog buff / cache to disk every interval seconds# default value is 60 secondssync_binlog_buff_interval=10# sync storage stat info to disk every interval seconds# default value is 300 secondssync_stat_file_interval=300# thread stack size, should &gt;= 512KB# default value is 512KBthread_stack_size=512KB# the priority as a source server for uploading file.# the lower this value, the higher its uploading priority.# default value is 10upload_priority=10# the NIC alias prefix, such as eth in Linux, you can see it by ifconfig -a# multi aliases split by comma. empty value means auto set by OS type# default values is emptyif_alias_prefix=# if check file duplicate, when set to true, use FastDHT to store file indexes# 1 or yes: need check# 0 or no: do not check# default value is 0check_file_duplicate=0# file signature method for check file duplicate## hash: four 32 bits hash code## md5: MD5 signature# default value is hash# since V4.01file_signature_method=hash# namespace for storing file indexes (key-value pairs)# this item must be set when check_file_duplicate is true / onkey_namespace=FastDFS# set keep_alive to 1 to enable persistent connection with FastDHT servers# default value is 0 (short connection)keep_alive=0# you can use &quot;#include filename&quot; (not include double quotes) directive to# load FastDHT server list, when the filename is a relative path such as# pure filename, the base path is the base path of current/this config file.# must set FastDHT server list when check_file_duplicate is true / on# please see INSTALL of FastDHT for detail##include /home/yuqing/fastdht/conf/fdht_servers.conf# if log to access log# default value is false# since V4.00use_access_log = false# if rotate the access log every day# default value is false# since V4.00rotate_access_log = false# rotate access log time base, time format: Hour:Minute# Hour from 0 to 23, Minute from 0 to 59# default value is 00:00# since V4.00access_log_rotate_time=00:00# if rotate the error log every day# default value is false# since V4.02rotate_error_log = false# rotate error log time base, time format: Hour:Minute# Hour from 0 to 23, Minute from 0 to 59# default value is 00:00# since V4.02error_log_rotate_time=00:00# rotate access log when the log file exceeds this size# 0 means never rotates log file by log file size# default value is 0# since V4.02rotate_access_log_size = 0# rotate error log when the log file exceeds this size# 0 means never rotates log file by log file size# default value is 0# since V4.02rotate_error_log_size = 0# keep days of the log files# 0 means do not delete old log files# default value is 0log_file_keep_days = 0# if skip the invalid record when sync file# default value is false# since V4.02file_sync_skip_invalid_record=false# if use connection pool# default value is false# since V4.05use_connection_pool = false# connections whose the idle time exceeds this time will be closed# unit: second# default value is 3600# since V4.05connection_pool_max_idle_time = 3600# use the ip address of this storage server if domain_name is empty,# else this domain name will ocur in the url redirected by the tracker serverhttp.domain_name=# the port of the web server on this storage server# 注意第二个端口号，8888为文件访问运行的端口信息http.server_port=8888-----------------分割线----------------bash4.2-# cat /etc/fdfs/storage.conf# is this config file disabled# false for enabled# true for disableddisabled=false# bind an address of this host# empty for bind all addresses of this hostbind_addr=# the tracker server portport=22122# connect timeout in seconds# default value is 30sconnect_timeout=30# network timeout in seconds# default value is 30snetwork_timeout=60# the base path to store data and log filesbase_path=/var/fdfs# max concurrent connections this server supportedmax_connections=256# accept thread count# default value is 1# since V4.07accept_threads=1# work thread count, should &lt;= max_connections# default value is 4# since V2.00work_threads=4# min buff size# default value 8KBmin_buff_size = 8KB# max buff size# default value 128KBmax_buff_size = 128KB# the method of selecting group to upload files# 0: round robin# 1: specify group# 2: load balance, select the max free space group to upload filestore_lookup=2# which group to upload file# when store_lookup set to 1, must set store_group to the group namestore_group=group2# which storage server to upload file# 0: round robin (default)# 1: the first server order by ip address# 2: the first server order by priority (the minimal)store_server=0# which path(means disk or mount point) of the storage server to upload file# 0: round robin# 2: load balance, select the max free space path to upload filestore_path=0# which storage server to download file# 0: round robin (default)# 1: the source storage server which the current file uploaded todownload_server=0# reserved storage space for system or other applications.# if the free(available) space of any stoarge server in# a group &lt;= reserved_storage_space,# no file can be uploaded to this group.# bytes unit can be one of follows:### G or g for gigabyte(GB)### M or m for megabyte(MB)### K or k for kilobyte(KB)### no unit for byte(B)### XX.XX% as ratio such as reserved_storage_space = 10%reserved_storage_space = 10%#standard log level as syslog, case insensitive, value list:### emerg for emergency### alert### crit for critical### error### warn for warning### notice### info### debuglog_level=info#unix group name to run this program,#not set (empty) means run by the group of current userrun_by_group=#unix username to run this program,#not set (empty) means run by current userrun_by_user=# allow_hosts can ocur more than once, host can be hostname or ip address,# &quot;*&quot; (only one asterisk) means match all ip addresses# we can use CIDR ips like 192.168.5.64/26# and also use range like these: 192.168.1.[0-254] and host[01-08,20-25].domain.com# for example:# allow_hosts=192.168.1.[1-15,20]# allow_hosts=host[01-08,20-25].domain.com# allow_hosts=192.168.5.64/26allow_hosts=*# sync log buff to disk every interval seconds# default value is 10 secondssync_log_buff_interval = 10# check storage server alive interval secondscheck_active_interval = 120# thread stack size, should &gt;= 64KB# default value is 64KBthread_stack_size = 64KB# auto adjust when the ip address of the storage server changed# default value is truestorage_ip_changed_auto_adjust = true# storage sync file max delay seconds# default value is 86400 seconds (one day)# since V2.00storage_sync_file_max_delay = 86400# the max time of storage sync a file# default value is 300 seconds# since V2.00storage_sync_file_max_time = 300# if use a trunk file to store several small files# default value is false# since V3.00use_trunk_file = false# the min slot size, should &lt;= 4KB# default value is 256 bytes# since V3.00slot_min_size = 256# the max slot size, should &gt; slot_min_size# store the upload file to trunk file when it&#39;s size &lt;=  this value# default value is 16MB# since V3.00slot_max_size = 16MB# the trunk file size, should &gt;= 4MB# default value is 64MB# since V3.00trunk_file_size = 64MB# if create trunk file advancely# default value is false# since V3.06trunk_create_file_advance = false# the time base to create trunk file# the time format: HH:MM# default value is 02:00# since V3.06trunk_create_file_time_base = 02:00# the interval of create trunk file, unit: second# default value is 38400 (one day)# since V3.06trunk_create_file_interval = 86400# the threshold to create trunk file# when the free trunk file size less than the threshold, will create# the trunk files# default value is 0# since V3.06trunk_create_file_space_threshold = 20G# if check trunk space occupying when loading trunk free spaces# the occupied spaces will be ignored# default value is false# since V3.09# NOTICE: set this parameter to true will slow the loading of trunk spaces# when startup. you should set this parameter to true when neccessary.trunk_init_check_occupying = false# if ignore storage_trunk.dat, reload from trunk binlog# default value is false# since V3.10# set to true once for version upgrade when your version less than V3.10trunk_init_reload_from_binlog = false# the min interval for compressing the trunk binlog file# unit: second# default value is 0, 0 means never compress# FastDFS compress the trunk binlog when trunk init and trunk destroy# recommand to set this parameter to 86400 (one day)# since V5.01trunk_compress_binlog_min_interval = 0# if use storage ID instead of IP address# default value is false# since V4.00use_storage_id = false# specify storage ids filename, can use relative or absolute path# since V4.00storage_ids_filename = storage_ids.conf# id type of the storage server in the filename, values are:## ip: the ip address of the storage server## id: the server id of the storage server# this paramter is valid only when use_storage_id set to true# default value is ip# since V4.03id_type_in_filename = ip# if store slave file use symbol link# default value is false# since V4.01store_slave_file_use_link = false# if rotate the error log every day# default value is false# since V4.02rotate_error_log = false# rotate error log time base, time format: Hour:Minute# Hour from 0 to 23, Minute from 0 to 59# default value is 00:00# since V4.02error_log_rotate_time=00:00# rotate error log when the log file exceeds this size# 0 means never rotates log file by log file size# default value is 0# since V4.02rotate_error_log_size = 0# keep days of the log files# 0 means do not delete old log files# default value is 0log_file_keep_days = 0# if use connection pool# default value is false# since V4.05use_connection_pool = false# connections whose the idle time exceeds this time will be closed# unit: second# default value is 3600# since V4.05connection_pool_max_idle_time = 3600# HTTP port on this tracker serverhttp.server_port=8080# check storage HTTP server alive interval seconds# &lt;= 0 for never check# default value is 30http.check_alive_interval=30# check storage HTTP server alive type, values are:#   tcp : connect to the storge server with HTTP port only,#        do not request and get response#   http: storage check alive url must return http status 200# default value is tcphttp.check_alive_type=tcp# check storage HTTP server alive uri/url# NOTE: storage embed HTTP server support uri: /status.htmlhttp.check_alive_uri=/status.htm-------------------分割线----------------------------bash4.2-# cat client.conf# connect timeout in seconds# default value is 30sconnect_timeout=30# network timeout in seconds# default value is 30snetwork_timeout=60# the base path to store log filesbase_path=/var/fdfs# tracker_server can ocur more than once, and tracker_server format is#  &quot;host:port&quot;, host can be hostname or ip addresstracker_server=192.168.0.197:22122#standard log level as syslog, case insensitive, value list:### emerg for emergency### alert### crit for critical### error### warn for warning### notice### info### debuglog_level=info# if use connection pool# default value is false# since V4.05use_connection_pool = false# connections whose the idle time exceeds this time will be closed# unit: second# default value is 3600# since V4.05connection_pool_max_idle_time = 3600# if load FastDFS parameters from tracker server# since V4.05# default value is falseload_fdfs_parameters_from_tracker=false# if use storage ID instead of IP address# same as tracker.conf# valid only when load_fdfs_parameters_from_tracker is false# default value is false# since V4.05use_storage_id = false# specify storage ids filename, can use relative or absolute path# same as tracker.conf# valid only when load_fdfs_parameters_from_tracker is false# since V4.05storage_ids_filename = storage_ids.conf#HTTP settingshttp.tracker_server_port=80#use &quot;#include&quot; directive to include HTTP other settiongs##include http.conf</code></pre><p>查看并修改完上述三个文件，storage.conf、tracker.conf、client.conf，退出容器。</p><p>最后防火墙开启端口</p><pre><code>$ sudo firewall-cmd --zone=public --add-port=8888/tcp --permanentsuccess$ sudo firewall-cmd --zone=public --add-port=22122/tcp --permanentsuccess$ sudo firewall-cmd --zone=public --add-port=23000/tcp --permanentsuccess$ sudo firewall-cmd --reloadsuccess</code></pre><p>最终进行测试，用一张test.png图片，通过本地客户端模拟图片上传。</p><p>首先将图片传入到storage容器中，重新进入到storage容器中，进行上传操作:</p><pre><code>$ docker cp test.png storage:/var/$ docker exec -it storage /bin/bashbash4.2-# /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /var/test.pnggroup1/M00/00/00/CgBC7F78uTiAegOJAAC3s0pdQHQ707.png</code></pre><p>上传操作后，返回一个地址信息，然后通过浏览器进行访问，如下：</p><p><img src="./imgs/fastdfs%E8%AE%BF%E9%97%AE%E6%95%88%E6%9E%9C.png" alt></p><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><h3 id="第一步：准备工作"><a href="#第一步：准备工作" class="headerlink" title="第一步：准备工作"></a>第一步：准备工作</h3><ol><li>libfastcommon</li></ol><p>从 FastDFS 和 FastDHT 中提取出来的公共 C 函数库，基础环境</p><p>在安装 FastDFS 前需要先安装这个</p><p>下载地址址：<a href="https://github.com/happyfish100/libfastcommon/releases" target="_blank" rel="noopener">https://github.com/happyfish100/libfastcommon/releases</a></p><ol start="2"><li>FastDFS</li></ol><ul><li><p>FastDFS 安装包</p></li><li><p>下载地址：<a href="https://github.com/happyfish100/fastdfs/releases" target="_blank" rel="noopener">https://github.com/happyfish100/fastdfs/releases</a></p></li></ul><ol start="3"><li>fastdfs-nginx-module</li></ol><ul><li><p>为了实现通过 HTTP 服务访问和下载 FastDFS 服务器中的文件</p></li><li><p>可以重定向文件链接到源服务器取文件，避免同一组 Storage 服务器同步延迟导致文件访问错误</p></li><li><p>下载地址：<a href="https://github.com/happyfish100/fastdfs-nginx-module/releases" target="_blank" rel="noopener">https://github.com/happyfish100/fastdfs-nginx-module/releases</a></p></li></ul><p>本次采用的安装包版本如下</p><ul><li><p>libfastcommon ：1.0.43</p></li><li><p>FastDFS ：6.06</p></li><li><p>fastdfs-ninx-module ：1.22</p></li><li><p>nginx:1.16.1</p></li></ul><p>将以上安装包拷贝到 CentOS7 下的 /home/centos/FastDFS 目录下</p><h3 id="第二步：安装libfastcommon"><a href="#第二步：安装libfastcommon" class="headerlink" title="第二步：安装libfastcommon"></a>第二步：安装libfastcommon</h3><p>libfastcommon 安装依赖于 gcc 和 perl，故要先安装这两个</p><pre><code>// 在线安装 gcc（安装系统时，选择开发工具安装和安全工具安装后，gcc自动安装）$ sudo yum -y install make zlib zlib-devel gcc-c++ libtool  openssl openssl-devel libevent// 上传已经下载好的libfastcommon安装包// 进入 root 目录下$ cd /home/centos/FastDFS // 解压 libfastcommon 压缩包$ tar zxvf libfastcommon-1.0.43.tar.gz //进入 libfastcommon 文件夹中，编译 libfastcommon 以及安装$ cd ./libfastcommon-1.0.43$ ./make.sh &amp;&amp; sudo ./make.sh install</code></pre><h3 id="第三步：安装-FastDFS"><a href="#第三步：安装-FastDFS" class="headerlink" title="第三步：安装 FastDFS"></a>第三步：安装 FastDFS</h3><pre><code>$ cd /home/centos/FastDFS // 上传已经下载好的FastDFS安装包// 解压 FastDFS 压缩包，编译以及安装$ tar zxvf fastdfs-6.06.tar.gz $ cd ./fastdfs-6.06$ ./make.sh &amp;&amp; sudo ./make.sh install// 查看是否已经安装$ cd /usr/bin &amp;&amp; ls |grep fdfs// 配置 Tracker// 创建 Tracker 的存储日志和数据的根目录$ mkdir -p /home/centos/FastDFS/data/tracker$ cd /etc/fdfs$ sudo cp tracker.conf.sample tracker.conf// 配置 tracker.conf$ sudo vim tracker.conf// 在这里，tracker.conf 只是修改一下 Tracker 存储日志和数据的路径// 启用配置文件（默认为 false，表示启用配置文件）disabled=false// Tracker 服务端口（默认为 22122），不需要更改port=22122// 存储日志和数据的根目录base_path=/home/centos/FastDFS/data/tracker// :wq保存退出// 配置 Storage// 创建 Storage 的存储日志和数据的根目录$ mkdir -p /home/centos/FastDFS/data/storage$ cd /etc/fdfs$ sudo cp storage.conf.sample storage.conf// 配置 storage.conf$ sudo vim storage.conf// 在这里，storage.conf 只是修改一下 storage 存储日志和数据的路径// 启用配置文件（默认为 false，表示启用配置文件）disabled=false// Storage 服务端口（默认为 23000）port=23000// 数据和日志文件存储根目录base_path=/home/centos/FastDFS/data/storage// 存储路径，访问时路径为 M00// store_path1 则为 M01，以此递增到 M99（如果配置了多个存储目录的话，这里只指定 1 个）store_path0=/home/centos/FastDFS/data/storage// Tracker 服务器 IP 地址和端口，单机搭建时也不要写 127.0.0.1// tracker_server 可以多次出现，如果有多个，则配置多个tracker_server=192.168.122.240:22122// 设置 HTTP 访问文件的端口。这个配置已经不用配置了，配置了也没什么用// 这也是为何 Storage 服务器需要 Nginx 来提供 HTTP 访问的原因http.server_port=8888// 启动 Tracker 和 Storage 服务// 启动 Tracker 服务// 其它操作则把 start 改为 stop、restart、reload、status 即可。Storage 服务相同$ sudo /etc/init.d/fdfs_trackerd start// 启动 Storage 服务$ sudo /etc/init.d/fdfs_storaged start</code></pre><h3 id="第四步：测试上传文件"><a href="#第四步：测试上传文件" class="headerlink" title="第四步：测试上传文件"></a>第四步：测试上传文件</h3><pre><code># 修改 Tracker 服务器客户端配置文件$ sudo cp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf$ sudo vim /etc/fdfs/client.conf// client.conf 中修改 base_path 和 Tracker 服务器的 IP 地址与端口号即可// 存储日志文件的基本路径base_path=/home/centos/FastDFS/data/tracker// Tracker 服务器 IP 地址与端口号tracker_server=192.168.122.240:22122// 拷贝一张图片到 /home/centos 目录下// 通过命令操作，存储到 FastDFS 服务器中$ sudo /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /home/centos/test.pnggroup1/M00/00/00/wKjlpltF-K-AZQQsAABhhboA1Kk469.png// 当返回文件 ID 号，如 group1/M00/00/00/wKjlpltF-K-AZQQsAABhhboA1Kk469.png 则表示上传成功</code></pre><h3 id="第五步：设置开机启动"><a href="#第五步：设置开机启动" class="headerlink" title="第五步：设置开机启动"></a>第五步：设置开机启动</h3><pre><code>$ sudo vim /etc/rc.d/rc.local// 文件最后进行追加操作/etc/init.d/fdfs_trackerd start/etc/init.d/fdfs_storaged start</code></pre><h3 id="第六步：编译安装nginx"><a href="#第六步：编译安装nginx" class="headerlink" title="第六步：编译安装nginx"></a>第六步：编译安装nginx</h3><p>准备文件，下载地址：<a href="http://nginx.org/en/download.html，选择1.16.1版本进行下载。" target="_blank" rel="noopener">http://nginx.org/en/download.html，选择1.16.1版本进行下载。</a></p><p>将以上安装包拷贝到 CentOS7 下的 /home/centos/FastDFS/ 目录下，将fastdfs-nginx-module的压缩包同样拷贝到这个目录下。</p><p>安装与配置 Nginx</p><pre><code>// 安装依赖信息$ sudo rpm -ivh pcre-devel-8.32-17.el7.x86_64.rpm zlib-devel-1.2.7-18.el7.x86_64.rpm$ cd /home/centos/FastDFS/$ tar -zxvf nginx-1.16.1.tar.gz // 找到nginx模块$ cd /home/centos/FastDFS/$ tar -zxvf fastdfs-nginx-module-1.22.tar.gz$ cd fastdfs-nginx-module-1.22/src$ vim config// 增加以下：:%s+/usr/local+/usr// 替换/usr目录为/usr/local目录// :wq保存退出$ cd /usr/local/nginx-1.16.1/// 给 Nginx 添加 fastdfs-nginx-module 模块$ ./configure --add-module=/home/centos/FastDFS/fastdfs-nginx-module-1.22/src --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_flv_module --with-http_gzip_static_module$ make &amp;&amp; sudo make install</code></pre><p>fastdfs-nginx-module 和 FastDFS 配置文件修改</p><pre><code># 复制 FastDFS 的部分配置文件到 /etc/fdfs$ cd /home/centos/FastDFS/fastdfs-nginx-module-1.22// 这条语句错误，修改如下$ sudo cp /home/centos/FastDFS/fastdfs-6.06/conf/http.conf /home/centos/FastDFS/fastdfs-6.06/conf/mime.types /etc/fdfs/$ cd /home/centos/FastDFS/data/storage$ ln -s /home/centos/FastDFS/data/storage/data/ /home/centos/FastDFS/data/storage/data/M00// 复制 fastdfs-nginx-module 源码中的配置文件到  /etc/fdfs 中$ sudo cp /usr/local/fastdfs-nginx-module-1.22/src/mod_fastdfs.conf /etc/fdfs/$ sudo vim /etc/fdfs/mod_fastdfs.conf// mod_fastdfs.conf 配置如下// Tracker 服务器IP和端口修改tracker_server=192.168.122.240:22122// url 中是否包含 group 名称，改为 true，包含 groupurl_have_group_name = true// 配置 Storage 信息，修改 store_path0 的信息store_path0=/home/centos/FastDFS/data/storage// 其它的一般默认即可，例如// :wq保存退出// 配置 Nginx$ sudo vim /usr/local/nginx/conf/nginx.conf// 添加如下配置  server {        listen       8888;        server_name  192.168.122.240;        location ~/group([0-9])/M00 {            root  /home/centos/FastDFS/data/storage/data;            ngx_fastdfs_module;        }    }// :wq保存退出// 启动 Nginx$ sudo /usr/local/nginx/sbin/nginx</code></pre><p>测试图片上传：</p><pre><code>$ vim /etc/fdfs/client.conf// 修改下面三个值信息base_path = /home/centos/FastDFS/data/trackertracker_server = 192.168.122.240:22122http.tracker_server_port = 8888// :wq保存// 上传图片$ sudo /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /var/test.pnggroup1/M00/00/00/CgBC7F78uTiAegOJAAC3s0pdQHQ707.png</code></pre><p>通过 HTTP 访问文件</p><p>根据 URL 访问之前上传的那张图片</p><p><a href="http://192.168.122.240:8888/group1/M00/00/00/CgBC7F78uTiAegOJAAC3s0pdQHQ707.png" target="_blank" rel="noopener">http://192.168.122.240:8888/group1/M00/00/00/CgBC7F78uTiAegOJAAC3s0pdQHQ707.png</a></p><p><strong>注意</strong>：</p><pre><code>// 每次开机时，手动打开 Tracker 服务$ sudo /etc/init.d/fdfs_trackerd start// 打开 Storage 服务$ sudo /etc/init.d/fdfs_storaged start// 启动 Nginx$ sudo /usr/local/nginx/sbin/nginx</code></pre><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li>docker安装参考链接：<a href="http://www.zuidaima.com/blog/4653383732808704.htm" target="_blank" rel="noopener">http://www.zuidaima.com/blog/4653383732808704.htm</a></li><li>原生方式安装参考链接：<a href="https://www.cnblogs.com/btxu/p/13061306.html" target="_blank" rel="noopener">https://www.cnblogs.com/btxu/p/13061306.html</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>jenkins迁移记录</title>
      <link href="/2020/06/12/jenkins-qian-yi-ji-lu/"/>
      <url>/2020/06/12/jenkins-qian-yi-ji-lu/</url>
      
        <content type="html"><![CDATA[<h1 id="jenkins备份和迁移"><a href="#jenkins备份和迁移" class="headerlink" title="jenkins备份和迁移"></a>jenkins备份和迁移</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="jenkins导出镜像"><a href="#jenkins导出镜像" class="headerlink" title="jenkins导出镜像"></a>jenkins导出镜像</h2><pre><code>$ docker commit jenkins_official jenkins/jenkins:2.208$ docker export jenkins_official &gt; jenkins_2.208.img$ scp -P 15555 jenkins_2.208.img centos@192.168.88.180:~/</code></pre><p>用上述方式导出后，发送到新的机器上时，然后尝试启动，发现不成功，如下：</p><pre><code>// 在另外一台机器上执行$ docker import - jenkins/jenkins:2.208 &lt; jenkins_2.208.img$ docker run -d --name jenkins_official -v /jenkins_home:/home/centos/docker/jenkins_home -v /data/maven:/home/centos/maven/apache-maven-3.6.2 -p 50001:50000 -p 9090:8080 --restart=always jenkins/jenkins:2.208docker: Error response from daemon: No command specified.See &#39;docker run --help&#39;.</code></pre><p>出现上述问题后，查看整个镜像的描述信息，如下:</p><pre><code>$ docker inspect b437[    {        &quot;Id&quot;: &quot;sha256:b43728bd35394ed1fd5a982fbaad893293cb926f2db00e395ff970a3f40a7653&quot;,        &quot;RepoTags&quot;: [            &quot;jenkins/jenkins:2.208&quot;        ],        &quot;RepoDigests&quot;: [],        &quot;Parent&quot;: &quot;&quot;,        &quot;Comment&quot;: &quot;Imported from -&quot;,        &quot;Created&quot;: &quot;2020-05-28T02:00:19.9158622Z&quot;,        &quot;Container&quot;: &quot;&quot;,        &quot;ContainerConfig&quot;: {            &quot;Hostname&quot;: &quot;&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;Tty&quot;: false,            &quot;OpenStdin&quot;: false,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: null,            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;&quot;,            &quot;Volumes&quot;: null,            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: null,            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: null        },        &quot;DockerVersion&quot;: &quot;19.03.5&quot;,        &quot;Author&quot;: &quot;&quot;,        &quot;Config&quot;: {            &quot;Hostname&quot;: &quot;&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;Tty&quot;: false,            &quot;OpenStdin&quot;: false,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: null,            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;&quot;,            &quot;Volumes&quot;: null,            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: null,            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: null        },        &quot;Architecture&quot;: &quot;amd64&quot;,        &quot;Os&quot;: &quot;linux&quot;,        &quot;Size&quot;: 742766101,        &quot;VirtualSize&quot;: 742766101,        &quot;GraphDriver&quot;: {            &quot;Data&quot;: {                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/d890ad5267d6e09d4c4e6d9264b85a282ffdf619b0f399de94ae685c17f9ba10/merged&quot;,                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/d890ad5267d6e09d4c4e6d9264b85a282ffdf619b0f399de94ae685c17f9ba10/diff&quot;,                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/d890ad5267d6e09d4c4e6d9264b85a282ffdf619b0f399de94ae685c17f9ba10/work&quot;            },            &quot;Name&quot;: &quot;overlay2&quot;        },        &quot;RootFS&quot;: {            &quot;Type&quot;: &quot;layers&quot;,            &quot;Layers&quot;: [                &quot;sha256:613397f3accc61821ab378b654c7f09f84dad36d603bd121a355a2d53c569335&quot;            ]        },        &quot;Metadata&quot;: {            &quot;LastTagTime&quot;: &quot;2020-05-28T10:00:20.275661145+08:00&quot;        }    }]</code></pre><p>可以看到镜像的cmd部分，命令为null，En导致启动失败。因此需要更换镜像的导出方式。</p><p>在执行docker commit命令后，使用其它的方式导出信息。</p><pre><code>// 在之前的机器上执行$ docker save c1746bb41658 &gt; jenkins.tar$ scp -P 15555 jenkins.tar centos@192.168.88.180:~/</code></pre><p>传输完成后，在后续机器上导入</p><pre><code>// 在另一台机器上执行$ docker load &lt; jenkins.tare4b20fcc48f4: Loading layer [==================================================&gt;]  105.6MB/105.6MB91ecdd7165d3: Loading layer [==================================================&gt;]   24.1MB/24.1MB73bfa217d66f: Loading layer [==================================================&gt;]  8.005MB/8.005MB5f3a5adb8e97: Loading layer [==================================================&gt;]  146.4MB/146.4MB9a11244a7e74: Loading layer [==================================================&gt;]   10.1MB/10.1MBb18043518924: Loading layer [==================================================&gt;]  3.584kB/3.584kB2ee490fbc316: Loading layer [==================================================&gt;]  205.6MB/205.6MB60fe639d301d: Loading layer [==================================================&gt;]  338.9kB/338.9kB167c0ea1e843: Loading layer [==================================================&gt;]  3.584kB/3.584kB37e1fe95171e: Loading layer [==================================================&gt;]  9.728kB/9.728kB65c4c6a6feb6: Loading layer [==================================================&gt;]  868.9kB/868.9kB811d818689d8: Loading layer [==================================================&gt;]  62.76MB/62.76MB06e095115ef5: Loading layer [==================================================&gt;]  3.584kB/3.584kBed68bc14f08b: Loading layer [==================================================&gt;]  9.728kB/9.728kB581bcef05992: Loading layer [==================================================&gt;]   5.12kB/5.12kB94dc46c85541: Loading layer [==================================================&gt;]  3.072kB/3.072kB87e68b9bebe3: Loading layer [==================================================&gt;]  7.168kB/7.168kB557f852b13c6: Loading layer [==================================================&gt;]  13.82kB/13.82kBd9caa2edb930: Loading layer [==================================================&gt;]  199.9MB/199.9MBLoaded image ID: sha256:c1746bb416581b8a9b81fee02ebb355a681752ffe9d38ba7a5adba02b89d3fc3$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE&lt;none&gt;              &lt;none&gt;              c1746bb41658        24 hours ago        750MBgitlab/gitlab-ce    12.2.5_101          3925ebbdc4d6        25 hours ago        1.75GBgitlab/gitlab-ce    12.2.5              a574a1869b14        4 months ago        1.75GB$ docker tag  c1746bb41658 jenkins/jenkins:2.208$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEjenkins/jenkins     2.208               c1746bb41658        24 hours ago        750MBgitlab/gitlab-ce    12.2.5_101          3925ebbdc4d6        25 hours ago        1.75GBgitlab/gitlab-ce    12.2.5              a574a1869b14        4 months ago        1.75GB$ docker inspect c1746bb41658[    {        &quot;Id&quot;: &quot;sha256:c1746bb416581b8a9b81fee02ebb355a681752ffe9d38ba7a5adba02b89d3fc3&quot;,        &quot;RepoTags&quot;: [            &quot;jenkins/jenkins:2.208&quot;        ],        &quot;RepoDigests&quot;: [],        &quot;Parent&quot;: &quot;&quot;,        &quot;Comment&quot;: &quot;&quot;,        &quot;Created&quot;: &quot;2020-05-27T02:18:31.310227817Z&quot;,        &quot;Container&quot;: &quot;cbbc3907c1bcbd577722d0dbed4459a9af5b5360e3bedbc03094528b3a0d2841&quot;,        &quot;ContainerConfig&quot;: {            &quot;Hostname&quot;: &quot;cbbc3907c1bc&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;jenkins&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;ExposedPorts&quot;: {                &quot;50000/tcp&quot;: {},                &quot;8080/tcp&quot;: {}            },            &quot;Tty&quot;: true,            &quot;OpenStdin&quot;: true,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: [                &quot;PATH=/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,                &quot;LANG=C.UTF-8&quot;,                &quot;JAVA_HOME=/usr/local/openjdk-8&quot;,                &quot;JAVA_VERSION=8u232&quot;,                &quot;JAVA_BASE_URL=https://github.com/AdoptOpenJDK/openjdk8-upstream-binaries/releases/download/jdk8u232-b09/OpenJDK8U-jdk_&quot;,                &quot;JAVA_URL_VERSION=8u232b09&quot;,                &quot;JENKINS_HOME=/var/jenkins_home&quot;,                &quot;JENKINS_SLAVE_AGENT_PORT=50000&quot;,                &quot;REF=/usr/share/jenkins/ref&quot;,                &quot;JENKINS_VERSION=2.208&quot;,                &quot;JENKINS_UC=https://updates.jenkins.io&quot;,                &quot;JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental&quot;,                &quot;JENKINS_INCREMENTALS_REPO_MIRROR=https://repo.jenkins-ci.org/incrementals&quot;,                &quot;COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log&quot;            ],            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;jenkins/jenkins&quot;,            &quot;Volumes&quot;: {                &quot;/var/jenkins_home&quot;: {}            },            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: [                &quot;/sbin/tini&quot;,                &quot;--&quot;,                &quot;/usr/local/bin/jenkins.sh&quot;            ],            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: {}        },        &quot;DockerVersion&quot;: &quot;19.03.2&quot;,        &quot;Author&quot;: &quot;&quot;,        &quot;Config&quot;: {            &quot;Hostname&quot;: &quot;cbbc3907c1bc&quot;,            &quot;Domainname&quot;: &quot;&quot;,            &quot;User&quot;: &quot;jenkins&quot;,            &quot;AttachStdin&quot;: false,            &quot;AttachStdout&quot;: false,            &quot;AttachStderr&quot;: false,            &quot;ExposedPorts&quot;: {                &quot;50000/tcp&quot;: {},                &quot;8080/tcp&quot;: {}            },            &quot;Tty&quot;: true,            &quot;OpenStdin&quot;: true,            &quot;StdinOnce&quot;: false,            &quot;Env&quot;: [                &quot;PATH=/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;,                &quot;LANG=C.UTF-8&quot;,                &quot;JAVA_HOME=/usr/local/openjdk-8&quot;,                &quot;JAVA_VERSION=8u232&quot;,                &quot;JAVA_BASE_URL=https://github.com/AdoptOpenJDK/openjdk8-upstream-binaries/releases/download/jdk8u232-b09/OpenJDK8U-jdk_&quot;,                &quot;JAVA_URL_VERSION=8u232b09&quot;,                &quot;JENKINS_HOME=/var/jenkins_home&quot;,                &quot;JENKINS_SLAVE_AGENT_PORT=50000&quot;,                &quot;REF=/usr/share/jenkins/ref&quot;,                &quot;JENKINS_VERSION=2.208&quot;,                &quot;JENKINS_UC=https://updates.jenkins.io&quot;,                &quot;JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental&quot;,                &quot;JENKINS_INCREMENTALS_REPO_MIRROR=https://repo.jenkins-ci.org/incrementals&quot;,                &quot;COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log&quot;            ],            &quot;Cmd&quot;: null,            &quot;Image&quot;: &quot;jenkins/jenkins&quot;,            &quot;Volumes&quot;: {                &quot;/var/jenkins_home&quot;: {}            },            &quot;WorkingDir&quot;: &quot;&quot;,            &quot;Entrypoint&quot;: [                &quot;/sbin/tini&quot;,                &quot;--&quot;,                &quot;/usr/local/bin/jenkins.sh&quot;            ],            &quot;OnBuild&quot;: null,            &quot;Labels&quot;: {}        },        &quot;Architecture&quot;: &quot;amd64&quot;,        &quot;Os&quot;: &quot;linux&quot;,        &quot;Size&quot;: 749645095,        &quot;VirtualSize&quot;: 749645095,        &quot;GraphDriver&quot;: {            &quot;Data&quot;: {                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/14e74494b7d5e6f6edade6abcd59118440346fa48d151642f4d6ca84a2763899/diff:/var/lib/docker/overlay2/160ca4b02111a4e91dd7cb72c8d903da556aa85b79121994c71d0f25dafecb6b/diff:/var/lib/docker/overlay2/768dac08d447e865a9456ea862b6fe8242e15bfc127e645e6136665b6e1e50ab/diff:/var/lib/docker/overlay2/5e63415ab060f1df0a4436a609cc277a68fb582ee04ac45479e45502703dbd1f/diff:/var/lib/docker/overlay2/1b3ddec07d1159647ab55af7dc1f413d314a7dfb140ab3cb3a10e73e6547707a/diff:/var/lib/docker/overlay2/d6d40054166e07dc1b81b01caeb373e27ccb2fabd1c438172a026f0b2fd14cee/diff:/var/lib/docker/overlay2/c8a667802affe38898c86cee24c17c911698536de7690cd6c00ba94d6b5a0e32/diff:/var/lib/docker/overlay2/46de4645b03414a8345932dac971c155077749f159bd95121bd94a9c61dbc332/diff:/var/lib/docker/overlay2/8820f1a5bd612cb450024dd06838d53e90bfc6b3905835333c5d086ae4718152/diff:/var/lib/docker/overlay2/1c45197624d037e0c5304be33a89b15f399f8aef5427f0052bd98c8f4e846e4a/diff:/var/lib/docker/overlay2/58108e3d56074702831819adc017cd76bf48c3e381c1926b5506c72d724d5619/diff:/var/lib/docker/overlay2/5aae50ad403940eb6665853bacc95f3b26fd107de435418460dbe5e7aeb2314a/diff:/var/lib/docker/overlay2/cc94aec27d7388468a38efc7d6a9fae4979e4fb9e8d095a4abe24d0b9545366b/diff:/var/lib/docker/overlay2/52e8cfa0b28387eb79384858333e4889fe7fe652c9c0ad11b81840aa9078057d/diff:/var/lib/docker/overlay2/538fe5e1f97fd5db5730c092d7c92cb5e7985d0cceb11fe5c488b935dfc3ec6d/diff:/var/lib/docker/overlay2/3cdfcac471d7bb04d6f1936f704d8c034ce36643f8da440791c1b940c7c0e639/diff:/var/lib/docker/overlay2/81604d5d12d8b8f266f6ec5e4fe17bb09c4be1b8f182737d80b6490b2bbbe602/diff:/var/lib/docker/overlay2/e38dbbc004152e1f7e6a3c325193ae1e9fe3388c853eecf46df8e26c730b6cec/diff&quot;,                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/7a8f041bc7472f86d7a15a6faf72dd565b89535f93551470e3ad1084941724e4/merged&quot;,                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/7a8f041bc7472f86d7a15a6faf72dd565b89535f93551470e3ad1084941724e4/diff&quot;,                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/7a8f041bc7472f86d7a15a6faf72dd565b89535f93551470e3ad1084941724e4/work&quot;            },            &quot;Name&quot;: &quot;overlay2&quot;        },        &quot;RootFS&quot;: {            &quot;Type&quot;: &quot;layers&quot;,            &quot;Layers&quot;: [                &quot;sha256:e4b20fcc48f4a225fd29ce3b3686cc51042ce1f076d88195b3705b5bb2f38c3d&quot;,                &quot;sha256:91ecdd7165d37f7ac4fb6051632936b95da73973f0261785e162b555458f4592&quot;,                &quot;sha256:73bfa217d66f35520cce481c1fbde51cbdba48113f549c41b68b33eca7bc1f0c&quot;,                &quot;sha256:5f3a5adb8e97eab0570fd2d83a112f270cb2d0b6e32cbb6b6770ebe7a3e9678e&quot;,                &quot;sha256:9a11244a7e74c1be586f2029f6ca6d9359afd5657403eec6fe6cf6658907427d&quot;,                &quot;sha256:b18043518924251be33af630922e6357383dc7e723864c79ba2e347197e5ce95&quot;,                &quot;sha256:2ee490fbc316cec390e0e2409b26d698f4825d673e14cf41055412accd902f1f&quot;,                &quot;sha256:60fe639d301de3b394e6adb800378832c86091e72ac712a5a4415a2bc1fff884&quot;,                &quot;sha256:167c0ea1e843e48675d88bd4d7ead1bf07c859e02c5b81384af4bad31ebcbd35&quot;,                &quot;sha256:37e1fe95171e3eb1b41b50ce512552635aed8879562bb55569ef10a494712cf0&quot;,                &quot;sha256:65c4c6a6feb60e4277a6d46ec7217ca5e89e68eeaa4bf20be2b4a38c58f441d6&quot;,                &quot;sha256:811d818689d836d0d7f247ee56d26518b000738a49685f87b39b104991dc2b05&quot;,                &quot;sha256:06e095115ef52045c10ad6d99d735f941eea9987fc01cd695da9589e66f93985&quot;,                &quot;sha256:ed68bc14f08ba0dc6c4b318819b10ca57145c63ac25cc9d95864bd0b3a9952bb&quot;,                &quot;sha256:581bcef0599240aa369b87716e81be3f70487b08476da5b1850e5fd303c955a4&quot;,                &quot;sha256:94dc46c85541c011448c5871401d83bec2589ebf5a4d28f6e00c70bea18d7af0&quot;,                &quot;sha256:87e68b9bebe3ec7ca616656bdba5c24b17ea1ac4795a86c5bf64e4001ed24e96&quot;,                &quot;sha256:557f852b13c6f82a43724259d13b6c36dff30f29b1b9493f586be1a061820ac2&quot;,                &quot;sha256:d9caa2edb9309e9e78275238d80c17ccce4d0a74dc9c2cef623f3f8d57945e7e&quot;            ]        },        &quot;Metadata&quot;: {            &quot;LastTagTime&quot;: &quot;2020-05-28T10:18:14.372683313+08:00&quot;        }    }]</code></pre><p>这样导出再导入的时候，Entrypoint就不为null了，重新启动jenkins尝试，如下：</p><pre><code>$ docker run -d --name jenkins_official -v /jenkins_home:/home/centos/docker/jenkins_home -v /data/maven:/home/centos/maven/apache-maven-3.6.2 -p 50001:50000 -p 9090:8080 --restart=always jenkins/jenkins:2.208</code></pre><h2 id="查看jenkins容器的启动命令"><a href="#查看jenkins容器的启动命令" class="headerlink" title="查看jenkins容器的启动命令"></a>查看jenkins容器的启动命令</h2><p>使用插件查看</p><p>启动命令<br>docker run -d –name jenkins_official -v /jenkins_home:/home/centos/docker/jenkins_home -v /data/maven:/home/centos/maven/apache-maven-3.6.2 -p 50001:50000/tcp -p 9090:8080/tcp –restart always -e ‘LANG=C.UTF-8’ jenkins/jenkins</p><h2 id="创建运行的文件夹"><a href="#创建运行的文件夹" class="headerlink" title="创建运行的文件夹"></a>创建运行的文件夹</h2><p>maven组件、创建文件夹，拷贝</p><h2 id="导入镜像到新机器中并启动"><a href="#导入镜像到新机器中并启动" class="headerlink" title="导入镜像到新机器中并启动"></a>导入镜像到新机器中并启动</h2><h2 id="初始化设置"><a href="#初始化设置" class="headerlink" title="初始化设置"></a>初始化设置</h2><p>按照之前的jenkins设置为一致，不安装插件，直接跳过。</p><p>配置插件安装地址</p><p>安装thinbackup插件</p><p>初始化设置thinbackup</p><h2 id="备份原有镜像内容"><a href="#备份原有镜像内容" class="headerlink" title="备份原有镜像内容"></a>备份原有镜像内容</h2><p>使用thinbackup的备份方式，点击BackupNow，将备份后的文件夹进行压缩，传输到目标机器中。</p><h2 id="重新覆盖新机器中的镜像信息"><a href="#重新覆盖新机器中的镜像信息" class="headerlink" title="重新覆盖新机器中的镜像信息"></a>重新覆盖新机器中的镜像信息</h2><p>通过thinbackup进行恢复，拷贝文件到文件夹中</p><p>文件解压，回到页面上点击restore</p><p><strong>注意：</strong> 在jenkins恢复时，在网页上无法判断是否恢复完毕，甚至存在浏览器不停转圈的情况。</p><p>面对这种情况，可以直接从docker镜像的日志中查看恢复进度，如下：</p><pre><code>$ docker logs -f --tail=100 jenkins_official2020-06-29 01:25:13.774+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;ssh-credentials&#39;.2020-06-29 01:25:13.779+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;workflow-job&#39;.2020-06-29 01:25:13.784+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;workflow-basic-steps&#39;.2020-06-29 01:25:13.788+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;jsch&#39;.2020-06-29 01:25:13.789+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;github-api&#39;.2020-06-29 01:25:13.790+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;gitlab-merge-request-jenkins&#39;.2020-06-29 01:25:13.795+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: Restore plugin &#39;workflow-cps&#39;.2020-06-29 01:25:13.796+0000 [id=71]    INFO    o.j.h.p.t.restore.HudsonRestore#installPlugin: R.................................2020-06-29 01:29:25.522+0000 [id=76]    INFO    h.model.UpdateCenter$DownloadJob#run: Starting the installation of pubsub-light on behalf of admin2020-06-29 01:29:26.737+0000 [id=76]    INFO    h.m.UpdateCenter$UpdateCenterConfiguration#download: Downloading pubsub-light2020-06-29 01:29:27.046+0000 [id=71]    INFO    o.j.h.p.t.ThinBackupMgmtLink#doRestore: Restore finished.</code></pre><p>直到日志中出现<em>Restore finished.</em>即为恢复完成！</p><h2 id="迁移后的剩余问题"><a href="#迁移后的剩余问题" class="headerlink" title="迁移后的剩余问题"></a>迁移后的剩余问题</h2><ol><li>jenkins的URL地址进行变更，变更为目标服务器的</li></ol><p>重新设置jenkins的URL地址</p><ol start="2"><li>账号凭据等失效的问题</li></ol><p>重建账号信息</p><ol start="3"><li>本地maven工具的失效的问题</li></ol><p>重新拷贝maven工具到容器中，然后在jenkins中重新配置</p><ol start="4"><li>pom文件执行错误的问题:构建时，使用了项目根目录下的pom.xml文件，并不是构建的对应目录下的文件</li></ol><p>报错信息如下：</p><pre><code>09:51:02 Parsing POMs09:51:02 using settings config with name Maven Local Settings09:51:02 Replacing all maven server entries not found in credentials list is true09:51:02 ERROR: Failed to parse POMs09:51:02 org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:09:51:02 [ERROR] Non-resolvable import POM: Failure to transfer org.springframework.cloud:spring-cloud-dependencies:pom:Hoxton.SR1 from https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced. Original error: Could not transfer artifact org.springframework.cloud:spring-cloud-dependencies:pom:Hoxton.SR1 from/to central (https://repo.maven.apache.org/maven2): Connect to repo.maven.apache.org:443 [repo.maven.apache.org/151.101.40.215] failed: Connection timed out (Connection timed out) @ com.testsoft:test-utils:1.0.0-SNAPSHOT, /var/jenkins_home/workspace/test-dict-develop/pom.xml, line 82, column 2509:51:02 [ERROR] Non-resolvable import POM: Failure to transfer com.alibaba.cloud:spring-cloud-alibaba-dependencies:pom:2.2.1.RELEASE from https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced. Original error: Could not transfer artifact com.alibaba.cloud:spring-cloud-alibaba-dependencies:pom:2.2.1.RELEASE from/to central (https://repo.maven.apache.org/maven2): Connect to repo.maven.apache.org:443 [repo.maven.apache.org/151.101.40.215] failed: Connection timed out (Connection timed out) @ com.testsoft:test-utils:1.0.0-SNAPSHOT, /var/jenkins_home/workspace/test-dict-develop/pom.xml, line 89, column 2509:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for org.springframework.cloud:spring-cloud-commons:jar is missing. @ com.testsoft:test-data-dictionary-server:[unknown-version], /var/jenkins_home/workspace/test-dict-develop/test-data-dictionary-server/pom.xml, line 29, column 2109:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for org.springframework.cloud:spring-cloud-starter-netflix-eureka-client:jar is missing. @ com.testsoft:test-data-dictionary-server:[unknown-version], /var/jenkins_home/workspace/test-dict-develop/test-data-dictionary-server/pom.xml, line 81, column 2109:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-config:jar is missing. @ com.testsoft:test-data-dictionary-server:[unknown-version], /var/jenkins_home/workspace/test-dict-develop/test-data-dictionary-server/pom.xml, line 85, column 2109:51:02 [ERROR] &#39;dependencies.dependency.version&#39; for org.springframework.cloud:spring-cloud-starter-sleuth:jar is missing. @ com.testsoft:test-utils:1.0.0-SNAPSHOT, /var/jenkins_home/workspace/test-dict-develop/pom.xml, line 74, column 2109:51:02 09:51:02     at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:383)09:51:02     at hudson.maven.MavenEmbedder.buildProjects(MavenEmbedder.java:370)09:51:02     at hudson.maven.MavenEmbedder.readProjects(MavenEmbedder.java:340)09:51:02     at hudson.maven.MavenModuleSetBuild$PomParser.invoke(MavenModuleSetBuild.java:1329)09:51:02     at hudson.maven.MavenModuleSetBuild$PomParser.invoke(MavenModuleSetBuild.java:1126)09:51:02     at hudson.FilePath.act(FilePath.java:1075)09:51:02     at hudson.FilePath.act(FilePath.java:1058)09:51:02     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.parsePoms(MavenModuleSetBuild.java:987)09:51:02     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.doRun(MavenModuleSetBuild.java:691)09:51:02     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:504)09:51:02     at hudson.model.Run.execute(Run.java:1853)09:51:02     at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:543)09:51:02     at hudson.model.ResourceController.execute(ResourceController.java:97)09:51:02     at hudson.model.Executor.run(Executor.java:427)</code></pre><p>处理方式：在maven命令部分，添加*<em>-U</em>参数，确保拉取时拉取最新的jar包</p><ol start="5"><li>迁移以后，jenkins中的ssh连接失效</li></ol><p>报错信息：</p><pre><code>com.jcraft.jsch.JSchException: SSH_MSG_DISCONNECT: 2 Too many authentication failures 13:18:37     at com.jcraft.jsch.Session.read(Session.java:1004)13:18:37     at com.jcraft.jsch.UserAuthPassword.start(UserAuthPassword.java:91)13:18:37     at com.jcraft.jsch.Session.connect(Session.java:470)13:18:37     at org.jvnet.hudson.plugins.CredentialsSSHSite.createSession(CredentialsSSHSite.java:132)13:18:37     at org.jvnet.hudson.plugins.CredentialsSSHSite.executeCommand(CredentialsSSHSite.java:208)13:18:37     at org.jvnet.hudson.plugins.SSHBuilder.perform(SSHBuilder.java:104)13:18:37     at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20)13:18:37     at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:741)13:18:37     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.build(MavenModuleSetBuild.java:946)13:18:37     at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.doRun(MavenModuleSetBuild.java:896)13:18:37     at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:504)13:18:37     at hudson.model.Run.execute(Run.java:1853)13:18:37     at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:543)13:18:37     at hudson.model.ResourceController.execute(ResourceController.java:97)13:18:37     at hudson.model.Executor.run(Executor.java:427)13:18:37 Build step &#39;Execute shell script on remote host using ssh&#39; marked build as failure</code></pre><p>处理方式：SSH密钥过期的问题，重新添加密钥即可</p><h2 id="关于jenkins中容器过大的问题调整"><a href="#关于jenkins中容器过大的问题调整" class="headerlink" title="关于jenkins中容器过大的问题调整"></a>关于jenkins中容器过大的问题调整</h2><p>backup内容过大的问题–&gt;删除过多的备份调整备份策略    31G  –&gt; 1.1G</p><ol><li>修改定时任务</li></ol><p>0 23 * * 1</p><p>每周一23点执行一次备份</p><ol start="2"><li>删除多余的备份</li></ol><p>jobs–&gt;产出物的积累问题，产出物jar包              7.9G   –&gt; 7.9G</p><ol><li><p>产出物jar包占用大小，后续构建完成后推送到nexus3镜像，并且删除本地的jar包</p></li><li><p>构建物的保留策略，需要重新制定，<a href="https://blog.csdn.net/liliwang90/article/details/104690491" target="_blank" rel="noopener">参考链接</a></p></li></ol><p>workspace–&gt; 排序，查看最大与最小的内容           14G    –&gt; 9.6G</p><ol><li><p>项目拆分，存在重复的情况</p></li><li><p>前端依赖问题，先备份前端依赖，再逐步删除依赖信息</p></li><li><p>快速批量删除Jenkins构建清理磁盘空间并按参数保留最近构建</p></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>jenkins备份相对不那么好做，由于历史的原因，一些配置的设定上并没有那么合理，需要在迁移完成后重新进行设置，并且一定构建一支工程进行测试，确保测试无误后，投入使用。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://blog.csdn.net/liliwang90/article/details/104690491" target="_blank" rel="noopener">快速批量删除Jenkins构建清理磁盘空间并按参数保留最近构建</a></li></ul><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li><p>时间更新</p></li><li><p>统一构建账户的调整</p></li><li><p>全局变量的参数化配置</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>gitlab迁移和转入操作记录</title>
      <link href="/2020/06/09/gitlab-qian-yi-he-zhuan-ru-cao-zuo-ji-lu/"/>
      <url>/2020/06/09/gitlab-qian-yi-he-zhuan-ru-cao-zuo-ji-lu/</url>
      
        <content type="html"><![CDATA[<h1 id="gitlab迁移和转入的操作指南"><a href="#gitlab迁移和转入的操作指南" class="headerlink" title="gitlab迁移和转入的操作指南"></a>gitlab迁移和转入的操作指南</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前涉及到之前平台的gitlab需要进行停机，转出所使用的的机器，引发了gitlab的迁移操作，最终决定迁移到云平台这边的gitlab中。</p><p>下面介绍下基本环境</p><ul><li>待迁入机器</li></ul><p>ubuntu 16.04，gitlab 10.x版本，使用普通安装方式，完整安装在操作系统中</p><ul><li>目标迁入机器</li></ul><p>centos 7.6，gitlab 12.2.x版本，使用docker进行安装</p><h2 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h2><h3 id="1-导出用户"><a href="#1-导出用户" class="headerlink" title="1. 导出用户"></a>1. 导出用户</h3><p>根据配置文件，定位相关信息</p><pre><code># cat /var/opt/gitlab/gitlab-rails/etc/database.yml// 找到gitlab-psql用户信息// 查看Gitlab对应的系统用户，确定是否存在gitlab-psql用户# cat /etc/passwd | grep gitlab// 根据信息登陆数据库// 切换用户# su - gitlab-psql// 登陆数据库（-h指定host，-d指定数据库）-sh-4.2$ psql -h /var/opt/gitlab/postgresql -d gitlabhq_production// 查看帮助信息gitlabhq_production=# \h// 查看数据库gitlabhq_production=# \l// 查看库中的表（执行命令后，按回车键显示更多表信息）gitlabhq_production=# \dt// 通过筛查，可在库中找到users表，相关用户信息都记录在表中！// 查看users表结构gitlabhq_production=# \d users// 查看表信息gitlabhq_production=# SELECT * FROM users;// 查看users表中的name字段gitlabhq_production=# SELECT name FROM users;// 登出数据库gitlabhq_production=# \q// 确定表表users中的 username , email , state , name 字段是需要提取的信息，进行导出操作-sh-4.2$  echo &#39;select username,email,state,name from users;&#39; | psql -h /var/opt/gitlab/postgresql -d gitlabhq_production &gt; userinfo.txt// 退出用户，回到同一个目录下，查看该文件-sh-4.2$ exit$ ls userinfo.txt</code></pre><p>将导出的信息重新整理，整理后的userinfo.txt如下：</p><pre><code>zhongyanyan123 zhongyanyan     zhongyanyan@testsoft.com     active  zhongyanyanzongquanxiang123 zongquanxiang   zongquanxiang@testsoft.com   active  zongquanxiangyangjiajie123 yangjiajie      yangjiajie@testsoft.com      active  yangjiajiesujingfang123 sujingfang      sujingfang@testsoft.com      active  sujingfangyangshuai123 yangshuai       yangshuai@testsoft.com       active  yangshuaiwangchao123 wangchao        wangchao@testsoft.com        active  wangchaozhanglianjiang123 zhanglianjiang  zhanglianjiang@testsoft.com  active  zhanglianjiangliulily123 liulily         liulili@testsoft.com         active  liulily</code></pre><h3 id="2-导入用户"><a href="#2-导入用户" class="headerlink" title="2. 导入用户"></a>2. 导入用户</h3><p>编写脚本信息，通过api调用的方式导入用户，首先需要在gitlab中用管理员用户，调用gitlab的api，申请对应的token信息，步骤如下：</p><p><img src="%E8%B0%83%E7%94%A8api%E4%BF%A1%E6%81%AF.png" alt></p><p>然后编写插入脚本，注意在private_token参数位置填写gitlab中申请的token信息，如下：</p><pre><code>$ vim gitlab_import_user.sh// 插入以下内容#!/bin/bash# 导入gitlab用户信息userinfo=&quot;userinfo.txt&quot;while read linedo    password=`echo $line | awk &#39;{print $1}&#39;`    username=`echo $line | awk &#39;{print $2}&#39;`    mail=`echo $line | awk &#39;{print $3}&#39;`    name=`echo $line | awk &#39;{print $5}&#39;`    # 这里private_token使用你上面生成的token，只能在24小时内有效    curl -d &quot;password=$password&amp;email=$mail&amp;username=$username&amp;name=$name&amp;private_token=m6UTzKvksmfnaA4Nx957&quot; &quot;http://192.168.88.159:8181/api/v4/users&quot;done &lt;$userinfo// :wq保存退出</code></pre><p>最后，将上述备份的userinfo.txt导入到目标服务器中，执行导入用户的脚本，如下：</p><pre><code>$ chmod +x gitlab_import_user.sh$ ./gitlab_import_user.sh</code></pre><p>当用户导入完毕后，由于我们没有配置邮箱信息，需要使用管理员用户，登录到新的gitlab页面，用管理员账户确认每一个用户（Confirm user），使这些用户可用。操作单个用户，示意如下：</p><p><img src="%E6%96%B0%E5%A2%9E%E7%94%A8%E6%88%B7.png" alt></p><p><img src="%E7%A1%AE%E8%AE%A4%E7%94%A8%E6%88%B7%E5%8F%AF%E7%94%A8.png" alt></p><p>用户导入后，对用户进行分组，这里设置CMMP和ICP_Mobile两个分组，将各自用户导入即可。</p><h3 id="3-导出项目"><a href="#3-导出项目" class="headerlink" title="3. 导出项目"></a>3. 导出项目</h3><p>直接备份目标服务器中的源码信息，进入gitlab所在的服务器中，定位到/var/opt/gitlab/git-data/repository路径下，对该路径下的所有文件夹进行打包。</p><pre><code># cd /var/opt/gitlab/git-data/repository/# tar -zxvf gitlab-code.tar.gz .</code></pre><p>将gitlab-code.tar.gz拷贝出来，使用scp传输到目标迁入机器上。</p><pre><code>$ scp -P 15555 gitlab-code.tar.gz centos@192.168.88.159:~/</code></pre><p>这样导出项目就完成了。</p><h3 id="4-导入项目"><a href="#4-导入项目" class="headerlink" title="4. 导入项目"></a>4. 导入项目</h3><p>首先将压缩过的代码在目标迁入机器上进行解压，分批传入镜像中进行导入。</p><pre><code>$ mkdir old_code &amp;&amp; cd old_code$ tar -zxvf gitlab-code.tar.gz ./</code></pre><p>以test-user文件夹下的代码为例子进行导入，内部拥有gac-wechat这一个项目。首先对test-user文件夹进行压缩，转为test-user.tar.gz。</p><pre><code>$ tar -zcvf test-user.tar.gz ./test-user</code></pre><p>然后传入docker镜像中,</p><pre><code>$ docker cp test-user.tar.gz gitlab:/</code></pre><p>最后创建要导入的文件夹，和repositories同级，把压缩包解压到这个文件夹中，执行导入的操作。</p><pre><code>$ docker exec -it gitlab /bin/bash# cd /var/opt/gitlab/git-data/// 创建要导入的文件夹// 这里test-instance_mobile对应我们在gitlab中创建的分组信息// 包含对应的用户# mkdir -p repository-import-2020-05-25/test-instance_mobile// 压缩包解压# cd repository-import-2020-05-25/test-instance_mobile &amp;&amp; tar zxvf /test-user.tar.gz .// 修改所有者权限，一定注意修改，否则将导致项目无法导入# cd ../.. &amp;&amp; chown git.git -R repository-import-2020-05-25// 执行导入命令# cd &amp;&amp; gitlab-rake gitlab:import:repos[&#39;/var/opt/gitlab/git-data/repository-import-2020-05-25/&#39;]</code></pre><p>导入完成后，在gitlab页面中查看该项目，然后将新添加的用户分配给该项目，这样项目的迁移就完成了。其它项目照此办理，逐步进行导入即可。</p><p><img src="%E6%B7%BB%E5%8A%A0%E7%94%A8%E6%88%B7%E5%88%B0%E7%BE%A4%E7%BB%84%E5%B9%B6%E5%85%B3%E8%81%94%E9%A1%B9%E7%9B%AE.png" alt></p><h3 id="5-现有项目修改远程连接地址"><a href="#5-现有项目修改远程连接地址" class="headerlink" title="5. 现有项目修改远程连接地址"></a>5. 现有项目修改远程连接地址</h3><p>项目迁移完成后，之前配置的远程url已经不能使用了，而且之前的ssh密钥同样需要进行更换。</p><h4 id="5-1-添加、更换ssh密钥"><a href="#5-1-添加、更换ssh密钥" class="headerlink" title="5.1 添加、更换ssh密钥"></a>5.1 添加、更换ssh密钥</h4><p>不更换ssh密钥，需要输入用户名密码进行登录操作。</p><h4 id="5-2-设置远程地址"><a href="#5-2-设置远程地址" class="headerlink" title="5.2 设置远程地址"></a>5.2 设置远程地址</h4><p>在你本地开发机上进行操作，设置远程地址，命令行如下：</p><pre><code>$ git remote -v$ git remote set-url origin http://192.168.88.159:8181/test-instance_mobile/test-user/gac-wechat.git</code></pre><h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3><p>在本地开发机测试拉取代码、提交代码等操作，即可。</p><h3 id="7-问题"><a href="#7-问题" class="headerlink" title="7. 问题"></a>7. 问题</h3><p>停止已迁移的gitlab运行，直接使用docker stop命令停止。在重启该docker镜像的时候，打开页面出现502的返回码，运行出现了问题，查看日志如下：</p><pre><code>==&gt; /var/log/gitlab/unicorn/unicorn_stdout.log &lt;==bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)==&gt; /var/log/gitlab/unicorn/current &lt;==2020-06-12_02:22:48.15146 starting new unicorn master2020-06-12_02:22:49.28312 master failed to start, check stderr log for details2020-06-12_02:22:50.31663 failed to start a new unicorn master2020-06-12_02:22:50.38006 starting new unicorn master2020-06-12_02:22:51.45315 master failed to start, check stderr log for details2020-06-12_02:22:52.47585 failed to start a new unicorn master2020-06-12_02:22:52.52884 starting new unicorn master2020-06-12_02:22:53.67915 master failed to start, check stderr log for details2020-06-12_02:22:54.69961 failed to start a new unicorn master2020-06-12_02:22:54.75306 starting new unicorn master==&gt; /var/log/gitlab/unicorn/unicorn_stdout.log &lt;==bundler: failed to load command: unicorn (/opt/gitlab/embedded/bin/unicorn)==&gt; /var/log/gitlab/unicorn/unicorn_stderr.log &lt;==ArgumentError: Already running on PID:436 (or pid=/opt/gitlab/var/unicorn/unicorn.pid is stale)  /opt/gitlab/embedded/lib/ruby/gems/2.6.0/gems/unicorn-5.4.1/lib/unicorn/http_server.rb:205:in `pid=&#39;  /opt/gitlab/embedded/lib/ruby/gems/2.6.0/gems/unicorn-5.4.1/lib/unicorn/http_server.rb:137:in `start&#39;  /opt/gitlab/embedded/lib/ruby/gems/2.6.0/gems/unicorn-5.4.1/bin/unicorn:126:in `&lt;top (required)&gt;&#39;  /opt/gitlab/embedded/bin/unicorn:23:in `load&#39;  /opt/gitlab/embedded/bin/unicorn:23:in `&lt;top (required)&gt;&#39;</code></pre><p>解决方式：</p><p>进入docker镜像，删除unicorn.pid文件，退出后再重启镜像，操作如下：</p><pre><code>$ docker exec -it gitlab /bin/bashroot@gitlab:/# rm -rf /opt/gitlab/var/unicorn/unicorn.pidroot@gitlab:/# exitexit$ docker restart gitlabgitlab</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><ul><li><a href="https://www.cnblogs.com/kazihuo/p/11200585.html" target="_blank" rel="noopener">https://www.cnblogs.com/kazihuo/p/11200585.html</a></li><li><a href="https://blog.csdn.net/hnmpf/article/details/80531444" target="_blank" rel="noopener">https://blog.csdn.net/hnmpf/article/details/80531444</a></li><li>单个项目导出：<a href="https://www.jianshu.com/p/848711844704" target="_blank" rel="noopener">https://www.jianshu.com/p/848711844704</a></li><li><a href="https://tsov.net/uupee/25512/" target="_blank" rel="noopener">https://tsov.net/uupee/25512/</a></li></ul><h2 id="docker-registry的备份和迁移"><a href="#docker-registry的备份和迁移" class="headerlink" title="docker registry的备份和迁移"></a>docker registry的备份和迁移</h2><p>docker registry的启动：</p><p>docker run –name=registry –volume=”/usr/local/registry:/var/lib/registry” -p 5000:5000 –restart=always registry:2 </p><p>备份docker registry镜像信息</p><p>主要是整体迁移，blob文件夹，以及打好的镜像信息。</p><p>安装前端页面</p><p>docker run <br>  -d <br>  -e ENV_DOCKER_REGISTRY_HOST=192.168.88.159 <br>  -e ENV_DOCKER_REGISTRY_PORT=5000 <br>  -p 5001:80 <br>  konradkleine/docker-registry-frontend:v2</p><p>是否保留之前的构建产物？保留规则是什么？</p><p>直接拷贝/usr/local/registry路径下的所有文件，打压缩包进行拷贝。</p><h2 id="nexus3-的备份和迁移"><a href="#nexus3-的备份和迁移" class="headerlink" title="nexus3 的备份和迁移"></a>nexus3 的备份和迁移</h2><p>备份nexus镜像</p><p>docker run -d –name=nexus3 –volume=”/kichun/nexus3/nexus-data:/var/nexus-data” –privileged -p 8081:8081 –restart=always sonatype/nexus3</p><p>docker cp nexus3:/nexus-data/ ./nexus-data/</p><p>直接拷贝/nexus-data/路径下的所有文件，打压缩包进行拷贝。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>一阶段前端构建和CI/CD流程优化</title>
      <link href="/2020/05/19/yi-jie-duan-qian-duan-gou-jian-he-ci-cd-liu-cheng-you-hua/"/>
      <url>/2020/05/19/yi-jie-duan-qian-duan-gou-jian-he-ci-cd-liu-cheng-you-hua/</url>
      
        <content type="html"><![CDATA[<h1 id="一阶段前端构建和CI-CD流程优化"><a href="#一阶段前端构建和CI-CD流程优化" class="headerlink" title="一阶段前端构建和CI/CD流程优化"></a>一阶段前端构建和CI/CD流程优化</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>深受前端构建时间过长的荼毒，导致前端构建极端浪费时间的情况，当发布一次前端项目时，都要承受等待20多分钟的情况，这是完全不能接受的。所以非常需要进行前端构建的优化，于是首先针对构建流程进行优化，其次针对前端打包的流程进行优化。</p><h2 id="原始构建方式分析"><a href="#原始构建方式分析" class="headerlink" title="原始构建方式分析"></a>原始构建方式分析</h2><h3 id="1-构建配置"><a href="#1-构建配置" class="headerlink" title="1. 构建配置"></a>1. 构建配置</h3><p>整体的构建是基于jenkins，通过nvm和npm插件进行构建的，然后去打包，最终构建docker镜像，去k8s中进行部署。整个配置图片如图：</p><p><img src="%E6%9E%84%E5%BB%BA%E9%85%8D%E7%BD%AE%E5%9B%BE%E7%89%87.png" alt></p><h3 id="2-构建时间分析"><a href="#2-构建时间分析" class="headerlink" title="2. 构建时间分析"></a>2. 构建时间分析</h3><p>然后进行构建的时间分析，以某次构建为案例进行构建分析，构建整体日志如下：</p><pre><code>13:50:42 Started by user admin13:50:42 Running as SYSTEM13:50:42 Building in workspace /var/jenkins_home/workspace/test-frontend-k8s13:50:42 using credential lisongyang-Test-CICD13:50:42  &gt; git rev-parse --is-inside-work-tree # timeout=1013:50:42 Fetching changes from the remote Git repository13:50:42  &gt; git config remote.origin.url http://192.168.238.159:8181/test/frontend/test1.0-ui.git # timeout=1013:50:42 Fetching upstream changes from http://192.168.238.159:8181/test/frontend/test1.0-ui.git13:50:42  &gt; git --version # timeout=1013:50:42 using GIT_ASKPASS to set credentials 13:50:42  &gt; git fetch --tags --progress -- http://192.168.238.159:8181/test/frontend/test1.0-ui.git +refs/heads/*:refs/remotes/origin/* # timeout=1013:50:43  &gt; git rev-parse refs/remotes/origin/release^{commit} # timeout=1013:50:43  &gt; git rev-parse refs/remotes/origin/origin/release^{commit} # timeout=1013:50:43 Checking out Revision 3a454a94861b96a5668cfe5606242f2f3955c0b0 (refs/remotes/origin/release)13:50:43  &gt; git config core.sparsecheckout # timeout=1013:50:43  &gt; git checkout -f 3a454a94861b96a5668cfe5606242f2f3955c0b0 # timeout=1013:50:45 Commit message: &quot;Merge branch &#39;dev&#39; into release&quot;13:50:45  &gt; git rev-list --no-walk 3a454a94861b96a5668cfe5606242f2f3955c0b0 # timeout=1013:50:45 [test-frontend-k8s] $ bash -c &quot;test -f /var/jenkins_home/nvm/nvm.sh&quot;13:50:45 NVM is already installed13:50:45 13:50:45 [test-frontend-k8s] $ bash -c &quot;export &gt; env.txt&quot;13:50:45 [test-frontend-k8s] $ bash -c &quot;NVM_DIR=/var/jenkins_home/nvm &amp;&amp; source $NVM_DIR/nvm.sh --no-use &amp;&amp; NVM_NODEJS_ORG_MIRROR=http://nodejs.org/dist nvm install 10.12.0 &amp;&amp; nvm use 10.12.0 &amp;&amp; export &gt; env.txt&quot;13:50:49 v10.12.0 is already installed.13:50:53 Now using node v10.12.0 (npm v6.4.1)13:50:55 Now using node v10.12.0 (npm v6.4.1)13:50:55 [test-frontend-k8s] $ /bin/sh -xe /tmp/jenkins5873642462170513819.sh13:50:55 + npm --registry https://registry.npm.taobao.org install -g yarn13:51:06 /var/jenkins_home/nvm/versions/node/v10.12.0/bin/yarn -&gt; /var/jenkins_home/nvm/versions/node/v10.12.0/lib/node_modules/yarn/bin/yarn.js13:51:06 /var/jenkins_home/nvm/versions/node/v10.12.0/bin/yarnpkg -&gt; /var/jenkins_home/nvm/versions/node/v10.12.0/lib/node_modules/yarn/bin/yarn.js13:51:06 + yarn@1.22.413:51:06 updated 1 package in 5.6s13:51:06 + yarn --registry https://registry.npm.taobao.org install13:51:06 yarn install v1.22.413:51:07 [1/4] Resolving packages...13:51:08 success Already up-to-date.13:51:08 $ opencollective-postinstall13:51:09 [96m[1mThank you for using vue-antd-pro![96m[1m13:51:09 [0m[96mIf you rely on this package, please consider supporting our open collective:[22m[39m13:51:09 &gt; [94mhttps://opencollective.com/ant-design-pro-vue/donate[0m13:51:09 13:51:09 Done in 2.33s.13:51:09 + yarn run build13:51:09 yarn run v1.22.413:51:09 $ vue-cli-service build13:51:22 13:51:22 -  Building for production...13:51:40 (node:23259) DeprecationWarning: Tapable.plugin is deprecated. Use new API on `.hooks` instead13:53:11  WARNING  Compiled with 3 warnings1:53:11 PM13:53:11 13:53:11  warning  in ./src/views/operationManagementCenter/README.md13:53:11 13:53:11 Module parse failed: Assigning to rvalue (2:0)13:53:11 You may need an appropriate loader to handle this file type, currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders13:53:11 13:53:11  @ ./src/views lazy ^\.\/.*$ namespace object13:53:11  @ ./src/router/generator-routers.js13:53:11  @ ./src/store/modules/async-router.js13:53:11  @ ./src/store/index.js13:53:11  @ ./src/main.js13:53:11  @ multi ./src/main.js13:53:11 13:53:11  warning  13:53:11 13:53:11 asset size limit: The following asset(s) exceed the recommended size limit (244 KiB).13:53:11 This can impact web performance.13:53:11 Assets: 13:53:11   css/app.e64713f9.css (478 KiB)13:53:11   js/chunk-7b215be7.2f6c4877.js (480 KiB)13:53:11   css/chunk-vendors.6a5a4912.css (745 KiB)13:53:11   js/chunk-vendors.896630e5.js (5.02 MiB)13:53:11   lib/ace/ace.js (363 KiB)13:53:11   lib/ace/mode-php.js (470 KiB)13:53:11   lib/ace/mode-php_laravel_blade.js (474 KiB)13:53:11   lib/ace/worker-coffee.js (324 KiB)13:53:11   lib/ace/worker-xquery.js (1.56 MiB)13:53:11 13:53:11  warning  13:53:11 13:53:11 entrypoint size limit: The following entrypoint(s) combined asset size exceeds the recommended limit (244 KiB). This can impact web performance.13:53:11 Entrypoints:13:53:11   app (6.45 MiB)13:53:11       css/chunk-vendors.6a5a4912.css13:53:11       js/chunk-vendors.896630e5.js13:53:11       css/app.e64713f9.css13:53:11       js/app.245e3f1d.js13:53:11 13:53:11 13:53:13   File                                      Size             Gzipped13:53:13 13:53:13   dist/lib/ace/mode-xquery.js               225.60 KiB       34.91 KiB13:53:13   dist/lib/ace/worker-html.js               211.77 KiB       48.72 KiB13:53:13   dist/lib/ace/worker-javascript.js         160.14 KiB       48.28 KiB13:53:13   dist/lib/ace/worker-css.js                137.18 KiB       35.86 KiB13:53:13   dist/lib/ace/mode-slim.js                 104.49 KiB       29.90 KiB13:53:13   dist/lib/ace/keybinding-vim.js            96.70 KiB        29.52 KiB13:53:13   dist/lib/ace/mode-html_elixir.js          76.08 KiB        20.79 KiB13:53:13   dist/lib/ace/worker-php.js                75.62 KiB        21.10 KiB13:53:13   dist/lib/ace/mode-markdown.js             70.67 KiB        20.78 KiB13:53:13   dist/lib/ace/mode-html_ruby.js            69.95 KiB        20.69 KiB13:53:13   dist/lib/ace/mode-ejs.js                  69.63 KiB        21.00 KiB13:53:13   dist/lib/ace/mode-soy_template.js         67.59 KiB        19.25 KiB13:53:13   dist/lib/ace/mode-luapage.js              67.53 KiB        20.13 KiB13:53:13   dist/lib/ace/mode-csound_document.js      66.97 KiB        19.52 KiB13:53:13   dist/lib/ace/mode-razor.js                66.18 KiB        19.67 KiB13:53:13   dist/lib/ace/mode-rhtml.js                64.23 KiB        18.94 KiB13:53:13   dist/lib/ace/mode-velocity.js             63.92 KiB        18.79 KiB13:53:13   dist/lib/ace/mode-twig.js                 62.84 KiB        18.72 KiB13:53:13   dist/lib/ace/mode-smarty.js               62.67 KiB        18.65 KiB13:53:13   dist/lib/ace/mode-autohotkey.js           62.47 KiB        16.37 KiB13:53:13   dist/lib/ace/mode-coldfusion.js           61.76 KiB        18.35 KiB13:53:13   dist/lib/ace/mode-handlebars.js           61.12 KiB        18.09 KiB13:53:13   dist/lib/ace/mode-django.js               60.58 KiB        18.03 KiB13:53:13   dist/lib/ace/mode-curly.js                60.28 KiB        17.92 KiB13:53:13   dist/lib/ace/mode-html.js                 59.29 KiB        17.77 KiB13:53:13   dist/lib/ace/mode-pgsql.js                55.09 KiB        17.29 KiB13:53:13   dist/lib/ace/mode-objectivec.js           53.76 KiB        18.01 KiB13:53:13   dist/lib/ace/worker-xml.js                53.60 KiB        16.32 KiB13:53:13   dist/lib/ace/mode-jade.js                 52.03 KiB        14.93 KiB13:53:13   dist/js/chunk-ffcd749a.9a38b6de.js        50.88 KiB        12.69 KiB13:53:13   dist/lib/ace/worker-lua.js                45.52 KiB        13.64 KiB13:53:13   dist/lib/ace/mode-mask.js                 41.59 KiB        13.21 KiB13:53:13   dist/lib/ace/mode-haml.js                 38.86 KiB        12.16 KiB13:53:13   dist/lib/ace/mode-jsp.js                  36.57 KiB        11.55 KiB13:53:13   dist/lib/ace/mode-csound_orchestra.js     36.44 KiB        10.57 KiB13:53:13   dist/lib/ace/snippets/lsl.js              34.91 KiB        6.97 KiB13:53:13   dist/lib/ace/ext-language_tools.js        34.74 KiB        10.97 KiB13:53:13   dist/lib/ace/mode-powershell.js           32.25 KiB        8.61 KiB13:53:13   dist/lib/ace/mode-ftl.js                  32.18 KiB        10.29 KiB13:53:13   dist/lib/ace/mode-liquid.js               31.85 KiB        10.09 KiB13:53:13   dist/lib/ace/mode-svg.js                  31.67 KiB        9.15 KiB13:53:13   dist/lib/ace/worker-json.js               31.64 KiB        9.40 KiB13:53:13   dist/lib/ace/mode-erlang.js               29.34 KiB        4.71 KiB13:53:13   dist/lib/ace/theme-ambiance.js            27.14 KiB        18.98 KiB13:53:13   dist/lib/ace/mode-lsl.js                  26.31 KiB        8.82 KiB13:53:13   dist/js/chunk-2394da78.9aaff4ed.js        26.02 KiB        4.22 KiB13:53:13   dist/js/chunk-4e4be866.01b60c14.js        25.66 KiB        6.04 KiB13:53:13   dist/js/chunk-23759318.77953657.js        25.47 KiB        6.13 KiB13:53:13   dist/lib/ace/mode-mel.js                  24.67 KiB        10.43 KiB13:53:13   dist/js/chunk-eb8257d2.c62af659.js        24.09 KiB        5.64 KiB13:53:13   dist/lib/ace/keybinding-emacs.js          23.70 KiB        6.74 KiB13:53:13   dist/js/chunk-682634f1.271074ae.js        23.52 KiB        5.89 KiB13:53:13   dist/lib/ace/mode-scala.js                22.53 KiB        7.69 KiB13:53:13   dist/lib/ace/mode-groovy.js               22.35 KiB        7.53 KiB13:53:13   dist/lib/ace/mode-less.js                 22.31 KiB        7.27 KiB13:53:13   dist/lib/ace/mode-java.js                 22.14 KiB        7.48 KiB13:53:13   dist/js/chunk-78733bee.c6949174.js        21.72 KiB        5.00 KiB13:53:13   dist/js/chunk-0201a64a.019142e7.js        21.38 KiB        5.42 KiB13:53:13   dist/lib/ace/mode-sjs.js                  21.22 KiB        6.88 KiB13:53:13   dist/lib/ace/ext-emmet.js                 21.16 KiB        7.13 KiB13:53:13   dist/lib/ace/snippets/ruby.js             21.10 KiB        5.84 KiB13:53:13   dist/js/chunk-213aeccc.602fc2e3.js        20.50 KiB        5.22 KiB13:53:13   dist/js/chunk-ec42ef58.de24853f.js        20.44 KiB        5.00 KiB13:53:13   dist/lib/ace/mode-matlab.js               20.40 KiB        8.81 KiB13:53:13   dist/lib/ace/mode-actionscript.js         20.39 KiB        7.92 KiB13:53:13   dist/lib/ace/mode-css.js                  20.32 KiB        6.88 KiB13:53:13   dist/lib/ace/mode-gobstones.js            20.30 KiB        6.84 KiB13:53:13   dist/lib/ace/mode-wollok.js               20.27 KiB        6.83 KiB13:53:13   dist/lib/ace/mode-tsx.js                  20.12 KiB        6.70 KiB13:53:13   dist/lib/ace/mode-typescript.js           19.83 KiB        6.64 KiB13:53:13   dist/lib/ace/snippets/css.js              19.51 KiB        4.00 KiB13:53:13   dist/js/chunk-60867039.42fd60bd.js        19.41 KiB        5.11 KiB13:53:13   dist/js/chunk-05bc1686.2fe54f4d.js        19.07 KiB        5.20 KiB13:53:13   dist/lib/ace/mode-ocaml.js                15.54 KiB        5.54 KiB13:53:13   dist/js/chunk-e625997c.e374ecd3.js        14.70 KiB        2.72 KiB13:53:13   dist/lib/ace/mode-scss.js                 14.57 KiB        5.29 KiB13:53:13   dist/lib/ace/mode-stylus.js               14.39 KiB        5.02 KiB13:53:13   dist/lib/ace/mode-dart.js                 14.37 KiB        4.67 KiB13:53:13   dist/lib/ace/ext-settings_menu.js         14.35 KiB        5.76 KiB13:53:13   dist/js/chunk-25ed122e.3e2e562c.js        13.98 KiB        3.65 KiB13:53:13   dist/lib/ace/mode-apache_conf.js          13.91 KiB        4.40 KiB13:53:13   dist/lib/ace/ext-options.js               13.79 KiB        5.61 KiB13:53:13   dist/js/chunk-f3f382b8.14ae366f.js        13.66 KiB        3.52 KiB13:53:13   dist/js/chunk-cf91679c.d4e659a9.js        13.54 KiB        3.22 KiB13:53:13   dist/lib/ace/mode-nix.js                  13.33 KiB        4.35 KiB13:53:13   dist/js/chunk-4a1e5c3b.0b038e96.js        13.30 KiB        3.88 KiB13:53:13   dist/lib/ace/mode-glsl.js                 13.21 KiB        4.54 KiB13:53:13   dist/js/chunk-9a54878e.9774f6d5.js        13.20 KiB        3.46 KiB13:53:13   dist/js/chunk-721b8175.890e4a39.js        13.13 KiB        2.43 KiB13:53:13   dist/js/user.9301b70a.js                  12.87 KiB        3.86 KiB13:53:13   dist/lib/ace/mode-protobuf.js             12.76 KiB        4.25 KiB13:53:13   dist/lib/ace/mode-red.js                  12.62 KiB        4.67 KiB13:53:13   dist/js/chunk-2fadf79e.b2035703.js        12.39 KiB        3.13 KiB13:53:13   dist/js/chunk-07d9aff0.7e3a95ec.js        12.37 KiB        3.24 KiB13:53:13   dist/js/chunk-49b7fd8f.b8456323.js        12.34 KiB        3.14 KiB13:53:13   dist/js/chunk-945a6b30.d0020035.js        12.20 KiB        3.12 KiB13:53:13   dist/js/chunk-319fd138.eb33bfee.js        12.11 KiB        3.10 KiB13:53:13   dist/js/chunk-6d233d0a.80c38f77.js        12.07 KiB        3.38 KiB13:53:13   dist/js/chunk-061a5fa2.3dc97429.js        12.03 KiB        2.53 KiB13:53:13   dist/lib/ace/mode-kotlin.js               11.97 KiB        2.83 KiB13:53:13   dist/js/chunk-4a00d4af.d3fab71e.js        11.93 KiB        3.22 KiB13:53:13   dist/lib/ace/mode-xml.js                  11.89 KiB        3.32 KiB13:53:13   dist/js/chunk-112948ec.81b3d619.js        11.73 KiB        2.65 KiB13:53:13   dist/lib/ace/ext-searchbox.js             11.64 KiB        3.49 KiB13:53:13   dist/lib/ace/mode-sass.js                 11.62 KiB        4.34 KiB13:53:13   dist/js/chunk-158513f2.c63e7bf4.js        11.61 KiB        2.96 KiB13:53:13   dist/js/chunk-574e585c.cd2faf98.js        11.55 KiB        2.91 KiB13:53:13   dist/lib/ace/mode-haskell.js              11.53 KiB        3.75 KiB13:53:13   dist/js/chunk-cbe1287c.6a5b89c0.js        11.36 KiB        3.17 KiB13:53:13   dist/lib/ace/mode-drools.js               11.24 KiB        3.12 KiB13:53:13   dist/js/chunk-7cb23b21.0baab3b1.js        11.20 KiB        4.19 KiB13:53:13   dist/lib/ace/mode-c_cpp.js                11.11 KiB        3.96 KiB13:53:13   dist/js/chunk-37598709.b71d6923.js        10.94 KiB        2.90 KiB13:53:13   dist/js/chunk-1afc103e.b838c85d.js        10.94 KiB        2.90 KiB13:53:13   dist/js/chunk-30c32723.7fc373ff.js        10.94 KiB        2.90 KiB13:53:13   dist/js/chunk-e83ecf78.ddc98804.js        10.94 KiB        2.90 KiB13:53:13   dist/js/chunk-3a8c83d5.d7fcb4d9.js        10.93 KiB        2.89 KiB13:53:13   dist/lib/ace/mode-praat.js                10.47 KiB        3.84 KiB13:53:13   dist/js/chunk-1d9fe70c.87f23114.js        10.43 KiB        3.25 KiB13:53:13   dist/js/chunk-f16e9740.8f01b904.js        10.34 KiB        3.13 KiB13:53:13   dist/lib/ace/mode-nsis.js                 10.31 KiB        3.91 KiB13:53:13   dist/js/chunk-5f261a47.17b02d91.js        10.30 KiB        3.10 KiB13:53:13   dist/js/chunk-7ea2dba3.4184708e.js        10.20 KiB        3.05 KiB13:53:13   dist/js/chunk-e7434404.e5c2f054.js        10.20 KiB        3.05 KiB13:53:13   dist/js/chunk-421e36d3.5b8c8b28.js        10.20 KiB        3.04 KiB13:53:13   dist/lib/ace/mode-ruby.js                 10.17 KiB        3.81 KiB13:53:13   dist/js/chunk-0fa7c848.341d5fff.js        10.08 KiB        2.97 KiB13:53:13   dist/js/chunk-2a999ff8.b86dff40.js        10.08 KiB        3.20 KiB13:53:13   dist/js/chunk-fa671de4.8e841360.js        9.76 KiB         2.23 KiB13:53:13   dist/js/chunk-1476d79a.46892e12.js        9.42 KiB         2.92 KiB13:53:13   dist/js/chunk-47294af0.399c5861.js        9.41 KiB         2.91 KiB13:53:13   dist/js/chunk-7793b159.61431453.js        9.24 KiB         2.86 KiB13:53:13   dist/js/chunk-608e2fe3.0ae4f9e3.js        9.16 KiB         2.12 KiB13:53:13   dist/lib/ace/ext-textarea.js              9.11 KiB         3.60 KiB13:53:13   dist/js/chunk-7775a5f9.00e80283.js        9.08 KiB         2.02 KiB13:53:13   dist/lib/ace/mode-d.js                    9.05 KiB         3.22 KiB13:53:13   dist/lib/ace/mode-assembly_x86.js         9.01 KiB         3.39 KiB13:53:13   dist/lib/ace/mode-csharp.js               8.94 KiB         2.78 KiB13:53:13   dist/lib/ace/mode-asl.js                  8.86 KiB         3.42 KiB13:53:13   dist/js/chunk-12effcec.12ef237e.js        8.80 KiB         1.95 KiB13:53:13   dist/js/chunk-d114fb66.b31b93ab.js        8.58 KiB         2.56 KiB13:53:13   dist/lib/ace/mode-fortran.js              8.55 KiB         3.59 KiB13:53:13   dist/js/chunk-2337c4ca.c69650d6.js        8.53 KiB         2.41 KiB13:53:13   dist/lib/ace/mode-prolog.js               8.49 KiB         2.50 KiB13:53:13   dist/lib/ace/mode-dockerfile.js           8.35 KiB         2.97 KiB13:53:13   dist/lib/ace/mode-asciidoc.js             8.28 KiB         2.57 KiB13:53:13   dist/lib/ace/mode-clojure.js              8.15 KiB         3.32 KiB13:53:13   dist/lib/ace/mode-redshift.js             8.12 KiB         2.77 KiB13:53:13   dist/js/chunk-1a25084c.61b9a82e.js        8.10 KiB         2.43 KiB13:53:13   dist/lib/ace/mode-sparql.js               7.99 KiB         2.55 KiB13:53:13   dist/js/chunk-bba7e5be.cefce3ce.js        7.81 KiB         2.76 KiB13:53:13   dist/lib/ace/mode-dot.js                  7.70 KiB         2.75 KiB13:53:13   dist/lib/ace/mode-julia.js                7.68 KiB         2.45 KiB13:53:13   dist/lib/ace/mode-csound_score.js         7.61 KiB         1.91 KiB13:53:13   dist/js/chunk-e26d2c80.a37f666c.js        7.58 KiB         2.92 KiB13:53:13   dist/lib/ace/mode-coffee.js               7.56 KiB         2.76 KiB13:53:13   dist/lib/ace/mode-perl.js                 7.52 KiB         2.91 KiB13:53:13   dist/lib/ace/mode-lua.js                  7.43 KiB         2.86 KiB13:53:13   dist/lib/ace/mode-sh.js                   7.34 KiB         2.71 KiB13:53:13   dist/lib/ace/mode-puppet.js               7.28 KiB         2.59 KiB13:53:13   dist/lib/ace/mode-forth.js                7.15 KiB         2.46 KiB13:53:13   dist/js/chunk-1273618c.52bf12a5.js        7.14 KiB         2.32 KiB13:53:13   dist/lib/ace/snippets/php.js              7.11 KiB         2.07 KiB13:53:13   dist/lib/ace/mode-jsx.js                  7.11 KiB         2.55 KiB13:53:13   dist/lib/ace/mode-golang.js               7.06 KiB         2.53 KiB13:53:13   dist/lib/ace/mode-swift.js                7.06 KiB         2.63 KiB13:53:13   dist/lib/ace/mode-terraform.js            6.97 KiB         2.36 KiB13:53:13   dist/js/chunk-14e07ffc.6aa6124e.js        6.89 KiB         1.95 KiB13:53:13   dist/lib/ace/mode-mushcode.js             6.87 KiB         3.04 KiB13:53:13   dist/lib/ace/mode-rust.js                 6.71 KiB         2.61 KiB13:53:13   dist/lib/ace/mode-makefile.js             6.71 KiB         2.28 KiB13:53:13   dist/lib/ace/mode-haxe.js                 6.69 KiB         2.38 KiB13:53:13   dist/lib/ace/mode-mysql.js                6.68 KiB         2.84 KiB13:53:13   dist/lib/ace/mode-scad.js                 6.67 KiB         2.23 KiB13:53:13   dist/lib/ace/theme-iplastic.js            6.65 KiB         3.85 KiB13:53:13   dist/lib/ace/mode-pig.js                  6.48 KiB         2.53 KiB13:53:13   dist/lib/ace/mode-bro.js                  6.38 KiB         2.27 KiB13:53:13   dist/lib/ace/mode-tcl.js                  6.29 KiB         1.95 KiB13:53:13   dist/lib/ace/mode-hjson.js                6.15 KiB         1.97 KiB13:53:13   dist/lib/ace/mode-jssm.js                 6.08 KiB         1.72 KiB13:53:13   dist/lib/ace/mode-abap.js                 5.99 KiB         2.60 KiB13:53:13   dist/js/chunk-0b27757e.087b2317.js        5.97 KiB         2.30 KiB13:53:13   dist/lib/ace/mode-jack.js                 5.94 KiB         2.06 KiB13:53:13   dist/lib/ace/mode-logiql.js               5.89 KiB         2.05 KiB13:53:13   dist/lib/ace/mode-io.js                   5.85 KiB         2.11 KiB13:53:13   dist/js/chunk-effdf402.389432cd.js        5.81 KiB         2.21 KiB13:53:13   dist/lib/ace/snippets/perl.js             5.71 KiB         2.12 KiB13:53:13   dist/js/chunk-30ec3699.c9d46488.js        5.67 KiB         2.02 KiB13:53:13   dist/lib/ace/mode-applescript.js          5.60 KiB         2.29 KiB13:53:13   dist/lib/ace/mode-yaml.js                 4.56 KiB         1.62 KiB13:53:13   dist/lib/ace/snippets/java.js             4.54 KiB         1.59 KiB13:53:13   dist/lib/ace/snippets/edifact.js          4.54 KiB         1.59 KiB13:53:13   dist/lib/ace/mode-c9search.js             4.30 KiB         1.59 KiB13:53:13   dist/lib/ace/ext-modelist.js              4.26 KiB         2.17 KiB13:53:13   dist/js/chunk-34986174.5704663c.js        4.25 KiB         1.46 KiB13:53:13   dist/lib/ace/snippets/django.js           4.23 KiB         1.38 KiB13:53:13   dist/js/chunk-6923b696.3386014d.js        4.11 KiB         1.47 KiB13:53:13   dist/lib/ace/snippets/javascript.js       4.08 KiB         1.52 KiB13:53:13   dist/lib/ace/ext-keybinding_menu.js       4.07 KiB         1.59 KiB13:53:13   dist/js/chunk-2d0c0afd.48291ccc.js        4.00 KiB         1.63 KiB13:53:13   dist/lib/ace/ext-beautify.js              3.97 KiB         1.70 KiB13:53:13   dist/lib/ace/ext-elastic_tabstops_lite    3.94 KiB         1.49 KiB13:53:13   .js13:53:13   dist/lib/ace/mode-snippets.js             3.92 KiB         1.41 KiB13:53:13   dist/lib/ace/snippets/python.js           3.91 KiB         1.35 KiB13:53:13   dist/lib/ace/mode-scheme.js               3.88 KiB         1.52 KiB13:53:13   dist/lib/ace/snippets/tex.js              3.87 KiB         1.30 KiB13:53:13   dist/lib/ace/ext-static_highlight.js      3.84 KiB         1.64 KiB13:53:13   dist/js/chunk-771e4ee6.ea048885.js        3.84 KiB         1.36 KiB13:53:13   dist/lib/ace/snippets/erlang.js           3.81 KiB         1.29 KiB13:53:13   dist/lib/ace/ext-split.js                 3.78 KiB         1.23 KiB13:53:13   dist/lib/ace/theme-tomorrow_night_brig    3.75 KiB         1.04 KiB13:53:13   ht.js13:53:13   dist/lib/ace/mode-graphqlschema.js        3.68 KiB         1.36 KiB13:53:13   dist/js/chunk-cad5ac02.66c4e86a.js        3.57 KiB         1.47 KiB13:53:13   dist/lib/ace/theme-tomorrow_night_eigh    3.48 KiB         0.94 KiB13:53:13   ties.js13:53:13   dist/lib/ace/theme-dreamweaver.js         3.39 KiB         1.03 KiB13:53:13   dist/lib/ace/theme-katzenmilch.js         3.38 KiB         1.03 KiB13:53:13   dist/lib/ace/snippets/vala.js             3.38 KiB         1.04 KiB13:53:13   dist/lib/ace/theme-tomorrow_night_blue    3.29 KiB         0.95 KiB13:53:13   .js13:53:13   dist/lib/ace/mode-rst.js                  3.27 KiB         1.05 KiB13:53:13   dist/lib/ace/mode-cirru.js                3.24 KiB         1.01 KiB13:53:13   dist/lib/ace/snippets/actionscript.js     3.23 KiB         1.26 KiB13:53:13   dist/lib/ace/theme-terminal.js            3.17 KiB         0.97 KiB13:53:13   dist/lib/ace/mode-verilog.js              3.15 KiB         1.32 KiB13:53:13   dist/lib/ace/theme-sqlserver.js           3.15 KiB         1.05 KiB13:53:13   dist/lib/ace/theme-chaos.js               3.09 KiB         1.00 KiB13:53:13   dist/lib/ace/mode-eiffel.js               3.09 KiB         1.18 KiB13:53:13   dist/lib/ace/mode-mixal.js                3.09 KiB         1.08 KiB13:53:13   dist/lib/ace/theme-tomorrow_night.js      3.08 KiB         0.95 KiB13:53:13   dist/lib/ace/theme-crimson_editor.js      3.07 KiB         0.98 KiB13:53:13   dist/lib/ace/theme-mono_industrial.js     3.05 KiB         0.96 KiB13:53:13   dist/js/chunk-1d68162b.43d062f2.js        3.05 KiB         1.21 KiB13:53:13   dist/lib/ace/mode-edifact.js              3.03 KiB         1.29 KiB13:53:13   dist/lib/ace/snippets/jsp.js              3.03 KiB         0.93 KiB13:53:13   dist/lib/ace/ext-whitespace.js            3.02 KiB         1.36 KiB13:53:13   dist/lib/ace/mode-tex.js                  3.00 KiB         0.96 KiB13:53:13   dist/lib/ace/theme-chrome.js              2.97 KiB         1.03 KiB13:53:13   dist/lib/ace/mode-ini.js                  2.96 KiB         1.08 KiB13:53:13   dist/lib/ace/snippets/c_cpp.js            2.92 KiB         0.99 KiB13:53:13   dist/lib/ace/theme-pastel_on_dark.js      2.90 KiB         0.97 KiB13:53:13   dist/lib/ace/snippets/r.js                2.89 KiB         0.85 KiB13:53:13   dist/lib/ace/theme-textmate.js            2.87 KiB         1.03 KiB13:53:13   dist/lib/ace/theme-dracula.js             2.84 KiB         0.95 KiB13:53:13   dist/lib/ace/theme-tomorrow.js            2.82 KiB         0.92 KiB13:53:13   dist/lib/ace/theme-twilight.js            2.76 KiB         0.96 KiB13:53:13   dist/lib/ace/theme-merbivore_soft.js      2.72 KiB         0.90 KiB13:53:13   dist/lib/ace/theme-clouds_midnight.js     2.70 KiB         0.89 KiB13:53:13   dist/lib/ace/mode-diff.js                 2.66 KiB         0.96 KiB13:53:13   dist/lib/ace/theme-gob.js                 2.64 KiB         0.98 KiB13:53:13   dist/lib/ace/mode-gherkin.js              2.64 KiB         0.98 KiB13:53:13   dist/lib/ace/theme-monokai.js             2.64 KiB         0.92 KiB13:53:13   dist/lib/ace/theme-solarized_light.js     2.63 KiB         0.88 KiB13:53:13   dist/lib/ace/theme-cobalt.js              2.62 KiB         0.94 KiB13:53:13   dist/lib/ace/theme-solarized_dark.js      2.59 KiB         0.89 KiB13:53:13   dist/lib/ace/mode-cobol.js                2.57 KiB         1.29 KiB13:53:13   dist/lib/ace/theme-kr_theme.js            2.56 KiB         0.93 KiB13:53:13   dist/lib/ace/mode-ada.js                  2.56 KiB         1.17 KiB13:53:13   dist/lib/ace/mode-haskell_cabal.js        2.55 KiB         0.92 KiB13:53:13   dist/lib/ace/theme-idle_fingers.js        2.52 KiB         0.89 KiB13:53:13   dist/lib/ace/theme-merbivore.js           2.51 KiB         0.87 KiB13:53:13   dist/lib/ace/theme-dawn.js                2.51 KiB         0.92 KiB13:53:13   dist/lib/ace/snippets/coffee.js           2.50 KiB         0.79 KiB13:53:13   dist/lib/ace/mode-space.js                2.49 KiB         0.85 KiB13:53:13   dist/lib/ace/theme-vibrant_ink.js         2.48 KiB         0.86 KiB13:53:13   dist/lib/ace/mode-toml.js                 2.46 KiB         0.96 KiB13:53:13   dist/lib/ace/theme-github.js              2.45 KiB         0.89 KiB13:53:13   dist/lib/ace/snippets/sqlserver.js        2.42 KiB         0.97 KiB13:53:13   dist/lib/ace/theme-eclipse.js             2.40 KiB         0.87 KiB13:53:13   dist/lib/ace/theme-clouds.js              2.35 KiB         0.85 KiB13:53:13   dist/lib/ace/mode-vhdl.js                 2.35 KiB         1.07 KiB13:53:13   dist/lib/ace/mode-textile.js              2.35 KiB         0.85 KiB13:53:13   dist/lib/ace/ext-rtl.js                   2.35 KiB         0.94 KiB13:53:13   dist/lib/ace/theme-kuroir.js              2.33 KiB         0.83 KiB13:53:13   dist/lib/ace/snippets/sh.js               2.32 KiB         0.93 KiB13:53:13   dist/lib/ace/snippets/clojure.js          2.32 KiB         0.80 KiB13:53:13   dist/lib/ace/snippets/haskell.js          2.25 KiB         0.91 KiB13:53:13   dist/lib/ace/snippets/markdown.js         2.25 KiB         0.80 KiB13:53:13   dist/lib/ace/mode-lisp.js                 2.23 KiB         0.91 KiB13:53:13   dist/lib/ace/theme-xcode.js               2.19 KiB         0.82 KiB13:53:13   dist/lib/ace/mode-sql.js                  2.11 KiB         0.97 KiB13:53:13   dist/js/chunk-11ebe073.a0e42fd1.js        2.07 KiB         0.91 KiB13:53:13   dist/lib/ace/snippets/xquery.js           2.00 KiB         0.66 KiB13:53:13   dist/lib/ace/snippets/jsoniq.js           2.00 KiB         0.66 KiB13:53:13   dist/lib/ace/snippets/tcl.js              1.97 KiB         0.73 KiB13:53:13   dist/lib/ace/theme-gruvbox.js             1.95 KiB         0.78 KiB13:53:13   dist/js/chunk-2d0c1f36.8b3907fe.js        1.83 KiB         0.90 KiB13:53:13   dist/lib/ace/mode-gcode.js                1.74 KiB         0.74 KiB13:53:13   dist/lib/ace/ext-themelist.js             1.68 KiB         0.71 KiB13:53:13   dist/lib/ace/mode-lucene.js               1.66 KiB         0.63 KiB13:53:13   dist/lib/ace/snippets/dart.js             1.61 KiB         0.61 KiB13:53:13   dist/lib/ace/ext-spellcheck.js            1.60 KiB         0.69 KiB13:53:13   dist/lib/ace/snippets/wollok.js           1.56 KiB         0.64 KiB13:53:13   dist/lib/ace/mode-csp.js                  1.53 KiB         0.69 KiB13:53:13   dist/lib/ace/snippets/io.js               1.51 KiB         0.59 KiB13:53:13   dist/lib/ace/mode-properties.js           1.39 KiB         0.51 KiB13:53:13   dist/lib/ace/snippets/csound_orchestra    1.38 KiB         0.46 KiB13:53:13   .js13:53:13   dist/lib/ace/ext-statusbar.js             1.25 KiB         0.60 KiB13:53:13   dist/lib/ace/snippets/abc.js              1.25 KiB         0.50 KiB13:53:13   dist/lib/ace/snippets/sql.js              1.24 KiB         0.48 KiB13:53:13   dist/js/chunk-2d0b6ecc.d84b63d8.js        1.24 KiB         0.70 KiB13:53:13   dist/js/chunk-2d0dcff1.8f257698.js        1.20 KiB         0.69 KiB13:53:13   dist/lib/ace/mode-gitignore.js            1.20 KiB         0.46 KiB13:53:13   dist/lib/ace/ext-linking.js               1.19 KiB         0.48 KiB13:53:13   dist/js/chunk-fb0f7f1a.8a629e9f.js        1.14 KiB         0.58 KiB13:53:13   dist/js/chunk-2d0aa1b9.11d1012d.js        1.07 KiB         0.72 KiB13:53:13   dist/lib/ace/snippets/graphqlschema.js    0.98 KiB         0.37 KiB13:53:13   dist/lib/ace/snippets/velocity.js         0.96 KiB         0.42 KiB13:53:13   dist/lib/ace/snippets/gobstones.js        0.92 KiB         0.39 KiB13:53:13   dist/js/chunk-2d207786.1df04e6d.js        0.91 KiB         0.61 KiB13:53:13   dist/js/chunk-2d230473.2d79c76f.js        0.86 KiB         0.41 KiB13:53:13   dist/lib/ace/snippets/diff.js             0.86 KiB         0.47 KiB13:53:13   dist/lib/ace/snippets/textile.js          0.85 KiB         0.42 KiB13:53:13   dist/lib/ace/mode-plain_text.js           0.82 KiB         0.36 KiB13:53:13   dist/lib/ace/snippets/lua.js              0.82 KiB         0.36 KiB13:53:13   dist/lib/ace/snippets/csound_score.js     0.47 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/assembly_x86.js     0.47 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/soy_template.js     0.47 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/html_elixir.js      0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/applescript.js      0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/apache_conf.js      0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/plain_text.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/powershell.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/dockerfile.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/properties.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/autohotkey.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/typescript.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/livescript.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/objectivec.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/handlebars.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/coldfusion.js       0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/gitignore.js        0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/batchfile.js        0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/html_ruby.js        0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/terraform.js        0.46 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/protobuf.js         0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/mushcode.js         0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/vbscript.js         0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/asciidoc.js         0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/c9search.js         0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/redshift.js         0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/verilog.js          0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/luapage.js          0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/fortran.js          0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/gherkin.js          0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/kotlin.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/sparql.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/elixir.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/turtle.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/smarty.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/stylus.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/prolog.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/scheme.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/pascal.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/golang.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/eiffel.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/puppet.js           0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/liquid.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/matlab.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/lucene.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/logiql.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/fsharp.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/groovy.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/csharp.js           0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/hjson.js            0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/jssm.js             0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/praat.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/mysql.js            0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/nsis.js             0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/latex.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/ocaml.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/julia.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/pgsql.js            0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/curly.js            0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/scala.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/mixal.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/cobol.js            0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/cirru.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/gcode.js            0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/forth.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/swift.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/rhtml.js            0.45 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/space.js            0.45 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/bro.js              0.44 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/csp.js              0.44 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/mask.js             0.44 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/text.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/abap.js             0.44 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/twig.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/lisp.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/yaml.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/rust.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/sass.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/scad.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/less.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/json.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/scss.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/slim.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/glsl.js             0.44 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/vhdl.js             0.44 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/haxe.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/jade.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/toml.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/jack.js             0.44 KiB         0.21 KiB13:53:13   dist/lib/ace/snippets/rdoc.js             0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/red.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/jsx.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/xml.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/sjs.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/ftl.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/dot.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/ini.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/nix.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/mel.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/elm.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/tsx.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/pig.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/asl.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/svg.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/ada.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/ejs.js              0.44 KiB         0.20 KiB13:53:13   dist/lib/ace/snippets/d.js                0.43 KiB         0.20 KiB13:53:13   dist/js/chunk-2d0da6a8.f46faf41.js        0.37 KiB         0.29 KiB13:53:13   dist/js/chunk-2d2253ae.81f78e55.js        0.37 KiB         0.28 KiB13:53:13   dist/js/fail.9d1b914c.js                  0.36 KiB         0.28 KiB13:53:13   dist/js/chunk-2d0e95df.acbc094b.js        0.33 KiB         0.26 KiB13:53:13   dist/lib/ace/ext-error_marker.js          0.33 KiB         0.15 KiB13:53:13   dist/lib/ace/mode-text.js                 0.32 KiB         0.15 KiB13:53:13   dist/js/lang-ja-JP.facbd014.js            0.21 KiB         0.21 KiB13:53:13   dist/js/chunk-2d0dd63f.eaf7c999.js        0.21 KiB         0.18 KiB13:53:13   dist/js/lang-fr-FR.7bb8cf92.js            0.20 KiB         0.17 KiB13:53:13   dist/js/lang-en-GB.f0851053.js            0.19 KiB         0.17 KiB13:53:13   dist/js/chunk-2d20977c.861ea0f1.js        0.17 KiB         0.15 KiB13:53:13   dist/css/chunk-vendors.6a5a4912.css       745.29 KiB       96.21 KiB13:53:13   dist/css/app.e64713f9.css                 477.93 KiB       98.08 KiB13:53:13   dist/css/chunk-7cb23b21.af37fb1c.css      4.75 KiB         2.61 KiB13:53:13   dist/css/user.c31e0589.css                4.18 KiB         2.46 KiB13:53:13   dist/css/chunk-6923b696.173cf026.css      3.70 KiB         2.32 KiB13:53:13   dist/css/chunk-2394da78.0e79d50a.css      2.48 KiB         0.38 KiB13:53:13   dist/css/chunk-7b215be7.5846bcc1.css      1.78 KiB         0.65 KiB13:53:13   dist/loading/loading.css                  1.62 KiB         0.54 KiB13:53:13   dist/css/chunk-1a25084c.7cf4d9c4.css      0.55 KiB         0.29 KiB13:53:13   dist/css/chunk-ffcd749a.7cf4d9c4.css      0.55 KiB         0.29 KiB13:53:13   dist/css/chunk-bba7e5be.60ba599d.css      0.48 KiB         0.25 KiB13:53:13   dist/css/chunk-421e36d3.aced983b.css      0.43 KiB         0.25 KiB13:53:13   dist/css/chunk-e7434404.45b80a38.css      0.43 KiB         0.25 KiB13:53:13   dist/css/chunk-0fa7c848.8c8de4cc.css      0.43 KiB         0.25 KiB13:53:13   dist/css/chunk-f16e9740.c7849a1f.css      0.43 KiB         0.25 KiB13:53:13   dist/css/chunk-5f261a47.fcc48993.css      0.43 KiB         0.25 KiB13:53:13   dist/css/chunk-7ea2dba3.f471e889.css      0.43 KiB         0.25 KiB13:53:13   dist/css/chunk-11ebe073.20670f24.css      0.34 KiB         0.20 KiB13:53:13   dist/css/chunk-4e4be866.a9dfd643.css      0.32 KiB         0.22 KiB13:53:13   dist/loading/option2/loading.css          0.30 KiB         0.18 KiB13:53:13   dist/css/chunk-60867039.1d961b54.css      0.29 KiB         0.21 KiB13:53:13   dist/css/chunk-213aeccc.99dfac86.css      0.26 KiB         0.20 KiB13:53:13   dist/css/chunk-34e01bd8.5a4006cf.css      0.23 KiB         0.19 KiB13:53:13   dist/css/chunk-671f39ca.8642fc4a.css      0.23 KiB         0.19 KiB13:53:13   dist/css/chunk-7e84b310.aace4747.css      0.20 KiB         0.16 KiB13:53:13   dist/css/chunk-12effcec.aace4747.css      0.20 KiB         0.16 KiB13:53:13   dist/css/chunk-f3f382b8.1e7a6bf2.css      0.19 KiB         0.16 KiB13:53:13   dist/css/chunk-e26d2c80.e52e2e15.css      0.19 KiB         0.15 KiB13:53:13   dist/css/chunk-cad5ac02.5eedec8d.css      0.18 KiB         0.14 KiB13:53:13   dist/css/chunk-6d233d0a.44bf8e75.css      0.16 KiB         0.14 KiB13:53:13   dist/css/chunk-7775a5f9.28de7f11.css      0.16 KiB         0.14 KiB13:53:13   dist/css/chunk-574e585c.28de7f11.css      0.16 KiB         0.14 KiB13:53:13   dist/css/chunk-0201a64a.71adeaf3.css      0.11 KiB         0.11 KiB13:53:13   dist/css/chunk-23759318.71adeaf3.css      0.11 KiB         0.11 KiB13:53:13   dist/css/chunk-4a1e5c3b.9190ca97.css      0.10 KiB         0.10 KiB13:53:13   dist/css/chunk-2337c4ca.e315ce2b.css      0.09 KiB         0.11 KiB13:53:13   dist/css/chunk-14e07ffc.e315ce2b.css      0.09 KiB         0.11 KiB13:53:13   dist/css/chunk-3942c521.e804fcd7.css      0.08 KiB         0.09 KiB13:53:13   dist/css/chunk-eb8257d2.159c0bfd.css      0.07 KiB         0.08 KiB13:53:13   dist/css/chunk-1273618c.be742cbe.css      0.06 KiB         0.07 KiB13:53:13   dist/css/chunk-5f1dd326.eff5faa3.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-66809a5c.be5a7a56.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-319fd138.20827ec7.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-0b27757e.9d60fcac.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-30ec3699.a920eb84.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-158513f2.a07d0447.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-2fadf79e.da3da5ec.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-c3b33c8e.094e5867.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-05bc1686.74338155.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-7793b159.6295b42e.css      0.05 KiB         0.07 KiB13:53:13   dist/css/chunk-78733bee.c5b0b76f.css      0.04 KiB         0.06 KiB13:53:13   dist/css/chunk-41b7bf83.c5b0b76f.css      0.04 KiB         0.06 KiB13:53:13   dist/css/chunk-682634f1.250c35f1.css      0.04 KiB         0.06 KiB13:53:13   dist/css/chunk-2a999ff8.250c35f1.css      0.04 KiB         0.06 KiB13:53:13   dist/css/chunk-fb0f7f1a.6f26ea36.css      0.04 KiB         0.06 KiB13:53:13 13:53:13   Images and other types of assets omitted.13:53:13 13:53:13  DONE  Build complete. The dist directory is ready to be deployed.13:53:13  INFO  Check out deployment instructions at https://cli.vuejs.org/guide/deployment.html13:53:13       13:53:13 Done in 124.33s.14:03:30 [Docker] INFO: Step 1/3 : FROM 192.168.238.159:5000/base_frontend:0.0.114:03:30 [Docker] INFO: 14:03:30 14:03:32 [Docker] INFO:  ---&gt; c283c15686cf14:03:32 14:03:32 [Docker] INFO: Step 2/3 : COPY dist/ /var/www14:03:32 [Docker] INFO: 14:03:32 14:07:10 [Docker] INFO:  ---&gt; Using cache14:07:10 14:07:10 [Docker] INFO:  ---&gt; ed96447e99da14:07:10 14:07:10 [Docker] INFO: Step 3/3 : ENTRYPOINT [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]14:07:10 [Docker] INFO: 14:07:10 14:07:44 [Docker] INFO:  ---&gt; Running in 4447e3f5208714:07:44 14:08:32 [Docker] INFO:  ---&gt; e9ff4657b42a14:08:32 14:12:30 [Docker] INFO: Successfully built e9ff4657b42a14:12:30 14:12:31 [Docker] INFO: Successfully tagged 192.168.238.159:5000/test-frontend-k8s:2114:12:31 14:12:31 [Docker] INFO: Build image id:e9ff4657b42a14:12:31 [Docker] INFO: Pushing image 192.168.238.159:5000/test-frontend-k8s:2114:12:39 [Docker] INFO: Done pushing image 192.168.238.159:5000/test-frontend-k8s:2114:12:39 [Docker] INFO: Removed image 192.168.238.159:5000/test-frontend-k8s:2114:12:39 Starting Kubernetes deployment14:12:45 Loading configuration: /var/jenkins_home/workspace/test-frontend-k8s/frontend-opt-k8s.yaml14:12:46 Applied V1Service: class V1Service {14:12:46     apiVersion: v114:12:46     kind: Service14:12:46     metadata: class V1ObjectMeta {14:12:46         annotations: null14:12:46         clusterName: null14:12:46         creationTimestamp: 2020-04-02T06:12:55.000Z14:12:46         deletionGracePeriodSeconds: null14:12:46         deletionTimestamp: null14:12:46         finalizers: null14:12:46         generateName: null14:12:46         generation: null14:12:46         initializers: null14:12:46         labels: {app=test-frontend}14:12:46         managedFields: null14:12:46         name: test-frontend14:12:46         namespace: test-all-service14:12:46         ownerReferences: null14:12:46         resourceVersion: 2699088814:12:46         selfLink: /api/v1/namespaces/test-all-service/services/test-frontend14:12:46         uid: 41f878bf-c4f1-491b-997b-e52a1038ad5c14:12:46     }14:12:46     spec: class V1ServiceSpec {14:12:46         clusterIP: 10.43.135.17014:12:46         externalIPs: null14:12:46         externalName: null14:12:46         externalTrafficPolicy: Cluster14:12:46         healthCheckNodePort: null14:12:46         loadBalancerIP: null14:12:46         loadBalancerSourceRanges: null14:12:46         ports: [class V1ServicePort {14:12:46             name: tcp14:12:46             nodePort: 3108014:12:46             port: 8014:12:46             protocol: TCP14:12:46             targetPort: 8014:12:46         }]14:12:46         publishNotReadyAddresses: null14:12:46         selector: {app=test-frontend}14:12:46         sessionAffinity: None14:12:46         sessionAffinityConfig: null14:12:46         type: NodePort14:12:46     }14:12:46     status: class V1ServiceStatus {14:12:46         loadBalancer: class V1LoadBalancerStatus {14:12:46             ingress: null14:12:46         }14:12:46     }14:12:46 }14:12:47 Applied V1Deployment: class V1Deployment {14:12:47     apiVersion: apps/v114:12:47     kind: Deployment14:12:47     metadata: class V1ObjectMeta {14:12:47         annotations: null14:12:47         clusterName: null14:12:47         creationTimestamp: 2020-04-26T10:08:53.000Z14:12:47         deletionGracePeriodSeconds: null14:12:47         deletionTimestamp: null14:12:47         finalizers: null14:12:47         generateName: null14:12:47         generation: 714:12:47         initializers: null14:12:47         labels: null14:12:47         managedFields: null14:12:47         name: test-frontend14:12:47         namespace: test-all-service14:12:47         ownerReferences: null14:12:47         resourceVersion: 2699088914:12:47         selfLink: /apis/apps/v1/namespaces/test-all-service/deployments/test-frontend14:12:47         uid: 25aa0a0d-3b88-4a00-ac4f-df731440d21c14:12:47     }14:12:47     spec: class V1DeploymentSpec {14:12:47         minReadySeconds: 1014:12:47         paused: null14:12:47         progressDeadlineSeconds: 60014:12:47         replicas: 114:12:47         revisionHistoryLimit: 1014:12:47         selector: class V1LabelSelector {14:12:47             matchExpressions: null14:12:47             matchLabels: {app=test-frontend}14:12:47         }14:12:47         strategy: class V1DeploymentStrategy {14:12:47             rollingUpdate: class V1RollingUpdateDeployment {14:12:47                 maxSurge: 114:12:47                 maxUnavailable: 014:12:47             }14:12:47             type: RollingUpdate14:12:47         }14:12:47         template: class V1PodTemplateSpec {14:12:47             metadata: class V1ObjectMeta {14:12:47                 annotations: null14:12:47                 clusterName: null14:12:47                 creationTimestamp: null14:12:47                 deletionGracePeriodSeconds: null14:12:47                 deletionTimestamp: null14:12:47                 finalizers: null14:12:47                 generateName: null14:12:47                 generation: null14:12:47                 initializers: null14:12:47                 labels: {app=test-frontend}14:12:47                 managedFields: null14:12:47                 name: null14:12:47                 namespace: null14:12:47                 ownerReferences: null14:12:47                 resourceVersion: null14:12:47                 selfLink: null14:12:47                 uid: null14:12:47             }14:12:47             spec: class V1PodSpec {14:12:47                 activeDeadlineSeconds: null14:12:47                 affinity: class V1Affinity {14:12:47                     nodeAffinity: null14:12:47                     podAffinity: null14:12:47                     podAntiAffinity: class V1PodAntiAffinity {14:12:47                         preferredDuringSchedulingIgnoredDuringExecution: [class V1WeightedPodAffinityTerm {14:12:47                             podAffinityTerm: class V1PodAffinityTerm {14:12:47                                 labelSelector: class V1LabelSelector {14:12:47                                     matchExpressions: [class V1LabelSelectorRequirement {14:12:47                                         key: app14:12:47                                         operator: In14:12:47                                         values: [app-test-frontend]14:12:47                                     }]14:12:47                                     matchLabels: null14:12:47                                 }14:12:47                                 namespaces: null14:12:47                                 topologyKey: kubernetes.io/hostname14:12:47                             }14:12:47                             weight: 114:12:47                         }]14:12:47                         requiredDuringSchedulingIgnoredDuringExecution: null14:12:47                     }14:12:47                 }14:12:47                 automountServiceAccountToken: null14:12:47                 containers: [class V1Container {14:12:47                     args: null14:12:47                     command: null14:12:47                     env: [class V1EnvVar {14:12:47                         name: GATEWAY_HOST14:12:47                         value: 192.168.238.241:3133314:12:47                         valueFrom: null14:12:47                     }]14:12:47                     envFrom: null14:12:47                     image: 192.168.238.159:5000/test-frontend-k8s:2114:12:47                     imagePullPolicy: Always14:12:47                     lifecycle: null14:12:47                     livenessProbe: null14:12:47                     name: test-frontend14:12:47                     ports: [class V1ContainerPort {14:12:47                         containerPort: 8014:12:47                         hostIP: null14:12:47                         hostPort: null14:12:47                         name: null14:12:47                         protocol: TCP14:12:47                     }]14:12:47                     readinessProbe: null14:12:47                     resources: class V1ResourceRequirements {14:12:47                         limits: {memory=Quantity{number=1610612736, format=DECIMAL_SI}}14:12:47                         requests: {memory=Quantity{number=1073741824, format=BINARY_SI}}14:12:47                     }14:12:47                     securityContext: null14:12:47                     stdin: null14:12:47                     stdinOnce: null14:12:47                     terminationMessagePath: /dev/termination-log14:12:47                     terminationMessagePolicy: File14:12:47                     tty: null14:12:47                     volumeDevices: null14:12:47                     volumeMounts: null14:12:47                     workingDir: null14:12:47                 }]14:12:47                 dnsConfig: null14:12:47                 dnsPolicy: ClusterFirst14:12:47                 enableServiceLinks: null14:12:47                 hostAliases: null14:12:47                 hostIPC: null14:12:47                 hostNetwork: null14:12:47                 hostPID: null14:12:47                 hostname: null14:12:47                 imagePullSecrets: null14:12:47                 initContainers: null14:12:47                 nodeName: null14:12:47                 nodeSelector: null14:12:47                 preemptionPolicy: null14:12:47                 priority: null14:12:47                 priorityClassName: null14:12:47                 readinessGates: null14:12:47                 restartPolicy: Always14:12:47                 runtimeClassName: null14:12:47                 schedulerName: default-scheduler14:12:47                 securityContext: class V1PodSecurityContext {14:12:47                     fsGroup: null14:12:47                     runAsGroup: null14:12:47                     runAsNonRoot: null14:12:47                     runAsUser: null14:12:47                     seLinuxOptions: null14:12:47                     supplementalGroups: null14:12:47                     sysctls: null14:12:47                     windowsOptions: null14:12:47                 }14:12:47                 serviceAccount: null14:12:47                 serviceAccountName: null14:12:47                 shareProcessNamespace: null14:12:47                 subdomain: null14:12:47                 terminationGracePeriodSeconds: 3014:12:47                 tolerations: null14:12:47                 volumes: null14:12:47             }14:12:47         }14:12:47     }14:12:47     status: class V1DeploymentStatus {14:12:47         availableReplicas: 114:12:47         collisionCount: null14:12:47         conditions: [class V1DeploymentCondition {14:12:47             lastTransitionTime: 2020-04-26T10:09:24.000Z14:12:47             lastUpdateTime: 2020-04-26T10:09:24.000Z14:12:47             message: Deployment has minimum availability.14:12:47             reason: MinimumReplicasAvailable14:12:47             status: True14:12:47             type: Available14:12:47         }, class V1DeploymentCondition {14:12:47             lastTransitionTime: 2020-04-26T10:09:05.000Z14:12:47             lastUpdateTime: 2020-05-07T01:33:25.000Z14:12:47             message: ReplicaSet &quot;test-frontend-5c458c5fbf&quot; has successfully progressed.14:12:47             reason: NewReplicaSetAvailable14:12:47             status: True14:12:47             type: Progressing14:12:47         }]14:12:47         observedGeneration: 614:12:47         readyReplicas: 114:12:47         replicas: 114:12:47         unavailableReplicas: null14:12:47         updatedReplicas: 114:12:47     }14:12:47 }14:12:48 Finished Kubernetes deployment14:12:49 Finished: SUCCESS</code></pre><p>分析上述日志，可以得到以下信息：</p><ol><li>yarn run build这个过程大约在2分钟到3分钟之间。</li><li>在完成上述构建后，到进入docker镜像构建的时候，存在一个很长的时间，10分钟左右，该位置时间节点原因未知。</li><li>在Dockerfile文件中，执行COPY命令时，发现持续时间很长，在4分钟左右。</li><li>完成镜像打包到推送到镜像仓库中，这个位置耗时较长，在4分钟左右。</li></ol><p>根据上述的信息，可以预见的是目前构建中存在如下问题：</p><ol><li>前端构建时生成文件过多，导致COPY时速度缓慢，这一点需要优化。</li><li>需要查看打包后生成的前端镜像是否过大。</li><li>完成构建到docker镜像构建过程时，耗时长的问题。</li></ol><h2 id="构建改造"><a href="#构建改造" class="headerlink" title="构建改造"></a>构建改造</h2><p>决定全部走docker镜像构建，剥离当前jenkins内的镜像对于构建的影响，主要是nvm插件的使用。使用两段式构建。基础镜像不做变更，使用nodejs的镜像，构建全部前端代码，然后使用基础镜像构建前端项目。编写Dockerfile，如下：</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile"># 第一层面构建，打包前端代码#### 1. 指定node镜像版本FROM node:10.12.0 AS builder# 2. 指定编译的工作空间WORKDIR /home/node/app# 3. 安装打包需要的yarn工具RUN npm --registry https://registry.npm.taobao.org install -g yarn# 4. 添加package.jsonCOPY package.json /home/node/app/# 5. 安装依赖，如果package.json未变更，将会沿用之前的镜像缓存RUN yarn --registry https://registry.npm.taobao.org install# 6. 添加剩余代码到工作空间COPY . /home/node/app# 7. 编译代码RUN yarn run build# 第二层面构建#### 1.拉取自定义镜像名称FROM 192.168.238.159:5000/base_frontend:0.0.1# 2.将打包后的代码复制到运行位置COPY --from=builder /home/node/app/dist /var/www# 3.启动nginxENTRYPOINT ["nginx","-g","daemon off;"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样就将上述的外部打包的工作，变成了镜像内工作，同时根据层次划分，将依赖安装和源码构建分开。对一个Docker镜像来说，Dockerfile中的每一个步骤都是一个分层，从第一个步骤开始，只有当其中一个步骤的操作发生了变化，这之后的每一个分层才会重新生成，而之前的分层都会被复用。这样就能利用之前缓存的层级信息，为后续构建加速！</p><p>随后，改造整个构建流程，拿掉和nodejs相关的构建步骤，只保留最后docker打包的部分，如下图：</p><p><img src="%E6%96%B0%E7%9A%84docker%E6%9E%84%E5%BB%BA%E9%85%8D%E7%BD%AE.png" alt></p><p>最后，进行构建即可。</p><h2 id="改造后效果"><a href="#改造后效果" class="headerlink" title="改造后效果"></a>改造后效果</h2><p>第一次构建非常慢，涉及到拉取各路依赖包信息，长达一个小时到两个小时左右，主要瓶颈点在于依赖的下载安装。下面进行两个测试，第一是通过修改一行代码来测试，第二是通过添加一个新的依赖包测试。</p><h3 id="1-修改一行代码的测试"><a href="#1-修改一行代码的测试" class="headerlink" title="1. 修改一行代码的测试"></a>1. 修改一行代码的测试</h3><p>修改一行代码后，其真正触发的是编译前端代码的部分，起始处日志如下：</p><pre><code>17:29:32 [Docker] INFO: Step 7/10 : RUN yarn run build</code></pre><p>会重新开始构建前端代码，这样以后到最终构建完成并部署，花费4分钟左右。比起之前的构建提升巨大，至少节省了15分钟以上。将构建流程压缩到了4分钟左右。</p><h3 id="2-添加新的依赖包的测试"><a href="#2-添加新的依赖包的测试" class="headerlink" title="2. 添加新的依赖包的测试"></a>2. 添加新的依赖包的测试</h3><p>测试修改package.json中的内容，添加一个dragable控件信息，如下：</p><pre><code>$ vim package.json// 添加下面的内容到dependencies节点.....&quot;dependencies&quot;: {  &quot;vue-draggable-resizable&quot;: &quot;^2.2.0&quot;,  ....}</code></pre><p>然后提交，进行构建。但是并没有提升什么构建速度，相反拖累了整个jenkins中对于前端构建的速度，又回到了一个小时的时候。这时候需要进行进一步的优化。</p><h3 id="3-其它尝试"><a href="#3-其它尝试" class="headerlink" title="3. 其它尝试"></a>3. 其它尝试</h3><h4 id="3-1-制作私有镜像仓库"><a href="#3-1-制作私有镜像仓库" class="headerlink" title="3.1 制作私有镜像仓库"></a>3.1 制作私有镜像仓库</h4><p>制作私有镜像仓库参考自<a href="https://www.jianshu.com/p/1674a6bc1c12" target="_blank" rel="noopener">该链接</a>。使用nexus3作为本地的npm镜像仓库，仓库地址为：<a href="http://192.168.238.159:8081/repository/npm-group/" target="_blank" rel="noopener">http://192.168.238.159:8081/repository/npm-group/</a> ，用户名为admin，密码为helloworld，账号的邮箱地址为<a href="mailto:lisongyang@abc.com" target="_blank" rel="noopener">lisongyang@abc.com</a>。</p><h4 id="3-2-在本地使用私有镜像仓库"><a href="#3-2-在本地使用私有镜像仓库" class="headerlink" title="3.2 在本地使用私有镜像仓库"></a>3.2 在本地使用私有镜像仓库</h4><pre><code>// 查看并修改镜像仓库地址$ npm config get registryhttp://registry.npmjs.org$ npm config set registry http://192.168.238.159:8081/repository/npm-group/// 登录镜像仓库// 如果不登录的话，拉取仓库中的依赖信息，会出现401 unauthorized的错误$ npm login// 输入上面提到的用户名密码以及邮箱地址Username: adminPassword:Email: (this IS public) lisongyang@abc.comLogged in as leinov on http://192.168.238.159:8081/repository/npm-group/.</code></pre><p>上述操作完成后，需要删除本地的缓存文件夹node_cache以及项目中的node_modules文件夹，这样做的目的是保证本地拉取远程的依赖包时，从我们自己搭建的nexus3镜像仓库中拉取，也是将配置的外网淘宝镜像的依赖，缓存在我们自己的镜像仓库中。这样后续在拉取依赖时，会从nexus3镜像仓库中拉取，不需要去访问外网了，只有本地没有的依赖信息才会主动去淘宝镜像拉取。</p><p>删除完成后，就可以开始构建了，运行npm install，会发现速度慢了很多。当第一次操作完成后，后续操作时会加速构建。</p><p>将npm换为yarn工具，操作是类似的。但是yarn工具不能在docker构建时使用，具体参考下面的章节。</p><h4 id="3-3-CI-CD过程中使用私有镜像仓库"><a href="#3-3-CI-CD过程中使用私有镜像仓库" class="headerlink" title="3.3 CI/CD过程中使用私有镜像仓库"></a>3.3 CI/CD过程中使用私有镜像仓库</h4><p>在nexus3镜像仓库搭建完成后，重新编写前端的dockerfile，如下：</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile"># 第一层面构建，打包前端代码#### 1. 指定node镜像版本FROM node:10.16.0 AS builder# 添加日期信息，如果需要更新缓存层，更新该处日期信息即可ENV REFRESH_DATE 2020-05-18_11:11:11# 2. 指定编译的工作空间WORKDIR /home/node/app# 安装打包需要的yarn工具# RUN npm --registry https://registry.npm.taobao.org install -g yarn# 3. 对yarn设置淘宝镜像RUN npm config set registry http://192.168.238.159:8081/repository/npm-group/ # 4. 添加登录nexus3镜像仓库的插件RUN npm install -g npm-cli-adduser --registry https://registry.npm.taobao.org# 5. 解决node-sass安装失败的问题RUN npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/# 6. 添加package.jsonCOPY package.json /home/node/app/# 7. 登录并安装依赖，如果package.json未变更，将会沿用之前的镜像缓存RUN NPM_USER=admin NPM_PASS=helloworld NPM_EMAIL=lisongyang@abc.com NPM_REGISTRY=http://192.168.238.159:8081/repository/npm-group/ npm-cli-adduser # 8. 安装依赖信息RUN npm --loglevel info install # 9. 添加剩余代码到工作空间COPY . /home/node/app# 10. 编译代码RUN npm run build# 第二层面构建#### 1.拉取自定义镜像名称FROM 192.168.238.159:5000/base_frontend:0.0.1# 2.将打包后的代码复制到运行位置COPY --from=builder /home/node/app/dist /var/www# 3.启动nginxENTRYPOINT ["nginx","-g","daemon off;"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对比之前的dockerfile，这里主要是将yarn用npm进行了代替，根据构建中出现的node-sass依赖错误的问题进行了优化。这样编写之后，整个前端从打包到构建镜像全部在docker中进行了，相应的前端构建项目也是要变更。创建新的分支，为change_ci_cd，然后进行构建测试，构建流程和之前的一致。</p><p>这样完成之后，就可以愉快的开启打包流程了。但是这时候会报错，如下：</p><pre><code>TypeError: Class extends value undefined is not a constructor or null    at Object.&lt;anonymous&gt; (/Users/glenn/github/minimalistic-devserver/node_modules/mini-css-extract-plugin/dist/index.js:30:47)    at Module._compile (/Users/glenn/github/minimalistic-devserver/node_modules/v8-compile-cache/v8-compile-cache.js:178:30)    at Object.Module._extensions..js (module.js:673:10)    at Module.load (module.js:575:32)    at tryModuleLoad (module.js:515:12)    at Function.Module._load (module.js:507:3)    at Module.require (module.js:606:17)    at require (/Users/glenn/github/minimalistic-devserver/node_modules/v8-compile-cache/v8-compile-cache.js:159:20)    at Object.&lt;anonymous&gt; (/Users/glenn/github/minimalistic-devserver/node_modules/mini-css-extract-plugin/dist/cjs.js:3:18)    at Module._compile (/Users/glenn/github/minimalistic-devserver/node_modules/v8-compile-cache/v8-compile-cache.js:178:30)    at Object.Module._extensions..js (module.js:673:10)    at Module.load (module.js:575:32)    at tryModuleLoad (module.js:515:12)    at Function.Module._load (module.js:507:3)    at Module.require (module.js:606:17)    at require (/Users/glenn/github/minimalistic-devserver/node_modules/v8-compile-cache/v8-compile-cache.js:159:20)    at Object.&lt;anonymous&gt; (/Users/glenn/github/minimalistic-devserver/webpack.config.js:8:30)    at Module._compile (/Users/glenn/github/minimalistic-devserver/node_modules/v8-compile-cache/v8-compile-cache.js:178:30)    at Object.Module._extensions..js (module.js:673:10)    at Module.load (module.js:575:32)    at tryModuleLoad (module.js:515:12)    at Function.Module._load (module.js:507:3)    at Module.require (module.js:606:17)    at require (/Users/glenn/github/minimalistic-devserver/node_modules/v8-compile-cache/v8-compile-cache.js:159:20)    at WEBPACK_OPTIONS (/Users/glenn/github/minimalistic-devserver/node_modules/webpack-cli/bin/convert-argv.js:133:13)    at requireConfig (/Users/glenn/github/minimalistic-devserver/node_modules/webpack-cli/bin/convert-argv.js:135:6)    at /Users/glenn/github/minimalistic-devserver/node_modules/webpack-cli/bin/convert-argv.js:142:17    at Array.forEach (&lt;anonymous&gt;)    at module.exports (/Users/glenn/github/minimalistic-devserver/node_modules/webpack-cli/bin/convert-argv.js:140:15)    at yargs.parse (/Users/glenn/github/minimalistic-devserver/node_modules/webpack-cli/bin/webpack.js:234:39)</code></pre><p>报错的原因是，我们自身的webpack配置，引发了mini-css-extract-plugin插件的错误。我们在package.json中限定webpack高于3.6.0版本，在jenkins中拉取时，拉去了webpack4以上的版本，这是令人奇怪的地方。看package.json中的匹配规则:</p><pre><code>...    &quot;webpack&quot;: &quot;^3.6.0&quot;,...</code></pre><p>这里引述两个符号解释：</p><pre><code>&#39;~&#39;（波浪符号）:他会更新到当前minor version（也就是中间的那位数字）中最新的版本。放到我们的例子中就是：&quot;exif-js&quot;: &quot;~2.3.0&quot;，这个库会去匹配更新到2.3.x的最新版本，如果出了一个新的版本为2.4.0，则不会自动升级。波浪符号是曾经npm安装时候的默认符号，现在已经变为了插入符号。&#39;^&#39;（插入符号）: 这个符号就显得非常的灵活了，他将会把当前库的版本更新到当前major version（也就是第一位数字）中最新的版本。放到我们的例子中就是：&quot;vue&quot;: &quot;^2.2.2&quot;, 这个库会去匹配2.x.x中最新的版本，但是他不会自动更新到3.0.0。作者：太阳的小号链接：https://www.jianshu.com/p/612eefd8ef40来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</code></pre><p>正常来说，webpack应该去匹配3.x相关的版本，而不是匹配4.x相关的版本。当webpack版本匹配到4.1.x版本时，导致了mini-css-extract-plugin插件报错了，这时候参考相关链接中“webpack版本升级的问题”，需要强制把webpack版本升到4.1.0才能构建通过。</p><p><strong>注意：</strong>该问题目前无解，只能临时进行尝试替换并在日常开发中测试，目前尚无明确的不良反应。</p><p>当webpack版本升级后，除第一次构建外，构建时间会大幅缩减，即使是修改package.json中的文件，也不会出现耗时过长的情况。更广泛的作用是，它加速了本地下载依赖的速度，优化了打包性能。</p><p>重新尝试打包的工作，分别使用npm和yarn进行打包。测试后发现yarn工具不会报错，而且能很好的执行构建工作，而npm会出现依赖报错的问题。但是webpack的版本还是会提升到4.1.0以上，不影响其它组件的使用。</p><p>在经过最终讨论和尝试后，我们确定统一使用yarn来构建我们的前端项目。然后尽量采用淘宝镜像来构建。yarn比npm在版本管理上更加科学，也更有效，能避免上述提到的依赖版本问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><ul><li>webpack版本升级的问题：<a href="https://github.com/webpack-contrib/mini-css-extract-plugin/issues/3" target="_blank" rel="noopener">https://github.com/webpack-contrib/mini-css-extract-plugin/issues/3</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 前端打包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于k8s环境中微服务调用问题排查方式</title>
      <link href="/2020/05/14/guan-yu-k8s-huan-jing-zhong-wei-fu-wu-diao-yong-wen-ti-pai-cha-fang-shi/"/>
      <url>/2020/05/14/guan-yu-k8s-huan-jing-zhong-wei-fu-wu-diao-yong-wen-ti-pai-cha-fang-shi/</url>
      
        <content type="html"><![CDATA[<h1 id="关于当前k8s环境中出现的服务调用失败的排查"><a href="#关于当前k8s环境中出现的服务调用失败的排查" class="headerlink" title="关于当前k8s环境中出现的服务调用失败的排查"></a>关于当前k8s环境中出现的服务调用失败的排查</h1><h2 id="问题概述"><a href="#问题概述" class="headerlink" title="问题概述"></a>问题概述</h2><p>在五一节前的时候，出现了opt相关平台无法登录的情况，总是出现登录失败的反馈，前端调用后端接口时出现了调用失败的情况。而且伴随着前端出现了弹框的异常信息。涉及到test-user-uaa、test-umc以及test-opt-agent三个服务。对比开发环境中的服务，发现异常出现在test-user-uaa的接口调用部分。做出了如下的排查手段并实践！</p><h2 id="排查方式"><a href="#排查方式" class="headerlink" title="排查方式"></a>排查方式</h2><p>总结下微服务中排查错误的常见方式，如下：</p><ol><li><p>查看全链路监控中的信息</p></li><li><p>查看相关服务的日志信息</p></li><li><p>查看服务的配置信息</p></li><li><p>对比git中的代码日志记录</p></li><li><p>对比开发库和测试库的数据信息，测试相同数据的结果</p></li><li><p>查看服务间是否有循环调用</p></li><li><p>查看服务部署时的部署文件</p></li><li><p>查看jar包的依赖信息</p></li><li><p>重新部署服务</p></li><li><p>打通k8s线上线下环境，流量引到线下，进行联调</p></li></ol><p>上述是我这里的顺序，下面在展开分析中并未按照上述的顺序执行，但基本步骤是一致的。</p><h2 id="展开分析"><a href="#展开分析" class="headerlink" title="展开分析"></a>展开分析</h2><h3 id="1-查看全链路监控中的信息"><a href="#1-查看全链路监控中的信息" class="headerlink" title="1. 查看全链路监控中的信息"></a>1. 查看全链路监控中的信息</h3><p>很遗憾，我们的skywalking又挂了，我不得不重新进行启动。重启完成后，进行监控。从浏览器登录，看到请求的接口如下：</p><pre><code>http://192.168.232.239:31081/api/uaa/oauth/token?checkCode=1111&amp;randomStr=502c6ed51866ddeedee60f802e408029</code></pre><p>然后找到Skywalking中关于该接口的监控信息如下：</p><p><img src="skywalking%E7%9B%91%E6%8E%A7.png" alt></p><p>由于后向请求出现错误，所以导致监控接口并无任何信息返回。</p><h3 id="2-查看相关服务的日志信息"><a href="#2-查看相关服务的日志信息" class="headerlink" title="2. 查看相关服务的日志信息"></a>2. 查看相关服务的日志信息</h3><p>由于我们尚未启用ELK日志系统，并不能在同一个位置查看所有的日志信息，这时候需要我们查看服务所在pod中的日志信息，需要先对pod进行缩减，从单一服务上进行查看。操作如下：</p><pre><code>$ kubectl get pod -n test-all-serviceNAME                                         READY   STATUS    RESTARTS   AGEtest-data-dictionary-7c7cf58696-7zrwd   1/1     Running   0          12dtest-filestore-opt-f9c5794f5-j2s2n      1/1     Running   0          8dtest-filestore-uc-9c89c7dc8-gdp7p       1/1     Running   0          8dtest-form-design-6457c7c855-6wj9k       1/1     Running   0          8dtest-frontend-5c458c5fbf-mbb7d          1/1     Running   0          2dtest-frontend-user-59d5fd489d-hnr6n     1/1     Running   0          2d14htest-id-generator-59ccb5b667-bbp4k      1/1     Running   0          12dtest-id-generator-59ccb5b667-qqfj5      1/1     Running   1          12dtest-id-generator-59ccb5b667-z62d2      1/1     Running   0          12dtest-opt-agent-6667bfd8c8-8g8cz         1/1     Running   0          8dtest-opt-center-657785b66c-wrttd        1/1     Running   0          8dtest-opt-uaa-6c87879dcb-dcs69           1/1     Running   0          8dtest-user-uaa-5fb57fb898-552nz          1/1     Running   0          16htest-user-uaa-5fb57fb898-8gl5b          1/1     Running   0          16htest-user-uaa-5fb57fb898-ptxng          1/1     Running   0          16htest-user-umc-bb58848d4-8nt5g           1/1     Running   0          16htest-user-umc-bb58848d4-gnmnb           1/1     Running   0          16htest-user-umc-bb58848d4-xn77n           1/1     Running   0          16htest-visual-model-7547ff9984-klgqp      1/1     Running   0          8dtest-workflow-5dd8d98b9f-wnnfp          1/1     Running   0          8d// 缩减相关服务数量$ kubectl scale deployment test-user-uaa --replicas=1 -n test-all-service$ kubectl scale deployment test-umc --replicas=1 -n test-all-service$ kubectl scale deployment test-opt-agent --replicas=1 -n test-all-service// 分别查看各个服务中的日志信息$ kubectl logs -f --tail=100 test-opt-agent -n test-all-service(无相关日志)$ kubectl logs -f --tail=100 test-umc -n test-all-service(无相关日志)$ kubectl logs -f --tail=100 test-umc -n test-all-service// 错误日志如下************************************************************Request received for POST &#39;/oauth/token?checkCode=neya&amp;randomStr=8a1ca2f6324261a4c7989e59773da07d&#39;:org.apache.catalina.connector.RequestFacade@330f414bservletPath:/oauth/tokenpathInfo:nullheaders:content-length: 149authorization: Basic aWNwLXVpOmhvdGVhbUAyMDE5accept: application/json, text/plain, */*origin: http://192.168.232.239:31081user-agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36timezone: 8content-type: application/x-www-form-urlencoded;charset=UTF-8referer: http://192.168.232.239:31081/user/login?redirect=%2Faccept-encoding: gzip, deflateaccept-language: en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7,es;q=0.6cookie: JSESSIONID=D6343899CA9DCEEA915300AC905FF141forwarded: proto=http;host=&quot;192.168.232.239:31081&quot;;for=&quot;10.42.0.0:41273&quot;x-forwarded-for: 10.42.0.0x-forwarded-proto: httpx-forwarded-prefix: /api/uaax-forwarded-port: 31081x-forwarded-host: 192.168.232.239:31081x-b3-traceid: 939275ae4315ffa7x-b3-spanid: 174e0e570d7587bdx-b3-parentspanid: 939275ae4315ffa7x-b3-sampled: 0host: 10.42.3.181:19060Security filter chain: [  WebAsyncManagerIntegrationFilter  SecurityContextPersistenceFilter  HeaderWriterFilter  LogoutFilter  ClientCredentialsTokenEndpointFilter  BasicAuthenticationFilter  RequestCacheAwareFilter  SecurityContextHolderAwareRequestFilter  AnonymousAuthenticationFilter  SessionManagementFilter  ExceptionTranslationFilter  FilterSecurityInterceptor]************************************************************************************************************************Request received for POST &#39;/error?checkCode=neya&amp;randomStr=8a1ca2f6324261a4c7989e59773da07d&#39;:org.apache.catalina.core.ApplicationHttpRequest@3b651af0servletPath:/errorpathInfo:nullheaders:content-length: 149accept: application/json, text/plain, */*origin: http://192.168.232.239:31081user-agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36authorization: Basic aWNwLXVpOmhvdGVhbUAyMDE5timezone: 8content-type: application/x-www-form-urlencoded;charset=UTF-8referer: http://192.168.232.239:31081/user/login?redirect=%2Faccept-encoding: gzip, deflateaccept-language: en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7,es;q=0.6cookie: JSESSIONID=D6343899CA9DCEEA915300AC905FF141forwarded: proto=http;host=&quot;192.168.232.239:31081&quot;;for=&quot;10.42.0.0:15508&quot;x-forwarded-for: 10.42.0.0x-forwarded-proto: httpx-forwarded-prefix: /api/uaax-forwarded-port: 31081x-forwarded-host: 192.168.232.239:31081x-b3-traceid: 469d17bb77397455x-b3-spanid: 242ce814c10dd625x-b3-parentspanid: 469d17bb77397455x-b3-sampled: 0host: 10.42.3.181:19060Security filter chain: [  WebAsyncManagerIntegrationFilter  SecurityContextPersistenceFilter  HeaderWriterFilter  CorsFilter  LogoutFilter  RequestCacheAwareFilter  SecurityContextHolderAwareRequestFilter  AnonymousAuthenticationFilter  SessionManagementFilter  ExceptionTranslationFilter  FilterSecurityInterceptor]************************************************************</code></pre><p>貌似是在请求/oauth/token接口，导致了错误，重定向到/error上了。但是这样并不能体现错误出现在哪里。</p><h3 id="3-尝试重新部署服务"><a href="#3-尝试重新部署服务" class="headerlink" title="3. 尝试重新部署服务"></a>3. 尝试重新部署服务</h3><p>通过jenkins手动触发服务的部署，但是并没有发现与之前有什么不同，错误始终是这样！</p><h3 id="4-对比git中的代码日志记录"><a href="#4-对比git中的代码日志记录" class="headerlink" title="4. 对比git中的代码日志记录"></a>4. 对比git中的代码日志记录</h3><p>利用sourcetree工具查看提交日志记录，k8s中部署的是release分支的功能，相对比较稳定，对比develop分支，不涉及相关服务代码的修改。所以暂时认为代码是稳定一致的！</p><h3 id="5-查看jar包的依赖信息"><a href="#5-查看jar包的依赖信息" class="headerlink" title="5. 查看jar包的依赖信息"></a>5. 查看jar包的依赖信息</h3><p>下载解压jenkins中构建的jar包，与本地打包的jar包进行比较。这部分缺乏合适的工具，只能是针对jar包反编译后，使用jd-gui工具，查看反编译后的java代码，利用vscode进行对比，结果发现并没有什么不同。实际上这一步可以不这样做，因为git提交日志基本相同，maven配置一致，基本上可以确定依赖是一样的。所以同小伙伴查看了是否有maven相关的依赖未提交到私有的依赖仓库，确定所有的依赖信息均正常。</p><h3 id="6-查看服务部署时的部署文件"><a href="#6-查看服务部署时的部署文件" class="headerlink" title="6. 查看服务部署时的部署文件"></a>6. 查看服务部署时的部署文件</h3><p>查看这三个服务以及涉及的前端项目部署文件，主要查看环境变量设置，尤其是针对链接的配置项。截取这几个服务的配置信息如下：</p><pre><code>// user-uaa部署配置信息部分....        env:            - name: IP_ADDR              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: 192.168.232.194:18848            - name: NACOS_NAMESPACE              value: 4cca5bd9-730b-499b-828e-875b0c0478d8            - name: SKYWALKING_NAMESPACE              value: test-k8s-test            - name: SKYWALKING_TARGET_SERVICE_NAME              value: test-user-uaa            - name: SKYWALKING_IP_PORT              value: 192.168.232.163:11800            - name: CHANNEL              value: standalone// user-umc部署配置信息部分....        env:            - name: IP_ADDR              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: 192.168.232.194:18848            - name: NACOS_NAMESPACE              value: 4cca5bd9-730b-499b-828e-875b0c0478d8            - name: SKYWALKING_NAMESPACE              value: test-k8s-test            - name: SKYWALKING_TARGET_SERVICE_NAME              value: test-user-umc            - name: SKYWALKING_IP_PORT              value: 192.168.232.163:11800            - name: CHANNEL              value: standalone// opt-agent部署配置信息部分....        env:            - name: IP_ADDR              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: 192.168.232.194:18848            - name: NACOS_NAMESPACE              value: 4cca5bd9-730b-499b-828e-875b0c0478d8            - name: SKYWALKING_NAMESPACE              value: test-k8s-test            - name: SKYWALKING_TARGET_SERVICE_NAME              value: test-opt-agent            - name: SKYWALKING_IP_PORT              value: 192.168.232.163:11800            - name: CHANNEL              value: standalone// 前端测试信息....        env:            - name: GATEWAY_HOST              value: 192.168.232.241:31333              #value: gateway.test-basic-gateway:19020 </code></pre><p>查看配置信息一切正常，对nacos的链接，对Gateway的链接，均配置正确。</p><p>这个时候基本排除是构建方面产生的问题！</p><h3 id="7-打通k8s线上线下环境，流量引到线下，进行联调"><a href="#7-打通k8s线上线下环境，流量引到线下，进行联调" class="headerlink" title="7. 打通k8s线上线下环境，流量引到线下，进行联调"></a>7. 打通k8s线上线下环境，流量引到线下，进行联调</h3><p>参考<a href="http://192.168.232.159:8181/test/document/env-dev/blob/master/01.%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/k8s%E7%9B%B8%E5%85%B3%E6%96%87%E6%A1%A3/kt-connect%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8--%E5%AE%8C%E6%95%B4%E6%95%99%E7%A8%8B.md" target="_blank" rel="noopener">kt-connect使用</a>的文章，对出现错误的user-uaa服务进行联调。</p><p>需要修改新的配置信息，将原有的standalone和基础配置文件换到peer0，修改对外连接数据库的地址信息，如下：</p><pre class="line-numbers language-test-user-uaa.yml"><code class="language-test-user-uaa.yml">spring:  application:    name: test-user-uaa  datasource:    username: root    password: ht@MySQL135    driver-class-name: com.mysql.jdbc.Driver    url: jdbc:mysql://192.168.232.180:3306/test?characterEncoding=utf8&useUnicode=true&useSSL=false    druid:      initial-size: 5 # 初始大小      min-idle: 5  # 最小      max-active: 9  # 最大      max-wait: 60000  # 连接超时时间      time-between-eviction-runs-millis: 60000  # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒      min-evictable-idle-time-millis: 300000  # 指定一个空闲连接最少空闲多久后可被清除，单位是毫秒      validationQuery: select 'x'      test-while-idle: true  # 当连接空闲时，是否执行连接测试      test-on-borrow: false  # 当从连接池借用连接时，是否测试该连接      test-on-return: false  # 在连接归还到连接池时是否测试该连接      filters: config,wall,stat  # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙      poolPreparedStatements: true      maxPoolPreparedStatementPerConnectionSize: 20      maxOpenPreparedStatements: 20      web-stat-filter:        enabled: true        url-pattern: /*        exclusions: /druid/*,*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico        session-stat-enable: true        session-stat-max-count: 10      stat-view-servlet:        enabled: true        url-pattern: /druid/*        reset-enable: true        login-username: root        login-password: ht@MySQL135      filter:        wall:          enabled: true #配置wall filter          db-type: mysql          config:            alter-table-allow: false            truncate-allow: false            drop-table-allow: false            none-base-statement-allow: false #是否允许非以上基本语句的其他语句，缺省关闭，通过这个选项就能够屏蔽DDL。            update-where-none-check: true #检查UPDATE语句是否无where条件，这是有风险的，但不是SQL注入类型的风险            select-into-outfile-allow: false #SELECT ... INTO OUTFILE 是否允许，这个是mysql注入攻击的常见手段，缺省是禁止的            metadata-allow: true #是否允许调用Connection.getMetadata方法，这个方法调用会暴露数据库的表信息          log-violation: true #对被认为是攻击的SQL进行LOG.error输出          throw-exception: true #对被认为是攻击的SQL抛出SQLExcepton  cache:    caffeine:      spec: initialCapacity=50,maximumSize=500,expireAfterAccess=5s,expireAfterWrite=10s,refreshAfterWrite=5s  redis:     host: 192.168.232.172    port: 6379    password: ht@Redis579    timeout: 60000    database: 0    lettuce:      pool:        max-active: 8        max-wait: -1        max-idle: 8        min-idle: 0    jedis:      pool:        max-active: 8        max-wait: -1        max-idle: 8        min-idle: 0mybatis-plus:  mapper-locations: classpath:mybatis/mapper/*Mapper.xml  configuration:    cache-enabled: true    call-setters-on-nulls: true    log-impl: org.apache.ibatis.logging.stdout.StdOutImpleureka:  instance:    prefer-ip-address: true    instance-id: ${spring.application.name}@${spring.cloud.client.ip-address}:${server.port}    ipAddress: ${spring.cloud.client.ip-address}  client:    service-url:      defaultZone: http://test:test2019@192.168.232.239:31011/eureka/swagger:  enable: truelogging:  level:   com:    testsoft: debugdebug: truehystrix:    command:      default:        execution:         timeout:          enabled: true         isolation:            thread:              timeoutInMilliseconds: 30000  threadpool:      default:        coreSize: 1000feign:  client:    config:      default:        connectTimeout: 30000        readTimeout: 30000ribbon:  ReadTimeout: 30000  ConnectTimeout: 30000  MaxAutoRetries: 3  MaxAutoRetriesNextServer: 1server:  port: 19060<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>准备好配置文件，准备好影子镜像，启动本地的服务，进行联调，发现是可以正常登录的！</p><p>这时候已经有些懵圈了！配置信息基本相同，只是将内网连接改为真实的ip地址连接，以便于线下运行的需要！</p><h3 id="8-查看服务间是否有循环调用"><a href="#8-查看服务间是否有循环调用" class="headerlink" title="8. 查看服务间是否有循环调用"></a>8. 查看服务间是否有循环调用</h3><p>同小伙伴确定了，这三个微服务中的接口不存在循环调用的情况，也就不会出现接口互相调用导致服务出现问题的可能。</p><h3 id="9-对比开发库和测试库的数据信息，测试相同数据的结果"><a href="#9-对比开发库和测试库的数据信息，测试相同数据的结果" class="headerlink" title="9. 对比开发库和测试库的数据信息，测试相同数据的结果"></a>9. 对比开发库和测试库的数据信息，测试相同数据的结果</h3><p>尝试将开发库的用户信息，复制到测试库中，修改为相同的密码设置，同时在开发环境和测试环境进行登录，发现两个系统均可以登录，基本排除前后端代码因为加密的问题，导致无法登录的情况。</p><p>最后停止调试，恢复集群中的正常服务！</p><h3 id="10-查看服务的配置信息"><a href="#10-查看服务的配置信息" class="headerlink" title="10. 查看服务的配置信息"></a>10. 查看服务的配置信息</h3><p>前面对比了user-uaa服务新的peer0的配置文件同k8s测试环境原有配置文件的情况，基本一致，这次对比的时候，对比开发环境和k8s测试环境中的配置文件信息，发现多出了以下内容：</p><pre><code>hystrix:    command:      default:        execution:         timeout:          enabled: true         isolation:            thread:              timeoutInMilliseconds: 30000  threadpool:      default:        coreSize: 1000feign:  client:    config:      default:        connectTimeout: 30000        readTimeout: 30000ribbon:  ReadTimeout: 30000  ConnectTimeout: 30000  MaxAutoRetries: 3  MaxAutoRetriesNextServer: 1</code></pre><p>我尝试删除这些配置信息，重新启动程序，发现并不能解决这个问题。</p><p>但是我的小伙伴对比了umc服务中开发环境和k8s测试环境中的配置文件信息，也是多出了上述内容，删除后，k8s测试环境中的登录信息恢复正常！</p><p>世界太平了，正常了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol start="0"><li><p>合理的排查信息的流程性总结，梳理正常的排查流程。</p></li><li><p>加强全链路监控的建设，选择合适的技术栈，全面掌握其使用。</p></li><li><p>启用统一的日志管理系统，增强日志收集和筛选能力。</p></li><li><p>强化对微服务的监控手段，强化报警能力，设定阈值信息。</p></li><li><p>非线上环境，可以将服务缩减为单个，然后监控单个服务的日志情况。</p></li></ol><h2 id="配置信息解析"><a href="#配置信息解析" class="headerlink" title="配置信息解析"></a>配置信息解析</h2><p>重新贴一下“多余”的配置信息，并逐条进行解释设定，添加默认值：</p><pre><code>hystrix:    command:      default:        execution:         timeout:          enabled: true   # 默认启动hystrix熔断        isolation:            thread:              timeoutInMilliseconds: 30000 # 该属性用来配 置HystrixCommand执行的超时时间， 单位为毫秒。当HystrixCommand执行 时间超过该配置值之后， Hystrix会将该执行命令标记为TIMEOUT并进入服务降级 处理逻辑。默认值1000  threadpool:      default:        coreSize: 1000      # 用来控制Hystrix命令所属线程池的配置feign:  client:    config:      default:        connectTimeout: 30000  # 接口调用超时时间        readTimeout: 30000ribbon:  ReadTimeout: 30000  ConnectTimeout: 30000  MaxAutoRetries: 3          #同一台实例最大重试次数,不包括首次调用  MaxAutoRetriesNextServer: 1  #重试负载均衡其他的实例最大重试次数,不包括首次调用</code></pre><p>根据我们的集群环境，结合上述配置信息，怀疑问题在以下三点上：</p><ol><li>超时时间的设置不合理。</li><li>重试次数设置不合理。</li><li>k8s中对单个pod的资源限制，导致配置文件中的配置项出现了问题。</li></ol><p>需要针对上述信息进行重试！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s中的etcd同步问题</title>
      <link href="/2020/05/14/k8s-zhong-de-etcd-tong-bu-wen-ti/"/>
      <url>/2020/05/14/k8s-zhong-de-etcd-tong-bu-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="关于k8s集群中，各个节点资源进行富集的问题"><a href="#关于k8s集群中，各个节点资源进行富集的问题" class="headerlink" title="关于k8s集群中，各个节点资源进行富集的问题"></a>关于k8s集群中，各个节点资源进行富集的问题</h2><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>先看一下集群中的机器情况：</p><pre><code>$ kubectl get node -o wideNAME          STATUS   ROLES                      AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME192.168.232.232   Ready    controlplane,etcd,worker   57d   v1.17.2   192.168.232.232   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6192.168.232.233   Ready    worker                     14d   v1.17.2   192.168.232.233   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.18.1.el7.x86_64   docker://19.3.8192.168.232.234   Ready    worker                     14d   v1.17.2   192.168.232.234   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.18.1.el7.x86_64   docker://19.3.8192.168.232.239   Ready    controlplane,etcd,worker   57d   v1.17.2   192.168.232.239   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6192.168.232.241   Ready    controlplane,etcd,worker   57d   v1.17.2   192.168.232.241   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6</code></pre><p>再去看一下，集群的资源使用情况：</p><pre><code>$ kubectl top nodeNAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%192.168.232.232   3990m        99%    8257Mi          52%192.168.232.233   3889m        97%    8646Mi          54%192.168.232.234   382m         9%     7066Mi          44%192.168.232.239   775m         19%    7993Mi          50%192.168.232.241   643m         16%    5664Mi          35%</code></pre><p>明显看到，232、233两台机器负载较高，而后面的234、239、241负载压力非常低。这样导致了以下问题：</p><ul><li>pod启动缓慢</li><li>pod部署时，无法进行部署</li><li>pod做集群时，多个相同pod运行在同一节点上</li></ul><h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><p>问题在于，pod的部署策略存在问题，未能将replicas不为1的服务，均匀部署到其它机器上，导致pod出现了富集，整个集群性能下降，启动缓慢，部署难。不能发挥整个k8s的集群优势！</p><h3 id="解决方式和策略"><a href="#解决方式和策略" class="headerlink" title="解决方式和策略"></a>解决方式和策略</h3><p>我们目前集群中存在所有的微服务相关的基础服务，以及我们的业务服务。首先要针对集群中的各个node节点，进行标签的划分，然后通过针对node节点进行亲和性设置，部署在特定节点上。</p><p>其次针对需要进行集群化部署的服务，需要对相同的pod进行反亲和性设定，确保它们部署在不同的机器上，再根据前面划定的机器标签，部署在工作节点上。</p><p>最后，针对特定业务，例如工作流、表单、业务建模这样的，进行反亲和性设定，互不影响。其它业务不做强制设定。</p><p>主要的调整点如下：</p><ul><li>针对资源申请的限制，我们限定了内存的请求，尚未限制CPU的使用</li><li>亲和性和反亲和性的设置，针对Pod层面以及node层面</li><li>对Pod进行自动伸缩，目前先做到针对CPU和内存的自动伸缩</li></ul><p>具体实施，以Eureka为例，演示针对node进行亲和性设置。以用户相关的服务，uaa和umc来演示pod之间的反亲和性。</p><ol><li>Eureka的部署调整</li></ol><p>首先对所在节点设置标签，选取232,239,241为Eureka的部署机器，调整如下：</p><pre><code>// 调整节点，打标签$ kubectl label node 192.168.232.232 192.168.232.239 192.168.232.241 basic-eureka=eurekanode/192.168.232.232 labelednode/192.168.232.239 labelednode/192.168.232.241 labeled// 配置文件设定$ vim server-register-eureka-k8s.yaml---apiVersion: v1kind: Servicemetadata:  name: eureka  namespace: test-basic-eureka  labels:    app: eurekaspec:  ports:    - port: 19011      name: eureka      targetPort: 19011  clusterIP: None  selector:    app: eureka---apiVersion: apps/v1kind: StatefulSetmetadata:  name: eureka  namespace: test-basic-eurekaspec:  serviceName: &quot;eureka&quot;  replicas: 3  selector:    matchLabels:      app: eureka  template:    metadata:      labels:        app: eureka    spec:      affinity:                                        # 设置节点的亲和性        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:              - matchExpressions:                  - key: basic-eureka            # 根据标签的名称进行设置                    operator: In                    values:                      - eureka      containers:        - name: eureka          image: 192.168.232.159:5000/server-register-k8s:$BUILD_NUMBER          ports:            - containerPort: 19011          lifecycle:            preStop:              httpGet:                port: 19011                path: /spring/shutdown          livenessProbe:            httpGet:              path: /actuator/health              port: 19011            initialDelaySeconds: 180            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          readinessProbe:            httpGet:              path: /actuator/health              port: 19011            initialDelaySeconds: 180            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          resources:            limits:                       # 增加对CPU、内存使用的限制              memory: 2Gi              cpu: 500m            requests:              memory: 1.5Gi              cpu: 250m          env:            - name: IP_ADDR              #value: eureka-*.eureka-headless.test-basic-eureka.svc.cluster.local              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: 192.168.232.194:18848            - name: NACOS_NAMESPACE              value: 4cca5bd9-730b-49sadfa32e-875b0c0478d8            - name: SKYWALKING_NAMESPACE              value: test-k8s-test            - name: SKYWALKING_TARGET_SERVICE_NAME              value: test-user-umc            - name: SKYWALKING_IP_PORT              value: 192.168.232.163:11800            - name: CHANNEL              value: standalone  podManagementPolicy: &quot;Parallel&quot;</code></pre><p>设置完成后，我们还需要对node节点进行调整才能进行部署，由于CPU占用资源过多的问题，需要我们对pod进行筛选：</p><pre><code>// 查看资源占用$ kubectl top pod -n test-all-serviceNAME                                         CPU(cores)   MEMORY(bytes)test-data-dictionary-7c7cf58696-7zrwd   11m          463Mitest-filestore-opt-84c5976859-l6w8q     9m           484Mitest-filestore-uc-8568f58df-2zggs       9m           493Mitest-form-design-d546c579d-wk8jt        342m         80Mitest-frontend-795b45b99d-tqhzb          0m           334Mitest-frontend-user-6c75fc8c4d-qnqdw     0m           334Mitest-id-generator-59ccb5b667-bbp4k      18m          421Mitest-id-generator-59ccb5b667-qqfj5      10m          425Mitest-id-generator-59ccb5b667-z62d2      13m          390Mitest-opt-agent-58d8dd9cdd-7d4rv         368m         154Mitest-opt-agent-58d8dd9cdd-df74m         9m           412Mitest-opt-agent-58d8dd9cdd-n6859         13m          410Mitest-opt-center-85899c4cc-dql54         3642m        971Mitest-opt-uaa-b4f6784cd-m6cx4            17m          419Mitest-user-uaa-58d65c79c-c4nhg           30m          438Mitest-user-uaa-58d65c79c-ptvwv           16m          420Mitest-user-uaa-58d65c79c-sx48w           27m          427Mitest-user-umc-8dddc6-7njpb              12m          423Mitest-user-umc-8dddc6-f8sdv              0m           0Mitest-user-umc-8dddc6-nxzdr              9m           410Mitest-visual-model-56d4447bdd-6xrjp      1011m        931Mitest-workflow-855f6c4496-zlj4s          1372m        896Mi// 将资源占用过大的服务进行缩减$ kubectl scale deploy test-visual-model --replicas=0 -n test-all-service// 同理，针对内部占用资源过多的情况，进行缩减// 最终结果$ kubectl top nodeNAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%192.168.232.232   1356m        33%    7651Mi          48%192.168.232.233   292m         7%     7732Mi          48%192.168.232.234   290m         7%     7099Mi          44%192.168.232.239   584m         14%    7846Mi          49%192.168.232.241   262m         6%     5860Mi          37%</code></pre><p>缩减完成后，重新提交Eureka的部署配置信息，进行CI/CD构建部署。</p><p>最终结果如下：</p><pre><code>$ kubectl get pod -A -o wide  | grep eurekatest-basic-eureka    eureka-0                                                   1/1     Running            0          3m37s   10.42.0.253   192.168.232.241   &lt;none&gt;           &lt;none&gt;test-basic-eureka    eureka-1                                                   1/1     Running            0          7m21s   10.42.2.91    192.168.232.232   &lt;none&gt;           &lt;none&gt;test-basic-eureka    eureka-2                                                   1/1     Running            0          10m     10.42.1.16    192.168.232.239   &lt;none&gt;           &lt;none&gt;</code></pre><p>调整部署后，eureka对应的三台服务器已经部署到含有”basic-eureka=eureka”标签的机器上了。</p><p>那么gateway部分来个照此办理，这里只贴gateway的配置了。如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> gateway  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>gateway  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> gateway<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19020</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> tcp      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">19020</span>      <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">31333</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> gateway<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> gateway  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>gateway<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10             </span><span class="token comment" spellcheck="true"># 等待10秒后进行操作</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10        </span><span class="token comment" spellcheck="true"># 保留10个滚动版本的历史记录</span>  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate           <span class="token comment" spellcheck="true"># 滚动更新配置</span>    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0           </span><span class="token comment" spellcheck="true"># 保证高可用，不允许有无法使用的pod</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> gateway  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> gateway    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>                                        <span class="token comment" spellcheck="true"># 设置节点的亲和性</span>        <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>          <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>            <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>              <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                  <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> basic<span class="token punctuation">-</span>eureka            <span class="token comment" spellcheck="true"># 根据标签的名称进行设置</span>                    <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                    <span class="token key atrule">values</span><span class="token punctuation">:</span>                      <span class="token punctuation">-</span> eureka      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> gateway          <span class="token key atrule">image</span><span class="token punctuation">:</span> 192.168.232.159<span class="token punctuation">:</span>5000/gateway<span class="token punctuation">-</span>k8s<span class="token punctuation">:</span>$BUILD_NUMBER          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>            <span class="token key atrule">preStop</span><span class="token punctuation">:</span>              <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>                <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19020</span>                <span class="token key atrule">path</span><span class="token punctuation">:</span> /spring/shutdown          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19020</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19020</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">resources</span><span class="token punctuation">:</span>            <span class="token key atrule">requests</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi              <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 500m            <span class="token key atrule">limits</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1.5Gi              <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 750m          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">19020</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> IP_ADDR              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> status.podIP            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_IP              <span class="token key atrule">value</span><span class="token punctuation">:</span> 192.168.232.194<span class="token punctuation">:</span><span class="token number">18848</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> 4cca5bd9<span class="token punctuation">-</span>730b<span class="token punctuation">-</span>499b<span class="token punctuation">-</span>828e<span class="token punctuation">-</span>adsafadfs78d8            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>k8s<span class="token punctuation">-</span>test            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_TARGET_SERVICE_NAME              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>gateway            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_IP_PORT              <span class="token key atrule">value</span><span class="token punctuation">:</span> 192.168.232.163<span class="token punctuation">:</span><span class="token number">11800</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CHANNEL              <span class="token key atrule">value</span><span class="token punctuation">:</span> standalone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>UAA服务和UMC的部署调整</li></ol><p>针对USER-UAA以及UMC服务二者进行调整，均匀部署到标签为”deploy=worker”的node节点上。opt相关的三个服务，以同样的方式进行处理。首先设置node节点所在标签，如下：</p><pre><code>$ kubectl label node 192.168.232.233 192.168.232.234 192.168.232.241 deploy=workernode/192.168.232.233 labelednode/192.168.232.234 labelednode/192.168.232.241 labeled</code></pre><p>以UMC项目为例子进行部署，既要使用node的亲和性，又要使用pod之间的反亲和性。配置信息如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19070</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> tcp      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">19070</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>all<span class="token punctuation">-</span>service<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>        <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>          <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 弱化亲和性</span>            <span class="token punctuation">-</span> <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>                <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname               <span class="token comment" spellcheck="true"># 根据pod的标签进行设置反亲和性</span>                <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                   <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                    <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app                      <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                      <span class="token key atrule">values</span><span class="token punctuation">:</span>                        <span class="token punctuation">-</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc              <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">1</span>        <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>          <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 强制亲和性</span>            <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>              <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                  <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> deploy            <span class="token comment" spellcheck="true"># 根据标签的名称进行设置亲和性</span>                    <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                    <span class="token key atrule">values</span><span class="token punctuation">:</span>                      <span class="token punctuation">-</span> worker      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> user<span class="token punctuation">-</span>umc          <span class="token key atrule">image</span><span class="token punctuation">:</span> 192.168.232.159<span class="token punctuation">:</span>5000/user<span class="token punctuation">-</span>umc<span class="token punctuation">-</span>k8s<span class="token punctuation">:</span>$BUILD_NUMBER          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>            <span class="token key atrule">preStop</span><span class="token punctuation">:</span>              <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>                <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19070</span>                <span class="token key atrule">path</span><span class="token punctuation">:</span> /spring/shutdown          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19070</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19070</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">resources</span><span class="token punctuation">:</span>            <span class="token key atrule">requests</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi              <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 350m            <span class="token key atrule">limits</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1.5Gi              <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 1000m          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">19070</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> IP_ADDR              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> status.podIP            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_IP              <span class="token key atrule">value</span><span class="token punctuation">:</span> 192.168.232.194<span class="token punctuation">:</span><span class="token number">18848</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> 4cca5bd9<span class="token punctuation">-</span>730b<span class="token punctuation">-</span>499b<span class="token punctuation">-</span><span class="token number">828e-87590878986</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>k8s<span class="token punctuation">-</span>test            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_TARGET_SERVICE_NAME              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>user<span class="token punctuation">-</span>umc            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> SKYWALKING_IP_PORT              <span class="token key atrule">value</span><span class="token punctuation">:</span> 192.168.232.163<span class="token punctuation">:</span><span class="token number">11800</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CHANNEL              <span class="token key atrule">value</span><span class="token punctuation">:</span> standalone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="最终总结"><a href="#最终总结" class="headerlink" title="最终总结"></a>最终总结</h3><ul><li><p>什么是亲和性和反亲和性？</p></li><li><p>pod部署需要注意的事项？如何根据场景进行决定？</p></li><li><p>番外/参考链接</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch集群化安装</title>
      <link href="/2020/05/14/elasticsearch-ji-qun-hua-an-zhuang/"/>
      <url>/2020/05/14/elasticsearch-ji-qun-hua-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h1 id="ElasticSearch集群化安装"><a href="#ElasticSearch集群化安装" class="headerlink" title="ElasticSearch集群化安装"></a>ElasticSearch集群化安装</h1><h2 id="机器准备"><a href="#机器准备" class="headerlink" title="机器准备"></a>机器准备</h2><table><thead><tr><th>主机地址</th><th>节点功能</th></tr></thead><tbody><tr><td>192.168.232.193</td><td>从节点</td></tr><tr><td>192.168.232.194</td><td>主节点</td></tr><tr><td>192.168.232.191</td><td>从节点</td></tr></tbody></table><h2 id="安装工具版本"><a href="#安装工具版本" class="headerlink" title="安装工具版本"></a>安装工具版本</h2><ul><li><p>操作系统：centos7.6</p></li><li><p>elasticsearch: 7.6.2</p></li></ul><h2 id="正式安装设置"><a href="#正式安装设置" class="headerlink" title="正式安装设置"></a>正式安装设置</h2><h3 id="0-解压并转移到-usr-local路径下"><a href="#0-解压并转移到-usr-local路径下" class="headerlink" title="0. 解压并转移到/usr/local路径下"></a>0. 解压并转移到/usr/local路径下</h3><pre><code>$ cd ~/skywalking-install/$ tar -zxvf elasticsearch-7.6.2-linux-x86_64.tar.gz$ sudo mv elasticsearch-7.6.2 /usr/local/</code></pre><h3 id="1-开放端口–9200、9300"><a href="#1-开放端口–9200、9300" class="headerlink" title="1. 开放端口–9200、9300"></a>1. 开放端口–9200、9300</h3><pre><code>$ sudo firewall-cmd --zone=public --add-port=9200/tcp --permanentsuccess$ sudo firewall-cmd --zone=public --add-port=9300/tcp --permanentsuccess$ sudo firewall-cmd --zone=public --add-port=9300/udp --permanentsuccess$ sudo firewall-cmd --zone=public --add-port=9200/udp --permanentsuccess$ sudo firewall-cmd --reloadsuccess</code></pre><h3 id="2-创建用户"><a href="#2-创建用户" class="headerlink" title="2. 创建用户"></a>2. 创建用户</h3><pre><code>$ sudo groupadd elasticsearch$ sudo useradd elasticsearch -g elasticsearch -p elasticsearch$ sudo passwd elasticsearchChanging password for user elasticsearch.New password: Retype new password:// 密码为ht@es123// 修改目录归属$ cd /usr/local/$ lsbin  elasticsearch-7.6.2  etc  games  include  lib  lib64  libexec  sbin  share  src$ sudo chown -R elasticsearch:elasticsearch elasticsearch-7.6.2/$ cd elasticsearch-7.6.2/$ ls -altotal 548drwxr-xr-x.  9 elasticsearch elasticsearch    155 Mar 26 14:36 .drwxr-xr-x. 13 root          root             158 Apr 20 11:17 ..drwxr-xr-x.  2 elasticsearch elasticsearch   4096 Apr 20 11:16 bindrwxr-xr-x.  2 elasticsearch elasticsearch    148 Mar 26 14:36 configdrwxr-xr-x.  9 elasticsearch elasticsearch    107 Mar 26 14:36 jdkdrwxr-xr-x.  3 elasticsearch elasticsearch   4096 Mar 26 14:36 lib-rw-r--r--.  1 elasticsearch elasticsearch  13675 Mar 26 14:28 LICENSE.txtdrwxr-xr-x.  2 elasticsearch elasticsearch      6 Mar 26 14:36 logsdrwxr-xr-x. 38 elasticsearch elasticsearch   4096 Mar 26 14:37 modules-rw-r--r--.  1 elasticsearch elasticsearch 523209 Mar 26 14:36 NOTICE.txtdrwxr-xr-x.  2 elasticsearch elasticsearch      6 Mar 26 14:36 plugins-rw-r--r--.  1 elasticsearch elasticsearch   8164 Mar 26 14:28 README.asciidoc</code></pre><h3 id="3-服务器参数设定"><a href="#3-服务器参数设定" class="headerlink" title="3. 服务器参数设定"></a>3. 服务器参数设定</h3><p>3.1 修改jvm</p><pre><code>// 切换用户，确保执行$ su elasticsearch// 创建数据目录$ mkdir data$ vim config/jvm.options// 修改下面两行信息-Xms10g-Xmx10g</code></pre><p>3.2 修改vm内存权限</p><pre><code>$ su centos$ sudo vim /etc/sysctl.conf// 追加以下信息vm.max_map_count=262144$ sudo sysctl -p// 执行后输出信息vm.max_map_count=262144</code></pre><p>3.3 修改切换到root用户修改配置limits.conf，添加文件打开数的配置</p><pre><code>$ sudo vim /etc/security/limits.conf// 追加以下信息* hard nofile 65536* soft nofile 65536</code></pre><p>后续切换到es的用户。进行启动</p><h3 id="4-配置文件设定"><a href="#4-配置文件设定" class="headerlink" title="4. 配置文件设定"></a>4. 配置文件设定</h3><p>切换到192.168.232.194机器上进行配置：</p><pre><code>$ cd /usr/local/$ su elasticsearch$ cd elasticsearch/$ sudo vim config/elasticsearch.ymlcluster.name: es-masternode.name: node-1node.attr.rack: r1path.data: /usr/local/elasticsearch-7.6.2/datapath.logs: /usr/local/elasticsearch-7.6.2/logs#bootstrap.memory_lock: truenetwork.host: 192.168.232.194http.port: 9200discovery.seed_hosts: [&quot;192.168.232.191:9300&quot;, &quot;192.168.232.193:9300&quot;]#index.number_of_shards: 3#index.number_of_replicas: 1transport.tcp.port: 9300cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;, &quot;node-3&quot;]discovery.zen.minimum_master_nodes: 2 gateway.recover_after_nodes: 3http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;</code></pre><p>配置名词解释<br> cluster.name: es-cluster #指定es集群名<br> node.name: xxxx #指定当前es节点名<br> node.data: false #非数据节点<br> node.master: false #非master节点<br> node.attr.rack: r1 #自定义的属性,这是官方文档中自带的<br> bootstrap.memory_lock: true #开启启动es时锁定内存<br> network.host: 172.17.0.5 #当前节点的ip地址<br> http.port: 9200 #设置当前节点占用的端口号，默认9200<br> discovery.seed_hosts: [“172.17.0.4:9300”,”172.17.0.2:9300”] #启动当前es节点时会去这个ip列表中去发现其他节点，此处不需配置自己节点的ip,这里支持ip和ip:port形式,不加端口号默认使用ip:9300去发现节点<br> cluster.initial_master_nodes: [“node-1”, “node-2”, “node-3”] #可作为master节点初始的节点名称,tribe-node不在此列<br> gateway.recover_after_nodes: 2 #设置集群中N个节点启动时进行数据恢复，默认为1。可选<br> path.data: /path/to/path  #数据保存目录<br> path.logs: /path/to/path #日志保存目录<br> transport.tcp.port: 9300 #设置集群节点发现的端口<br> index.number_of_replicas: 1 是数据备份数，如果只有一台机器，设置为0<br> index.number_of_shards: 3  是数据分片数，默认为5，有时候设置为3<br> discovery.zen.minimum_master_nodes: 2 # 通过配置大多数节点(节点总数/ 2 + 1)来防止脑裂<br> gateway.recover_after_nodes: 3 # 在一个完整的集群重新启动到N个节点开始之前，阻止初始恢复<br> http.cors.enabled: true   # 允许跨域的设置<br> http.cors.allow-origin: “*” # 允许跨域的设置</p><p>这里不去配置bootstrap.memory_lock，以及index.number_of_replicas、index.number_of_shards这三个选项，配置后导致ES无法启动，有异常日志！</p><p>切换到192.168.232.193机器上配置为：</p><pre><code>cluster.name: es-masternode.name: node-2node.attr.rack: r1path.data: /usr/local/elasticsearch-7.6.2/datapath.logs: /usr/local/elasticsearch-7.6.2/logs#bootstrap.memory_lock: truenetwork.host: 192.168.232.193http.port: 9200#index.number_of_shards: 3#index.number_of_replicas: 1discovery.seed_hosts: [&quot;192.168.232.194:9300&quot;, &quot;192.168.232.191:9300&quot;]cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;, &quot;node-3&quot;]discovery.zen.minimum_master_nodes: 2gateway.recover_after_nodes: 3</code></pre><p>切换到192.168.232.191机器上配置为：</p><pre><code>cluster.name: es-masternode.name: node-3node.attr.rack: r1path.data: /usr/local/elasticsearch-7.6.2/datapath.logs: /usr/local/elasticsearch-7.6.2/logs#bootstrap.memory_lock: truenetwork.host: 192.168.232.191http.port: 9200#index.number_of_shards: 3#index.number_of_replicas: 1discovery.seed_hosts: [&quot;192.168.232.194:9300&quot;, &quot;192.168.232.193:9300&quot;]cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;, &quot;node-3&quot;]discovery.zen.minimum_master_nodes: 2gateway.recover_after_nodes: 3</code></pre><h3 id="5-启动不同机器的ES"><a href="#5-启动不同机器的ES" class="headerlink" title="5. 启动不同机器的ES"></a>5. 启动不同机器的ES</h3><p>在三台机器上同时进行启动</p><pre><code>$ su elasticsearch$ cd /usr/local/elasticsearch-7.6.2/bin$ ./elasticsearch -d</code></pre><p>最后进行测试，访问<a href="http://192.168.232.194:9200/_cat/nodes?pretty地址，查看es的信息。输出如下：" target="_blank" rel="noopener">http://192.168.232.194:9200/_cat/nodes?pretty地址，查看es的信息。输出如下：</a></p><pre><code>192.168.232.193 8 64 3 0.51 0.34 0.38 dilm - node-2192.168.232.194 3 51 0 0.00 0.09 0.16 dilm * node-1192.168.232.191 3 59 1 0.00 0.07 0.11 dilm - node-3</code></pre><p>看到主节点确定在192.168.232.194机器上，说明选举成功，集群已经启动！</p><p>针对各台机器进行验证，如下：</p><pre><code>$ curl http://192.168.232.191:9200/_cat/health?vepoch      timestamp cluster   status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1587395840 15:17:20  es-master green           3         3      0   0    0    0        0             0                  -                100.0%$ curl http://192.168.232.193:9200/_cat/health?vepoch      timestamp cluster   status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1587395840 15:17:20  es-master green           3         3      0   0    0    0        0             0                  -                100.0%$ curl http://192.168.232.194:9200/_cat/health?vepoch      timestamp cluster   status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1587395840 15:17:20  es-master green           3         3      0   0    0    0        0             0                  -                100.0%</code></pre><h2 id="必要插件安装"><a href="#必要插件安装" class="headerlink" title="必要插件安装"></a>必要插件安装</h2><ol><li>安装head插件</li></ol><p>可以在开发机上，拉取head前端代码，并运行，操作如下：</p><pre><code>$ git clone git://github.com/mobz/elasticsearch-head.git$ cd elasticsearch-head$ npm install --registry https://registry.npm.taobao.org$ npm run start</code></pre><p>安装完成后，从浏览器访问<a href="http://localhost:9100/即可。如下图：" target="_blank" rel="noopener">http://localhost:9100/即可。如下图：</a></p><p><img src="%E9%A6%96%E9%A1%B5.png" alt></p><p>填写主节点地址到输入框中，即可进行访问，效果如下：</p><p><img src="%E7%99%BB%E9%99%86%E5%90%8E%E9%A1%B5%E9%9D%A2.png" alt></p><p>注意：ES节点一定要配置允许跨域访问，如下：</p><pre><code>http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;</code></pre><p>否则前端无法进行访问。</p><ol start="2"><li>安装ik分词器</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>安装并不困难，但是在访问的时候需要配置跨域访问的内容。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>项目迁移到k8s的配置修改和问题记录</title>
      <link href="/2020/05/11/guan-yu-wei-fu-wu-ji-qun-ip-di-zhi-qian-yi-de-nei-rong/"/>
      <url>/2020/05/11/guan-yu-wei-fu-wu-ji-qun-ip-di-zhi-qian-yi-de-nei-rong/</url>
      
        <content type="html"><![CDATA[<h1 id="关于ip地址迁移的内容"><a href="#关于ip地址迁移的内容" class="headerlink" title="关于ip地址迁移的内容"></a>关于ip地址迁移的内容</h1><h2 id="涉及到修改IP地址的内容"><a href="#涉及到修改IP地址的内容" class="headerlink" title="涉及到修改IP地址的内容"></a>涉及到修改IP地址的内容</h2><ol><li>代码的硬编码中包含的ip地址信息</li></ol><p>包括目前在代码中的硬编码的ip地址</p><ol start="2"><li>关于nacos配置文件中的配置的ip地址信息</li></ol><p>包括所有服务涉及的ip地址信息，例如数据库、缓存等连接地址</p><ol start="3"><li>各个工具内部设置的ip地址</li></ol><p>包括：</p><ul><li>jenkins中配置的ip地址信息，包括各个服务配置中的ip地址、总体configure配置中的ip地址</li><li>gitlab中配置的ip地址信息，包括webhook配置的URL</li><li>其它待补充（nexus、registry、sonarqube等等）</li></ul><ol start="4"><li>Linux服务器中涉及的IP地址</li></ol><p>包括：</p><ul><li>/etc/hosts中的ip地址</li><li>docker配置，/etc/docker/daemon.json</li><li>mysql、redis、mongodb中的ip配置</li><li>其它待补充</li></ul><ol start="5"><li>待增加</li></ol><h2 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h2><ol><li>备份信息</li></ol><ul><li>gitlab备份，代码+镜像</li><li>jenkins备份，插件列表+项目配置</li><li>数据库备份</li></ul><ol start="2"><li>重建k8s集群</li></ol><p>IP地址更改导致证书以及集群环境的变化，导致k8s必须重新进行部署</p><ol start="3"><li>是否能够分批进行地址迁移？</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>整体平台性能提升方式</title>
      <link href="/2020/05/10/zheng-ti-ping-tai-xing-neng-ti-sheng-fang-shi/"/>
      <url>/2020/05/10/zheng-ti-ping-tai-xing-neng-ti-sheng-fang-shi/</url>
      
        <content type="html"><![CDATA[<h1 id="整体平台性能提升方式"><a href="#整体平台性能提升方式" class="headerlink" title="整体平台性能提升方式"></a>整体平台性能提升方式</h1><h2 id="整体上–核心大改"><a href="#整体上–核心大改" class="headerlink" title="整体上–核心大改"></a>整体上–核心大改</h2><h3 id="响应式编程"><a href="#响应式编程" class="headerlink" title="响应式编程"></a>响应式编程</h3><p>Vert.x</p><h3 id="事件驱动"><a href="#事件驱动" class="headerlink" title="事件驱动"></a>事件驱动</h3><p>消息队列–kafka、RocketMQ</p><h2 id="具体实施"><a href="#具体实施" class="headerlink" title="具体实施"></a>具体实施</h2><h3 id="缓存提升"><a href="#缓存提升" class="headerlink" title="缓存提升"></a>缓存提升</h3><ul><li>分布式缓存的使用，缓存集群，多级缓存的使用</li><li>硬件提升（SSD、CPU超频）</li></ul><h3 id="减少调用链路信息"><a href="#减少调用链路信息" class="headerlink" title="减少调用链路信息"></a>减少调用链路信息</h3><p>尽可能减少单个服务接口的后向调用接口数量，从全链路层面进行限定</p><h3 id="提升核心服务地位"><a href="#提升核心服务地位" class="headerlink" title="提升核心服务地位"></a>提升核心服务地位</h3><p>对核心服务进行资源倾斜，发挥整个集群的性能优势，从部署方式和服务的</p><h3 id="提升单个服务承载能力"><a href="#提升单个服务承载能力" class="headerlink" title="提升单个服务承载能力"></a>提升单个服务承载能力</h3><p>提升单个服务对于资源的限定，同上述进行权衡</p><h3 id="服务动态伸缩"><a href="#服务动态伸缩" class="headerlink" title="服务动态伸缩"></a>服务动态伸缩</h3><ul><li>根据CPU和内存限定进行伸缩</li><li>根据具体的参数进行伸缩</li></ul><h3 id="提升服务的启动时间"><a href="#提升服务的启动时间" class="headerlink" title="提升服务的启动时间"></a>提升服务的启动时间</h3><p>降低服务启动时间，更换虚拟机，优化镜像打包</p><h3 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h3><p>减少HTTP相关调用，转为使用消息队列的方式进行传递</p>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装dashboard以及规避google限制拉取镜像</title>
      <link href="/2020/04/18/an-zhuang-dashboard-yi-ji-gui-bi-google-xian-zhi-la-qu-jing-xiang/"/>
      <url>/2020/04/18/an-zhuang-dashboard-yi-ji-gui-bi-google-xian-zhi-la-qu-jing-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="dashboard安装以及规避google限制拉取镜像"><a href="#dashboard安装以及规避google限制拉取镜像" class="headerlink" title="dashboard安装以及规避google限制拉取镜像"></a>dashboard安装以及规避google限制拉取镜像</h1><h2 id="工具链"><a href="#工具链" class="headerlink" title="工具链"></a>工具链</h2><ul><li>dashboard版本–2.0.0-rc7</li><li>kubernetes版本–1.17.2</li></ul><h2 id="下载官方部署文件"><a href="#下载官方部署文件" class="headerlink" title="下载官方部署文件"></a>下载官方部署文件</h2><p>找到<a href="https://github.com/kubernetes/dashboard/releases" target="_blank" rel="noopener">官方github仓库</a>地址，转到dashboard/aio/deploy目录下，也可以直接访问<a href="https://github.com/kubernetes/dashboard/tree/master/aio/deploy" target="_blank" rel="noopener">该链接</a>。下载recommended.yaml文件，网络不好的时候也可以打开该文件，直接复制里面的内容，创建即可。</p><p>下载文件完成后，需要编辑该文件，主要更改的点在于：</p><ul><li>对Service设置nodePort方式对外暴露端口</li><li>调整镜像拉取策略，从Always更换为IfNotPresent</li></ul><p>更改完成之后的配置文件内容如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># Copyright 2017 The Kubernetes Authors.</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># Licensed under the Apache License, Version 2.0 (the "License");</span><span class="token comment" spellcheck="true"># you may not use this file except in compliance with the License.</span><span class="token comment" spellcheck="true"># You may obtain a copy of the License at</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># Unless required by applicable law or agreed to in writing, software</span><span class="token comment" spellcheck="true"># distributed under the License is distributed on an "AS IS" BASIS,</span><span class="token comment" spellcheck="true"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="token comment" spellcheck="true"># See the License for the specific language governing permissions and</span><span class="token comment" spellcheck="true"># limitations under the License.</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Namespace<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">443</span>      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8443</span>      <span class="token key atrule">nodePort</span><span class="token punctuation">:</span> <span class="token number">30001</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>csrf  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token key atrule">data</span><span class="token punctuation">:</span>  <span class="token key atrule">csrf</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Secret<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>key<span class="token punctuation">-</span>holder  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">type</span><span class="token punctuation">:</span> Opaque<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ConfigMap<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>settings  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Role<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Allow Dashboard to get, update and delete Dashboard exclusive secrets.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"secrets"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"kubernetes-dashboard-key-holder"</span><span class="token punctuation">,</span> <span class="token string">"kubernetes-dashboard-certs"</span><span class="token punctuation">,</span> <span class="token string">"kubernetes-dashboard-csrf"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">,</span> <span class="token string">"update"</span><span class="token punctuation">,</span> <span class="token string">"delete"</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"configmaps"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"kubernetes-dashboard-settings"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">,</span> <span class="token string">"update"</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Allow Dashboard to get metrics.</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"services"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"heapster"</span><span class="token punctuation">,</span> <span class="token string">"dashboard-metrics-scraper"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"proxy"</span><span class="token punctuation">]</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">""</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"services/proxy"</span><span class="token punctuation">]</span>    <span class="token key atrule">resourceNames</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"heapster"</span><span class="token punctuation">,</span> <span class="token string">"http:heapster:"</span><span class="token punctuation">,</span> <span class="token string">"https:heapster:"</span><span class="token punctuation">,</span> <span class="token string">"dashboard-metrics-scraper"</span><span class="token punctuation">,</span> <span class="token string">"http:dashboard-metrics-scraper"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">]</span><span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">rules</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Allow Metrics Scraper to get metrics from the Metrics server</span>  <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"metrics.k8s.io"</span><span class="token punctuation">]</span>    <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"pods"</span><span class="token punctuation">,</span> <span class="token string">"nodes"</span><span class="token punctuation">]</span>    <span class="token key atrule">verbs</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"get"</span><span class="token punctuation">,</span> <span class="token string">"list"</span><span class="token punctuation">,</span> <span class="token string">"watch"</span><span class="token punctuation">]</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> RoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> Role  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> rbac.authorization.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRoleBinding<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">roleRef</span><span class="token punctuation">:</span>  <span class="token key atrule">apiGroup</span><span class="token punctuation">:</span> rbac.authorization.k8s.io  <span class="token key atrule">kind</span><span class="token punctuation">:</span> ClusterRole  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">subjects</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">kind</span><span class="token punctuation">:</span> ServiceAccount    <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard    <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard          <span class="token key atrule">image</span><span class="token punctuation">:</span> kubernetesui/dashboard<span class="token punctuation">:</span>v2.0.0<span class="token punctuation">-</span>rc7          <span class="token comment" spellcheck="true">#imagePullPolicy: Always</span>          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> IfNotPresent          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8443</span>              <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP          <span class="token key atrule">args</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>auto<span class="token punctuation">-</span>generate<span class="token punctuation">-</span>certificates            <span class="token punctuation">-</span> <span class="token punctuation">-</span><span class="token punctuation">-</span>namespace=kubernetes<span class="token punctuation">-</span>dashboard            <span class="token comment" spellcheck="true"># Uncomment the following line to manually specify Kubernetes API server Host</span>            <span class="token comment" spellcheck="true"># If not specified, Dashboard will attempt to auto discover the API server and connect</span>            <span class="token comment" spellcheck="true"># to it. Uncomment only if the default does not work.</span>            <span class="token comment" spellcheck="true"># - --apiserver-host=http://my-address:port</span>          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /certs              <span class="token comment" spellcheck="true"># Create on-disk volume to store exec logs</span>            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /tmp              <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">scheme</span><span class="token punctuation">:</span> HTTPS              <span class="token key atrule">path</span><span class="token punctuation">:</span> /              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8443</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">1001</span>            <span class="token key atrule">runAsGroup</span><span class="token punctuation">:</span> <span class="token number">2001</span>      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs          <span class="token key atrule">secret</span><span class="token punctuation">:</span>            <span class="token key atrule">secretName</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token punctuation">-</span>certs        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">"beta.kubernetes.io/os"</span><span class="token punctuation">:</span> linux      <span class="token comment" spellcheck="true"># Comment the following tolerations if Dashboard must not be deployed on master</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master          <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">name</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8000</span>      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8000</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper<span class="token punctuation">---</span><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">name</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper      <span class="token key atrule">annotations</span><span class="token punctuation">:</span>        <span class="token key atrule">seccomp.security.alpha.kubernetes.io/pod</span><span class="token punctuation">:</span> <span class="token string">'runtime/default'</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> dashboard<span class="token punctuation">-</span>metrics<span class="token punctuation">-</span>scraper          <span class="token key atrule">image</span><span class="token punctuation">:</span> kubernetesui/metrics<span class="token punctuation">-</span>scraper<span class="token punctuation">:</span>v1.0.4          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8000</span>              <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">scheme</span><span class="token punctuation">:</span> HTTP              <span class="token key atrule">path</span><span class="token punctuation">:</span> /              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8000</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span>          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /tmp            <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>            <span class="token key atrule">readOnlyRootFilesystem</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>            <span class="token key atrule">runAsUser</span><span class="token punctuation">:</span> <span class="token number">1001</span>            <span class="token key atrule">runAsGroup</span><span class="token punctuation">:</span> <span class="token number">2001</span>      <span class="token key atrule">serviceAccountName</span><span class="token punctuation">:</span> kubernetes<span class="token punctuation">-</span>dashboard      <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span>        <span class="token key atrule">"beta.kubernetes.io/os"</span><span class="token punctuation">:</span> linux      <span class="token comment" spellcheck="true"># Comment the following tolerations if Dashboard must not be deployed on master</span>      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master          <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> tmp<span class="token punctuation">-</span>volume          <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="重做dashboard相关的镜像"><a href="#重做dashboard相关的镜像" class="headerlink" title="重做dashboard相关的镜像"></a>重做dashboard相关的镜像</h2><p>由于众所周知的网络问题，当需要去拉取相应镜像的时候，非常耗时且存在无法拉取的情况，总是在dockerhub上捡别人现成的镜像也不好。这时需要制作自己的docker镜像。</p><h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>准备好自己的github账号，并且登录，然后注册一个<a href="https://hub.docker.com/" target="_blank" rel="noopener">DockerHub</a>的账号。DockerHub可能存在访问慢的问题，请自行科学上网。</p><h3 id="创建github项目"><a href="#创建github项目" class="headerlink" title="创建github项目"></a>创建github项目</h3><p>以kubernetesui/dashboard:v2.0.0-rc7镜像为例子进行创建，首先在github上创建空项目，项目名称为：k8s-dashboard-2.0.0-rc7，然后克隆到本地进行操作。</p><p>在项目目录下新增一个Dockerfile文件，并进行编辑，添加下面的两行信息：</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile">FROM kubernetesui/dashboard:v2.0.0-rc7MAINTAINER lsy1234567@163.com<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>保存后将文件提交版本控制，并推送到github仓库中，这时候我们就完成了镜像的配置文件编写。</p><h3 id="制作docker镜像"><a href="#制作docker镜像" class="headerlink" title="制作docker镜像"></a>制作docker镜像</h3><p>登录Dockerhub，如下：</p><p><img src="dockerhub%E4%B8%AA%E4%BA%BA%E9%A1%B5%E9%9D%A2.png" alt></p><p>点击Create Repository，进入创建docker镜像的流程。操作如下：</p><p><img src="%E5%88%9B%E5%BB%BAdocker%E9%95%9C%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C.png" alt></p><p>创建完成后，点击Create &amp;Build即可完成。如下：</p><p><img src="%E5%AE%8C%E6%88%90%E5%88%9B%E5%BB%BAdocker%E9%95%9C%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C.png" alt></p><p>完成后回到首页，点击Builds看到我们的镜像已经开始构建了，构建完成后，结果如下：</p><p><img src="%E6%9E%84%E5%BB%BA%E5%AE%8C%E6%88%90.png" alt></p><p>这时候docker镜像就制作完成了。可以回到我们的服务器进行拉取镜像的测试，进行如下操作：</p><pre><code>$ docker pull dockerhtsoft/k8s-test:latest</code></pre><p>两个镜像都用这种方法去创建，最终形成以下两个镜像，如下图：</p><p><img src="%E9%95%9C%E5%83%8F%E4%BF%A1%E6%81%AF.png" alt></p><p>镜像制作完成后，需要拉取镜像到对应的机器，这里先把镜像拉取到一台机器上，然后对镜像改名，变更为k8s部署配置文件中对应的名称，最后推送到各个节点上。例如拉取了我们自己的镜像dockerhtsoft/k8s-dashboard-2.0.0-rc7，然后更名为kubernetesui/dashboard:v2.0.0-rc7，推送到各个节点机器上的操作如下：</p><pre><code>$ docker pull dockerhtsoft/k8s-dashboard-2.0.0-rc7$ docker tag dockerhtsoft/k8s-dashboard-2.0.0-rc7:latest kubernetesui/dashboard:v2.0.0-rc7$ docker save kubernetesui/dashboard:v2.0.0-rc7 | bzip2 | ssh -p 15555 centos@192.168.123.239 &#39;bunzip2 | docker load&#39;// 推送到其它机器的时候，重复以上操作即可</code></pre><p>同样的dockerhtsoft/k8s-metrics-scraper-v1.0.4，拉取后更名为kubernetesui/metrics-scraper:v1.0.4，推送到各个节点机器上。</p><h3 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h3><p>当上述所有的内容改造完毕后，这时候开始正式的安装。操作如下：</p><pre><code>$ kubectl apply -f recommended.yaml</code></pre><p>安装完毕后，查看pod和service状态。操作如下：</p><pre><code>$ kubectl get pods,svc -n kubernetes-dashboard -o wideNAME                                            READY   STATUS    RESTARTS   AGE     IP            NODE          NOMINATED NODE   READINESS GATESpod/dashboard-metrics-scraper-b68468655-jj85v   1/1     Running   0          2d14h   10.42.1.182   192.168.123.239   &lt;none&gt;           &lt;none&gt;pod/kubernetes-dashboard-6f659467f6-qvvjx       1/1     Running   0          2d14h   10.42.1.181   192.168.123.239   &lt;none&gt;           &lt;none&gt;NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE     SELECTORservice/dashboard-metrics-scraper   ClusterIP   10.43.150.148   &lt;none&gt;        8000/TCP        2d14h   k8s-app=dashboard-metrics-scraperservice/kubernetes-dashboard        NodePort    10.43.254.199   &lt;none&gt;        443:30001/TCP   2d14h   k8s-app=kubernetes-dashboard</code></pre><h3 id="如何访问"><a href="#如何访问" class="headerlink" title="如何访问"></a>如何访问</h3><p>由上述信息可知，我们的dashboard已经部署在192.168.123.239这台机器上了。通过<a href="https://192.168.123.239:30001可以进行访问，需要接受浏览器的风险提示才能进行访问。如下：" target="_blank" rel="noopener">https://192.168.123.239:30001可以进行访问，需要接受浏览器的风险提示才能进行访问。如下：</a></p><p><img src="dashboard%E9%A6%96%E9%A1%B5.png" alt></p><p>这时候可以进行正常访问了，可以使用默认用户访问，也可以自行创建管理员用户进行。下面自行创建管理员用户：</p><pre><code>$ vim create-admin.yaml// 填入以下内容apiVersion: v1kind: ServiceAccountmetadata:  name: admin-user  namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: admin-userroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: admin-user  namespace: kubernetes-dashboard// :wq保存退出// 执行以下命令$ kubectl apply -f create-admin.yaml</code></pre><p>管理员用户创建完成后，需要获取器sa和secret值，通过token进行登录，获取操作如下：</p><pre><code>$ kubectl get sa,secrets -n kubernetes-dashboardNAME                                  SECRETS   AGEserviceaccount/admin-user             1         2d15hserviceaccount/default                1         2d15hserviceaccount/kubernetes-dashboard   1         2d15hNAME                                      TYPE                                  DATA   AGEsecret/admin-user-token-fqvxr             kubernetes.io/service-account-token   3      2d15hsecret/default-token-c76qr                kubernetes.io/service-account-token   3      2d15hsecret/kubernetes-dashboard-certs         Opaque                                0      2d15hsecret/kubernetes-dashboard-csrf          Opaque                                1      2d15hsecret/kubernetes-dashboard-key-holder    Opaque                                2      2d15hsecret/kubernetes-dashboard-token-h4fzt   kubernetes.io/service-account-token   3      2d15h$ kubectl describe secret admin-user-token-fqvxr  -n kubernetes-dashboardName:         admin-user-token-fqvxrNamespace:    kubernetes-dashboardLabels:       &lt;none&gt;Annotations:  kubernetes.io/service-account.name: admin-user              kubernetes.io/service-account.uid: 36e39ee5-6ba1-42c5-a6a6-d6698f07620aType:  kubernetes.io/service-account-tokenData====ca.crt:     1017 bytesnamespace:  20 bytestoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlZFOFRYYWhHR3d5UFM4dmZ4NGNXRGlRMnEzU2dYRHpEM0RNTjZmN1R3aWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWZxdnhyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzNmUzOWVlNS02YmExLTQyYzUtYTZhNi1kNjY5OGYwNzYyMGEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.mZZwwOObHFLDcbZZ5ukj9zljm47EcNL4KPkGAKfR65g-kkICg9rClRpjUHfIM5v_1N4Cnvo9uh_GFwnuZ01Pm4zb9UrvYv4VwyUqKGktc24caWGz9auhylT9POYuFXzuZi7TfHTY7NyOMq8PSBBgg73eDuVewayZQCRpwH9xJEe4FYFJ8xjQIYA94b1IYgg72xqWGM0L0rVkoMIchAGLzy5acDhK2DjtL_LSmr22SnIJ1jaYIMWa4wQIeZgMl5WbUNU1S_3nAwQdbvo2oSLlQkBdTPDLDGXXaObPBcJZxVS19s8WXrFMIKsFD7knJBX_E_bjWZBz8WZJw</code></pre><p>这样复制<strong>token:</strong>后的字符串信息，在登录页面选择token登录，粘贴到输入框中，如下：</p><p><img src="%E8%BE%93%E5%85%A5token%E4%BF%A1%E6%81%AF.png" alt></p><p>点击Sign in，登录到系统中，如下：</p><p><img src="%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%85%A8%E6%99%AF.png" alt></p><p>整个安装过程就完成了！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>项目迁移到k8s的配置修改和问题记录</title>
      <link href="/2020/04/16/xiang-mu-qian-yi-dao-k8s-de-pei-zhi-xiu-gai-he-wen-ti-ji-lu/"/>
      <url>/2020/04/16/xiang-mu-qian-yi-dao-k8s-de-pei-zhi-xiu-gai-he-wen-ti-ji-lu/</url>
      
        <content type="html"><![CDATA[<h1 id="项目迁移到k8s的配置修改和问题记录"><a href="#项目迁移到k8s的配置修改和问题记录" class="headerlink" title="项目迁移到k8s的配置修改和问题记录"></a>项目迁移到k8s的配置修改和问题记录</h1><h1 id="项目迁移到k8s指南，配置修改"><a href="#项目迁移到k8s指南，配置修改" class="headerlink" title="项目迁移到k8s指南，配置修改"></a>项目迁移到k8s指南，配置修改</h1><h2 id="配置信息中各类连接的修改"><a href="#配置信息中各类连接的修改" class="headerlink" title="配置信息中各类连接的修改"></a>配置信息中各类连接的修改</h2><ol><li>nacos的连接地址，统一修改为以下链接：</li></ol><p><a href="http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848" target="_blank" rel="noopener">http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848</a></p><p>注意：在服务日志中会出现nacos连接超时的情况，可以忽略</p><ol start="2"><li>eureka的连接地址，统一修改为以下链接：</li></ol><p><a href="http://test:test2019@eureka-0.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-1.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-2.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/" target="_blank" rel="noopener">http://test:test2019@eureka-0.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-1.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-2.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/</a></p><ol start="3"><li>mysql的连接信息，示例：</li></ol><p>mysql-outer.test-basic-mysql-outer:3306</p><ol start="4"><li>redis的连接信息</li></ol><p>redis-outer.test-basic-redis-outer:6379</p><ol start="5"><li>mongo的连接信息</li></ol><p>mongo-outer.test-basic-mongo-outer:27017</p><h2 id="数据库的迁移"><a href="#数据库的迁移" class="headerlink" title="数据库的迁移"></a>数据库的迁移</h2><p>数据库MySQL和MongoDB的迁移！</p><h2 id="CI-CD的改造"><a href="#CI-CD的改造" class="headerlink" title="CI/CD的改造"></a>CI/CD的改造</h2><p>参考<a href="http://192.168.123.159:8181/test/document/env-dev/blob/master/01.%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/k8s%E7%9B%B8%E5%85%B3%E6%96%87%E6%A1%A3/CI-CD%E7%9A%84%E6%94%B9%E9%80%A0.md" target="_blank" rel="noopener">CI-CD的改造</a>一文。</p><h2 id="迁移问题"><a href="#迁移问题" class="headerlink" title="迁移问题"></a>迁移问题</h2><ol><li>kaptcha生成验证码，使用openjdk-8-alpine镜像遇到的关于字体的问题</li></ol><p>gateway项目中，在Dockerfile中添加<strong>apk –no-cache add ttf-dejavu fontconfig</strong>内容，将字体包进行安装。</p><p>参考链接：<a href="https://blog.csdn.net/huofuman960209/article/details/100738712" target="_blank" rel="noopener">https://blog.csdn.net/huofuman960209/article/details/100738712</a></p><ol start="2"><li>关于部分第三方jar包，如javax.mail、xalan:xalan:jar，推送的nexus依赖库中</li></ol><p>单独把特定jar包推送到仓库中，例如</p><pre><code>$ mvn deploy:deploy-file -DgroupId=com.xy.oracle -DartifactId=ojdbc14 -Dversion=10.2.0.4.0 -Dpackaging=jar -Dfile=E:\ojdbc14.jar -Durl=http://192.168.123.159:8081/nexus/content/repositories/thirdparty/ -DrepositoryId=thirdparty</code></pre><p>或者直接通过nexus的在线页面，通过upload选项直接进行上传！</p><p>参考链接：<a href="https://www.cnblogs.com/rwxwsblog/p/6029636.html" target="_blank" rel="noopener">https://www.cnblogs.com/rwxwsblog/p/6029636.html</a></p><ol start="3"><li>k8s滚动升级时，单个pod更新耗时过长的问题，导致升级速度过于慢。</li></ol><ol start="4"><li>关于前端构建nvm无法获取nodejs版本问题</li></ol><p>前端构建失败，错误日志如下：</p><pre><code>NVM is already installed[test-frontend-develop] $ bash -c &quot;export &gt; env.txt&quot;[test-frontend-develop] $ bash -c &quot;NVM_DIR=/var/jenkins_home/nvm &amp;&amp; source $NVM_DIR/nvm.sh --no-use &amp;&amp; NVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist nvm install 10.12.0 &amp;&amp; nvm use 10.12.0 &amp;&amp; export &gt; env.txt&quot;Version &#39;10.12.0&#39; not found - try `nvm ls-remote` to browse available versions.ERROR: Failed to fork bash </code></pre><p>这种情况偶尔出现，常见于自动触发的方式，手动触发未出现问题。使用的是nodejs的官方镜像，两个参数如下：</p><pre><code>    NVM_NODEJS_ORG_MIRROR    =   https://nodejs.org/dist    NVM_IOJS_ORG_MIRROR     =   https://iojs.org/dist</code></pre><p>使用临时的解决方式，修改NVM_NODEJS_ORG_MIRROR里面，将https变更为http。如下：</p><pre><code>    NVM_NODEJS_ORG_MIRROR    =   http://nodejs.org/dist    NVM_IOJS_ORG_MIRROR     =   https://iojs.org/dist</code></pre><p>参考链接：<a href="https://blog.csdn.net/qq_43897372/article/details/104526660" target="_blank" rel="noopener">https://blog.csdn.net/qq_43897372/article/details/104526660</a></p><ol start="5"><li>需要优化jenkins中前端构建耗时过长的问题</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CI/CD的改造--通过jenkins部署到k8s</title>
      <link href="/2020/04/10/ci-cd-de-gai-zao/"/>
      <url>/2020/04/10/ci-cd-de-gai-zao/</url>
      
        <content type="html"><![CDATA[<h1 id="CI-CD的改造"><a href="#CI-CD的改造" class="headerlink" title="CI/CD的改造"></a>CI/CD的改造</h1><h2 id="前期介绍"><a href="#前期介绍" class="headerlink" title="前期介绍"></a>前期介绍</h2><p>使用jenkins+gitlab+sonar+docker的方式保证了前期的持续集成构建流程的顺利展开，但是在面向k8s时，将docker换成k8s时，需要对整个构建流程进行改造。</p><p>其核心聚焦在最终的部署上，由于我们的jenkins部署在k8s之外，需要将自有的微服务部署在k8s中，需要借助kubectl进行改造。</p><p>灵感来自我们当时针对Rancher的部署，部署完成后在192.168.88.240机器上，使用kubectl工具以及指定的配置文件对k8s集群进行管理！</p><h2 id="方案确定"><a href="#方案确定" class="headerlink" title="方案确定"></a>方案确定</h2><h3 id="方案选择"><a href="#方案选择" class="headerlink" title="方案选择"></a>方案选择</h3><p>将deployment.yaml放在对应服务的根路径下，且最好以模板的可替换的方式导入。</p><ol><li><p>将kubectl工具和k8s生成的配置文件kube_config_cluster.yaml，导入到jenkins所在镜像，然后在构建流水线中启动本地shell并转到$WORKSPACE对应的路径，使用kubectl进行部署</p></li><li><p>借助jenkins插件的方式，Kubernetes Continous Deploy，需要将服务的部署文件随源代码上传到gitlab中</p></li><li><p>使用Helm工具进行安装部署，拉取私有镜像仓库中的镜像，部署到k8s中</p></li></ol><p>第一种方式由于jenkins镜像的不确定性，如果把文件通过拷贝的方式放到k8s中，一旦jenkins镜像出现问题，恢复会比较困难，但是这种方式最简单，也最迅速！</p><p>第三种方式我们需要把部署的yaml文件远程传输到目标服务器，这样部署复杂程度高，不易操作。</p><p>因此选择第二种方式进行操作</p><h3 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h3><h4 id="1-安装插件"><a href="#1-安装插件" class="headerlink" title="1. 安装插件"></a>1. 安装插件</h4><p>优先测试第二种方式，首先离线安装这个Kubernetes Continous Deploy插件，需要下载两个插件信息azure-commons和kubernetes-cd，从<a href="https://plugins.jenkins.io/" target="_blank" rel="noopener">下载地址</a>中搜索并下载这两个插件，通过jenkins插件离线安装的方式进行，点击Manage Jenkins，转到如下图：</p><p><img src="%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6.png" alt></p><p>然后点击Manage Plugins，转到插件管理页面，开始上传.hpi文件，安装插件，如下：</p><p><img src="%E4%B8%8A%E4%BC%A0%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6.png" alt></p><p>安装完成后，需要重启jenkins使插件生效。</p><h4 id="2-对于具体构建的改造"><a href="#2-对于具体构建的改造" class="headerlink" title="2. 对于具体构建的改造"></a>2. 对于具体构建的改造</h4><h4 id="2-1-添加部署到k8s的执行块"><a href="#2-1-添加部署到k8s的执行块" class="headerlink" title="2.1 添加部署到k8s的执行块"></a>2.1 添加部署到k8s的执行块</h4><p>对于之前的项目，我们把生成的docker镜像通过ssh连接到目标服务器，执行shell脚本代码进行部署，如下：</p><p><img src="%E6%89%A7%E8%A1%8Cshell%E4%BB%A3%E7%A0%81%E9%83%A8%E7%BD%B2.png" alt></p><p>现在需要更换方式，利用Kubernetes Continous Deploy插件进行部署，这样就需要删除这个执行脚本的shell，添加k8s部署的执行块。</p><p>首先点击Add post-build step，在下拉菜单中找到Deploy to Kubernetes，点击添加执行块，如下：</p><p><img src="%E7%82%B9%E5%87%BB%E6%B7%BB%E5%8A%A0%E9%83%A8%E7%BD%B2%E5%88%B0k8s%E7%9A%84%E6%89%A7%E8%A1%8C%E5%9D%97.png" alt></p><p><img src="k8s%E6%89%A7%E8%A1%8C%E5%9D%97.png" alt></p><p>然后我们需要添加k8s的配置文件，主要是k8s集群外部连接的相关信息。在192.168.88.240机器上可以找到该文件，名称为kube_config_cluster.yml。我们需要把该文件的内容添加到系统中，找到Kubeconfig一栏，当前系统中没有相关的配置文件，点击右侧的Add按钮进行添加，如下图：</p><p><img src="%E6%B7%BB%E5%8A%A0kubeconfig01.png" alt></p><p><img src="%E6%B7%BB%E5%8A%A0kubeconfig02.png" alt></p><p><img src="%E6%B7%BB%E5%8A%A0kubeconfig03.png" alt></p><p><img src="%E6%B7%BB%E5%8A%A0kubeconfig04.png" alt></p><p>经过以上四步，我们添加kubeconfig已经完成，下面就可以选择该config信息，使Kubernetes Continous Deploy插件可以连接到我们的k8s集群，如下图：</p><p><img src="%E9%80%89%E6%8B%A9kubeconfig%E7%94%9F%E6%95%88.png" alt></p><p>选择完毕后，这样就可以生效了。下一步我们需要指定Config文件，由于我们以test-business01项目进行构建测试，这里设置该Config文件的名称为<em>business01-deployment.yaml</em>。你会发现目前路径下并没有这个配置文件，我们稍后会创建该文件并添加到项目中。设置完成后如下图：</p><p><img src="%E6%8C%87%E5%AE%9A%E9%83%A8%E7%BD%B2%E6%96%87%E4%BB%B6.png" alt></p><p>这样jenkins部分就已经设置完成了，由于我们在k8s集群中已经添加了docker registry的连接信息，这里不需要设置连接镜像仓库的内容，也不需要指定目标的命名空间，点击Verify Configuration进行验证，如下图：</p><p><img src="%E7%82%B9%E5%87%BB%E9%AA%8C%E8%AF%81%E6%88%90%E5%8A%9F.png" alt></p><p>这样就设置完成了，点击最下方的Save按钮保存配置即可！</p><h4 id="2-2-对项目的改造"><a href="#2-2-对项目的改造" class="headerlink" title="2.2 对项目的改造"></a>2.2 对项目的改造</h4><p>回到项目中，我们需要在项目的根目录创建business01-deployment.yaml。记住一定是在项目的根目录创建，目前Kubernetes Continous Deploy插件并不支持根据路径进行查找！文件内容如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Service<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>test<span class="token punctuation">-</span>business  <span class="token key atrule">labels</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span><span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">ports</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19030</span>      <span class="token key atrule">name</span><span class="token punctuation">:</span> tcp      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">19030</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span><span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span>  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>test<span class="token punctuation">-</span>business<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">strategy</span><span class="token punctuation">:</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> RollingUpdate    <span class="token key atrule">rollingUpdate</span><span class="token punctuation">:</span>      <span class="token key atrule">maxUnavailable</span><span class="token punctuation">:</span> <span class="token number">0</span>      <span class="token key atrule">maxSurge</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">selector</span><span class="token punctuation">:</span>    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>      <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span>  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>      <span class="token key atrule">labels</span><span class="token punctuation">:</span>        <span class="token key atrule">app</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">affinity</span><span class="token punctuation">:</span>        <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>          <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>                <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname                <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>                  <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>                    <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app                      <span class="token key atrule">operator</span><span class="token punctuation">:</span> In                      <span class="token key atrule">values</span><span class="token punctuation">:</span>                        <span class="token punctuation">-</span> app<span class="token punctuation">-</span>test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span>              <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">1</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business<span class="token punctuation">-</span><span class="token number">01</span>          <span class="token key atrule">image</span><span class="token punctuation">:</span> 192.168.88.159<span class="token punctuation">:</span>5000/test<span class="token punctuation">-</span>business01<span class="token punctuation">-</span>k8s<span class="token punctuation">:</span>$BUILD_NUMBER          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span>            <span class="token key atrule">preStop</span><span class="token punctuation">:</span>              <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>                <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19030</span>                <span class="token key atrule">path</span><span class="token punctuation">:</span> /spring/shutdown          <span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19030</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span>            <span class="token key atrule">httpGet</span><span class="token punctuation">:</span>              <span class="token key atrule">path</span><span class="token punctuation">:</span> /actuator/health              <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19030</span>            <span class="token key atrule">initialDelaySeconds</span><span class="token punctuation">:</span> <span class="token number">180</span>            <span class="token key atrule">periodSeconds</span><span class="token punctuation">:</span> <span class="token number">5</span>            <span class="token key atrule">timeoutSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span>            <span class="token key atrule">successThreshold</span><span class="token punctuation">:</span> <span class="token number">1</span>            <span class="token key atrule">failureThreshold</span><span class="token punctuation">:</span> <span class="token number">5</span>          <span class="token key atrule">resources</span><span class="token punctuation">:</span>            <span class="token key atrule">requests</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi            <span class="token key atrule">limits</span><span class="token punctuation">:</span>              <span class="token key atrule">memory</span><span class="token punctuation">:</span> 1Gi          <span class="token key atrule">ports</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">19030</span>          <span class="token key atrule">env</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> IP_ADDR              <span class="token key atrule">valueFrom</span><span class="token punctuation">:</span>                <span class="token key atrule">fieldRef</span><span class="token punctuation">:</span>                  <span class="token key atrule">fieldPath</span><span class="token punctuation">:</span> status.podIP            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_IP              <span class="token key atrule">value</span><span class="token punctuation">:</span> http<span class="token punctuation">:</span>//nacos<span class="token punctuation">-</span>0.nacos<span class="token punctuation">-</span>headless.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>nacos.svc.cluster.local<span class="token punctuation">:</span><span class="token number">8848</span><span class="token punctuation">,</span>http<span class="token punctuation">:</span>//nacos<span class="token punctuation">-</span>1.nacos<span class="token punctuation">-</span>headless.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>nacos.svc.cluster.local<span class="token punctuation">:</span><span class="token number">8848</span><span class="token punctuation">,</span>http<span class="token punctuation">:</span>//nacos<span class="token punctuation">-</span>2.nacos<span class="token punctuation">-</span>headless.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>nacos.svc.cluster.local<span class="token punctuation">:</span><span class="token number">8848</span>            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> NACOS_NAMESPACE              <span class="token key atrule">value</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>dev            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> CHANNEL              <span class="token key atrule">value</span><span class="token punctuation">:</span> standalone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后提交代码到版本控制，整个这部分内容就完成了，目录结构如下：</p><p><img src="%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.png" alt></p><h4 id="2-3-触发构建"><a href="#2-3-触发构建" class="headerlink" title="2.3 触发构建"></a>2.3 触发构建</h4><p>回到jenkins中，在test-business01的job中点击“立即构建”（Build Now），开始构建，最终关于k8s的输出日志如下：</p><pre><code>Starting Kubernetes deploymentLoading configuration: /var/jenkins_home/workspace/test-business01-k8s/business01-deployment.yamlApplied V1Service: class V1Service {    apiVersion: v1    kind: Service    metadata: class V1ObjectMeta {        annotations: null        clusterName: null        creationTimestamp: 2020-04-06T04:11:47.000Z        deletionGracePeriodSeconds: null        deletionTimestamp: null        finalizers: null        generateName: null        generation: null        initializers: null        labels: {app=test-business-01}        managedFields: null        name: test-business-01        namespace: test-test-business        ownerReferences: null        resourceVersion: 16222470        selfLink: /api/v1/namespaces/test-test-business/services/test-business-01        uid: 90eddf16-be68-442f-8112-f009d55b46d2    }    spec: class V1ServiceSpec {        clusterIP: 10.43.50.235        externalIPs: null        externalName: null        externalTrafficPolicy: null        healthCheckNodePort: null        loadBalancerIP: null        loadBalancerSourceRanges: null        ports: [class V1ServicePort {            name: tcp            nodePort: null            port: 19030            protocol: TCP            targetPort: 19030        }]        publishNotReadyAddresses: null        selector: {app=test-business-01}        sessionAffinity: None        sessionAffinityConfig: null        type: ClusterIP    }    status: class V1ServiceStatus {        loadBalancer: class V1LoadBalancerStatus {            ingress: null        }    }}Applied V1Deployment: class V1Deployment {    apiVersion: apps/v1    kind: Deployment    metadata: class V1ObjectMeta {        annotations: null        clusterName: null        creationTimestamp: 2020-04-08T01:30:06.000Z        deletionGracePeriodSeconds: null        deletionTimestamp: null        finalizers: null        generateName: null        generation: 4        initializers: null        labels: null        managedFields: null        name: test-business-01        namespace: test-test-business        ownerReferences: null        resourceVersion: 16222472        selfLink: /apis/apps/v1/namespaces/test-test-business/deployments/test-business-01        uid: 7fca5704-6be8-49fa-9454-03a2a522ac7b    }    spec: class V1DeploymentSpec {        minReadySeconds: 10        paused: null        progressDeadlineSeconds: 600        replicas: 3        revisionHistoryLimit: 10        selector: class V1LabelSelector {            matchExpressions: null            matchLabels: {app=test-business-01}        }        strategy: class V1DeploymentStrategy {            rollingUpdate: class V1RollingUpdateDeployment {                maxSurge: 1                maxUnavailable: 0            }            type: RollingUpdate        }        template: class V1PodTemplateSpec {            metadata: class V1ObjectMeta {                annotations: null                clusterName: null                creationTimestamp: null                deletionGracePeriodSeconds: null                deletionTimestamp: null                finalizers: null                generateName: null                generation: null                initializers: null                labels: {app=test-business-01}                managedFields: null                name: null                namespace: null                ownerReferences: null                resourceVersion: null                selfLink: null                uid: null            }            spec: class V1PodSpec {                activeDeadlineSeconds: null                affinity: class V1Affinity {                    nodeAffinity: null                    podAffinity: null                    podAntiAffinity: class V1PodAntiAffinity {                        preferredDuringSchedulingIgnoredDuringExecution: [class V1WeightedPodAffinityTerm {                            podAffinityTerm: class V1PodAffinityTerm {                                labelSelector: class V1LabelSelector {                                    matchExpressions: [class V1LabelSelectorRequirement {                                        key: app                                        operator: In                                        values: [app-test-business-01]                                    }]                                    matchLabels: null                                }                                namespaces: null                                topologyKey: kubernetes.io/hostname                            }                            weight: 1                        }]                        requiredDuringSchedulingIgnoredDuringExecution: null                    }                }                automountServiceAccountToken: null                containers: [class V1Container {                    args: null                    command: null                    env: [class V1EnvVar {                        name: IP_ADDR                        value: null                        valueFrom: class V1EnvVarSource {                            configMapKeyRef: null                            fieldRef: class V1ObjectFieldSelector {                                apiVersion: v1                                fieldPath: status.podIP                            }                            resourceFieldRef: null                            secretKeyRef: null                        }                    }, class V1EnvVar {                        name: NACOS_IP                        value: http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848                        valueFrom: null                    }, class V1EnvVar {                        name: NACOS_NAMESPACE                        value: test-dev                        valueFrom: null                    }, class V1EnvVar {                        name: CHANNEL                        value: standalone                        valueFrom: null                    }]                    envFrom: null                    image: 192.168.88.159:5000/test-business01-k8s:17                    imagePullPolicy: Always                    lifecycle: class V1Lifecycle {                        postStart: null                        preStop: class V1Handler {                            exec: null                            httpGet: class V1HTTPGetAction {                                host: null                                httpHeaders: null                                path: /spring/shutdown                                port: 19030                                scheme: HTTP                            }                            tcpSocket: null                        }                    }                    livenessProbe: class V1Probe {                        exec: null                        failureThreshold: 5                        httpGet: class V1HTTPGetAction {                            host: null                            httpHeaders: null                            path: /actuator/health                            port: 19030                            scheme: HTTP                        }                        initialDelaySeconds: 180                        periodSeconds: 5                        successThreshold: 1                        tcpSocket: null                        timeoutSeconds: 10                    }                    name: test-business-01                    ports: [class V1ContainerPort {                        containerPort: 19030                        hostIP: null                        hostPort: null                        name: null                        protocol: TCP                    }]                    readinessProbe: class V1Probe {                        exec: null                        failureThreshold: 5                        httpGet: class V1HTTPGetAction {                            host: null                            httpHeaders: null                            path: /actuator/health                            port: 19030                            scheme: HTTP                        }                        initialDelaySeconds: 180                        periodSeconds: 5                        successThreshold: 1                        tcpSocket: null                        timeoutSeconds: 10                    }                    resources: class V1ResourceRequirements {                        limits: {memory=Quantity{number=1073741824, format=BINARY_SI}}                        requests: {memory=Quantity{number=1073741824, format=BINARY_SI}}                    }                    securityContext: null                    stdin: null                    stdinOnce: null                    terminationMessagePath: /dev/termination-log                    terminationMessagePolicy: File                    tty: null                    volumeDevices: null                    volumeMounts: null                    workingDir: null                }]                dnsConfig: null                dnsPolicy: ClusterFirst                enableServiceLinks: null                hostAliases: null                hostIPC: null                hostNetwork: null                hostPID: null                hostname: null                imagePullSecrets: null                initContainers: null                nodeName: null                nodeSelector: null                preemptionPolicy: null                priority: null                priorityClassName: null                readinessGates: null                restartPolicy: Always                runtimeClassName: null                schedulerName: default-scheduler                securityContext: class V1PodSecurityContext {                    fsGroup: null                    runAsGroup: null                    runAsNonRoot: null                    runAsUser: null                    seLinuxOptions: null                    supplementalGroups: null                    sysctls: null                    windowsOptions: null                }                serviceAccount: null                serviceAccountName: null                shareProcessNamespace: null                subdomain: null                terminationGracePeriodSeconds: 30                tolerations: null                volumes: null            }        }    }    status: class V1DeploymentStatus {        availableReplicas: 1        collisionCount: null        conditions: [class V1DeploymentCondition {            lastTransitionTime: 2020-04-08T01:30:33.000Z            lastUpdateTime: 2020-04-08T01:37:10.000Z            message: ReplicaSet &quot;test-business-01-67b4cd9d4c&quot; has successfully progressed.            reason: NewReplicaSetAvailable            status: True            type: Progressing        }, class V1DeploymentCondition {            lastTransitionTime: 2020-04-08T02:03:24.000Z            lastUpdateTime: 2020-04-08T02:03:24.000Z            message: Deployment has minimum availability.            reason: MinimumReplicasAvailable            status: True            type: Available        }]        observedGeneration: 3        readyReplicas: 1        replicas: 1        unavailableReplicas: null        updatedReplicas: 1    }}Finished Kubernetes deploymentFinished: SUCCESS</code></pre><p>说明k8s中已经部署了我们的镜像，在k8s中查看如下：</p><pre><code>$ kubectl get pod -n test-test-businessNAME                                READY   STATUS    RESTARTS   AGEtest-business-01-57b478d68d-27x8g   1/1     Running   0          81mtest-business-01-57b478d68d-5qzs8   1/1     Running   3          87mtest-business-01-57b478d68d-8jt5t   1/1     Running   0          77m</code></pre><p>部署成功！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这样的方式，让我们的构建更简单，调整的部分也更少，后续的工作也能很好的开展。</p><p>这里需要注意的是，在每一个项目的yaml配置文件中，一定要指定部署在k8s集群中的namespace名称！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kt-connect的安装和使用</title>
      <link href="/2020/04/09/kt-connect-de-an-zhuang-he-shi-yong-wan-zheng-jiao-cheng/"/>
      <url>/2020/04/09/kt-connect-de-an-zhuang-he-shi-yong-wan-zheng-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="kt-connect的安装和使用"><a href="#kt-connect的安装和使用" class="headerlink" title="kt-connect的安装和使用"></a>kt-connect的安装和使用</h1><h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><p>开发机：win7、win10</p><p>工具版本:</p><ul><li>kt-connect: 0.0.12</li><li>kubectl: 1.17.2</li><li>命令行工具：MobaXterm v11.1，cmder</li></ul><p>python版本：python3.7，安装virtualenv和sshuttle。</p><h2 id="本文适用于"><a href="#本文适用于" class="headerlink" title="本文适用于"></a>本文适用于</h2><ul><li>使用windows进行开发的java开发者</li><li>idea作为主力开发的ide</li><li>Kubernetes作为开发测试环境</li><li>开发的应用包含多个微服务，且存在相互调用关系</li></ul><h2 id="资料下载"><a href="#资料下载" class="headerlink" title="资料下载"></a>资料下载</h2><p>文中涉及的所有配置文件、工具，和文章放在同一个目录下，会上传到gitlab中。在同目录下的tools文件夹中，解压即可用！</p><h2 id="配置环境并安装"><a href="#配置环境并安装" class="headerlink" title="配置环境并安装"></a>配置环境并安装</h2><h3 id="0-搭建python环境"><a href="#0-搭建python环境" class="headerlink" title="0. 搭建python环境"></a>0. 搭建python环境</h3><p>首先下载<a href="https://www.python.org/ftp/python/3.7.0/python-3.7.0-amd64.exe" target="_blank" rel="noopener">python3.7</a>的安装包，进行安装。请自行勾选添加到环境变量的工作，并且允许该计算机上的所有用户使用。</p><p>安装完成后，在命令行工具下进行测试，如下：</p><pre><code>$ pythonPython 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt;</code></pre><p>说明python环境搭建完成。</p><p>然后需要创建python运行的虚拟环境，用来安装sshuttle工具。下面首先需要安装virtualenv工具并创建虚拟环境：</p><pre><code>$ pip install virtualenv// 转到你要的路径下$ cd D:\dev\Java\ktctl_windows_amd64// 创建虚拟环境，虚拟环境名称为vevn$ virtualenv venv// 使虚拟环境生效$ .\venv\Scripts\activate// 查看虚拟环境路径下的信息(venv) $ lsvenv</code></pre><p>虚拟环境生效后，在命令行前面会出现<strong>(venv)</strong>的字样。说明虚拟环境已经生效，这时候在虚拟环境下进行操作，不会影响外部的python运行环境，起到隔离的作用！</p><p>最后我们需要安装sshuttle工具，直接使用python所带的pip工具进行安装，如下：</p><pre><code>(venv) $ pip install sshuttle</code></pre><p>安装完成后即可使用。</p><h3 id="1-安装kubectl"><a href="#1-安装kubectl" class="headerlink" title="1. 安装kubectl"></a>1. 安装kubectl</h3><p>下载<a href="https://storage.googleapis.com/kubernetes-release/release/v1.17.2/bin/windows/amd64/kubectl.exe" target="_blank" rel="noopener">kubectl工具</a>，然后放到与venv相同路径下。</p><p>然后获取k8s中的配置信息，这里因为使用的Rancher中的rke工具进行安装，配置文件名称为：kube_config_cluster.yml，所以直接copy出配置文件一并放到与venv相同路径下。</p><p>现在就可以查看当前的k8s集群中的内容了，进行如下设置：</p><pre><code>// 指定配置文件的路径$ set KUBECONFIG=./kube_config_cluster.yml// 查看集群中某个namespace下的内容$ kubectl.exe get pod -n test-test-businessNAME                                READY   STATUS    RESTARTS   AGEtest-business-01-84cf9fcbb7-bcgs2   0/1     Running   285        25htest-business-02-58655d78c6-jhl7s   1/1     Running   0          5d1h</code></pre><p>这样kubectl就配置完成了。</p><h3 id="2-安装kt-connect"><a href="#2-安装kt-connect" class="headerlink" title="2. 安装kt-connect"></a>2. 安装kt-connect</h3><p><a href="https://github.com/alibaba/kt-connect/releases/download/kt-0.0.12/ktctl_windows_amd64.tar.gz" target="_blank" rel="noopener">下载kt-connect</a>并解压到之前venv所在的路径下，然后查看文件夹下的内容：</p><pre><code>$ lsktctl_windows_amd64.exe*  kube_config_cluster.yml  kubectl.exe*  venv/</code></pre><p>需要配置环境变量，将环境变量的路径指向该目录下，我这里的目录是<strong>D:\dev\Java\ktctl_windows_amd64</strong>，其实之前执行的set命令也可以配置到环境变量中，这个自行配置下即可。但是令人不太满意的是，在真正运行的时候，还是需要指定全路径信息！</p><h3 id="3-部署测试镜像"><a href="#3-部署测试镜像" class="headerlink" title="3. 部署测试镜像"></a>3. 部署测试镜像</h3><p>下面我们就可以部署测试的镜像，可以使用刚才配置的kubectl工具进行部署，操作如下：</p><pre><code>$ kubectl create ns kt-connect-test$ kubectl run tomcat --image=registry.cn-hangzhou.aliyuncs.com/rdc-product/kt-connect-tomcat9:1.0 --expose --port=8080 -n kt-connect-testservice &quot;tomcat&quot; createddeployment.apps &quot;tomcat&quot; created$ kubectl get deployments -o wide --selector run=tomcat -n kt-connect-testNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES                                                                 SELECTORtomcat    1         1         1            1           18s       tomcat       registry.cn-hangzhou.aliyuncs.com/rdc-product/kt-connect-tomcat9:1.0   run=tomcat$ kubectl get pods -o wide --selector run=tomcat -n kt-connect-testNAME                      READY     STATUS    RESTARTS   AGE       IP             NODEtomcat-54d87b848c-2mc9b   1/1       Running   0          1m        172.23.2.234   cn-beijing.192.168.0.8$ kubectl get svc tomcat -n kt-connect-testNAME      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGEtomcat    ClusterIP   172.21.6.39   &lt;none&gt;        8080/TCP   1m</code></pre><p>这样根据以上展示的所有信息，可以进行下面的启动和连接。</p><h3 id="4-启动kt-connect"><a href="#4-启动kt-connect" class="headerlink" title="4. 启动kt-connect"></a>4. 启动kt-connect</h3><p>开始操作之前，要知道windows的限制，Windwos环境下KT Connect只支持使用SOCKS5代理模式，在该模式下用户可以直接在本地访问PodIP和ClusterIP,但是无法直接使用DNS。在操作之前，请先确定本机已经安装kubectl并且能够正常与Kubernetes集群交互，而且一定确定使你之前配置的虚拟环境进行生效！首先在正式操作之前，检查下环境是否都准备好了。当出现<em>KT Connect is ready, enjoy it!</em>时，显示可以进行直接连接操作，如下：</p><pre><code>(venv) λ D:\dev\Java\ktctl_windows_amd64\ktctl_windows_amd64.exe --kubeconfig D:\dev\Java\ktctl_windows_amd64\kube_config_cluster.yml -n kt-connect-test -d connecct --method=socks5 --proxy 42222 --port 423331:24PM INF Connect Start At 218921:24PM INF Client address 10.0.93.1931:24PM INF deploy shadow deployment kt-connect-daemon-ieljt in namespace kt-connect-test1:24PM INF pod label: kt=kt-connect-daemon-ieljt1:24PM INF pod:  is running,but not ready1:24PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:24PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:24PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:24PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:24PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:24PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:25PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:25PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:25PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:25PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:25PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:25PM INF pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is running,but not ready1:25PM INF Shadow pod: kt-connect-daemon-ieljt-65b8f8956-s7js2 is ready.1:25PM DBG Child, os.Args = [D:\dev\Java\ktctl_windows_amd64\ktctl_windows_amd64.exe --kubeconfig D:\dev\Java\ktctl_windows_amd64\kube_config_cluster.yml -n kt-connect-test -d connect --method=socks5 --proxy 42222 --port 42333]1:25PM DBG Child, cmd.Args = [kubectl --kubeconfig=D:\dev\Java\ktctl_windows_amd64\kube_config_cluster.yml -n kt-connect-test port-forward kt-connect-daemon-ieljt-65b8f8956-s7js2 42333:22]Forwarding from 127.0.0.1:42333 -&gt; 22Forwarding from [::1]:42333 -&gt; 221:25PM INF port-forward start at pid: 39201:25PM INF ==============================================================1:25PM INF Start SOCKS5 Proxy: export http_proxy=socks5://127.0.0.1:422221:25PM INF ==============================================================1:25PM DBG Child, os.Args = [D:\dev\Java\ktctl_windows_amd64\ktctl_windows_amd64.exe --kubeconfig D:\dev\Java\ktctl_windows_amd64\kube_config_cluster.yml -n kt-connect-test -d connect --method=socks5 --proxy 42222 --port 42333]1:25PM DBG Child, cmd.Args = [ssh -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -i C:\Users\ht/.kt_id_rsa -D 42222 root@127.0.0.1 -p42333 sh loop.sh] Handling connection for 42333Warning: Permanently added &#39;[127.0.0.1]:42333&#39; (ECDSA) to the list of known hosts.1:25PM INF vpn(ssh) start at pid: 107241:25PM INF KT proxy start successful</code></pre><p>当没有错误出现，且出现的信息如上时，可以执行后续的测试连接操作。</p><h3 id="5-测试连接"><a href="#5-测试连接" class="headerlink" title="5. 测试连接"></a>5. 测试连接</h3><p>这时候需要另开一个终端界面，进行下面的操作：</p><pre><code>// 参考上面日志中的提示Start SOCKS5 Proxy: export http_proxy=socks5://127.0.0.1:42222// windows需要使用set替换export命令$ set http_proxy=socks5://127.0.0.1:42222// 测试$ curl http://172.21.6.39:8080kt-connect demo from tomcat9</code></pre><p>这时候出现了<strong>kt-connect demo from tomcat9</strong>，说明访问上面部署的tomcat镜像成功，本地联调也已经成功了。这样我们就可以进行服务的联调了。</p><h2 id="具体案例操作"><a href="#具体案例操作" class="headerlink" title="具体案例操作"></a>具体案例操作</h2><h3 id="1-测试服务介绍"><a href="#1-测试服务介绍" class="headerlink" title="1. 测试服务介绍"></a>1. 测试服务介绍</h3><p>以test-business01为案例进行测试，在本地启动test-business01服务，替换线上的test-business01服务，然后从线下服务中打印日志，表示服务已经切换到线下开发机的服务中了。</p><h3 id="2-测试服务线上启动"><a href="#2-测试服务线上启动" class="headerlink" title="2. 测试服务线上启动"></a>2. 测试服务线上启动</h3><p>首先连接到线上的namespace中，名称为test-test-business。执行connect命令：</p><pre><code>(venv) $ D:\dev\Java\ktctl_windows_amd64\ktctl_windows_amd64.exe --kubeconfig D:\dev\Java\ktctl_windows_amd64\kube_config_cluster.yml -n test-test-business --d connect --method=socks5 --proxy 42222 --port 42333</code></pre><p>然后使用本地服务替换线上的服务，新打开一个命令行终端，命令如下：</p><pre><code>// 先在命令行中配置代理信息// 参考上面日志中的提示Start SOCKS5 Proxy: export http_proxy=socks5://127.0.0.1:42222// windows需要使用set替换export命令// $ set http_proxy=socks5://127.0.0.1:42222// 不需要设置代理信息即可使用// 启动虚拟环境$ ./venv/Scripts/activate // 启动新的影子镜像替换旧的镜像(venv) $ ktct_windows_amd64.exe --kubeconfig D:\dev\Java\ktctl_windows_amd64\kube_config_cluster.yml --debug --namespace=test-test-business exchange test-business-01 --expose 19030</code></pre><p>启动后，我们查看一下线上服务是否被替换，如下：</p><pre><code>$ kubectl get pod -n test-test-businessNAME                                        READY   STATUS    RESTARTS   AGEkt-connect-daemon-acbhz-6648c86848-t4kzr    1/1     Running   0          86mtest-business-01-kt-tccwe-658f9c4b5-l8sdc   1/1     Running   0          32mtest-business-02-58655d78c6-jhl7s           1/1     Running   0          5d6h</code></pre><p>test-business01已经添加上了<em>kt-</em>标志，说明已经替换成我们自己的了！完成后需要对nacos和eureka以及网关等进行配置上的修改！</p><p>目前nacos和eureka并未对外暴露，服务可能无法注册进去，需要nacos和eureka进行调整。其次在本地服务启动时，需要重新导入nacos和eureka的配置信息。针对nacos进行配置，新建一个namespace，导入test-business相关的配置信息，将eureka、数据库、缓存的连接更改为对外暴露的地址信息，以peer0为标志，加载该标志对应的配置信息。</p><p>由于nacos部署的是headless服务信息，所以无法使用nodePort模式进行访问。再考虑到我们这里使用的socks5的代理方式，最终只能使用clusterIP的方式进行连接。首先需要获取nacos中的clusterIP，如下：</p><pre><code>$ kubectl get svc -n test-basic-nacosNAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGEmysql            ClusterIP   10.43.252.60   &lt;none&gt;        3306/TCP   18dnacos-headless   ClusterIP   None           &lt;none&gt;        8848/TCP   18d$ kubectl describe svc nacos-headless -n test-basic-nacosName:              nacos-headlessNamespace:         test-basic-nacosLabels:            app=nacosAnnotations:       kubectl.kubernetes.io/last-applied-configuration:                     {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;app&quot;:&quot;nacos&quot;},&quot;name&quot;:&quot;nacos-headless&quot;,&quot;namespace&quot;:&quot;test-b...Selector:          app=nacosType:              ClusterIPIP:                NonePort:              server  8848/TCPTargetPort:        8848/TCPEndpoints:         10.42.0.203:8848,10.42.1.15:8848,10.42.2.79:8848Session Affinity:  NoneEvents:            &lt;none&gt;</code></pre><p>Endpoints一栏表示了，当前的nacos对应的ip地址和端口信息，根据headless服务的特点，Endpoint的ip在重新部署时会发生变更，这时候需要经常查看以进行修改。考虑到我们要在后面设置程序的代理信息，通过代理是可以连接到集群内部的nacos服务，因此采用这种方式来处理。如果idea控制台中的日志出现了连接超时的问题，临时可以忽略，不影响程序运行！</p><p>然后在test-business01中更改nacos的地址，变更为k8s集群中对外暴露的地址，如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token key atrule">application</span><span class="token punctuation">:</span>    <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>business01  <span class="token key atrule">cloud</span><span class="token punctuation">:</span>    <span class="token key atrule">nacos</span><span class="token punctuation">:</span>      <span class="token key atrule">config</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 测试环境k8s使用</span>        <span class="token key atrule">server-addr</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>nacos_ip<span class="token punctuation">:</span>10.42.1.15<span class="token punctuation">:</span><span class="token number">8848</span><span class="token punctuation">}</span>        <span class="token key atrule">namespace</span><span class="token punctuation">:</span> $<span class="token punctuation">{</span>nacos_namespace<span class="token punctuation">:</span>test<span class="token punctuation">-</span>dev<span class="token punctuation">-</span>local<span class="token punctuation">}</span>        <span class="token comment" spellcheck="true"># 开发环境使用</span><span class="token comment" spellcheck="true">#        server-addr: ${nacos_ip:192.168.88.161:18848}</span><span class="token comment" spellcheck="true">#        namespace: ${nacos_namespace:858b37b1-35be-4564-a24e-dc2c322d5784}</span>        <span class="token key atrule">file-extension</span><span class="token punctuation">:</span> yml<span class="token key atrule">feign</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#  httpclient:</span><span class="token comment" spellcheck="true">#    enabled: false</span><span class="token comment" spellcheck="true">#  okhttp:</span><span class="token comment" spellcheck="true">#    enabled: true</span>  <span class="token key atrule">hystrix</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">client</span><span class="token punctuation">:</span>    <span class="token key atrule">config</span><span class="token punctuation">:</span>      <span class="token key atrule">remote-service</span><span class="token punctuation">:</span>        <span class="token key atrule">connectTimeout</span><span class="token punctuation">:</span> <span class="token number">1000</span>        <span class="token key atrule">readTimeout</span><span class="token punctuation">:</span> <span class="token number">12000</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中10.42.1.15代表的是k8s集群内的nacos地址信息，是某一个nacos的连接地址，因为通过kt-connect连接集群后，使用socks5方式进行代理，只能通过内部的clusterIP的方式进行访问！但是该ip是变化的，根据实际情况进行设置。</p><h3 id="3-idea接入测试服务进行调试"><a href="#3-idea接入测试服务进行调试" class="headerlink" title="3. idea接入测试服务进行调试"></a>3. idea接入测试服务进行调试</h3><p>上述设置完成后，开始使用本地服务进行测试！</p><p>官网推荐使用<a href="https://plugins.jetbrains.com/plugin/13482-jvm-inject" target="_blank" rel="noopener">jvm-inject</a>插件，但是我们从2019.1.3版本试到2019.3.4版本，均无法安装该插件。</p><p>在前面的操作中，在venv所在路径下，生成了.jvmrc文件，打开查看，内容如下：</p><pre><code>-DsocksProxyHost=127.0.0.1-DsocksProxyPort=42222</code></pre><p>然后通过反编译获取jvm-inject插件的核心源码，如下：</p><pre><code>//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//package io.github.jvm.inject;import com.google.common.base.Joiner;import com.intellij.execution.Executor;import com.intellij.execution.configurations.JavaParameters;import com.intellij.execution.configurations.RunConfiguration;import com.intellij.execution.configurations.RunProfile;import com.intellij.execution.runners.JavaProgramPatcher;import java.io.File;import java.io.IOException;import java.util.List;import org.apache.commons.io.FileUtils;public class JvmInject extends JavaProgramPatcher {    public static final String RC_FILE = &quot;.jvmrc&quot;;    public JvmInject() {    }    public void patchJavaParameters(Executor executor, RunProfile configuration, JavaParameters javaParameters) {        if (configuration instanceof RunConfiguration) {            RunConfiguration runConfiguration = (RunConfiguration)configuration;            String root = runConfiguration.getProject().getBaseDir().getPath();            File file = FileUtils.getFile(new String[]{root, &quot;.jvmrc&quot;});            if (file.exists()) {                try {                    List&lt;String&gt; lines = FileUtils.readLines(file, &quot;UTF-8&quot;);                    String result = Joiner.on(&quot; &quot;).join(lines);                    javaParameters.getVMParametersList().addParametersString(result);                } catch (IOException var9) {                    var9.printStackTrace();                }            }        }    }}</code></pre><p>实际上就是向启动命令中注入了.jvmrc中包含的信息<strong>getVMParametersList()</strong>判断是通过ide中配置VMOptions的参数进行的，那么我们可以手动进行启动信息的注入！如下图：</p><p><img src="%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF.png" alt></p><p>然后启动服务的时候，在VM Options中的参数会自动注入到启动命令中，如下：</p><pre><code>D:\dev\Java\jdk1.8.0_201\bin\java.exe -DsocksProxyHost=127.0.0.1 -DsocksProxyPort=42222 -XX:TieredStopAtLevel=1 -noverify -Dspring.profiles.active=--spring.profiles.active=peer0 -Dspring.output.ansi.enabled=always -Dcom.sun.management.jmxremote -Dspring.jmx.enabled=true</code></pre><p>这样代理已经设置完毕，结合上面的内容，配置文件bootstrap.yml替换了nacos的地址信息，这时进行启动该服务！启动完成后查看eureka服务，看服务是否已经注册了，如下图：</p><p><img src="eureka%E6%B3%A8%E5%86%8C%E4%BF%A1%E6%81%AF.png" alt></p><p>服务已经注册，192.168.88.193所示的服务就是我们自己线下启动的服务，这时候通过接口进行测试，完成一次查询的操作，看接口是否能连通，插入数据的时候，日志如下：</p><pre><code>2020-04-07 18:48:30.850  INFO [test-business01,,,] 12464 --- [erListUpdater-0] c.netflix.config.ChainedDynamicProperty  : Flipping property: test-business02.ribbon.ActiveConnectionsLimit to use NEXT property: niws.loadbalancer.availabilityFilteringRule.activeConnectionsLimit = 2147483647Creating a new SqlSessionSqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@62210e0b] was not registered for synchronization because synchronization is not activeJDBC Connection [com.alibaba.druid.proxy.jdbc.ConnectionProxyImpl@49a685f] will not be managed by Spring==&gt;  Preparing: SELECT id,name FROM roles ==&gt; Parameters: &lt;==    Columns: id, name&lt;==        Row: 5, ROLE_ADMIN&lt;==        Row: 4, ROLE_USER&lt;==      Total: 2Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@62210e0b]</code></pre><p>测试插入和查询的接口，日志均能进行打印！所以到这里已经成功了！</p><h3 id="4-终止调试"><a href="#4-终止调试" class="headerlink" title="4. 终止调试"></a>4. 终止调试</h3><p>最后要终止调试，并恢复之前的服务，在启动影子镜像的命令行中，使用ctrl+c终止运行</p><pre><code>(venv) $ ktct_windows_amd64.exe --kubeconfig D:\dev\Java\ktctl_windows_amd64\kube_config_cluster.yml --debug --namespace=test-test-business exchange test-business-01 --expose 19030</code></pre><p>上述命令行中的运行程序被终止，程序会退出并清理之前部署的镜像信息，查看镜像时，会发现测试镜像被替换，恢复原来的镜像信息，如下：</p><pre><code>$ kubectl get pod -n test-test-businessNAME                                         READY   STATUS              RESTARTS   AGEkt-connect-daemon-dbjpv-666fdf5c7d-f5n55     1/1     Running             0          21mtest-business-01-67b4cd9d4c-smjfg            0/1     ContainerCreating   0          14s// 该镜像被清理且终止test-business-01-kt-jbizj-6cb8d9d898-w6rkw   1/1     Terminating         0          17mtest-business-02-58655d78c6-jhl7s            1/1     Running             0          5d22h$ kubectl get pod -n test-test-businessNAME                                       READY   STATUS    RESTARTS   AGEkt-connect-daemon-dbjpv-666fdf5c7d-f5n55   1/1     Running   0          25mtest-business-01-67b4cd9d4c-smjfg          0/1     Running   0          4m54stest-business-02-58655d78c6-jhl7s          1/1     Running   0          5d22h</code></pre><p>这时候再访问，发现已经恢复为以前的镜像！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前只是使用到了k8s部分的内容，下一步添加ServiceMesh时，利用istio工具实现对流量的切换，需要对当前的内容进行升级！</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><p>介绍：<a href="https://www.v2ex.com/t/584314" target="_blank" rel="noopener">https://www.v2ex.com/t/584314</a></p></li><li><p>排查问题：<a href="https://alibaba.github.io/kt-connect/#/zh-cn/troubleshoot" target="_blank" rel="noopener">https://alibaba.github.io/kt-connect/#/zh-cn/troubleshoot</a></p></li><li><p>windows支持：<a href="https://alibaba.github.io/kt-connect/#/zh-cn/guide/windows-support" target="_blank" rel="noopener">https://alibaba.github.io/kt-connect/#/zh-cn/guide/windows-support</a></p></li><li><p>快速开始，测试镜像：<a href="https://alibaba.github.io/kt-connect/#/zh-cn/quickstart" target="_blank" rel="noopener">https://alibaba.github.io/kt-connect/#/zh-cn/quickstart</a></p></li></ul><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li>多人同时调试一个服务的时候，上线多个相同服务，如何进行调试？如何对流量进行控制？</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>微服务教程大纲</title>
      <link href="/2020/04/08/wei-fu-wu-ke-cheng-da-gang/"/>
      <url>/2020/04/08/wei-fu-wu-ke-cheng-da-gang/</url>
      
        <content type="html"><![CDATA[<h1 id="微服务教程大纲"><a href="#微服务教程大纲" class="headerlink" title="微服务教程大纲"></a>微服务教程大纲</h1><h2 id="自学内容"><a href="#自学内容" class="headerlink" title="自学内容"></a>自学内容</h2><ol><li>Java语言，jdk1.8</li><li>SpringBoot，以2.0.9版本为例子</li><li>Spring Cloud，Finchely版本</li></ol><h2 id="微服务开发相关"><a href="#微服务开发相关" class="headerlink" title="微服务开发相关"></a>微服务开发相关</h2><ol start="0"><li><p>概念和优势、引入的问题并解决–概念讲解</p></li><li><p>基础设施</p></li></ol><ul><li>服务注册和发现</li><li>配置中心</li><li>网关</li><li>服务间调用</li><li>消息总线，消息队列</li><li>熔断、限流、降级</li></ul><ol start="2"><li>具体服务使用和对接，运作流程</li></ol><ul><li>服务注册</li><li>配置推送和拉取</li><li>网关调度</li></ul><ol start="3"><li>服务间调用</li></ol><ul><li>远程HTTP调用</li><li>rpc调用</li><li>远程调用规则</li><li>异步调用</li></ul><ol start="4"><li>自建的基础设施</li></ol><ul><li>分布式id生成器、数据字典、统一日志处理、统一异常处理</li><li>用户中心</li><li>运营中心</li><li>业务建模</li><li>表单</li><li>工作流</li><li>应用管理</li><li>多租户</li><li>国际化-多语言、多时区</li></ul><ol start="5"><li>最佳实践</li></ol><ul><li>领域驱动设计，微服务划分</li><li>事件驱动，消息队列</li><li>分布式事务</li><li>服务容错</li><li>DevOps</li><li>运维监控</li><li>生产完备</li></ul><h2 id="微服务部署相关"><a href="#微服务部署相关" class="headerlink" title="微服务部署相关"></a>微服务部署相关</h2><ol start="0"><li>容器化部署</li></ol><ul><li>docker部署</li><li>CI/CD改造</li></ul><ol><li><p>容器编排-k8s</p></li><li><p>流量管理-istio</p></li><li><p>后续改造</p></li></ol><ul><li>语言无关性，基础设施下沉</li><li>大规模集群管理</li></ul><h2 id="微服务运维相关"><a href="#微服务运维相关" class="headerlink" title="微服务运维相关"></a>微服务运维相关</h2><ol start="0"><li><p>运维保障体系构成</p></li><li><p>全链路监控系统</p></li><li><p>统一日志监控管理系统</p></li><li><p>日常运维监控</p></li><li><p>全面运维平台</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>微服务从docker到k8s的改造历程</title>
      <link href="/2020/04/04/wei-fu-wu-cong-docker-dao-k8s-de-gai-zao-li-cheng/"/>
      <url>/2020/04/04/wei-fu-wu-cong-docker-dao-k8s-de-gai-zao-li-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="微服务从docker到k8s的改造历程"><a href="#微服务从docker到k8s的改造历程" class="headerlink" title="微服务从docker到k8s的改造历程"></a>微服务从docker到k8s的改造历程</h1><h2 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h2><p>目前，我们所有的微服务已经全部用容器进行部署，接入了以jenkins+gitlab+sonar为主的自动构建部署体系，能够实现自动打包，推送到对应服务器上进行部署。整个体系能正常走通，比较符合我们目前的使用需求。但是依旧面临以下问题：</p><ol><li>调试不方便。需要先将开发环境线上服务停止后，将自己开发机的服务启动并注册到线上环境。调试方式不优雅，且影响当前服务的正常使用。</li><li>针对容器的资源限定做的不好，例如jvm内存占用过高的问题。</li><li>在运行的服务集群中，不能做到服务的弹性伸缩，也不能很好的控制流量的转移与分配。</li></ol><p>针对于以上问题，我们决定引入k8s，先完成k8s的安装部署和微服务升级到k8s中，然后使用istio进行网络层面的改造。</p><h2 id="原则和目标"><a href="#原则和目标" class="headerlink" title="原则和目标"></a>原则和目标</h2><p>优先在k8s上部署单个微服务项目，临时决定剥离存储层面的内容。最终达到以三集群部署为主，即每个微服务部署三套，同时对外提供服务。</p><p>目前采用的策略是，临时将所有微服务相关的组件和服务，均一股脑放到k8s中，先做到这个中间形态。在这个阶段完成之后，通过对各项基础设施组件进行拆除，例如配置中心、服务注册和发现中心、网关等，将这些基础设施性质的内容沉淀到PaaS层，也就是目前依托的k8s层面的组件进行代替，例如istio的使用，最终目标是形成一套语言无关的微服务支撑平台。</p><p>在基础设施沉淀后，需要进行整个监控系统的优化，要形成从主机、集群、服务、存储、网络、日志信息等各个层面的监控。尤其是在后续形成多个k8s集群的时候，要统一进行管理。</p><p>最终目标，把这一部分扩展，构建统一的运维平台，利用该运维平台可以进行动态申请资源，配置各个服务的动态伸缩规则，真正做到4个9的高可用！</p><h2 id="各微服务业务部署"><a href="#各微服务业务部署" class="headerlink" title="各微服务业务部署"></a>各微服务业务部署</h2><h3 id="改造要点"><a href="#改造要点" class="headerlink" title="改造要点"></a>改造要点</h3><ol><li>对于Eureka、nacos、gateway这些基础组件优先部署到k8s集群，优先实现eureka的互相注册。</li><li>对于微服务远程接口调用来说，在基础设施不变的情况下，远程接口调用暂时不需要更改。后续变更基础设施时，需要使用完整url链接进行调用，尤其是Feign组件进行远程接口调用的变更。服务间通信，跨命名空间的服务通信，需要使用完整的服务名进行调用，不能使用ClusterIP进行调用。</li><li>关于存储层从k8s剥离，使用原生方式部署，启用高可用方式。对于微服务来说，要允许连接到集群外部的数据库、缓存、分布式存储等存储工具。</li><li>要求重新配置服务监控信息，对于微服务的日志收集，统一展示，对于各个微服务的链路监控信息。</li><li>要求能够将线下开发机接入集群进行调试，方便日后开发环境的联调工作。</li><li>要求能够接入目前的CI/CD体系，能够实现服务自动部署到k8s集群。</li></ol><h3 id="namespace划分"><a href="#namespace划分" class="headerlink" title="namespace划分"></a>namespace划分</h3><ol><li>基础设施：test-basic（nacos、Eureka、gateway），例如：test-basic-nacos</li><li>现有release版本的服务：test-all-services</li><li>测试微服务：test-test-business</li><li>对外连接mysql、redis、mongodb等：test-basic-&lt;服务名称&gt;-outer，例如：test-basic-mysql-outer</li></ol><p>nacos、Eureka均需要对外暴露，使用nodePort的模式对外暴露。至于gateway来说，对于后端服务而言，由于是内部进行调用，可以暂不对gateway开放暴露服务，但是对于前端服务，仍然需要对外暴露，同样使用nodePort方式。由于目前没有独立的DNS服务以及尚未引入https，所以这里采用nodePort的方式进行。</p><h3 id="基础设施划分"><a href="#基础设施划分" class="headerlink" title="基础设施划分"></a>基础设施划分</h3><h4 id="1-针对nacos的改造"><a href="#1-针对nacos的改造" class="headerlink" title="1. 针对nacos的改造"></a>1. 针对nacos的改造</h4><p>参考阿里官方的文档：<a href="https://github.com/nacos-group/nacos-k8s" target="_blank" rel="noopener">https://github.com/nacos-group/nacos-k8s</a></p><p>核心要义：用单机数据库，部署headless无头应用程序，利用ingress对外提供负载均衡服务。</p><p>首先创建nfs存储并进行挂载，随后进行操作的时候将地址定位到具体的节点地址上。参考。</p><p>然后下载<a href="https://github.com/nacos-group/nacos-k8s.git" target="_blank" rel="noopener">nacos-k8s</a>源码，首先转到deploy/nfs文件夹中，依次安装文件夹中的三个文件：</p><pre><code>$ kubectl create ns test-basic-nacos$ kubectl apply -f rbac.yaml -n test-basic-nacos$ kubectl apply -f class.yaml -n test-basic-nacos// 注意：要先修改deployment.yaml中的文件，将nfs部分的设置替换为自己的$ kubectl apply -f deployment.yaml -n test-basic-nacos</code></pre><p>其次转到deploy/mysql文件夹，执行下面的命令：</p><pre><code>// 注意：要先修改mysql-nfs.yaml中的文件，将nfs部分的设置替换为自己的$  kubectl apply -f mysql-nfs.yaml -n test-basic-nacos</code></pre><p>最后，转到deploy/nacos文件夹中，执行下面的命令：</p><pre><code>// 需要先对文件做如下修改，注释掉//  #annotations://  #  service.alpha.kubernetes.io/tolerate-unready-endpoints: &quot;true&quot;// 在spec部分添加publishNotReadyAddresses: true//spec://  #type: NodePort//  publishNotReadyAddresses: true$ kubectl apply -f nacos-pvc-nfs.yaml -n test-basic-nacos</code></pre><p>在该配置文件中，StatefulSet的配置中，serviceName被指定为<em>nacos-headless</em>。</p><p>除此之外，需要将nacos对外进行暴露，于是我自己在deploy文件夹中创建了ingress文件夹，并在ingress文件夹中创建nacos-ingress.yaml，操作如下：</p><pre><code>$ vim nacos-ingress.yaml// 输入以下信息apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: dashboard-nacos-nginx  namespace: test-basic-nacos  annotations:    kubernetes.io/ingress.class: nginx    ingress.kubernetes.io/ssl-redirect: &quot;false&quot;spec:  rules:  - http:      paths:      - path: /nacos        backend:          serviceName: nacos-headless          servicePort: 8848</code></pre><p>依托于集群中已有的ingress-nginx支持，通过ingress对外暴露服务，这样访问集群内任何一台机器的nacos服务均可，例如：10。0.88.241/nacos，即可看到所有的nacos服务。</p><p>对于内部访问，nacos的服务内部访问地址</p><p>nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848<br>nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848<br>nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848</p><!-- 尝试将mysql主从配置分别写入不同的nfs生成的pv和pvc，发现无法进行访问，是rbac的关系，联想之前的内容mysql主从写到了同一个文件夹。分离mysql主从写入不同的nfs位置，创建多个Deployment和storageclass，然后对mysql主从创建特定的pv和pvc，进行写入。 --><!-- https://www.cnblogs.com/cuishuai/p/9152277.html --><!-- 不再使用主从模式，！ --><p>默认端口号为：8848</p><p>注意在配置文件中，填写内部地址时，首先需要加上<strong>http://</strong>，然后是多个nacos连接地址的时候，需要用**”,”*分割！</p><p>例如完整的配置链接如下：</p><pre><code>http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848</code></pre><p>这样nacos就正式配置完成了。但是目前使用的nacos版本偏高，为1.2.0版本，当前开发环境使用的是1.1.2版本的nacos。随后将开发环境中的配置文件导出，添加到现在的nacos中！另外需要从开发环境中导入测试服务相关的配置文件，例如business01、business02相关的配置信息。</p><p><strong>注意：</strong>如果拉取镜像存在问题时，可以通过管理机192.168.88.240拉取，拉取后推送到各个集群主机即可。</p><p>例如针对MySQL的镜像，可以通过下面的命令进行，例如在192.168.88.240机器上执行下面的命令，192.168.88.232为某台集群机器：</p><pre><code>// 镜像拉取$ docker pull mysql:5.7// 镜像推送，目标机器为192.168.88.232，其它机器同理$ docker save mysql:5.7 | bzip2 | ssh -p 15555 centos@192.168.88.232 &#39;bunzip2 | docker load&#39;</code></pre><h3 id="Eureka、Gateway"><a href="#Eureka、Gateway" class="headerlink" title="Eureka、Gateway"></a>Eureka、Gateway</h3><p>nacos作为配置中心使用，需要对Eureka和gateway进行改造，使它们的docker镜像都能动态配置nacos连接信息。首先修改下docker镜像信息，原来的Dockerfile为</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile">FROM openjdk:8-jre-alpineWORKDIR /app#ARG DEPENDENCY=/target# Copy project dependencies from the build stageCOPY ./target/test-serverregister-0.0.1-SNAPSHOT.jar /appENTRYPOINT ["java","-Dspring.profiles.active=standalone","-jar","test-serverregister-0.0.1-SNAPSHOT.jar"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>现在的Dockerfile为</p><pre class="line-numbers language-Dockerfile"><code class="language-Dockerfile">FROM openjdk:8-alpineWORKDIR /app# 添加全局变量ENV CHANNEL=""ENV IP_ADDR=""# nacos配置# 默认为测试环境的地址ENV NACOS_IP="192.168.88.161:18848"ENV NACOS_NAMESPACE="858b37b1-35be-4564-a24e-dc2c322d5784"ENV CHANNEL="standalone"#ARG DEPENDENCY=/target# Copy project dependencies from the build stageCOPY ./target/test-serverregister-0.0.1-SNAPSHOT.jar /appENTRYPOINT ["sh", "-c", "java -Dspring.profiles.active=${CHANNEL} -Dspring.cloud.client.ip-address=${IP_ADDR} -Dnacos_ip=${NACOS_IP} -Dnacos_namespace=${NACOS_NAMESPACE} -jar test-serverregister-0.0.1-SNAPSHOT.jar"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要是修改启动命令信息，添加启动的配置信息。</p><p>下面开始Eureka的安装，安装之前需要注意，当前未搭建k8s中的私有的harbor镜像仓库，继续使用之前的docker registry镜像仓库，需要在节点机器上配置镜像仓库地址，修改/etc/docker/daemon.json文件，如下：</p><pre><code>$ vim /etc/docker/daemon.json// 添加以下有效信息{    &quot;registry-mirrors&quot;: [        &quot;https://docker.mirrors.ustc.edu.cn&quot;,        &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,        &quot;https://registry.docker-cn.com&quot;    ],    &quot;insecure-registries&quot;: [        &quot;192.168.88.159:5000&quot;    ],    &quot;live-restore&quot;: true,    &quot;log-driver&quot;: &quot;json-file&quot;,    &quot;log-opts&quot;: {        &quot;max-size&quot;: &quot;50m&quot;,        &quot;max-file&quot;: &quot;3&quot;    }}</code></pre><p>随后重启docker服务</p><pre><code>$ sudo systemctl restart docker</code></pre><p>下面针对Eureka服务进行配置。</p><p>首先编辑Eureka的安装文件，修改服务注册中的nacos配置选项，然后创建eureka的部署文件eureka.yaml，最后修改nacos中的关于eureka的配置信息，主要修改url地址信息。</p><pre><code>$ cd ./Eureka &amp;&amp; vim k8s-eureka.yaml// 在文件中输入以下内容---apiVersion: v1kind: Servicemetadata:  name: eureka  namespace: test-basic-eureka  labels:    app: eurekaspec:  ports:    - port: 19011      name: eureka      targetPort: 19011  clusterIP: None  selector:    app: eureka---apiVersion: apps/v1kind: StatefulSetmetadata:  name: eureka  namespace: test-basic-eurekaspec:  serviceName: &quot;eureka&quot;  replicas: 3  selector:    matchLabels:      app: eureka  template:    metadata:      labels:        app: eureka    spec:      containers:        - name: eureka          image: 192.168.88.159:5000/server-register-k8s:4          ports:            - containerPort: 19011          resources:            limits:              memory: 1Gi          env:            - name: IP_ADDR              #value: eureka-*.eureka-headless.test-basic-eureka.svc.cluster.local              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              #value: nacos-headless.test-basic-nacos              value: http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848            - name: NACOS_NAMESPACE              value: test-dev            - name: CHANNEL              value: standalone  podManagementPolicy: &quot;Parallel&quot;</code></pre><p>需要注意两个点，一个是构建Eureka镜像，是单独在jenkins服务中创建的构建任务进行生成的，和日常用的Eureka镜像并不太一样。单独创建k8s-dev-apps工作空间，创建eureka的构建服务，依托之前创建的服务来进行构建新的镜像。第二个就是配置nacos集群ip地址，如上面所示。第三就是指定配置文件信息。</p><p>在完成上面工作后，在命令行执行以下命令，安装Eureka服务：</p><pre><code>$ kubectl create ns test-basic-eureka$ kubectl apply -f k8s-eureka.yaml -n test-basic-eureka</code></pre><p>完成后，需要在nacos中导入目前沿用的配置信息并进行修改，由于我这里指定的是standalone尾缀的配置文件，需要找到<strong>test-register-standalone.yml</strong>进行修改，最终修改后如下：</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">eureka</span><span class="token punctuation">:</span>  <span class="token key atrule">instance</span><span class="token punctuation">:</span>     <span class="token key atrule">preferIpAddress</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>    <span class="token comment" spellcheck="true"># hostname: ${spring.cloud.client.ip-address}</span>    <span class="token comment" spellcheck="true"># appname: ${spring.application.name}</span>  <span class="token key atrule">client</span><span class="token punctuation">:</span>    <span class="token key atrule">register-with-eureka</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">fetch-registry</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">serviceUrl</span><span class="token punctuation">:</span>      <span class="token key atrule">defaultZone</span><span class="token punctuation">:</span> http<span class="token punctuation">:</span>//test<span class="token punctuation">:</span>test2019@eureka<span class="token punctuation">-</span>0.eureka.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>eureka.svc.cluster.local<span class="token punctuation">:</span>19011/eureka/<span class="token punctuation">,</span>http<span class="token punctuation">:</span>//test<span class="token punctuation">:</span>test2019@eureka<span class="token punctuation">-</span>1.eureka.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>eureka.svc.cluster.local<span class="token punctuation">:</span>19011/eureka/<span class="token punctuation">,</span>http<span class="token punctuation">:</span>//test<span class="token punctuation">:</span>test2019@eureka<span class="token punctuation">-</span>2.eureka.test<span class="token punctuation">-</span>basic<span class="token punctuation">-</span>eureka.svc.cluster.local<span class="token punctuation">:</span>19011/eureka/  <span class="token key atrule">server</span><span class="token punctuation">:</span>    <span class="token key atrule">enable-self-preservation</span><span class="token punctuation">:</span> <span class="token boolean important">false      </span><span class="token key atrule">server</span><span class="token punctuation">:</span>  <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">19011</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要是配置Eureka的连接信息，使他们进行相互注册。修改完成后，点击publish按钮，发布后，服务会自动更新配置信息。</p><p>最后设置对外访问，使用nodePort模式，配置如下：</p><pre><code>$ vim eureka-nodeport.yaml// 设置以下信息apiVersion: v1kind: Servicemetadata:  name: eureka-nodeport  namespace: test-basic-eurekaspec:  selector:    app: eureka  type: NodePort  ports:    - port: 19011      targetPort: 19011      nodePort: 31011      protocol: TCP// 执行$ kubectl apply -f eureka-nodeport.yaml -n test-basic-eureka</code></pre><p>这样就完成了eureka集群的部署，访问任何一台集群机器内的eureka服务，如192.168.88.232:31011，即可看到该服务信息，应该看到当前相互注册的三个eureka服务。</p><!-- eureka不使用无状态服务（headless）进行部署！！！！ --><h4 id="gateway部署"><a href="#gateway部署" class="headerlink" title="gateway部署"></a>gateway部署</h4><!-- 针对镜像的改造，将java启动命令的信息拆出来，优化分级。 --><!-- 问题：配置信息问题，配置信息可以放在nacos中，也可以放在启动文件的yaml中，这个如何进行权衡？ --><p>注意：设置连接外部服务时，需要将service名称和endpoints名称设置为一致，才能使服务生效。</p><p>参考链接：<a href="https://qingmu.io/2019/08/07/Run-Spring-Cloud-Gateway-on-kubernetes-2/" target="_blank" rel="noopener">https://qingmu.io/2019/08/07/Run-Spring-Cloud-Gateway-on-kubernetes-2/</a></p><p>针对gateway的代码并不需要进行变更，需要变更Dockerfile镜像文件，改造方式同Eureka服务，只是jar包名称不同。</p><p>对gateway的配置信息进行修改，主要是修改Eureka的连接信息，修改<strong>test-gateway-standalone.yml</strong>如下：</p><pre><code>server:  port: 19020eureka:  client:    service-url:      defaultZone: http://test:test2019@eureka-0.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-1.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-2.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/</code></pre><p>然后针对gateway重新创建构建，利用该构建创建新的docker镜像，进行部署。在服务器上编写部署文件如下：</p><pre><code>$ vim gateway.deployment.yaml// 输入以下信息---apiVersion: v1kind: Servicemetadata:  name: gateway  namespace: test-basic-gateway  labels:    app: gatewayspec:  ports:    - port: 19020      name: tcp      targetPort: 19020  selector:    app: gateway---apiVersion: apps/v1kind: Deploymentmetadata:  name: gateway  namespace: test-basic-gatewayspec:  revisionHistoryLimit: 10  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 25%      maxSurge: 25%  replicas: 3  selector:    matchLabels:      app: gateway  template:    metadata:      labels:        app: gateway    spec:      affinity:        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:            - podAffinityTerm:                topologyKey: kubernetes.io/hostname                labelSelector:                  matchExpressions:                    - key: app                      operator: In                      values:                        - app-gateway              weight: 1      containers:        - name: gateway          image: 192.168.88.159:5000/gateway-k8s:1          imagePullPolicy: Always          lifecycle:            preStop:              httpGet:                port: 19020                path: /spring/shutdown          livenessProbe:            httpGet:              path: /actuator/health              port: 19020            initialDelaySeconds: 300            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          readinessProbe:            httpGet:              path: /actuator/health              port: 19020            initialDelaySeconds: 300            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          resources:            requests:              memory: 1Gi            limits:              memory: 1Gi          ports:            - containerPort: 19020          env:            - name: IP_ADDR              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848            - name: NACOS_NAMESPACE              value: test-dev            - name: CHANNEL              value: standalone</code></pre><p>设置完成后，需要对gateway服务对外进行暴露，由于后续要通过gateway访问其它的服务，这里使用域名的方式进行配置，防止把访问的url信息也带到gateway的解析过程中，导致服务找不到，如下：</p><!-- 初始使用带url的路径的ingress方式进行测试，发现访问不通，后续将路径去掉，使用域名的方式进行访问，这样可以对要部署的内容进行正常访问。 --><pre><code>$ vim ingress-gateway.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata:  name: gateway-nginx  namespace: test-basic-gateway  annotations:    kubernetes.io/ingress.class: nginx    ingress.kubernetes.io/ssl-redirect: &quot;false&quot;spec:  rules:  - host: www.test-gateway.net    http:      paths:      - path: /        backend:          serviceName: gateway          servicePort: 19020</code></pre><p>设置完成后，需要在本地配置host文件，指向gateway的地址，如下：</p><pre><code>// 在host文件中添加下面的配置192.168.88.232 www.test-gateway.net192.168.88.239 www.test-gateway.net192.168.88.241 www.test-gateway.net</code></pre><p>这样就配置完成了，首先去已经部署的eureka的地址，查看eureka中是否已经存在该服务。然后访问gateway的信息，例如使用postman直接访问<a href="http://www.test-gateway.net，这里会报错，显示服务调用失败，如下：" target="_blank" rel="noopener">www.test-gateway.net，这里会报错，显示服务调用失败，如下：</a></p><pre><code>{    &quot;message&quot;: &quot;服务调用失败&quot;,    &quot;code&quot;: -1,    &quot;timestamp&quot;: 1585209902994}</code></pre><p>说明配置成功！</p><p><strong>注意：</strong>需要对前面的内容进行说明，将gateway对外暴露，使用nodePort的方式，原因是如果单纯使用“service名称+namespace名称+端口号”的形式进行调用，会出现无法连接的情况。</p><p>nodePort方式如下，修改gateway的部署文件：</p><pre><code>$ vim gateway.deployment.yaml// 修改以下信息，添加type为NodePort，指定nodePort端口号---apiVersion: v1kind: Servicemetadata:  name: gateway  namespace: test-basic-gateway  labels:    app: gatewayspec:  type: NodePort  ports:    - port: 19020      name: tcp      targetPort: 19020      nodePort: 31333  selector:    app: gateway</code></pre><p>这样在调用gateway的时候，可以通过192.168.88.239:31333来进行调用。nodePort方式和ingress方式可以共存。</p><h3 id="各微服务的改造，以测试项目为例子进行改造"><a href="#各微服务的改造，以测试项目为例子进行改造" class="headerlink" title="各微服务的改造，以测试项目为例子进行改造"></a>各微服务的改造，以测试项目为例子进行改造</h3><p>下面拿出一个测试案例，该案例中有两个微服务BUSINESS-01和BUSINESS-02，有服务间进行调用的接口，以此来测试服务是否能正常运行。</p><p>首先需要对项目进行改造，切换分支到feature.ICP.change_name，使其能够符合现行的框架约定，主要是添加nacos的调用信息，剥离配置文件到nacos中。然后选择在jenkins创建构建项目，打包生成镜像信息。</p><p><strong>注意：在拥有多个子项目且有子项目之间互相依赖时，需要先在最外层执行mvn clean install，然后到某个单项微服务中去执行构建命令。</strong></p><p>随后开始编写第一个项目的部署文档，如下：</p><pre><code>$ vim test-business-01.yaml---apiVersion: v1kind: Servicemetadata:  name: test-business-01  namespace: test-test-business  labels:    app: test-business-01spec:  ports:    - port: 19030      name: tcp      targetPort: 19030  selector:    app: test-business-01---apiVersion: apps/v1kind: Deploymentmetadata:  name: test-business-01  namespace: test-test-businessspec:  revisionHistoryLimit: 10  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 25%      maxSurge: 25%  replicas: 1  selector:    matchLabels:      app: test-business-01  template:    metadata:      labels:        app: test-business-01    spec:      affinity:        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:            - podAffinityTerm:                topologyKey: kubernetes.io/hostname                labelSelector:                  matchExpressions:                    - key: app                      operator: In                      values:                        - app-test-business-01              weight: 1      containers:        - name: test-business-01          image: 192.168.88.159:5000/test-business01-k8s:6          imagePullPolicy: Always          lifecycle:            preStop:              httpGet:                port: 19030                path: /spring/shutdown          livenessProbe:            httpGet:              path: /actuator/health              port: 19030            initialDelaySeconds: 300            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          readinessProbe:            httpGet:              path: /actuator/health              port: 19030            initialDelaySeconds: 300            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          resources:            requests:              memory: 1Gi            limits:              memory: 1Gi          ports:            - containerPort: 19030          env:            - name: IP_ADDR              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848            - name: NACOS_NAMESPACE              value: test-dev            - name: CHANNEL              value: standalone</code></pre><p>首先部署单个项目，操作如下：</p><pre><code>// 创建namespace$ kubectl create ns test-test-business// 创建服务$ kubectl apply -f test-business-01.yaml -n test-test-business// 获取test服务信息$ kubectl get pod -n test-test-businessNAME                                READY   STATUS    RESTARTS   AGEtest-business-01-656c496946-g5l26   1/1     Running   0          149m</code></pre><p>这样第一个服务部署成功，查看Eureka的服务信息，服务已经上线，服务名称为<strong>TEST-BUSINESS01</strong>，说明部署成功。</p><p>然后部署另外一个项目，配置信息与上面基本一致，区别在于服务名称由business01变更为business02，镜像也有所改变，为192.168.88.159:5000/test-business02-k8s:9，端口号根据配置信息更改为19040。操作与上面一致，最终结果如下：</p><pre><code>$ kubectl get pod -n test-test-businessNAME                                READY   STATUS    RESTARTS   AGEtest-business-01-656c496946-g5l26   1/1     Running   0          149mtest-business-02-58655d78c6-5x8zq   1/1     Running   0          156m</code></pre><p>查看Eureka的服务信息，第二个服务已经上线，说明部署成功。</p><p>随后使用postman或者命令行的方式进行调用，操作如下：</p><p><img src="%E6%8E%A5%E5%8F%A3%E8%AF%B7%E6%B1%82.png" alt></p><p>最后看到可以进行调用。</p><h3 id="连接外部的MySQL、Redis、MongoDB"><a href="#连接外部的MySQL、Redis、MongoDB" class="headerlink" title="连接外部的MySQL、Redis、MongoDB"></a>连接外部的MySQL、Redis、MongoDB</h3><p>以MySQL为例子，演示k8s中的微服务连接外部的存储。</p><pre><code>$ vim mysql-connection/mysql-endpoints.yaml// 输入以下信息apiVersion: v1kind: Endpointsmetadata:  name: mysql-outer  namespace: test-basic-mysql-outersubsets:  - addresses:    - ip: 192.168.88.190    ports:      - port: 3306</code></pre><p>将k8s集群外部的mysql连接配置为endpoint，执行命令：</p><pre><code>$ kubectl create ns test-basic-mysql-outer$ kubectl apply -f mysql-connection/mysql-endpoints.yaml -n test-basic-mysql-outer</code></pre><p>随后将该endpoint作为服务提供给k8s集群中的微服务使用。如下：</p><pre><code>$ vim mysql-connection/mysql-service.yaml// 输入以下信息apiVersion: v1kind: Servicemetadata:  name: mysql-outer  namespace: test-basic-mysql-outerspec:  ports:    - port: 3306</code></pre><p>执行以下命令：</p><pre><code>kubectl apply -f mysql-connection/mysql-service.yaml -n test-basic-mysql-outer</code></pre><p>这样MySQL的外部连接就配置好了，需要我们对MySQL的连接字符串进行配置，原来的连接配置字符串为：</p><pre><code>jdbc:mysql://192.168.88.159:3306/elk?useSSL=false&amp;useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC</code></pre><p>新的连接字符串配置为：</p><pre><code>jdbc:mysql://mysql-outer.test-basic-mysql-outer:3306/elk?useSSL=false&amp;useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC</code></pre><p><strong>注意：</strong>必须使用service名称+namespace名称的方式进行连接，否则会存在找不到服务的情况！例如redis配置后的连接为：</p><pre><code>redis-outer.test-basic-redis-outer</code></pre><h3 id="升级和回滚操作"><a href="#升级和回滚操作" class="headerlink" title="升级和回滚操作"></a>升级和回滚操作</h3><p>首先需要对部署的deployment进行跟踪。</p><p>第一，可以直接修改yaml配置文件中的镜像信息，使用kubectl apply -f命令执行升级。</p><p>第二，使用kubectl edit命令来进行升级，操作类似vim，替换image位置的镜像信息，修改后即可生效</p><p>实际操作中，根据我们在对应namespace中创建的deployment信息，进行操作，例如对gateway进行扩容，操作命令如下：</p><pre><code>kubectl scale deployment test-gateway --replicas=5 -n test-basic-gateway</code></pre><p>将之前的三个pod扩展为五个pod的操作。</p><p>操作方式参考：<a href="https://www.cnblogs.com/Tempted/p/7831604.html" target="_blank" rel="noopener">https://www.cnblogs.com/Tempted/p/7831604.html</a></p><h3 id="关于微服务编码上的影响"><a href="#关于微服务编码上的影响" class="headerlink" title="关于微服务编码上的影响"></a>关于微服务编码上的影响</h3><ol><li>链接需要更改为服务名称，尤其连接nacos、eureka的连接，必须使用完整的链路进行，这部分只需要修改配置信息即可，也不需要进行额外配置。</li><li>连接k8s集群外部的数据库、缓存等内容，需要先配置内部连接的EndPoint，提供连接的service，内部通过“service名称+namespace名称”进行调用。</li></ol><h3 id="微服务：分布式id的部署示例"><a href="#微服务：分布式id的部署示例" class="headerlink" title="微服务：分布式id的部署示例"></a>微服务：分布式id的部署示例</h3><ol><li>zookeeper的安装</li></ol><p>首先创建pv，创建存储空间，使用本地文件路径的方式，操作如下</p><pre><code>$ cd zookeeper &amp;&amp; vim pv-zk.yaml// 输入以下信息---apiVersion: v1kind: PersistentVolumemetadata:  name: pv-zk1  annotations:    volume.beta.kubernetes.io/storage-class: &quot;anything&quot;       labels:    type: localspec:  capacity:    storage: 2Gi  accessModes:    - ReadWriteOnce  hostPath:    path: &quot;/opt/data/zookeeper&quot;               persistentVolumeReclaimPolicy: Recycle---apiVersion: v1kind: PersistentVolumemetadata:  name: pv-zk2  annotations:    volume.beta.kubernetes.io/storage-class: &quot;anything&quot;  labels:    type: localspec:  capacity:    storage: 2Gi  accessModes:    - ReadWriteOnce  hostPath:    path: &quot;/opt/data/zookeeper&quot;                persistentVolumeReclaimPolicy: Recycle---apiVersion: v1kind: PersistentVolumemetadata:  name: pv-zk3  annotations:    volume.beta.kubernetes.io/storage-class: &quot;anything&quot;  labels:    type: localspec:  capacity:    storage: 2Gi  accessModes:    - ReadWriteOnce  hostPath:    path: &quot;/opt/data/zookeeper&quot;  persistentVolumeReclaimPolicy: Recycle// 执行下面的命令进行生效$ kubectl create namespace test-zookeeper$ kubectl apply -f pv-zk.yaml -n test-zookeeper</code></pre><p>随后创建zookeeper服务，操作如下：</p><pre><code>$ vim k8s-zk.yaml// 输入以下信息---apiVersion: v1kind: Servicemetadata:  name: zk-hs  labels:    app: zkspec:  ports:  - port: 2888    name: server  - port: 3888    name: leader-election  clusterIP: None  selector:    app: zk---apiVersion: v1kind: Servicemetadata:  name: zk-cs  labels:    app: zkspec:  ports:  - port: 2181    name: client  selector:    app: zk---apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata:  name: zk-pdbspec:  selector:    matchLabels:      app: zk  maxUnavailable: 1---apiVersion: apps/v1kind: StatefulSetmetadata:  name: zkspec:  selector:    matchLabels:      app: zk  serviceName: zk-hs  replicas: 3  updateStrategy:    type: RollingUpdate  podManagementPolicy: OrderedReady     template:    metadata:      labels:        app: zk    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            - labelSelector:                matchExpressions:                  - key: &quot;app&quot;                    operator: In                    values:                    - zk              topologyKey: &quot;kubernetes.io/hostname&quot;      containers:      - name: kubernetes-zookeeper        imagePullPolicy: Always        image: &quot;k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10&quot;        resources:          requests:            memory: &quot;1Gi&quot;            cpu: &quot;0.5&quot;        ports:        - containerPort: 2181          name: client        - containerPort: 2888          name: server        - containerPort: 3888          name: leader-election        command:        - sh        - -c        - &quot;start-zookeeper \          --servers=3 \          --data_dir=/var/lib/zookeeper/data \          --data_log_dir=/var/lib/zookeeper/data/log \          --conf_dir=/opt/zookeeper/conf \          --client_port=2181 \          --election_port=3888 \          --server_port=2888 \          --tick_time=2000 \          --init_limit=10 \          --sync_limit=5 \          --heap=512M \          --max_client_cnxns=60 \          --snap_retain_count=3 \          --purge_interval=12 \          --max_session_timeout=40000 \          --min_session_timeout=4000 \          --log_level=INFO&quot;        readinessProbe:          exec:            command:            - sh            - -c            - &quot;zookeeper-ready 2181&quot;          initialDelaySeconds: 10          timeoutSeconds: 5        livenessProbe:          exec:            command:            - sh            - -c            - &quot;zookeeper-ready 2181&quot;          initialDelaySeconds: 10          timeoutSeconds: 5        volumeMounts:        - name: datadir          mountPath: /var/lib/zookeeper  volumeClaimTemplates:  - metadata:      name: datadir      annotations:        volume.beta.kubernetes.io/storage-class: &quot;anything&quot;      spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      resources:        requests:          storage: 2Gi// 执行下面的命令生效$ kubectl apply -f k8s-zk.yaml -n test-zookeeper</code></pre><ol start="2"><li>分布式id服务的部署</li></ol><p>该分布式id项目基于<a href="https://github.com/Meituan-Dianping/Leaf" target="_blank" rel="noopener">美团的开源的分布式id生成器</a>，使用雪花算法生成id信息。</p><p>在zookeeper安装完成后，继续进行分布式id的部署，分布式id使用雪花算法生成id信息，需要依赖zookeeper服务进行生成。首先更改配置文件，连接zookeeper如下：</p><pre class="line-numbers language-properties"><code class="language-properties"><span class="token attr-name">spring.freemarker.cache</span><span class="token punctuation">=</span><span class="token attr-value">false</span><span class="token attr-name">spring.freemarker.charset</span><span class="token punctuation">=</span><span class="token attr-value">UTF-8</span><span class="token attr-name">spring.freemarker.check-template-location</span><span class="token punctuation">=</span><span class="token attr-value">true</span><span class="token attr-name">spring.freemarker.content-type</span><span class="token punctuation">=</span><span class="token attr-value">text/html</span><span class="token attr-name">spring.freemarker.expose-request-attributes</span><span class="token punctuation">=</span><span class="token attr-value">true</span><span class="token attr-name">spring.freemarker.expose-session-attributes</span><span class="token punctuation">=</span><span class="token attr-value">true</span><span class="token attr-name">spring.freemarker.request-context-attribute</span><span class="token punctuation">=</span><span class="token attr-value">request</span><span class="token comment" spellcheck="true"># 1.Eureka配置信息</span><span class="token comment" spellcheck="true">#spring.application.name=distributedIDGenerator2</span><span class="token comment" spellcheck="true">#spring.application.name=DISTRIBUTEDIDGENERATOR</span><span class="token attr-name">spring.application.name</span><span class="token punctuation">=</span><span class="token attr-value">test-id-generator</span><span class="token comment" spellcheck="true"># 注意：在这里更改时，需要在leaf.properties配置文件中更改端口信息</span><span class="token attr-name">server.port</span><span class="token punctuation">=</span><span class="token attr-value">8080</span><span class="token attr-name">eureka.instance.prefer-ip-address</span><span class="token punctuation">=</span><span class="token attr-value">true</span><span class="token attr-name">eureka.instance.instance-id</span><span class="token punctuation">=</span><span class="token attr-value">${spring.application.name}@${spring.cloud.client.ip-address}:${server.port}</span><span class="token attr-name">eureka.client.service-url.defaultZone</span><span class="token punctuation">=</span><span class="token attr-value">http://test:test2019@eureka-0.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-1.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/,http://test:test2019@eureka-2.eureka.test-basic-eureka.svc.cluster.local:19011/eureka/</span><span class="token attr-name">eureka.instance.ipAddress</span><span class="token punctuation">=</span><span class="token attr-value">${spring.cloud.client.ip-address}</span><span class="token comment" spellcheck="true">#eureka.client.service-url.defaultZone=http://test:test2019@192.168.123.93.153:19011/eureka/</span><span class="token comment" spellcheck="true"># 2. Feign中的Ribbon配置</span><span class="token comment" spellcheck="true"># 让 Hystrix 的超时时间大于 Ribbon 的超时时间，否则 Hystrix 命令超时后，该命令直接熔断，重试机制就没有任何意义了。</span><span class="token attr-name">ribbon.ConnectTimeout</span><span class="token punctuation">=</span><span class="token attr-value">500</span><span class="token attr-name">ribbon.ReadTimeout</span><span class="token punctuation">=</span><span class="token attr-value">2000</span><span class="token attr-name">distributed-id-generator.ribbon.MaxAutoRetries</span><span class="token punctuation">=</span><span class="token attr-value">2</span><span class="token comment" spellcheck="true"># 3  Hystrix配置</span><span class="token comment" spellcheck="true"># 配置全局的超时时间</span><span class="token attr-name">hystrix.command.default.execution.isolation.thread.timeoutinMilliseconds</span><span class="token punctuation">=</span><span class="token attr-value">5000</span><span class="token attr-name">hystrix.command.distributed-id-generator.execution.isolation.thread.timeoutinMilliseconds</span><span class="token punctuation">=</span><span class="token attr-value">5000</span><span class="token comment" spellcheck="true"># 启用全局hystrix配置</span><span class="token attr-name">feign.hystrix.enabled</span><span class="token punctuation">=</span><span class="token attr-value">true</span><span class="token comment" spellcheck="true"># 4. id生成器自己的配置信息</span><span class="token attr-name">leaf.name</span><span class="token punctuation">=</span><span class="token attr-value">com.sankuai.leaf.opensource.test</span><span class="token attr-name">leaf.segment.enable</span><span class="token punctuation">=</span><span class="token attr-value">false</span><span class="token comment" spellcheck="true">#leaf.jdbc.url=</span><span class="token comment" spellcheck="true">#leaf.jdbc.username=</span><span class="token comment" spellcheck="true">#leaf.jdbc.password=</span><span class="token comment" spellcheck="true"># 启用雪花算法，设置zookeeper的地址</span><span class="token attr-name">leaf.snowflake.enable</span><span class="token punctuation">=</span><span class="token attr-value">true</span><span class="token comment" spellcheck="true">#### 主要是修改这个位置的zookeeper连接，使用服务名+namespace名称+端口号的方式进行调用</span><span class="token attr-name">leaf.snowflake.zk.address</span><span class="token punctuation">=</span><span class="token attr-value">zk-cs.test-zookeeper:2181</span><span class="token attr-name">leaf.snowflake.port</span><span class="token punctuation">=</span><span class="token attr-value">8080</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里zookeeper地址指定为<strong>zk-cs.test-zookeeper:2181</strong>。下面开始部署id生成器：</p><pre><code>$ vim distributed-id-gen.yaml// 输入以下信息---apiVersion: v1kind: Servicemetadata:  name: test-id-generator  namespace: test-all-service  labels:    app: test-id-generatorspec:  ports:    - port: 8080      name: tcp      targetPort: 8080  selector:    app: test-id-generator---apiVersion: apps/v1kind: Deploymentmetadata:  name: test-id-generator  namespace: test-all-servicespec:  revisionHistoryLimit: 10  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 25%      maxSurge: 25%  replicas: 3  selector:    matchLabels:      app: test-id-generator  template:    metadata:      labels:        app: test-id-generator    spec:      affinity:        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:            - podAffinityTerm:                topologyKey: kubernetes.io/hostname                labelSelector:                  matchExpressions:                    - key: app                      operator: In                      values:                        - app-test-id-generator              weight: 1      containers:        - name: distributed-id-gen          image: 192.168.88.159:5000/distributed-id-gen-k8s:3          imagePullPolicy: Always          lifecycle:            preStop:              httpGet:                port: 8080                path: /spring/shutdown          livenessProbe:            httpGet:              path: /actuator/health              port: 8080            initialDelaySeconds: 300            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          readinessProbe:            httpGet:              path: /actuator/health              port: 8080            initialDelaySeconds: 300            periodSeconds: 5            timeoutSeconds: 10            successThreshold: 1            failureThreshold: 5          resources:            requests:              memory: 500Mi            limits:              memory: 1Gi          ports:            - containerPort: 8080          env:            - name: IP_ADDR              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NACOS_IP              value: http://nacos-0.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-1.nacos-headless.test-basic-nacos.svc.cluster.local:8848,http://nacos-2.nacos-headless.test-basic-nacos.svc.cluster.local:8848            - name: NACOS_NAMESPACE              value: test-dev            - name: SKYWALKING_NAMESPACE              value: test-test            - name: SKYWALKING_TARGET_SERVICE_NAME              value: test-ditributed-id-gen            - name: SKYWALKING_IP_PORT              value: 192.168.88.163:11800            - name: CHANNEL              value: standalone// 执行下面的信息生效$ kubectl apply -f distributed-id-gen.yaml -n test-all-service</code></pre><p>这样就部署完成的分布式id服务！</p><h2 id="基础镜像仓库–Harbor（待完成）"><a href="#基础镜像仓库–Harbor（待完成）" class="headerlink" title="基础镜像仓库–Harbor（待完成）"></a>基础镜像仓库–Harbor（待完成）</h2><ol><li><p>helm 配置</p></li><li><p>harbor文件配置修改</p></li><li><p>pv和pvc创建</p></li><li><p>镜像拉取并推送到各个节点</p></li><li><p>访问</p></li><li><p>导入registry镜像数据–基础镜像和数据</p></li></ol><h2 id="CI-CD构建改造（待完成）"><a href="#CI-CD构建改造（待完成）" class="headerlink" title="CI/CD构建改造（待完成）"></a>CI/CD构建改造（待完成）</h2><p>问题：对于镜像版本的管理方式</p><p>使用jenkins的k8s插件进行构建。</p><p>微服务变更：</p><ol><li>修改配置信息，主要针对Eureka的连接、数据库连接、缓存连接、mongodb的连接。</li><li>k8s部署的yaml信息进行配置，最好考虑用模板的方式进行，能传入版本号，保持和jenkins的版本一致。</li><li>镜像名称更改并重新打包。</li><li>检查远程调用的内容，查看是否需要修改。</li></ol><p>jenkins构建时，保留前面的流程，从docker部署的位置进行修改，以数据字典为例，删除含有以下代码的shell脚本执行块：</p><pre><code>#--------------------------------------------------------------------------# 判断是否存在镜像# docker ps -a | grep -w data-dictionary-server &amp;&gt; /dev/null# 如果存在先停止运行并删除镜像if [ &quot;$(docker ps -a | grep -w data-dictionary-server)&quot; ]; then    echo &quot;data-dictionary-server is exsited!!&quot;    docker stop `docker ps -a | grep -w data-dictionary-server | awk &#39;{print $1}&#39;`    docker rm `docker ps -a | grep -w data-dictionary-server | awk &#39;{print $1}&#39;`    docker image rm `docker images | grep -w data-dictionary-server | awk &#39;{print $3}&#39;`fiecho &quot;pull the data-dictionary-server image&quot;docker pull 192.168.88.159:5000/data-dictionary-server:$BUILD_NUMBERdocker run --restart=on-failure:10 -d -p 19090:19090 -e CHANNEL=&quot;standalone&quot; -e IP_ADDR=&quot;192.168.88.193&quot; -e NACOS_IP=&quot;192.168.88.194:18848&quot; -e NACOS_NAMESPACE=&quot;aa853012-28dd-404a-a941-e4e36324f615&quot;  -e SKYWALKING_NAMESPACE=&quot;test-test&quot; -e SKYWALKING_TARGET_SERVICE_NAME=&quot;test-data-dict&quot; -e SKYWALKING_IP_PORT=&quot;192.168.88.163:11800&quot; 192.168.88.159:5000/data-dictionary-server:$BUILD_NUMBER</code></pre><p>保存后，安装jenkins中的kubernetes插件。</p><p><strong>更新：</strong>kubernetes插件并不能将微服务部署到k8s集群中，这里预留两种思路进行：</p><ol><li><p>利用helm工具，以harbor镜像仓库为支撑，微服务打包成docker image后，推送到harbor中。然后使用k8s集群中某台机器，通过ssh登录后，利用helm安装到对应的namespace下。</p></li><li><p>在k8s集群中选择一台机器，针对gitlab代码仓库中各个微服务的部署文件，例如deployment.yaml，单独拉取该文件信息。在镜像打包完毕后，生成版本信息。将该版本信息，设置到部署文件中，替换原来部署文件的版本信息。使用kubectl apply命令让部署文件生效。替换原来的镜像，使新的镜像生效。</p></li></ol><h2 id="后续的微服务改造思路"><a href="#后续的微服务改造思路" class="headerlink" title="后续的微服务改造思路"></a>后续的微服务改造思路</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>语言无关性，将java侧Spring Cloud框架的具体依赖信息，下沉到k8s层面，利用k8s提供的各项功能替换，只在开发业务时选择语言。</p><h3 id="改造思路"><a href="#改造思路" class="headerlink" title="改造思路"></a>改造思路</h3><ol><li>Eureka下沉到k8s，利用k8s自身的服务发现进行替代</li><li>nacos信息使用k8s中的ConfigMap进行替代</li><li>gateway使用k8s中的网关来替代</li><li>建设统一的管理界面，需要对管理面板选型，目前感觉rancher还是不够用</li><li>对于监控层面，全链路监控沿用skywalking，日常监控使用promethus+grafana</li><li>引入多个k8s集群，解决跨集群通信访问的问题，尽量让关联性强的服务部署在同一个集群中</li></ol><h2 id="针对k8s的培训计划"><a href="#针对k8s的培训计划" class="headerlink" title="针对k8s的培训计划"></a>针对k8s的培训计划</h2><ol start="0"><li>先讲解k8s是什么，优秀特性以及如何进行搭建。</li><li>先以一个最简单的应用进行培训，例如nginx，穿插讲解涉及到的知识，尤其是对配置文件进行解释，入门级。</li><li>讲解具体到使用的基础设施层面，例如Eureka和gateway的时候，如何进行部署，如何进行伸缩，如何进行故障转移。</li><li>讲解具体的服务如何部署，并进行联动，设置服务间相互调用的例子，设置服务调用外部存储的例子，讲解如何进行负载均衡，多种均衡方式，如何暴露端口和url使其可以进行对外访问。</li><li>讲解前端的部署，静态资源的优化，穿插讲解configMap等知识。</li><li>讲解有状态服务和无状态服务的区别，演示mysql单体部署和集群部署，Redis的集群部署。</li><li>讲解如何进行线下调试，使用kt-connect工具进行测试。</li><li>现有的CI/CD体系如何进行接入。</li><li>未涉及的内容统一进行讲解，课程总结。<!-- 6. 总结所有的k8s相关知识点 --></li></ol><h2 id="附录，修改docker-daemon-json不重启镜像的操作"><a href="#附录，修改docker-daemon-json不重启镜像的操作" class="headerlink" title="附录，修改docker daemon.json不重启镜像的操作"></a>附录，修改docker daemon.json不重启镜像的操作</h2><p>vim  clean_exited_docker_containers.sh</p><p>docker rm $(docker ps -q -f status=exited)</p><p>所有机器添加以下选项，一个是私有仓库，另一个是保证docker daemon进程重启时不要重启其它的正在运行的镜像，但是只对设置后的生效，需要先配置：</p><pre><code># vim /etc/docker/daemon.json// 添加以下信息&quot;insecure-registries&quot;: [    &quot;192.168.88.159:5000&quot;],&quot;live-restore&quot;: true,</code></pre><p>还需要添加该公有仓库信息：”<a href="https://registry.cn-hangzhou.aliyuncs.com&quot;,。但是要注意，如果之前没有进行配置，重启docker服务，容器还是会重启，配置后重启docker服务，当前运行的容器不重启。" target="_blank" rel="noopener">https://registry.cn-hangzhou.aliyuncs.com&quot;,。但是要注意，如果之前没有进行配置，重启docker服务，容器还是会重启，配置后重启docker服务，当前运行的容器不重启。</a></p><p>需要单独搭建Harbor集群，放在单独的k8s镜像中</p><h2 id="附录，关于jenkins安装插件超时的问题"><a href="#附录，关于jenkins安装插件超时的问题" class="headerlink" title="附录，关于jenkins安装插件超时的问题"></a>附录，关于jenkins安装插件超时的问题</h2><p>替换url</p><p><a href="https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json</a></p><p><a href="http://mirror.esuni.jp/jenkins/updates/update-center.json" target="_blank" rel="noopener">http://mirror.esuni.jp/jenkins/updates/update-center.json</a></p><h2 id="附录，持续集成流程修改"><a href="#附录，持续集成流程修改" class="headerlink" title="附录，持续集成流程修改"></a>附录，持续集成流程修改</h2><ol start="0"><li>前面的打包流程不做变更，删除docker镜像部署的部分，更改为部署到k8s的流程</li><li>搭建harbor镜像仓库，配置到helm中</li><li>打包推送镜像完毕后，在目标机上通过helm进行部署</li></ol><p>目前存在的问题</p><ol><li>配置信息的修改，是否把配置信息同项目分离</li><li>helm所在目标机存在单点故障的情况</li></ol><p>替换镜像信息的操作如下：</p><p>sed ‘s#192.168.88.159:5000/distributed-id-gen-k8s:latest#’$BUILDIMG’#’ $WORKSPACE/k8s/deployment.yaml | kubectl apply -f -</p><h2 id="附录：资源不足问题"><a href="#附录：资源不足问题" class="headerlink" title="附录：资源不足问题"></a>附录：资源不足问题</h2><ol><li><p>报错：Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 Insufficient cpu.</unknown></p></li><li><p>解决方式：</p></li></ol><p>修改yaml中的cpu设置信息</p><pre><code>      resources:        requests:          memory: 500Mi          cpu: 100m        limits:          memory: 1Gi          cpu: 500m</code></pre><p><strong>更正：</strong>直接删除对cpu的资源限制，只对内存进行限制。如下：</p><pre><code>      resources:        requests:          memory: 500Mi        limits:          memory: 1Gi</code></pre><!-- ## jenkins中针对sonar的命令调整1. 临时删除前端中对sonar的代码检测npm --registry https://registry.npm.taobao.org install -g sonarqube-scanner && sonar-scanner \        -Dsonar.projectKey=test-frontend-develop \        -Dsonar.projectName=test-frontend-develop   \        -Dsonar.host.url=http://192.168.88.159:9000 \        -Dsonar.login=jenkins    \        -Dsonar.login=d3830ce5b21ca809290798ed7f093dd3f4396edf \        -Dsonar.sources=.   \        -Dsonar.tests=./tests/unit2. 去掉后端代码中的最前面的    mvn clean install -DskipTest调整sonar检测执行的顺序，放在post build之后。 --><h2 id="关于多个gateway的负载均衡"><a href="#关于多个gateway的负载均衡" class="headerlink" title="关于多个gateway的负载均衡"></a>关于多个gateway的负载均衡</h2><p>当部署了前端项目的时候，前端页面将对后端的请求通过nginx转发到后端，这时候nginx无法直接对k8s内的gateway进行请求。</p><p>目前gateway使用域名的方式对外暴露，考虑到dns问题，需要使用nodeport的方式对外暴露，但是这样需要在nginx侧设置对这些nodePort的统一代理，保证访问gateway的时候，可以进行负载均衡！</p><p>注意：由于arp病毒的影响，容易导致某台机器存在不可用的情况，所以在配置时需要检测机器状态。</p><!-- 1. gateway、Eureka基础设施重启2. 配置信息详细检查3. 服务重新部署 --><h2 id="最后需要排查的隐患"><a href="#最后需要排查的隐患" class="headerlink" title="最后需要排查的隐患"></a>最后需要排查的隐患</h2><ol><li>关于zookeeper镜像问题，时钟无法同步，时区未设置，差8小时，对分布式id服务的影响，目前未知。—-更换zookeeper镜像，重做。</li><li>关于服务过多的时候，存在问题，数据库出现：too many connections问题。—拆数据库，不同服务拥有自己的数据库。</li><li>镜像版本控制的问题，历史追踪设置。—</li></ol><h2 id="下一步改造"><a href="#下一步改造" class="headerlink" title="下一步改造"></a>下一步改造</h2><ol><li><p>日志，链路监控</p><ul><li><p>skywalking接入，ELK+file-pilot接入</p></li><li><p>ElasticSearch原生部署，两套集群的方式进行部署</p></li></ul></li><li><p>链路控制，共同开发。本地启动和线上调试的问题。kt-connect</p></li><li><p>培训内容</p></li><li><p>镜像仓库替换为harbor（能做做，不能做后放）</p></li><li><p>k8s限制和为何使用istio？统一运维平台内的信息。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s存储搭建</title>
      <link href="/2020/03/11/k8s-cun-chu-da-jian/"/>
      <url>/2020/03/11/k8s-cun-chu-da-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="为k8s提供常见存储——单机nfs以及Rook-Ceph存储集群的安装和配置"><a href="#为k8s提供常见存储——单机nfs以及Rook-Ceph存储集群的安装和配置" class="headerlink" title="为k8s提供常见存储——单机nfs以及Rook+Ceph存储集群的安装和配置"></a>为k8s提供常见存储——单机nfs以及Rook+Ceph存储集群的安装和配置</h1><h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><ul><li>操作系统:centos7.6</li><li>服务器列表</li></ul><table><thead><tr><th>主机名称</th><th>ip地址</th><th>作用</th></tr></thead><tbody><tr><td>k8s-nfs-provider</td><td>192.168.238.236</td><td>nfs单机安装</td></tr><tr><td>k8s-rke-node-start</td><td>192.168.238.240</td><td>rke管理机，管理k8s</td></tr><tr><td>k8s-node-01</td><td>192.168.238.232</td><td>k8s集群机器1</td></tr><tr><td>k8s-node-00</td><td>192.168.238.239</td><td>k8s集群机器2</td></tr><tr><td>k8s-node-02</td><td>192.168.238.241</td><td>k8s集群机器3</td></tr></tbody></table><ul><li><p>工具及其版本列表</p><ol><li><p>nfs版本，遵循yum版本信息</p></li><li><p>Rook：1.2.5</p></li><li><p>Ceph: 14.2.7</p></li></ol></li><li><p>前提：请参考**[Rancher集群化部署–在线安装.md]*文档，进行安装配置k8s和rancher。已经设置了240机器可以访问232、239、241三台机器，配置了ssh互通。</p></li><li><p>注意尽量给每台机器较大的硬盘信息，尽量不低于30GB。</p></li></ul><h2 id="nfs的安装配置"><a href="#nfs的安装配置" class="headerlink" title="nfs的安装配置"></a>nfs的安装配置</h2><h3 id="1-nfs服务端的安装"><a href="#1-nfs服务端的安装" class="headerlink" title="1. nfs服务端的安装"></a>1. nfs服务端的安装</h3><p>登录192.168.238.236的机器，如下：</p><pre><code>$ ssh -p 15555 centos@192.168.238.236</code></pre><p>首先检查并安装NFS，</p><pre><code>// 查看之前是否安装$ sudo rpm -aq nfs-utils rpcbind// 更新源信息$ sudo yum update// 安装$ sudo yum install  nfs-utils rpcbind -y</code></pre><p>其次启动RPC服务和NFS服务并检查，</p><pre><code>// 启动rpc服务并设置开机启动$ sudo systemctl start rpcbind.service$ sudo systemctl enable rpcbind.service// 启动NFS服务并设置开机启动$ sudo systemctl start nfs.service$ sudo systemctl enable nfs.service// 检查已经设置开机启动的是否生效$ sudo systemctl list-unit-files|grep enabled// 查看rpc是否启动成功$ sudo netstat   -lntup|grep rpc// 输出以下信息tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      808/rpcbindtcp        0      0 0.0.0.0:20048           0.0.0.0:*               LISTEN      2281/rpc.mountdtcp        0      0 0.0.0.0:58642           0.0.0.0:*               LISTEN      2214/rpc.statdtcp6       0      0 :::37967                :::*                    LISTEN      2214/rpc.statdtcp6       0      0 :::111                  :::*                    LISTEN      808/rpcbindtcp6       0      0 :::20048                :::*                    LISTEN      2281/rpc.mountdudp        0      0 127.0.0.1:703           0.0.0.0:*                           2214/rpc.statdudp        0      0 0.0.0.0:970             0.0.0.0:*                           808/rpcbindudp        0      0 0.0.0.0:59572           0.0.0.0:*                           2214/rpc.statdudp        0      0 0.0.0.0:20048           0.0.0.0:*                           2281/rpc.mountdudp        0      0 0.0.0.0:111             0.0.0.0:*                           808/rpcbindudp6       0      0 :::970                  :::*                                808/rpcbindudp6       0      0 :::44176                :::*                                2214/rpc.statdudp6       0      0 :::20048                :::*                                2281/rpc.mountdudp6       0      0 :::111                  :::*                                808/rpcbind// 查看rpc监控信息$ sudo rpcinfo -p localhost// 输出以下信息program vers proto   port  service100000    4   tcp    111  portmapper100000    3   tcp    111  portmapper100000    2   tcp    111  portmapper100000    4   udp    111  portmapper100000    3   udp    111  portmapper100000    2   udp    111  portmapper100024    1   udp  59572  status100024    1   tcp  58642  status100005    1   udp  20048  mountd100005    1   tcp  20048  mountd100005    2   udp  20048  mountd100005    2   tcp  20048  mountd100005    3   udp  20048  mountd100005    3   tcp  20048  mountd100003    3   tcp   2049  nfs100003    4   tcp   2049  nfs100227    3   tcp   2049  nfs_acl100003    3   udp   2049  nfs100003    4   udp   2049  nfs100227    3   udp   2049  nfs_acl100021    1   udp  43983  nlockmgr100021    3   udp  43983  nlockmgr100021    4   udp  43983  nlockmgr100021    1   tcp  45822  nlockmgr100021    3   tcp  45822  nlockmgr100021    4   tcp  45822  nlockmgr</code></pre><p>必须先启动rpcbind服务，后启动nfs服务，因为rpcbind服务启动之后，监控nfs服务</p><p>最后，设置共享文件夹，使nfs生效。操作如下：</p><pre><code>// 创建nfs-share文件夹$ sudo mkdir /nfs-share// 设置其可读写$ sudo chmod -R 777 /nfs-share// 配置/etc/exports$ sudo su# cat  &gt;&gt;/etc/exports&lt;&lt;EOF/data 192.168.238.0/24(rw,sync,all_squash)  EOF// 配置信息说明//  /data这是需要共享的目录，192.168.238.0/24允许访问的客户端，这里表示整个网段的都可以访问，也可以指定单个地址，也可以用星号表示所有用户都可以访问。// rw可读可行，sync实时写的的磁盘，all_squash不管访问NFS Server共享目录的用户身份如何，它的权限都被压缩成匿名用户，同时它的UID和GID都会变成nfsnobody账号身份。在早期多个NFS客户端同时读写NFS Server数据时，这个参数很有用。// 重启nfs服务$ sudo systemctl reload nfs.service// 查看showmount -e localhost //  输出 结果:Export list for localhost:/data 172.16.1.0/24</code></pre><p>这样nfs服务端就已经安装完成了。这样该服务器的可用硬盘容量就是该nfs服务的可用容量了。</p><h3 id="2-nfs客户端的安装配置"><a href="#2-nfs客户端的安装配置" class="headerlink" title="2. nfs客户端的安装配置"></a>2. nfs客户端的安装配置</h3><p>在所有目标机器（192.168.238.232，192.168.238.239，192.168.238.241）中进行操作。</p><p>首先检查并安装NFS，如下：</p><pre><code>// 查看之前是否安装$ sudo rpm -aq nfs-utils rpcbind// 更新源信息$ sudo yum update// 安装$ sudo yum install  nfs-utils rpcbind -y// 额外需要安装的包$ sudo yum install -y cifs-utils</code></pre><p>其次启动RPC服务和NFS服务并检查，</p><pre><code>// 启动rpc服务并设置开机启动$ sudo systemctl start rpcbind.service$ sudo systemctl enable rpcbind.service</code></pre><p>然后开始检查服务端的NFS，</p><pre><code>$ showmount -e 192.168.238.236// 出现下面结果，则成功：Export list for 192.168.238.236:/nfs-share 192.168.238.0/24</code></pre><p>倒数第二步，对该磁盘进行挂载</p><pre><code>// 挂载// -t 挂载的类型，表示以nfs模式运行$ sudo mount -t nfs 192.168.238.236:/nfs-share /mnt   // 查看$ df -h// 在列表中能找到下面的信息192.168.238.236:/nfs-share   146G  1.7G  145G   2% /mnt</code></pre><p>最后，把挂载点写入到开机自启动</p><pre><code># echo &quot;mount -t nfs 192.168.238.236:/nfs-share /mnt&quot; &gt;&gt; /etc/rc.local</code></pre><p>这样完成后，添加nfs存储已经成功。</p><h3 id="3-测试文件写入"><a href="#3-测试文件写入" class="headerlink" title="3. 测试文件写入"></a>3. 测试文件写入</h3><p>例如我们在192.168.238.232机器上创建一个文件并写入信息，例如：</p><pre><code>[centos@k8s-cluster-node-02 /]$ cd /mnt[centos@k8s-cluster-node-02 mnt]$ touch nfs-test.txt[centos@k8s-cluster-node-02 mnt]$ echo &quot;test nfs hello world&quot; &gt; nfs-test.txt[centos@k8s-cluster-node-02 mnt]$ lsnfs-test.txt</code></pre><p>然后我们回到192.168.238.236机器上，打开文件夹查看：</p><pre><code>[centos@k8s-nfs-provider ~]$ cd /nfs-share/[centos@k8s-nfs-provider nfs-share]$ lsnfs-test.txt[centos@k8s-nfs-provider nfs-share]$ cat nfs-test.txt// 输出信息test nfs hello world</code></pre><p>这样说明我们可以成功写入文件信息。</p><h3 id="4-添加nfs到rancher"><a href="#4-添加nfs到rancher" class="headerlink" title="4. 添加nfs到rancher"></a>4. 添加nfs到rancher</h3><p>rancher会在存储中，自动发现nfs存储，直接显示主机名称<strong>k8s-nfs-1</strong>。</p><p>后续添加存储，添加卷信息，都可以基于该nfs存储进行。</p><h2 id="Rook-Ceph进行集群存储调度管理"><a href="#Rook-Ceph进行集群存储调度管理" class="headerlink" title="Rook+Ceph进行集群存储调度管理"></a>Rook+Ceph进行集群存储调度管理</h2><p>我们需要先登录到192.168.238.240机器上进行操作。如下：</p><pre><code>$ ssh -p 15555 centos@192.168.238.240</code></pre><h3 id="1-第一种方式：使用helm安装rook-ceph"><a href="#1-第一种方式：使用helm安装rook-ceph" class="headerlink" title="1. 第一种方式：使用helm安装rook-ceph"></a>1. 第一种方式：使用helm安装rook-ceph</h3><p>这里可以使用helm快速安装rook-ceph。操作如下</p><pre><code>// 创建rook-ceph-system命名空间$ kubectl create namespace rook-ceph-system// 添加rook安装仓库信息$ helm repo add rook-stable https://charts.rook.io/release// 更新仓库信息$ helm update// 安装rook-ceph管理工具$ helm install rook --namespace rook-ceph-system rook-stable/rook-ceph// check$  kubectl --namespace rook-ceph-system get pods -l &quot;app=rook-ceph-operator&quot;</code></pre><p>请注意, rook-ceph-system 中的所有 Pod 都应该是 Running 或者Completed 状态,不应存在 restarts 或error 的情况。</p><h3 id="2-第二种方式：使用官方推荐的配置文件安装ceph存储"><a href="#2-第二种方式：使用官方推荐的配置文件安装ceph存储" class="headerlink" title="2. 第二种方式：使用官方推荐的配置文件安装ceph存储"></a>2. 第二种方式：使用官方推荐的配置文件安装ceph存储</h3><p>下载官方压缩包，解压后操作。如下：</p><pre><code>// 下载压缩包并解压$ wget https://github.com/rook/rook/archive/v1.2.5.tar.gz$ tar -zxvf v1.2.5.tar.gz$ cd cluster/examples/kubernetes/ceph/// 执行三联$ kubectl create -f common.yaml$ kubectl create -f operator.yaml$ kubectl create -f cluster.yaml// 查看安装效果$ kubectl get pods -n rook-cephNAME                                                    READY   STATUS      RESTARTS   AGEcsi-cephfsplugin-7sjpz                                  3/3     Running     0          5m54scsi-cephfsplugin-jjzpn                                  3/3     Running     0          5m42scsi-cephfsplugin-lvshq                                  3/3     Running     0          5m51scsi-cephfsplugin-provisioner-66c94d9784-mgn95           4/4     Running     0          5m38scsi-cephfsplugin-provisioner-66c94d9784-rxljk           4/4     Running     0          6m5scsi-rbdplugin-4zfl2                                     3/3     Running     0          6mcsi-rbdplugin-n2tmw                                     3/3     Running     0          5m59scsi-rbdplugin-provisioner-b4cfc4fd5-7pvw5               5/5     Running     0          4m49scsi-rbdplugin-provisioner-b4cfc4fd5-q6jmn               5/5     Running     0          6m10scsi-rbdplugin-x696n                                     3/3     Running     0          6m3srook-ceph-crashcollector-192.168.238.232-6b4b667d48-c8ltm   1/1     Running     0          61mrook-ceph-crashcollector-192.168.238.239-64db57cb89-kckzb   1/1     Running     0          62mrook-ceph-crashcollector-192.168.238.241-6d6697b8d8-hzxb7   1/1     Running     0          61mrook-ceph-mgr-a-75997f59cd-pk6rx                        1/1     Running     0          61mrook-ceph-mon-a-6f6b5c6cb8-zj2cd                        1/1     Running     0          62mrook-ceph-mon-b-8cb89bd84-6xzzp                         1/1     Running     0          62mrook-ceph-mon-c-d95686f54-vcttv                         1/1     Running     0          61mrook-ceph-operator-784cfd5c7d-msggb                     1/1     Running     1          6m17srook-ceph-osd-prepare-192.168.238.232-kcz6x                 0/1     Completed   0          3m32srook-ceph-osd-prepare-192.168.238.239-fxn5t                 0/1     Completed   0          3m26srook-ceph-osd-prepare-192.168.238.241-2qqsg                 0/1     Completed   0          3m21srook-discover-55rlx                                     1/1     Running     0          65mrook-discover-8cnnx                                     1/1     Running     0          65mrook-discover-s7thp                                     1/1     Running     0          65m</code></pre><p>请注意, rook-ceph-system 中的所有 Pod 都应该是 Running 或者Completed 状态,不应存在 restarts 或error 的情况。</p><p><strong>问题：</strong></p><pre><code>[centos@k8s-rke-node-start ceph]$ kubectl get pods -n rook-cephNAME                                                    READY   STATUS             RESTARTS   AGEcsi-cephfsplugin-mk622                                  1/3     CrashLoopBackOff   26         45mcsi-cephfsplugin-nhvrr                                  1/3     CrashLoopBackOff   26         45mcsi-cephfsplugin-provisioner-7b8fbf88b4-4wlpq           4/4     Running            0          45mcsi-cephfsplugin-provisioner-7b8fbf88b4-824jv           4/4     Running            0          45mcsi-cephfsplugin-z7nc4                                  1/3     CrashLoopBackOff   26         45mcsi-rbdplugin-8qt6t                                     1/3     CrashLoopBackOff   26         45mcsi-rbdplugin-hbgvt                                     1/3     CrashLoopBackOff   26         45mcsi-rbdplugin-provisioner-6b8b4d558c-p6kq8              5/5     Running            0          45mcsi-rbdplugin-provisioner-6b8b4d558c-tc954              5/5     Running            0          45mcsi-rbdplugin-s8c7h                                     1/3     CrashLoopBackOff   26         45mrook-ceph-crashcollector-192.168.238.232-6b4b667d48-c8ltm   1/1     Running            0          42mrook-ceph-crashcollector-192.168.238.239-64db57cb89-kckzb   1/1     Running            0          43mrook-ceph-crashcollector-192.168.238.241-6d6697b8d8-hzxb7   1/1     Running            0          42mrook-ceph-mgr-a-75997f59cd-pk6rx                        1/1     Running            0          42mrook-ceph-mon-a-6f6b5c6cb8-zj2cd                        1/1     Running            0          43mrook-ceph-mon-b-8cb89bd84-6xzzp                         1/1     Running            0          43mrook-ceph-mon-c-d95686f54-vcttv                         1/1     Running            0          42mrook-ceph-operator-7dcd87699d-cwdk8                     1/1     Running            0          45mrook-ceph-osd-prepare-192.168.238.232-vc8lm                 0/1     Completed          0          41mrook-ceph-osd-prepare-192.168.238.239-6tc99                 0/1     Completed          0          41mrook-ceph-osd-prepare-192.168.238.241-88fhz                 0/1     Completed          0          41mrook-discover-55rlx                                     1/1     Running            0          45mrook-discover-8cnnx                                     1/1     Running            0          45mrook-discover-s7thp                                     1/1     Running            0          45m$ kubectl -n rook-ceph logs csi-cephfsplugin-mk622Error from server (BadRequest): a container name must be specified for pod csi-cephfsplugin-mk622, choose one of: [driver-registrar csi-cephfsplugin liveness-prometheus]$ kubectl -n rook-ceph logs csi-cephfsplugin-mk622 -c csi-cephfspluginI0305 03:41:37.740687       1 cephcsi.go:104] Driver version: v1.2.2 and Git version: f8c854dc7d6ffff02cb2eed6002534dc0473f111I0305 03:41:37.740869       1 cachepersister.go:45] cache-perister: using kubernetes configmap as metadata cache persisterI0305 03:41:37.743887       1 cephcsi.go:131] Initial PID limit is set to -1I0305 03:41:37.743983       1 cephcsi.go:140] Reconfigured PID limit to -1 (max)I0305 03:41:37.743996       1 cephcsi.go:159] Starting driver type: cephfs with name: rook-ceph.cephfs.csi.ceph.comI0305 03:41:37.747336       1 volumemounter.go:159] loaded mounter: kernelI0305 03:41:37.763570       1 volumemounter.go:167] loaded mounter: fuseI0305 03:41:37.763847       1 mountcache.go:59] mount-cache: name: rook-ceph.cephfs.csi.ceph.com, version: v1.2.2, mountCacheDir: /mount-cache-dirI0305 03:41:37.763937       1 mountcache.go:99] mount-cache: successfully remounted 0 volumesF0305 03:41:37.764345       1 httpserver.go:25] listen tcp 192.168.238.232:9091: bind: address already in useI0305 03:41:37.764687       1 server.go:118] Listening for connections on address: &amp;net.UnixAddr{Name:&quot;//csi/csi.sock&quot;, Net:&quot;unix&quot;}</code></pre><p>端口被占用的问题。查找三个文件，cluster.yaml、operator.yaml、common.yaml。然后找到operator.yaml中，存在的端口号9091、9081、9090、9080重新进行设置，一定保证设置为未使用的端口信息：</p><pre><code>$ vim operator.yaml// 修改下面的信息    # Configure CSI cephfs grpc and liveness metrics port    - name: CSI_CEPHFS_GRPC_METRICS_PORT      value: &quot;10091&quot;    - name: CSI_CEPHFS_LIVENESS_METRICS_PORT      value: &quot;10081&quot;    # Configure CSI rbd grpc and liveness metrics port    - name: CSI_RBD_GRPC_METRICS_PORT      value: &quot;10090&quot;    - name: CSI_RBD_LIVENESS_METRICS_PORT      value: &quot;10080&quot;</code></pre><p>设置完成后，重新执行operator.yaml。</p><pre><code>$ kubectl apply -f operator.yaml</code></pre><p>查看效果：</p><pre><code>$ kubectl get pods -n rook-cephNAME                                                    READY   STATUS      RESTARTS   AGEcsi-cephfsplugin-7sjpz                                  3/3     Running     0          5m54scsi-cephfsplugin-jjzpn                                  3/3     Running     0          5m42scsi-cephfsplugin-lvshq                                  3/3     Running     0          5m51scsi-cephfsplugin-provisioner-66c94d9784-mgn95           4/4     Running     0          5m38scsi-cephfsplugin-provisioner-66c94d9784-rxljk           4/4     Running     0          6m5scsi-rbdplugin-4zfl2                                     3/3     Running     0          6mcsi-rbdplugin-n2tmw                                     3/3     Running     0          5m59scsi-rbdplugin-provisioner-b4cfc4fd5-7pvw5               5/5     Running     0          4m49scsi-rbdplugin-provisioner-b4cfc4fd5-q6jmn               5/5     Running     0          6m10scsi-rbdplugin-x696n                                     3/3     Running     0          6m3srook-ceph-crashcollector-192.168.238.232-6b4b667d48-c8ltm   1/1     Running     0          61mrook-ceph-crashcollector-192.168.238.239-64db57cb89-kckzb   1/1     Running     0          62mrook-ceph-crashcollector-192.168.238.241-6d6697b8d8-hzxb7   1/1     Running     0          61mrook-ceph-mgr-a-75997f59cd-pk6rx                        1/1     Running     0          61mrook-ceph-mon-a-6f6b5c6cb8-zj2cd                        1/1     Running     0          62mrook-ceph-mon-b-8cb89bd84-6xzzp                         1/1     Running     0          62mrook-ceph-mon-c-d95686f54-vcttv                         1/1     Running     0          61mrook-ceph-operator-784cfd5c7d-msggb                     1/1     Running     1          6m17srook-ceph-osd-prepare-192.168.238.232-kcz6x                 0/1     Completed   0          3m32srook-ceph-osd-prepare-192.168.238.239-fxn5t                 0/1     Completed   0          3m26srook-ceph-osd-prepare-192.168.238.241-2qqsg                 0/1     Completed   0          3m21srook-discover-55rlx                                     1/1     Running     0          65mrook-discover-8cnnx                                     1/1     Running     0          65mrook-discover-s7thp                                     1/1     Running     0          65m</code></pre><h3 id="3-安装toolbox和dashboard"><a href="#3-安装toolbox和dashboard" class="headerlink" title="3. 安装toolbox和dashboard"></a>3. 安装toolbox和dashboard</h3><h4 id="安装toolbox"><a href="#安装toolbox" class="headerlink" title="安装toolbox"></a>安装toolbox</h4><p>直接依据这个rook-ceph-tools.yaml进行安装：</p><pre><code>$ kubectl apply -f toolbox.yaml</code></pre><p>一旦 toolbox 的 Pod 运行成功后，我们就可以使用下面的命令进入到工具箱内部进行操作：</p><pre><code>$ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath=&#39;{.items[0].metadata.name}&#39;) bash// 查看安装后状态$ kubectl get pods -n rook-cephrook-ceph-tools-7d764c8647-vdz5r                        1/1     Running     0          3d20h// 进入下面的窗口，执行ceph status[root@rook-ceph-tools-7d764c8647-vdz5r /]# ceph statuscluster:    id:     18c1fa7d-944b-4a2e-8ef0-e6699c49ac67    health: HEALTH_OKservices:    mon: 3 daemons, quorum a,b,c (age 2d)    mgr: a(active, since 13h)    mds: myfs:1 {0=myfs-a=up:active} 1 up:standby-replay    osd: 3 osds: 3 up (since 2d), 3 in (since 2d)    rgw: 1 daemon active (my.store.a)data:    pools:   10 pools, 80 pgs    objects: 264 objects, 13 KiB    usage:   16 GiB used, 177 GiB / 192 GiB avail    pgs:     80 active+cleanio:    client:   853 B/s rd, 1 op/s rd, 0 op/s wr// 执行ceph osd status[root@rook-ceph-tools-7d764c8647-vdz5r /]# ceph osd status+----+-------------+-------+-------+--------+---------+--------+---------+-----------+| id |     host    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |+----+-------------+-------+-------+--------+---------+--------+---------+-----------+| 0  | 192.168.238.239 | 5405M | 44.8G |    0   |     0   |    2   |   106   | exists,up || 1  | 192.168.238.232 | 5235M | 35.9G |    0   |     0   |    0   |     0   | exists,up || 2  | 192.168.238.241 | 5368M | 95.8G |    0   |     0   |    1   |     0   | exists,up |+----+-------------+-------+-------+--------+---------+--------+---------+-----------+// 执行ceph df[root@rook-ceph-tools-7d764c8647-vdz5r /]# ceph dfRAW STORAGE:  CLASS     SIZE        AVAIL       USED       RAW USED     %RAW USED    hdd       192 GiB     177 GiB     16 GiB       16 GiB          8.13    TOTAL     192 GiB     177 GiB     16 GiB       16 GiB          8.13POOLS:  POOL                            ID     STORED      OBJECTS     USED        %USED     MAX AVAIL    my-store.rgw.control             1         0 B           8         0 B         0        53 GiB    my-store.rgw.meta                2       373 B           2       373 B         0        53 GiB    my-store.rgw.log                 3        50 B         210        50 B         0        53 GiB    my-store.rgw.buckets.index       4         0 B           0         0 B         0        53 GiB    my-store.rgw.buckets.non-ec      5         0 B           0         0 B         0        53 GiB    .rgw.root                        6     3.7 KiB          16     3.7 KiB         0        53 GiB    my-store.rgw.buckets.data        7         0 B           0         0 B         0        53 GiB    myfs-metadata                    9     9.2 KiB          22     9.2 KiB         0        53 GiB    myfs-data0                      10         0 B           0         0 B         0        53 GiB    replicapool                     11        36 B           6        36 B         0        53 GiB</code></pre><p>工具箱中的所有可用工具命令均已准备就绪，可满足您的故障排除需求。</p><h4 id="安装dashboard"><a href="#安装dashboard" class="headerlink" title="安装dashboard"></a>安装dashboard</h4><p>查看已有的服务信息，再确定dashboard的安装方式，如下：</p><pre><code>    $ kubectl get service -n rook-cephNAME                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGEcsi-cephfsplugin-metrics                 ClusterIP   10.43.49.186    &lt;none&gt;        8080/TCP,8081/TCP   3d22hcsi-rbdplugin-metrics                    ClusterIP   10.43.156.212   &lt;none&gt;        8080/TCP,8081/TCP   3d22hrook-ceph-mgr                            ClusterIP   10.43.112.170   &lt;none&gt;        9283/TCP            3d22hrook-ceph-mgr-dashboard                  ClusterIP   10.43.99.56     &lt;none&gt;        8443/TCP            3d22hrook-ceph-mon-a                          ClusterIP   10.43.132.112   &lt;none&gt;        6789/TCP,3300/TCP   3d22hrook-ceph-mon-b                          ClusterIP   10.43.86.9      &lt;none&gt;        6789/TCP,3300/TCP   3d22hrook-ceph-mon-c                          ClusterIP   10.43.188.99    &lt;none&gt;        6789/TCP,3300/TCP   3d22hrook-ceph-rgw-my-store                   ClusterIP   10.43.53.12     &lt;none&gt;        80/TCP              2d23h</code></pre><p>默认dashboard使用https安装，如下：</p><pre><code>$ kubectl apply -f dashboard-external-https.yaml$ kubectl -n rook-ceph get service -o wideNAME                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE     SELECTORrook-ceph-mgr-dashboard                  ClusterIP   10.43.99.56     &lt;none&gt;        8443/TCP            3h48m   app=rook-ceph-mgr,rook_cluster=rook-cephrook-ceph-mgr-dashboard-external-https   NodePort    10.43.209.164   &lt;none&gt;        8443:32166/TCP      42s     app=rook-ceph-mgr,rook_cluster=rook-ceph</code></pre><p>获取dashboard访问密码：<br>$ kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=”{[‘data’][‘password’]}” | base64 –decode &amp;&amp; echo<br>stU6jhkSVK</p><p>用任意一个集群节点的ip地址均可以访问，例如:<a href="https://192.168.238.232:32166，输入用户名：admin，密码：stU6jhkSVK。" target="_blank" rel="noopener">https://192.168.238.232:32166，输入用户名：admin，密码：stU6jhkSVK。</a></p><h3 id="4-创建存储"><a href="#4-创建存储" class="headerlink" title="4. 创建存储"></a>4. 创建存储</h3><pre><code>//  创建对象存储$ kubectl apply -f object.yaml// 以及用户$ kubectl apply -f object-user.yaml// 创建Ceph pool（块存储池）// 使用的kind是CephBlockPool$ kubectl apply -f pool.yaml// 创建文件存储$ kubectl apply -f filesystem.yaml// 创建rbd存储信息$ cd csi/rbd/$ kubectl apply -f storageclass.yaml// 出现以下提示cephblockpool.ceph.rook.io/replicapool configuredstorageclass.storage.k8s.io/rook-ceph-block created// 创建pvc$ kubectl apply -f pvc.yaml$ kubectl get pvcNAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGErbd-pvc   Bound    pvc-22e52bf9-429a-4ff9-8978-9f647d8354ca   2Gi        RWO            rook-ceph-block   11s</code></pre><p>这样存储就创建完毕了。我们来查看下创建的osd存储，如下：</p><pre><code>// 登录节点主机$ ssh -p 15555 centos@192.168.238.232// 查看osd存储$ sudo ls /home/centos/rook-storage/osd1/ -lh// 输出以下信息total 5.1G-rw-------   1 root root   37 Mar  6 15:50 ceph_fsiddrwxr-xr-x 164 root root 4.0K Mar  6 16:52 current-rw-r--r--   1 root root   37 Mar  6 15:50 fsid-rw-r--r--   1 root root 5.0G Mar  9 09:37 journal-rw-r--r--   1 root root   56 Mar  6 15:50 keyring-rw-------   1 root root   21 Mar  6 15:50 magic-rw-------   1 root root    6 Mar  6 15:50 ready-rw-------   1 root root    3 Mar  6 15:51 require_osd_release-rw-r--r--   1 root root 1.2K Mar  8 19:49 rook-ceph.config-rw-------   1 root root    4 Mar  6 15:50 store_version-rw-------   1 root root   53 Mar  6 15:50 superblockdrwxr--r--   2 root root   29 Mar  6 15:50 tmp-rw-------   1 root root   10 Mar  6 15:50 type-rw-------   1 root root    2 Mar  6 15:50 whoami</code></pre><p>查看其它节点的存储信息类似操作即可。</p><p><strong>问题：</strong></p><pre><code>$ kubectl get pvcNAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS              AGEmyclaim   Pending                                      rook-ceph-retain-bucket   11m$ kubectl describe pvc myclaimName:          myclaimNamespace:     defaultStorageClass:  rook-ceph-retain-bucketStatus:        PendingVolume:Labels:        &lt;none&gt;Annotations:   volume.beta.kubernetes.io/storage-provisioner: ceph.rook.io/bucketFinalizers:    [kubernetes.io/pvc-protection]Capacity:Access Modes:VolumeMode:    FilesystemMounted By:    &lt;none&gt;Events:  Type    Reason                Age                   From                         Message  ----    ------                ----                  ----                         -------  Normal  ExternalProvisioning  2m29s (x43 over 12m)  persistentvolume-controller  waiting for a volume to be created, either by external provisioner &quot;ceph.rook.io/bucket&quot; or manually created by system administrator</code></pre><pre><code>// 首先需要删除$ cd /home/centos/rook/rook-1.2.5/cluster/examples/kubernetes/ceph$ kubectl delete -f pvc-example.yaml$ kubectl delete -f storageclass-bucket-retain.yaml// 可以不进行删除$ kubectl delete -f pools.yaml// 对cluster.yml文件配置进行修改$ vim cluster.yaml // 修改服务配置的文件目录，要有操作权限directories:- path: /home/centos/rook-storage// 然后修改默认的位置，由于ceph v14.2.7不支持useAllDevices: trueuseAllDevices: false// 重新执行cluster的配置$ kubectl apply -f cluster.yaml// 然后回到ceph镜像中，查看$ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath=&#39;{.items[0].metadata.name}&#39;) bash</code></pre><pre><code># ceph status  cluster:    id:     18c1fa7d-944b-4a2e-8ef0-e6699c49ac67    health: HEALTH_OK  services:    mon: 3 daemons, quorum a,b,c (age 64m)    mgr: a(active, since 13m)    mds: myfs:1 {0=myfs-a=up:active} 1 up:standby-replay    osd: 3 osds: 3 up (since 56m), 3 in (since 56m)    rgw: 1 daemon active (my.store.a)  data:    pools:   10 pools, 80 pgs    objects: 226 objects, 7.9 KiB    usage:   16 GiB used, 177 GiB / 192 GiB avail    pgs:     80 active+clean  io:    client:   853 B/s rd, 1 op/s rd, 0 op/s wr</code></pre><p>此时，修改完成。</p><p>同时解决下面的问题：</p><pre><code>$ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath=&#39;{.items[0].metadata.name}&#39;)  bashbash: warning: setlocale: LC_CTYPE: cannot change locale (en_US.UTF-8): No such file or directorybash: warning: setlocale: LC_COLLATE: cannot change locale (en_US.UTF-8): No such file or directorybash: warning: setlocale: LC_MESSAGES: cannot change locale (en_US.UTF-8): No such file or directorybash: warning: setlocale: LC_NUMERIC: cannot change locale (en_US.UTF-8): No such file or directorybash: warning: setlocale: LC_TIME: cannot change locale (en_US.UTF-8): No such file or directory[root@rook-ceph-tools-7d764c8647-vdz5r /]# ceph -w  cluster:    id:     18c1fa7d-944b-4a2e-8ef0-e6699c49ac67    health: HEALTH_WARN            OSD count 0 &lt; osd_pool_default_size 3  services:    mon: 3 daemons, quorum a,b,c (age 2h)    mgr: a(active, since 86m)    osd: 0 osds: 0 up, 0 in  data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:</code></pre><p>HEALTH_WARN OSD count 0 &lt; osd_pool_default_size 3 尚未有大的影响，后续注意</p><p>问题位置：<a href="https://github.com/rook/rook/issues/3348" target="_blank" rel="noopener">https://github.com/rook/rook/issues/3348</a></p><p><a href="https://github.com/rook/rook/issues/4963" target="_blank" rel="noopener">https://github.com/rook/rook/issues/4963</a></p><p>原因是：ceph镜像版本为14.2.7版本的，不允许开启useAllDevices</p><p>见官网<a href="https://rook.io/docs/rook/v1.2/ceph-quickstart.html" target="_blank" rel="noopener">https://rook.io/docs/rook/v1.2/ceph-quickstart.html</a></p><p>对于Dashboard上enable一些功能后，通常需要重启Dashboard服务才能使用。使用下面的命令来重启Dashboard：</p><pre><code>// 在toolbox容器里执行[root@AI03 /]# ceph mgr module disable dashboard[root@AI03 /]# ceph mgr module enable dashboard</code></pre><!-- op-mgr: failed modules: "orchestrator modules". failed to set rook orchestrator backend: failed to set rook as the orchestrator backend: failed to complete command: context deadline exceeded --><h2 id="番外：清理rook-ceph命名空间以及删除"><a href="#番外：清理rook-ceph命名空间以及删除" class="headerlink" title="番外：清理rook-ceph命名空间以及删除"></a>番外：清理rook-ceph命名空间以及删除</h2><p>helm安装时，需要比较暴力的进行删除，直接删除命名空间rook-ceph-system中，所有的pods，最后删除改命名空间。<strong>切记，线上环境慎用该操作！</strong></p><p>如果是通过源码配置安装的，参考下面的方式删除。</p><pre><code>$ kubectl delete -f cluster.yaml$ kubectl delete -f operator.yaml$ kubectl delete -f common.yaml// 需要到集群的每个节点机器上执行下面的操作$ cd /var/lib/rook/$ sudo rm -rf ./*</code></pre><p>参考链接：<a href="https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#troubleshooting" target="_blank" rel="noopener">https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#troubleshooting</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>搭建rook+ceph时，一定要耐心看官网教程！</p><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><ul><li><a href="https://time.geekbang.org/column/article/39724" target="_blank" rel="noopener">https://time.geekbang.org/column/article/39724</a></li><li><a href="https://zhuanlan.zhihu.com/p/81417869" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/81417869</a></li><li><a href="https://www.cnblogs.com/passzhang/p/12191332.html" target="_blank" rel="noopener">https://www.cnblogs.com/passzhang/p/12191332.html</a></li><li><a href="http://www.yangguanjun.com/2018/12/28/rook-ceph-practice-part2/" target="_blank" rel="noopener">http://www.yangguanjun.com/2018/12/28/rook-ceph-practice-part2/</a></li><li><a href="https://www.cnblogs.com/kevincaptain/p/10655721.html" target="_blank" rel="noopener">https://www.cnblogs.com/kevincaptain/p/10655721.html</a></li><li><a href="http://www.yangguanjun.com/2018/12/22/rook-ceph-practice-part1/" target="_blank" rel="noopener">http://www.yangguanjun.com/2018/12/22/rook-ceph-practice-part1/</a></li><li><a href="https://rook.github.io/docs/rook/v1.2/" target="_blank" rel="noopener">https://rook.github.io/docs/rook/v1.2/</a></li><li><a href="https://www.qikqiak.com/post/deploy-ceph-cluster-with-rook/" target="_blank" rel="noopener">https://www.qikqiak.com/post/deploy-ceph-cluster-with-rook/</a></li><li><a href="https://blog.fleeto.us/post/the-ultimate-rook-and-ceph-survival-guide/" target="_blank" rel="noopener">https://blog.fleeto.us/post/the-ultimate-rook-and-ceph-survival-guide/</a></li><li><a href="https://fkpwolf.net/cloud/db/2018/09/14/rook.html" target="_blank" rel="noopener">https://fkpwolf.net/cloud/db/2018/09/14/rook.html</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于docker内jvm内存耗尽导致服务停机的问题解决</title>
      <link href="/2020/02/19/guan-yu-docker-nei-jvm-nei-cun-hao-jin-dao-zhi-fu-wu-ting-ji-de-wen-ti-jie-jue/"/>
      <url>/2020/02/19/guan-yu-docker-nei-jvm-nei-cun-hao-jin-dao-zhi-fu-wu-ting-ji-de-wen-ti-jie-jue/</url>
      
        <content type="html"><![CDATA[<h1 id="关于docker内jvm内存耗尽导致服务停机的问题解决"><a href="#关于docker内jvm内存耗尽导致服务停机的问题解决" class="headerlink" title="关于docker内jvm内存耗尽导致服务停机的问题解决"></a>关于docker内jvm内存耗尽导致服务停机的问题解决</h1><h2 id="最开始的设置情况"><a href="#最开始的设置情况" class="headerlink" title="最开始的设置情况"></a>最开始的设置情况</h2><p>jvm设置参数如下：</p><p>-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=8</p><p>在docker启动命令中设置。</p><p>但是由于jdk版本的变化，事情出现了严重的问题，-XX:MaxRAMFraction=8该配置不起作用！</p><h2 id="测试环境现象"><a href="#测试环境现象" class="headerlink" title="测试环境现象"></a>测试环境现象</h2><ol><li>服务器假死，可以ping通，无法连接ssh</li><li>监控服务器日常使用，随日期和使用量导致内存上升</li><li>观察EXSI中的内存占用，近1小时内内存飙升</li><li>交换空间问题，重启后发现内存依旧在高位且交换空间占用率不低</li></ol><h2 id="预先考虑的解决方式"><a href="#预先考虑的解决方式" class="headerlink" title="预先考虑的解决方式"></a>预先考虑的解决方式</h2><ol><li>本地服务器java虚拟机的jvm设置，-xmx、-xms对docker的影响，包括docker设置启动项的问题</li><li>对docker直接写死镜像的内存信息，例如启动项中添加-XX:MaxRAMPercentage=75.0 -XX:MaxRAM=1000m（<a href="https://medium.com/adorsys/usecontainersupport-to-the-rescue-e77d6cfea712）" target="_blank" rel="noopener">https://medium.com/adorsys/usecontainersupport-to-the-rescue-e77d6cfea712）</a></li><li>对docker基础镜像进行改造，使其镜像内部安装jmap、jstack、jsp等工具，能够直接对单个镜像进行排查获取内存信息。额外，对镜像进行改造，减少镜像体积！</li><li>更换docker镜像，尝试openj9等镜像（<a href="https://zhuanlan.zhihu.com/p/62652957），降低内存占用" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62652957），降低内存占用</a></li><li>对服务器添加内存，虚拟机扩容</li></ol><h2 id="排查手段"><a href="#排查手段" class="headerlink" title="排查手段"></a>排查手段</h2><ol><li>top、htop、iostat等命令工具</li><li>jvm内存排查：jmap、jstack等，dump内存看一下问题</li><li>可视化客户端工具：jconsole、jvisualvm</li></ol><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><h3 id="0-服务器重启后，查看-var-log-messages日志信息"><a href="#0-服务器重启后，查看-var-log-messages日志信息" class="headerlink" title="0. 服务器重启后，查看/var/log/messages日志信息"></a>0. 服务器重启后，查看/var/log/messages日志信息</h3><p>没啥用，通过搜索并未发现有什么问题，这个日志记录还需要重新思考下如何进行监控。</p><h3 id="1-服务器重启后，对sshd进程进行优先级提升，保证死机时有足够空间操作ssh"><a href="#1-服务器重启后，对sshd进程进行优先级提升，保证死机时有足够空间操作ssh" class="headerlink" title="1. 服务器重启后，对sshd进程进行优先级提升，保证死机时有足够空间操作ssh"></a>1. 服务器重启后，对sshd进程进行优先级提升，保证死机时有足够空间操作ssh</h3><p>建议使用 <strong>nice</strong> 命令或者<strong>renice</strong>命令将 sshd 进程的优先级调高，这样当系统内存紧张时，还能勉强登陆服务器进行调试，然后分析故障。操作如下：</p><pre><code>// 找到该进程信息$ ps -aux | grep sshd// 展示下面的信息root      2567  0.0  0.0 158936  5684 ?        Ss   08:34   0:00 sshd: centos [priv]centos    2601  0.0  0.0 158936  2452 ?        S    08:35   0:00 sshd: centos@pts/0root      2994  0.0  0.0 158936  5684 ?        Ss   08:51   0:00 sshd: centos [priv]centos    3000  0.0  0.0 158936  2452 ?        S    08:51   0:00 sshd: centos@pts/1centos    4474  0.0  0.0 112712   964 pts/1    S+   09:36   0:00 grep --color=auto sshdroot     23773  0.0  0.0 112920   460 ?        Ss   Mar11   0:00 /usr/sbin/sshd -D// 查看进程优先级$  ps ax -o pid,nice,comm | grep sshd 2567   0 sshd 2601   0 sshd 2994   0 sshd 3000   0 sshd23773   0 sshd// 修改第一个进程的id信息，将优先级提升到最高$ sudo renice -20 -p 23773// 查看进程优先级$  ps ax -o pid,nice,comm | grep sshd 2567   0 sshd 2601   0 sshd 2994   0 sshd 3000   0 sshd23773 -20 sshd// 退出后重新登录，再查看$ ps aux | grep sshdroot      2567  0.0  0.0 158936  5684 ?        Ss   08:34   0:00 sshd: centos [priv]centos    2601  0.0  0.0 158936  2452 ?        S    08:35   0:00 sshd: centos@pts/0root      4944  0.5  0.0 158936  5684 ?        S&lt;s  09:40   0:00 sshd: centos [priv]centos    4959  0.0  0.0 158936  2300 ?        S&lt;   09:41   0:00 sshd: centos@pts/1centos    4997  0.0  0.0 112716   960 pts/1    S&lt;+  09:41   0:00 grep --color=auto sshdroot     23773  0.0  0.0 112920   460 ?        S&lt;s  Mar11   0:00 /usr/sbin/sshd -D$ ps ax -o pid,nice,comm | grep sshd2567   0 sshd2601   0 sshd4944 -20 sshd4959 -20 sshd23773 -20 sshd</code></pre><p>可以看到的是sshd进程中，出现<strong>S&lt;s</strong>，”&lt;”代表高优先级进程标志，说明测试成功。</p><p>参考链接：</p><ul><li>关于进程相关命令的说明：<a href="https://www.cnblogs.com/alongdidi/p/linux_process.html" target="_blank" rel="noopener">https://www.cnblogs.com/alongdidi/p/linux_process.html</a></li></ul><h3 id="3-重启之前的服务"><a href="#3-重启之前的服务" class="headerlink" title="3. 重启之前的服务"></a>3. 重启之前的服务</h3><p>这里使用的是docker来进行部署项目，由于之前的未进行详细的设置，导致这些镜像在系统重启的时候，不能自动重启镜像，所以需要对之前的镜像启动命令进行改造，添加–restart=always选项。</p><p>后续需要对镜像进行优化以及启动命令进行优化。</p><h3 id="4-htop长期观察内存变化，测试人员正常进行功能测试"><a href="#4-htop长期观察内存变化，测试人员正常进行功能测试" class="headerlink" title="4. htop长期观察内存变化，测试人员正常进行功能测试"></a>4. htop长期观察内存变化，测试人员正常进行功能测试</h3><p>安装htop命令，手动观察内存变化。</p><pre><code>$ sudo yum install -y psmisc$ sudo yum install -y htop// 使用htop命令$ htop</code></pre><p>效果如下图：</p><p><img src="htop.png" alt="htop示意图"></p><p>在长期观察中，很明显的看到<strong>Mem</strong>部分占比非常高且<strong>Swp</strong>部分使用率不低，以此为根据怀疑是内存占用过高导致服务器死机。</p><p>服务器配置为4c32G，内存使用量在26G至30G之间，交换空间使用率在1/4左右，如果是走一个稍快的操作或者走一个轻量的压测会发现内存波动较大，交换空间上涨较快。</p><p>还可以看到java相关的进程在图中展示的MEM一列，大概占用在10%左右，在该服务器上一共部署了12个Java微服务，加上一个zookeeper，目前是13个微服务。单个微服务内存占比在10%左右，实际上一估算已经超出了现有内存，但是有部分服务未被调用，只会占用最低的内存量。所以可以维持现状，但是一旦开始使用，内存压力可想而知了，内存很容易被压爆。</p><h3 id="5-隔夜查看内存信息变化"><a href="#5-隔夜查看内存信息变化" class="headerlink" title="5. 隔夜查看内存信息变化"></a>5. 隔夜查看内存信息变化</h3><p>经过隔夜对内存的观察，发现内存还是维持在高位，且交换空间有上涨。</p><h3 id="4-停止部分服务，查看内存占用"><a href="#4-停止部分服务，查看内存占用" class="headerlink" title="4. 停止部分服务，查看内存占用"></a>4. 停止部分服务，查看内存占用</h3><p>尝试停止部分服务，只维持核心的业务逻辑运转。撤掉两个用户相关服务，UMC和USER-UAA。尚待证实！</p><h3 id="5-使用jstack、jmap等命令进行dump操作"><a href="#5-使用jstack、jmap等命令进行dump操作" class="headerlink" title="5. 使用jstack、jmap等命令进行dump操作"></a>5. 使用jstack、jmap等命令进行dump操作</h3><p>在使用这些命令的时候，发现并不能使用。</p><p>问题1：</p><pre><code>[centos@localhost bin]$ sudo ./jstack -F 14299[sudo] password for centos:Attaching to process ID 14299, please wait...Error attaching to process: Doesn&#39;t appear to be a HotSpot VM (could not find symbol &quot;gHotSpotVMTypes&quot; in remote process)sun.jvm.hotspot.debugger.DebuggerException: Doesn&#39;t appear to be a HotSpot VM (could not find symbol &quot;gHotSpotVMTypes&quot; in remote process)        at sun.jvm.hotspot.HotSpotAgent.setupVM(HotSpotAgent.java:411)        at sun.jvm.hotspot.HotSpotAgent.go(HotSpotAgent.java:305)        at sun.jvm.hotspot.HotSpotAgent.attach(HotSpotAgent.java:140)        at sun.jvm.hotspot.tools.Tool.start(Tool.java:185)        at sun.jvm.hotspot.tools.Tool.execute(Tool.java:118)        at sun.jvm.hotspot.tools.JStack.main(JStack.java:92)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:498)        at sun.tools.jstack.JStack.runJStackTool(JStack.java:140)        at sun.tools.jstack.JStack.main(JStack.java:106)    [centos@localhost bin]$ java -version    java version &quot;1.8.0_202&quot;    Java(TM) SE Runtime Environment (build 1.8.0_202-b08)    Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)    [centos@localhost bin]$ docker exec -it a0c9 /bin/sh    /app # java -version    openjdk version &quot;1.8.0_212&quot;    OpenJDK Runtime Environment (IcedTea 3.12.0) (Alpine 8.212.04-r0)    OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode)</code></pre><p>两边java版本不对应，况且本地java版本过低。但是更改后还是不行，在查看时还是报错。</p><p>关于docker中进程pid为1时，无法操作jmap、jstack等jvm工具。怀疑docker镜像构建的问题，需要排查。</p><p><a href="https://www.cqmaple.com/201905/docker-jvm-memory-limit.html" target="_blank" rel="noopener">https://www.cqmaple.com/201905/docker-jvm-memory-limit.html</a></p><p>问题2：安装openjdk后找不到jmap等工具</p><p>yum install -y java-1.8.0-openjdk-devel</p><p>使用openjdk排查依旧不行，切换为root用户依旧不能操作。最后考虑要么更改基础镜像中的jdk信息，安装相关工具，要么使用jmx来进行监控。</p><p>解决方式：</p><ol><li>对docker镜像进行改造，从只包含jre的镜像替换为具有完整的jdk镜像信息。</li><li>对docker内进程进行优化，使用tini来占据pid 1的进程，避免无法被监控的问题</li></ol><p>参考链接：</p><ul><li><a href="https://blog.no42.org/code/docker-java-signals-pid1/" target="_blank" rel="noopener">https://blog.no42.org/code/docker-java-signals-pid1/</a></li></ul><h3 id="5-对单个服务进行监控，使用jconsole和jvisualvm"><a href="#5-对单个服务进行监控，使用jconsole和jvisualvm" class="headerlink" title="5. 对单个服务进行监控，使用jconsole和jvisualvm"></a>5. 对单个服务进行监控，使用jconsole和jvisualvm</h3><p>以业务建模为例子，改造其启动命令，添加jvm参数信息。原版的服务启动命令如下：</p><pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -javaagent:/app/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.namespace=${SKYWALKING_NAMESPACE} -Dskywalking.agent.service_name=${SKYWALKING_TARGET_SERVICE_NAME} -Dskywalking.collector.backend_service=${SKYWALKING_IP_PORT}  -Dspring.profiles.active=${CHANNEL} -Dspring.cloud.client.ip-address=${IP_ADDR} -Dnacos_ip=${NACOS_IP} -Dnacos_namespace=${NACOS_NAMESPACE} -jar test-opt-visualmodel-0.0.1-SNAPSHOT.jar&quot;]</code></pre><p>改造后的服务启动命令如下：</p><pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;java -Djava.rmi.server.hostname=192.168.232.193 -Dcom.sun.management.jmxremote.port=10001 -Dcom.sun.management.jmxremote.rmi.port=10001 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -javaagent:/app/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.namespace=${SKYWALKING_NAMESPACE} -Dskywalking.agent.service_name=${SKYWALKING_TARGET_SERVICE_NAME} -Dskywalking.collector.backend_service=${SKYWALKING_IP_PORT}  -Dspring.profiles.active=${CHANNEL} -Dspring.cloud.client.ip-address=${IP_ADDR} -Dnacos_ip=${NACOS_IP} -Dnacos_namespace=${NACOS_NAMESPACE} -jar test-opt-visualmodel-0.0.1-SNAPSHOT.jar&quot;]</code></pre><p>变化在于，设置了jmx的连接选项。解释如下：</p><ul><li>-Djava.rmi.server.hostname=192.168.232.193          jmx服务器所在地址信息</li><li>-Dcom.sun.management.jmxremote.port=10001       jmx内部服务端口</li><li>-Dcom.sun.management.jmxremote.rmi.port=10001   jmx对外暴露的服务端口</li><li>-Dcom.sun.management.jmxremote.ssl=false        不使用ssl连接</li><li>-Dcom.sun.management.jmxremote.authenticate=false  不添加授权信息</li></ul><p><strong>一定注意，在内网操作这些信息，不要在外网暴露，容易引起安全问题。</strong></p><p>针对docker的启动命令需要修改为下面的信息，设置端口进行对应。如下：</p><pre><code>docker run -d --restart=on-failure:10 -p 20020:20020 -p 10001:10001 -e SKYWALKING_NAMESPACE=&quot;test-test&quot; -e SKYWALKING_TARGET_SERVICE_NAME=&quot;test-visualmodel&quot; -e SKYWALKING_IP_PORT=&quot;192.168.232.163:11800&quot; -e CHANNEL=&quot;standalone&quot; -e IP_ADDR=&quot;192.168.232.193&quot;  -e NACOS_IP=&quot;192.168.232.194:18848&quot; -e NACOS_NAMESPACE=&quot;aa853012-28dd-404a-a941-e4e36324f615&quot; 192.168.232.159:5000/visualmodel-re:$BUILD_NUMBER</code></pre><p>主要是添加了“ -p 10001:10001 ”对应于该服务的jmx端口，设置对外暴露。</p><p>在本地先打开jconsole进行调试，填入连接信息，如下：</p><p><img src="jconsole%E8%BF%9E%E6%8E%A5.png" alt="jconsole连接"></p><p>点击连接，查看内存信息。如下图：</p><p><img src="jconsole_real_mem.jpg" alt="jconsole内存信息"></p><p>这样就看到了单个服务提交了3.2G的内存分配请求，最大到7.5G，但是实际使用还不到500M。</p><p>问题发现，内存占比过高是因为宿主机的jvm的MaxRamFraction选项，默认为1/4内存，在设置了-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap两个选项后，并未设置镜像需要的内存信息，导致docker镜像还是去读取了宿主机的jvm设置，沿用了宿主机的jvm设置信息。</p><p>参考链接：</p><ul><li><a href="https://blog.kelu.org/tech/2018/05/30/running-a-jvm-in-a-container-without-getting-killed.html" target="_blank" rel="noopener">https://blog.kelu.org/tech/2018/05/30/running-a-jvm-in-a-container-without-getting-killed.html</a></li><li><a href="https://www.li-rui.top/2019/08/22/docker/oracle-jdk8%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6%E6%B5%8B%E8%AF%95/" target="_blank" rel="noopener">https://www.li-rui.top/2019/08/22/docker/oracle-jdk8%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6%E6%B5%8B%E8%AF%95/</a></li><li><a href="https://zhuanlan.zhihu.com/p/61408911https://zhuanlan.zhihu.com/p/61408911" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61408911https://zhuanlan.zhihu.com/p/61408911</a></li></ul><h3 id="如果更改宿主机的jvm信息，是否会改变docker中的内存占用？"><a href="#如果更改宿主机的jvm信息，是否会改变docker中的内存占用？" class="headerlink" title="如果更改宿主机的jvm信息，是否会改变docker中的内存占用？"></a>如果更改宿主机的jvm信息，是否会改变docker中的内存占用？</h3><p>尚未测试</p><h3 id="最终的解决方式"><a href="#最终的解决方式" class="headerlink" title="最终的解决方式"></a>最终的解决方式</h3><ol><li><p>针对docker镜像中的启动命令，添加*<em>-XX:MaxRAMPercentage=85.0 -XX:MaxRAM=1000m</em>选项，启动命令变为</p><p> java -XX:MaxRAMPercentage=85.0 -XX:MaxRAM=1000m -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -javaagent:/app/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.namespace=${SKYWALKING_NAMESPACE} -Dskywalking.agent.service_name=${SKYWALKING_TARGET_SERVICE_NAME} -Dskywalking.collector.backend_service=${SKYWALKING_IP_PORT}  -Dspring.profiles.active=${CHANNEL} -Dspring.cloud.client.ip-address=${IP_ADDR} -Dnacos_ip=${NACOS_IP} -Dnacos_namespace=${NACOS_NAMESPACE} -jar test-opt-visualmodel-0.0.1-SNAPSHOT.jar”</p></li></ol><p>表示将该镜像的最大内存占用限制在1000m，留有85%的内存给该微服务运行使用。设置后效果如下：</p><p><img src="jconsole_solve.png" alt="jconsole解决方式"></p><p>明显降低了内存占用，且服务基本不受影响，需要推广到其它服务进行测试。</p><ol start="2"><li><p>改造Dockerfile，使用具有完整功能的jdk镜像，将基础镜像更改为OpenJ9或者Fabric镜像，进一步降低内存占用。</p></li><li><p>完善监控机制，保证无死角监控。</p></li><li><p>后续排查这类问题遵循由内到外、工具现行的理念。</p></li></ol><h3 id="番外：jenkins僵尸进程问题"><a href="#番外：jenkins僵尸进程问题" class="headerlink" title="番外：jenkins僵尸进程问题"></a>番外：jenkins僵尸进程问题</h3><p>起因：由于服务器挂掉，导致在构建中的任务无法连接目标服务器，导致该任务成为僵尸进程，无法操作。点击红色叉号并不能停止该构建。</p><p>操作方式：脚本方式删除，还有通过monitor插件进行删除，均无效。只有在上述操作过后，重新启动jenkins才能删除之前的僵尸进程！</p><p>脚本方式删除：</p><pre><code>job = Jenkins.instance.getItemByFullName(&quot;test-visualmodel-peer0&quot;)buildNum = job.getBuildByNumber(68)buildNum.doStop()</code></pre><p>解决方式：针对产生僵尸进程的问题，通过设置ssh的超时时间解决。</p><p>jenkins重启链接：<a href="http://192.168.232.159:9090/restart" target="_blank" rel="noopener">http://192.168.232.159:9090/restart</a></p><h3 id="番外：提到的jvm配置选项的含义"><a href="#番外：提到的jvm配置选项的含义" class="headerlink" title="番外：提到的jvm配置选项的含义"></a>番外：提到的jvm配置选项的含义</h3><p>待添加</p><h3 id="关于Docker镜像的优化方式"><a href="#关于Docker镜像的优化方式" class="headerlink" title="关于Docker镜像的优化方式"></a>关于Docker镜像的优化方式</h3><p>待添加</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="http://101.201.252.212/Question/426707" target="_blank" rel="noopener">http://101.201.252.212/Question/426707</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>rancher集群化部署</title>
      <link href="/2020/02/18/rancher-ji-qun-hua-bu-shu/"/>
      <url>/2020/02/18/rancher-ji-qun-hua-bu-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="Rancher集群化部署–在线安装"><a href="#Rancher集群化部署–在线安装" class="headerlink" title="Rancher集群化部署–在线安装"></a>Rancher集群化部署–在线安装</h1><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>目前我们的工作进展有序，已经实现了利用Docker部署各微服务，结合CI/CD进行微服务构建。</p><p>但是面对当前的部署方式，依旧存在局限性：</p><ol><li>集群问题，部署时启动多套有难度</li><li>微服务停机或者升级时，或者微服务出现故障时，导致出现问题后不能及时上报</li><li>目前的部署方式必须进行停机更新，不满足服务不停机更新，且不满足滚动部署或者蓝绿部署的要求</li><li>微服务不能进行弹性伸缩，在熔断、限流、降级方面做的不足</li></ol><p>针对以上的问题，引入k8s进行微服务的集群化部署。利用Rancher来针对k8s集群进行统一管理，对整个集群进行运维。</p><h3 id="规划容量"><a href="#规划容量" class="headerlink" title="规划容量"></a>规划容量</h3><p>我们目前预测大概要到100多个的微服务集群信息，参考Rancher官网的容量规划，进行配置。初步确定服务器大概为7台，其中三台安装k8s，剩余一台做管理机，其它三台做备用机，进行日常添加删除主机的使用。</p><p>配置图如下：</p><table><thead><tr><th>主机名称</th><th>主机配置</th><th>主机ip</th><th>部署服务</th><th>备注</th></tr></thead><tbody><tr><td>k8s-rke-node-start</td><td>4c 16G 200G</td><td>192.168.238.240</td><td>nginx访问入口，rke、kubectl、helm安装工具</td><td>安装机和日常运维机器</td></tr><tr><td>k8s-master-node00</td><td>4c 16G 200G</td><td>192.168.238.239</td><td>k8s主节点工具、rancher</td><td>集群主节点1号</td></tr><tr><td>k8s-master-node01</td><td>4c 16G 200G</td><td>192.168.238.232</td><td>k8s主节点工具、rancher</td><td>集群主节点2号</td></tr><tr><td>k8s-master-node02</td><td>4c 16G 200G</td><td>192.168.238.241</td><td>k8s主节点工具、rancher</td><td>集群主节点3号</td></tr><tr><td>k8s-master-cluster-node01</td><td>2c 8G 100G</td><td>192.168.238.233</td><td>k8s从节点</td><td>集群从节点1号，未使用</td></tr><tr><td>k8s-master-cluster-node02</td><td>2c 8G 100G</td><td>192.168.238.234</td><td>k8s从节点</td><td>集群从节点2号，未使用</td></tr><tr><td>k8s-master-cluster-node03</td><td>2c 8G 100G</td><td>192.168.238.235</td><td>k8s从节点</td><td>集群从节点3号，未使用</td></tr></tbody></table><h3 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h3><ul><li>操作系统：CentOS 7.6 1810，以Basic Server方式安装</li><li>docker版本：19.03.6</li><li>kubernetes版本：1.17.2</li><li>rancher版本：2.3.5</li><li>nginx版本：1.16.1</li><li>工具链<ul><li>rke: 1.0.4</li><li>kubectl: 1.16.6</li><li>helm: 3.1.0</li></ul></li></ul><h3 id="一、服务器准备"><a href="#一、服务器准备" class="headerlink" title="一、服务器准备"></a>一、服务器准备</h3><p>参考 Linux日常运维操作 进行服务器准备，优先设置系统初始化的相关安装，以及docker软件的安装，这里不再说明，请参考。随后从docker安装完成后进行准备说明。在docker安装完成后，要对其进行锁定，避免因为使用yum update命令导致docker版本变动。</p><p><strong>下面的操作每一台服务器上都要进行操作。</strong></p><h4 id="1-docker的基本配置"><a href="#1-docker的基本配置" class="headerlink" title="1. docker的基本配置"></a>1. docker的基本配置</h4><p>daemon.json默认位于/etc/docker/daemon.json，如果没有可手动创建，基于systemd管理的系统都是相同的路径。通过修改daemon.json来改过Docker配置，也是Docker官方推荐的方法。</p><p>1、配置私有仓库（目前并未使用私有仓库，暂时不进行配置）<br>Docker默认只信任TLS加密的仓库地址(https)，所有非https仓库默认无法登陆也无法拉取镜像。insecure-registries字面意思为不安全的仓库，通过添加这个参数对非https仓库进行授信。可以设置多个insecure-registries地址，以数组形式书写，地址不能添加协议头(http)。<br>{<br>    “insecure-registries”: [“harbor.xxx.cn:30002”]<br>}</p><p>2、配置存储驱动<br>OverlayFS是一个新一代的联合文件系统，类似于AUFS，但速度更快，实现更简单。Docker为OverlayFS提供了两个存储驱动程序:旧版的overlay，新版的overlay2(更稳定)。</p><p>先决条件:</p><p>overlay2: Linux内核版本4.0或更高版本，或使用内核版本3.10.0-514+的RHEL或CentOS。<br>overlay: 主机Linux内核版本3.18+</p><p>支持的磁盘文件系统</p><p>ext4(仅限RHEL 7.1)<br>xfs(RHEL7.2及更高版本)，需要启用d_type=true。</p><p>{<br>    “storage-driver”: “overlay2”,<br>    “storage-opts”: [“overlay2.override_kernel_check=true”]<br>}</p><p>3、配置日志驱动（由于设置后会导致docker服务无法启动，暂时未进行设置）</p><p>容器在运行时会产生大量日志文件，很容易占满磁盘空间。通过配置日志驱动来限制文件大小与文件的数量。 &gt;限制单个日志文件为50M,最多产生3个日志文件<br>{<br>    “log-driver”: “json-file”,<br>    “log-opts”: {<br>        “max-size”: “50m”,<br>        “max-file”: “3”<br>    }<br>}</p><p>4、配置国内公共镜像</p><p> “registry-mirrors”: [<br>        “<a href="https://registry.cn-hangzhou.aliyuncs.com&quot;" target="_blank" rel="noopener">https://registry.cn-hangzhou.aliyuncs.com&quot;</a>,<br>        “<a href="https://docker.mirrors.ustc.edu.cn&quot;" target="_blank" rel="noopener">https://docker.mirrors.ustc.edu.cn&quot;</a>,<br>        “<a href="https://registry.docker-cn.com&quot;" target="_blank" rel="noopener">https://registry.docker-cn.com&quot;</a><br>    ],</p><p>最终配置文件如下：</p><pre><code>{    &quot;registry-mirrors&quot;: [        &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,        &quot;https://docker.mirrors.ustc.edu.cn&quot;,        &quot;https://registry.docker-cn.com&quot;    ],    &quot;insecure-registries&quot;: [&quot;harbor.xxx.cn:30002&quot;]，    &quot;log-driver&quot;: &quot;json-file&quot;,    &quot;log-opts&quot;: {        &quot;max-size&quot;: &quot;50m&quot;,        &quot;max-file&quot;: &quot;3&quot;    }}</code></pre><p>操作如下：</p><pre><code># vim /etc/docker/daemon.json// 输入以下内容{&quot;registry-mirrors&quot;: [    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,    &quot;https://docker.mirrors.ustc.edu.cn&quot;,    &quot;https://registry.docker-cn.com&quot;],&quot;insecure-registries&quot;: [&quot;harbor.xxx.cn:30002&quot;]，&quot;log-driver&quot;: &quot;json-file&quot;,&quot;log-opts&quot;: {    &quot;max-size&quot;: &quot;50m&quot;,    &quot;max-file&quot;: &quot;3&quot;    }}// :wq进行保存</code></pre><p>设置完成后，需要重启docker服务，操作如下：</p><pre><code>$ sudo systemctl daemon-reload$ sudo systemctl restart docker</code></pre><h4 id="2-服务器名称修改"><a href="#2-服务器名称修改" class="headerlink" title="2. 服务器名称修改"></a>2. 服务器名称修改</h4><pre><code>// 变更服务器hostname# hostnamectl set-hostname k8s-node-1// 修改host文件# vim /etc/hosts// 在文件后续追加以下内容192.168.238.239 k8s-master-node00192.168.238.240 k8s-rke-node-start192.168.238.232 k8s-cluster-node01192.168.238.241 k8s-cluster-node02</code></pre><h4 id="3-开放端口"><a href="#3-开放端口" class="headerlink" title="3. 开放端口"></a>3. 开放端口</h4><pre><code>// 根据官方文档开放端口信息$ sudo firewall-cmd --zone=public --add-port=80/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=443/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=2376/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=6443/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=2379/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=2380/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=8472/udp --permanent$ sudo firewall-cmd --zone=public --add-port=9099/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=10250/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=10254/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent$ sudo firewall-cmd --zone=public --add-port=30000-32767/udp --permanent// 使配置生效$ sudo firewall-cmd --reload</code></pre><h4 id="4-关闭selinux"><a href="#4-关闭selinux" class="headerlink" title="4. 关闭selinux"></a>4. 关闭selinux</h4><pre><code>// 获取linux状态$ sudo getenforce// 命令行设置关闭selinux$ sudo setenforce 0// 修改配置文件$ sudo vim /etc/selinux/config// 将文件中的SELINUX=enforcing改为SELINUX=disabled// :wq 保存文件退出</code></pre><h4 id="5-禁用swap"><a href="#5-禁用swap" class="headerlink" title="5. 禁用swap"></a>5. 禁用swap</h4><pre><code>// 删除 swap 区所有内容# swapoff -a删除 swap 挂载，这样系统下次启动不会再挂载 swap// 编辑挂载文件，用 “#” 注释 swap 行# vim /etc/fstab// 找到/dev/mapper/centos-swap swap这一行，用“#”进行注释// 重启系统，测试# reboot// 重启后查看$ free -h            total        used        free      shared  buff/cache   availableMem:           3.7G        203M        3.1G        8.5M        384M        3.3GSwap:            0B          0B          0B</code></pre><h4 id="7-操作系统及kernel调优"><a href="#7-操作系统及kernel调优" class="headerlink" title="7. 操作系统及kernel调优"></a>7. 操作系统及kernel调优</h4><pre><code>// 针对文件打开数调优。# vim /etc/security/limits.conf// 输入以下内容root soft nofile 65535root hard nofile 65535*    soft nofile 65535*    hard nofile 65535// :wq保存退出// 调整最大进程数# sed -i &#39;s#4096#65535#g&#39; /etc/security/limits.d/20-nproc.conf// kernel调优# cat &gt;&gt; /etc/sysctl.conf&lt;&lt;EOFnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1vm.swappiness=0vm.max_map_count=655360EOF// 使修改生效$ sudo sysctl -p</code></pre><h4 id="8-服务器免密登录"><a href="#8-服务器免密登录" class="headerlink" title="8. 服务器免密登录"></a>8. 服务器免密登录</h4><p>这一步操作从192.168.238.240所在的服务器进行操作，由此机器将ssh密钥生成并发送到各个目标机器上。</p><pre><code>// 生成密钥$ ssh-keygen -t rsa$ ssh-copy-id -p 15555 centos@192.168.238.232$ ssh-copy-id -p 15555 centos@192.168.238.239$ ssh-copy-id -p 15555 centos@192.168.238.241</code></pre><h3 id="二、安装工具链–rke、kubectl、helm"><a href="#二、安装工具链–rke、kubectl、helm" class="headerlink" title="二、安装工具链–rke、kubectl、helm"></a>二、安装工具链–rke、kubectl、helm</h3><h4 id="1-下载rke工具"><a href="#1-下载rke工具" class="headerlink" title="1. 下载rke工具"></a>1. 下载rke工具</h4><pre><code>// 在线下载，也可以下载完成后通过ftp传输到服务器上$ wget https://github.com/rancher/rke/releases/download/v1.0.4/rke_linux-amd64// 添加可执行权限$ sudo chmod +x rke_linux-amd64// 添加到环境变量，或者直接在~/.bashrc文件中用export命令暴露该工具$ sudo mv rke_linux-amd64 /usr/local/bin/rke// 测试是否已经安装完毕$ rke --version// 输出信息：rke version v1.0.4</code></pre><h4 id="2-下载kubectl工具"><a href="#2-下载kubectl工具" class="headerlink" title="2. 下载kubectl工具"></a>2. 下载kubectl工具</h4><p>1.16.6版本对应</p><pre><code>// 有时候会下载不下来，多尝试几次$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.6/bin/linux/amd64/kubectl$ sudo chmod +x kubectl$ sudo mv ./kubectl /usr/local/bin/kubectl// 引入自动补全// 安装自动补全工具$ sudo yum install bash-completion -y$ echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc</code></pre><h4 id="3-安装Helm"><a href="#3-安装Helm" class="headerlink" title="3. 安装Helm"></a>3. 安装Helm</h4><p>访问 Helm Github 下载页面 <a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">https://github.com/helm/helm/releases</a> 找到最新的客户端，里面有不同系统下的包，这里我们选择 Linux amd64，然后在 Linux 系统中使用 Wget 命令进行下载。</p><pre><code>// 下载Helm客户端$ wget https://get.helm.sh/helm-v3.1.0-linux-amd64.tar.gz</code></pre><p>接下来解压下载的包，然后将客户端放置到 /usr/local/bin/ 目录下：</p><pre><code>// 解压 Helm$ tar -zxvf helm-v3.1.0-linux-amd64.tar.gz// 复制客户端执行文件到 bin 目录下，方便在系统下能执行 helm 命令$ sudo mv linux-amd64/helm /usr/local/bin/helm// 测试是否安装成功$ helm version// 输出以下内容version.BuildInfo{Version:&quot;v3.1.0&quot;, GitCommit:&quot;b29d20baf09943e134c2fa5e1e1cab3bf93315fa&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.13.7&quot;}// 下面需要添加Chart仓库$ helm repo add  elastic    https://helm.elastic.co$ helm repo add  gitlab     https://charts.gitlab.io$ helm repo add  harbor     https://helm.goharbor.io$ helm repo add  bitnami    https://charts.bitnami.com/bitnami$ helm repo add  incubator  http://mirror.azure.cn/kubernetes/charts-incubator/$ helm repo add  stable     http://mirror.azure.cn/kubernetes/charts-incubator/$ helm repo add  rancher-stable    http://releases.rancher.com/server-charts/stable$ helm repo update</code></pre><h3 id="三、利用rke安装k8s"><a href="#三、利用rke安装k8s" class="headerlink" title="三、利用rke安装k8s"></a>三、利用rke安装k8s</h3><h4 id="1-生成配置文件并进行集群部署"><a href="#1-生成配置文件并进行集群部署" class="headerlink" title="1. 生成配置文件并进行集群部署"></a>1. 生成配置文件并进行集群部署</h4><p>可以用两种方式创建配置文件，一种是直接编写yml文件，另一种是通过交互式方式生成yml文件。我这边在实践的过程中，发现通过交互式生成生成的配置文件，在安装过程中出现了各种各样的问题。利用交互式生成配置文件，然后进行进行裁剪，参考官网步骤进行配置。这里直接进行编写。</p><pre><code>$ mkdir cluster// 运行rke config交互式配置向导$ rke config// 配置完成后编辑生成的文件cluster.yml$ vim cluster.yml// 文件最终内容如下：</code></pre><pre><code># If you intened to deploy Kubernetes in an air-gapped environment,               # please consult the documentation on how to configure custom RKE images.         nodes:                                                                            - address: 192.168.238.239                                                              port: &quot;15555&quot;                                                                     internal_address: &quot;&quot;                                                              role:                                                                             - controlplane                                                                    - worker                                                                          - etcd                                                                            hostname_override: &quot;&quot;                                                             user: centos                                                                      docker_socket: /var/run/docker.sock                                               ssh_key: &quot;&quot;                                                                       ssh_key_path: ~/.ssh/id_rsa                                                       ssh_cert: &quot;&quot;                                                                      ssh_cert_path: &quot;&quot;                                                                 labels: {}                                                                        taints: []                                                                      - address: 192.168.238.232                                                              port: &quot;15555&quot;                                                                     internal_address: &quot;&quot;                                                              role:                                                                             - controlplane                                                                    - worker                                                                          - etcd                                                                            hostname_override: &quot;&quot;                                                             user: centos                                                                      docker_socket: /var/run/docker.sock                                               ssh_key: &quot;&quot;                                                                       ssh_key_path: ~/.ssh/id_rsa                                                       ssh_cert: &quot;&quot;                                                                      ssh_cert_path: &quot;&quot;                                                                 labels: {}                                                                        taints: []                                                                      - address: 192.168.238.241                                                              port: &quot;15555&quot;                                                                     internal_address: &quot;&quot;                                                              role:                                                                             - controlplane                                                                    - worker                                                                          - etcd                                                                            hostname_override: &quot;&quot;  user: centos  docker_socket: /var/run/docker.sock  ssh_key: &quot;&quot;  ssh_key_path: ~/.ssh/id_rsa  ssh_cert: &quot;&quot;  ssh_cert_path: &quot;&quot;  labels: {}  taints: []services:  etcd:    snapshot: true    retention: 24h    creation: 6h# 如果这里你使用自带的ssl签名或者使用公用签名，可以不添加下面的选项ingress:  provider: nginx  options:    use-forwarded-headers: &quot;true&quot;</code></pre><p>设置完成后，下面可以正式开始部署了。执行下面的命令开始安装：</p><pre><code>// 部署并启动k8s集群$ rke up --config ./cluster.yml</code></pre><p>这时候开始进入部署状态。将会在三台服务器上进行部署，拉取相应镜像进行操作。</p><p>但是在部署过程中会存在各种各样的问题，下面单独设置一个章节讲解。假设现在一切顺利，安装完成了。最终在控制台上显示</p><pre><code>INFO[0172] Finished building Kubernetes cluster successfully</code></pre><p>说明安装完成。这时候k8s集群就正式完成了。部署完成后，在同级别目录下会存在三个文件，其中两个是主动生成的，分别为：</p><pre><code>cluster.yml：RKE集群配置文件。kube_config_cluster.yml：集群的Kubeconfig文件，此文件包含完全访问集群的凭据。cluster.rkestate：Kubernetes集群状态文件，此文件包含集群部署的状态，用于升级和更新。    </code></pre><h4 id="2-问题总结"><a href="#2-问题总结" class="headerlink" title="2. 问题总结"></a>2. 问题总结</h4><ul><li><strong>前提</strong>：如果出现错误，请先准备好清理脚本，重新进行安装。在232、239、241服务器上进行下面创建脚本的操作。</li></ul><p>如果有错误，先停止所有机器的镜像，无论是否在运行，通过执行下面的清理脚本恢复服务器初始状态。操作如下：</p><pre><code>$ vim rancher_clean-dirs.sh // 输入以下内容#!/bin/shif [ $(id -u) -ne 0 ]; then    echo &quot;Must be run as root!&quot;    exitfi DLIST=&quot;/var/lib/etcd /etc/kubernetes /etc/cni /opt/cni /var/lib/cni /var/run/calico /opt/rke&quot;for dir in $DLIST; do    echo &quot;Removing $dir&quot;    rm -rf $dirdone// :wq进行保存退出$ vim rancher_clean-docker.sh// 输入以下内容#!/bin/shCLIST=$(docker ps -qa)if [ &quot;x&quot;$CLIST == &quot;x&quot; ]; then    echo &quot;No containers exist - skipping container cleanup&quot;else    docker rm -f $CLISTfiILIST=$(docker images -a -q)if [ &quot;x&quot;$ILIST == &quot;x&quot; ]; then    echo &quot;No images exist - skipping image cleanup&quot;else    docker rmi $ILISTfiVLIST=$(docker volume ls -q)if [ &quot;x&quot;$VLIST == &quot;x&quot; ]; then    echo &quot;No volumes exist - skipping volume cleanup&quot;else    docker volume rm -f $VLISTfi// :wq进行保存退出// 针对每个文件添加可执行权限$ sudo chmod +x rancher_clean-dirs.sh $ sudo chmod +x rancher_clean-docker.sh </code></pre><p>这样在具体执行的时候，在目标服务器上，进行以下操作：</p><pre><code>$ sudo ./rancher_clean-docker.sh$ sudo ./rancher_clean-docker.sh </code></pre><p>即可进行服务器清理操作，对目标服务器清理完成后，再回到240服务器进行执行部署命令。</p><ul><li>问题列表</li></ul><ol><li>安装报错：</li></ol><p>如果这一步报错下边的内容：</p><pre><code>if the SSH server version is at least version 6.7 or higher. If you are using RedHat/CentOS, you can&#39;t use the user `root`. Please refer to the documentation for more instructions</code></pre><p>则可能是系统的openssh版本太低，只需执行如下命令升级即可：</p><pre><code>$ ssh -VOpenSSH_6.6.1p1, OpenSSL 1.0.1e-fips 11 Feb 2013  // 低于上边要求的6.7// 切换管理员权限$ sudo su# yum -y update openssh# ssh -VOpenSSH_7.4p1, OpenSSL 1.0.2k-fips  26 Jan 2017</code></pre><p>然后再切回centos用户执行安装即可！</p><ol start="2"><li>安装报错：</li></ol><p>错误日志如下：</p><pre><code>WARN[0934] Failed to create Docker container [etcd-fix-perm] on host [192.168.238.239]: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?WARN[0934] Failed to create Docker container [etcd-fix-perm] on host [192.168.238.239]: Error response from daemon: Conflict. The container name &quot;/etcd-fix-perm&quot; is already in use by container &quot;3d0a7d0a9d12fb088eeda6a02c5f18c98a561149fadf9d4fa0b261b0f79273b5&quot;. You have to remove (or rename) that container to be able to reuse that name.WARN[0934] Failed to create Docker container [etcd-fix-perm] on host [192.168.238.239]: Error response from daemon: Conflict. The container name &quot;/etcd-fix-perm&quot; is already in use by container &quot;3d0a7d0a9d12fb088eeda6a02c5f18c98a561149fadf9d4fa0b261b0f79273b5&quot;. You have to remove (or rename) that container to be able to reuse that name.FATA[0934] [etcd] Failed to bring up Etcd Plane: Failed to create [etcd-fix-perm] container on host [192.168.238.239]: Failed to create Docker container [etcd-fix-perm] on host [192.168.238.239]: &lt;nil&gt;</code></pre><p>只在239机器上执行清理脚本后，再转回到240机器，运行rke up –config cluster.yml。</p><ol start="3"><li>安装报错：</li></ol><p>错误日志如下：</p><pre><code>FATA[0700] Failed to get job complete status for job rke-network-plugin-deploy-job in namespace kube-system</code></pre><p>根据之前cluster.yml中的配置，创建docker网络，参考service_cluster_ip_range的配置项，默认为10.43.0.0/16</p><pre><code>docker network create --driver=bridge --subnet=10.43.0.0/16 br0_rke</code></pre><p>如果不创建也可以，重新执行rke up –name cluster.yml命令。</p><ol start="4"><li>安装报错</li></ol><p>清理完成后还是报错：</p><pre><code>WARN[0224] [etcd] host [192.168.238.239] failed to check etcd health: failed to get /health for host [192.168.238.239]: Get https://192.168.238.239:2379/health: remote error: tls: bad certificateWARN[0239] [etcd] host [192.168.238.232] failed to check etcd health: failed to get /health for host [192.168.238.232]: Get https://192.168.238.232:2379/health: remote error: tls: bad certificateWARN[0255] [etcd] host [192.168.238.241] failed to check etcd health: failed to get /health for host [192.168.238.241]: Get https://192.168.238.241:2379/health: remote error: tls: bad certificateFATA[0255] [etcd] Failed to bring up Etcd Plane: etcd cluster is unhealthy: hosts [192.168.238.239,192.168.238.232,192.168.238.241] failed to report healthy. Check etcd container logs on each host for more information</code></pre><p>请检查时钟同步问题，第二个是检查是否有未开放的端口信息。如果还有问题，需要重新执行清理脚本后，再运行rke config –name rancher-cluster.yml。</p><h4 id="3-配置信息指定"><a href="#3-配置信息指定" class="headerlink" title="3. 配置信息指定"></a>3. 配置信息指定</h4><p>安装完成后，查看各个节点信息，执行以下命令：</p><pre><code>$ kubectl get nodes</code></pre><p>可能会出现以下报错</p><pre><code>The connection to the server localhost:8080 was refused - did you specify the right host or port?</code></pre><p>或者报错情况如下： </p><pre><code>Config not found: /home/centos/kube_config_cluster.ymlThe connection to the server localhost:8080 was refused - did you specify the right host or port?</code></pre><p>原因：kubenetes master没有与本机绑定，集群初始化的时候没有设置</p><p>解决办法：</p><pre><code>// 在命令行直接执行：$ export KUBECONFIG=/home/centos/kube_config_cluster.conf// 或者将该命令放到~/.bashrc中 $ vim ~/.bashrc// 文件末尾追加export KUBECONFIG=/home/centos/kube_config_cluster.conf// :wq保存// 使配置生效$ source ~/.bashrc// 或者 在执行命令时手动指定配置文件，利用--kubeconfig。例如：$ kubectl get nodes --kubeconfig=/home/centos/kube_config_cluster.conf</code></pre><!-- /etc/kubernetes/admin.conf这个文件主要是集群初始化的时候用来传递参数的 --><p>解决完上述问题，这时重新执行命令即可。</p><pre><code>$ kubectl get nodesNAME          STATUS   ROLES                      AGE   VERSION192.168.238.232   Ready    controlplane,etcd,worker   36m   v1.16.6192.168.238.239   Ready    controlplane,etcd,worker   36m   v1.16.6192.168.238.241   Ready    controlplane,etcd,worker   36m   v1.16.6$  kubectl get pods --all-namespacesNAMESPACE       NAME                                      READY   STATUS      RESTARTS   AGEingress-nginx   default-http-backend-67cf578fc4-2g498     1/1     Running     0          7m4singress-nginx   nginx-ingress-controller-44wzn            1/1     Running     0          7m5singress-nginx   nginx-ingress-controller-cf84g            1/1     Running     0          7m5singress-nginx   nginx-ingress-controller-dxpxq            1/1     Running     0          7m5skube-system     canal-28m8p                               2/2     Running     0          32mkube-system     canal-479px                               2/2     Running     0          32mkube-system     canal-vdcvf                               2/2     Running     0          32mkube-system     coredns-7c5566588d-flshv                  1/1     Running     0          7m29skube-system     coredns-7c5566588d-t6vx6                  1/1     Running     0          6m35skube-system     coredns-autoscaler-65bfc8d47d-bnj2h       1/1     Running     0          7m26skube-system     metrics-server-6b55c64f86-fgpkd           1/1     Running     0          7m15skube-system     rke-coredns-addon-deploy-job-vtnrs        0/1     Completed   0          7m34skube-system     rke-ingress-controller-deploy-job-d556q   0/1     Completed   0          7m13skube-system     rke-metrics-addon-deploy-job-dpdhq        0/1     Completed   0          7m23skube-system     rke-network-plugin-deploy-job-xkmgs       0/1     Completed   0          33m</code></pre><p>说明已经完成。需要注意的是，有些服务并未处在Running状态，一定要保证全部处于Running状态时，才能进行下一步操作。</p><h3 id="四、Rancher集群的安装"><a href="#四、Rancher集群的安装" class="headerlink" title="四、Rancher集群的安装"></a>四、Rancher集群的安装</h3><p>在进行Rancher集群安装之前，需要说明，这里采用私有签发CA证书的方式来实现SSL访问。首先进行证书的签发，然后进行rancher集群的安装。</p><h4 id="1-证书签发"><a href="#1-证书签发" class="headerlink" title="1. 证书签发"></a>1. 证书签发</h4><p>使用openssl创建自己的CA certificate</p><pre><code>// # ① 生成CA私钥文件$ openssl genrsa -out ca.key 2048</code></pre><p>值得注意的是，Common Name一定要填写*.exmaple.org，以表示这个证书是一个通配证书（Wildcard Certificates），浏览器检查一个证书是否匹配，检查的就是Common Name。</p><pre><code>// # ② 生成CA证书文件$ openssl req -new -x509 -key ca.key -out ca.crt</code></pre><p>这时候根证书已经完成，下面利用根证书来进行签发CA证书。操作如下：</p><pre><code>// 签发对应域名的私有密钥$ openssl genrsa -out rancher.mine.com.key 2048// 签发csr证书文件$ openssl req -new -key rancher.mine.com.key -out rancher.mine.com.csr// 编辑cnf配置文件$ vim rancher.mine.com.cnf// 输入以下信息basicConstraints=CA:FALSEsubjectAltName=@my_subject_alt_namessubjectKeyIdentifier = hash[ my_subject_alt_names ]DNS.1 = *.rancher.mine.comDNS.2 = rancher.mine.com// :wq进行保存// 这里两个DNS配置表示，进行通配证书的签发// 最后执行，生成公钥信息$ openssl x509 -req -in rancher.mine.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out rancher.mine.com.x4.crt -extfile rancher.mine.com.cnf -sha256// 验证证书$ openssl verify -CAfile ca.crt rancher.mine.com.x4.crt</code></pre><p>这样在一个文件夹下，就有多个个证书文件了，分别是下面的几个：</p><pre><code>ca.crt  ca.srl      rancher.mine.com.cnf  rancher.mine.com.keyca.key  rancher.mine.com.csr  rancher.mine.com.x4.crt</code></pre><p>我们在这里用到的是<strong>rancher.mine.com.key</strong>和<strong>rancher.mine.com.x4.crt</strong>文件。</p><h4 id="2-rancher集群部署"><a href="#2-rancher集群部署" class="headerlink" title="2. rancher集群部署"></a>2. rancher集群部署</h4><p>执行以下命令：</p><pre><code>// 添加rancher的源仓库$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable// 在k8s中创建cattle-system命名空间，只能是这个名字$ kubectl create namespace cattle-system// 以私有证书的方式进行rancher的安装，标志为：--set ingress.tls.source=secret$ helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher.mine.com --set ingress.tls.source=secret// 输出如下NAME: rancherLAST DEPLOYED: Sun Feb 23 13:32:13 2020NAMESPACE: cattle-systemSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:Rancher Server has been installed.NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued and Ingress comes up.Check out our docs at https://rancher.com/docs/rancher/v2.x/en/Browse to https://rancher.mine.orgHappy Containering!</code></pre><p>当出现<strong>Happy Containering!</strong>的时候，就说明安装已经完成了。这时候可以通过以下命令进行测试：</p><pre><code>$ kubectl -n cattle-system rollout status deploy/rancher// 输出以下内容时，可认为安装完成Waiting for deployment &quot;rancher&quot; rollout to finish: 0 of 3 updated replicas are available...deployment &quot;rancher&quot; successfully rolled out// 查看服务状态$ kubectl -n cattle-system get deploy rancherNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGErancher   3         3         3            3           3m</code></pre><p>这样当Rancher安装完毕后，需要将私有的签名信息添加到Rancher集群的访问中。操作如下：</p><pre><code>$ kubectl -n cattle-system create secret tls tls-rancher-ingress --cert=rancher.mine.com.x4.crt --key=rancher.mine.com.key// 输出下面的内容secret/tls-rancher-ingress created</code></pre><p>证书生效后，创建tls-rancher-ingress，这样Rancher就可以在服务器内部被访问了。可以随便在232、239、241的服务器上执行：</p><pre><code>$ curl -L rancher.mine.com -k// 可以出现JSON数据</code></pre><h3 id="五、Nginx负载均衡配置"><a href="#五、Nginx负载均衡配置" class="headerlink" title="五、Nginx负载均衡配置"></a>五、Nginx负载均衡配置</h3><h4 id="1-Nginx安装配置"><a href="#1-Nginx安装配置" class="headerlink" title="1. Nginx安装配置"></a>1. Nginx安装配置</h4><pre><code>// 安装nginx $ sudo yum install -y nginx$ sudo systemctl enable nginx// 将上面生成的证书拷贝到nginx安装目录下$ cd /etc/nginx$ sudo mkdir cert$ sudo cp ~/nginx/rancher.mine.com.key /etc/nginx/cert$ sudo cp ~/nginx/rancher.mine.com.x4.crt /etc/nginx/cert// 修改配置文件如下$ sudo mv nginx.conf nginx.conf.bak$ sudo vim /etc/nginx/nginx.conf// 将以下信息填入user nginx;worker_processes 4;worker_rlimit_nofile 40000;events {    worker_connections 8192;}http {    upstream rancher_servers {        least_conn;        server 192.168.238.232:80 max_fails=3 fail_timeout=5s;        server 192.168.238.239:80 max_fails=3 fail_timeout=5s;        server 192.168.238.241:80 max_fails=3 fail_timeout=5s;    }    map $http_upgrade $connection_upgrade {        default Upgrade;        &#39;&#39;      close;    }    gzip on;    gzip_disable &quot;msie6&quot;;    gzip_disable &quot;MSIE [1-6].(?!.*SV1)&quot;;    gzip_vary on;    gzip_static on;    gzip_proxied any;    gzip_min_length 0;    gzip_comp_level 8;    gzip_buffers 16 8k;    gzip_http_version 1.1;    gzip_types text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml application/font-woff text/javascript application/javascript application/x-javascript text/x-json application/json application/x-web-app-manifest+json text/css text/plain text/x-component font/opentype application/x-font-ttf application/vnd.ms-fontobject font/woff2 image/x-icon image/png image/jpeg;    server {        listen         80;        server_name    rancher.mine.com;        rewrite ^(.*) https://$server_name$1 permanent;    }    server {        listen     443 ssl;        server_name    rancher.mine.com;        ssl_certificate cert/rancher.mine.com.x4.crt;        ssl_certificate_key cert/rancher.mine.com.key;        ssl_session_timeout 5m;        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;        ssl_protocols TLSv1 TLSv1.1 TLSv1.2;        ssl_prefer_server_ciphers on;        location / {            proxy_set_header Host $host;            proxy_set_header X-Forwarded-Proto $scheme;            proxy_set_header X-Forwarded-Port $server_port;            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;            proxy_pass http://rancher_servers;            proxy_http_version 1.1;            proxy_set_header Upgrade $http_upgrade;            proxy_set_header Connection $connection_upgrade;        }    }}// :wq进行保存后退出                                                 // 测试nginx配置文件是否正常$ sudo nginx -t// 正常后重启Nginx服务$ sudo systemctl restart nginx</code></pre><h4 id="2-设置hosts文件地址映射"><a href="#2-设置hosts文件地址映射" class="headerlink" title="2. 设置hosts文件地址映射"></a>2. 设置hosts文件地址映射</h4><p>rancher.com就是后面访问rancher的域名，需要在232、239、241服务器上的/etc/hosts文件中添加关联：</p><pre><code># echo &quot;192.168.238.240 rancher.mine.com&quot; &gt;&gt; /etc/hosts</code></pre><p>在你访问Rancher的web页面的机器上，也要配置hosts，例如在win10中，修改hosts文件，添加</p><pre><code>192.168.238.240 rancher.mine.com</code></pre><p>到文件末尾保存即可。这时打开浏览器，直接访问rancher.mine.com即可。如下图：</p><p>最后设置管理员账号密码即可，默认密码为admin，调整为我们常用的123456，完成。</p><p>后续登录的用户名为admin，密码为123456。地址为rancher.mine.com，记住一定设置host文件将ip和域名地址对应。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本次安装选择了较新的版本，一方面是屏蔽之前的bug，另一方面是该版本对于istio的支持较好，后续实施istio的时候可以比较方便。</p><h2 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h2><p>一、<strong>在后续运行的过程中，发现有部分服务始终处于CrashLoopBackOff的状态并且重启次数过多。</strong>如下：</p><pre><code>$ kubectl get pod -n cattle-systemNAME                                    READY   STATUS             RESTARTS   AGEcattle-cluster-agent-568688f988-lfg6b   0/1     CrashLoopBackOff   627        3d3hcattle-node-agent-8h4jg                 0/1     CrashLoopBackOff   888        3d3hcattle-node-agent-9qthg                 0/1     CrashLoopBackOff   889        3d3hcattle-node-agent-jp2rx                 0/1     CrashLoopBackOff   889        3d3hrancher-7d769c47d6-9cdss                1/1     Running            2          3d22hrancher-7d769c47d6-rn6kn                1/1     Running            1          3d22hrancher-7d769c47d6-wrtbd                1/1     Running            1          3d22h</code></pre><p>并且有时在观察过程中node-agent或者cluster-agent处在Running状态，过一会儿又切换为CrashLoopBackOff。这时需要进一步进行排查。操作如下：</p><pre><code>$ kubectl logs -f cattle-cluster-agent-568688f988-lfg6b -n cattle-systemINFO: Environment: CATTLE_ADDRESS=10.42.2.4 CATTLE_CA_CHECKSUM= CATTLE_CLUSTER=true CATTLE_INTERNAL_ADDRESS= CATTLE_K8S_MANAGED=true CATTLE_NODE_NAME=cattle-cluster-agent-568688f988-lfg6b CATTLE_SERVER=https://rancher.mine.comINFO: Using resolv.conf: nameserver 10.43.0.10 search cattle-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5ERROR: https://rancher.mine.com/ping is not accessible (Failed to connect to rancher.mine.com port 443: Connection timed out)</code></pre><p>cluster-agent并不能进行通讯，看到443问题，猜想是证书出了问题，随便查看一个node-agent的状态，如下：</p><pre><code>$ kubectl logs -f cattle-node-agent-9qthg -n cattle-systemINFO: Environment: CATTLE_ADDRESS=192.168.188.239 CATTLE_AGENT_CONNECT=true CATTLE_CA_CHECKSUM= CATTLE_CLUSTER=false CATTLE_INTERNAL_ADDRESS= CATTLE_K8S_MANAGED=true CATTLE_NODE_NAME=192.168.188.239 CATTLE_SERVER=https://rancher.mine.comINFO: Using resolv.conf: nameserver 114.114.114.114 nameserver 8.8.8.8 nameserver 1.1.1.1INFO: https://rancher.mine.com/ping is accessibleINFO: rancher.mine.com resolves to 192.168.188.240time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Rancher agent version v2.3.5 is starting&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Listening on /tmp/log.sock&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Option customConfig=map[address:192.168.188.239 internalAddress: label:map[] roles:[] taints:[]]&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Option etcd=false&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Option controlPlane=false&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Option worker=false&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Option requestedHostname=192.168.188.239&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Certificate details from https://rancher.mine.com&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Certificate #0 (https://rancher.mine.com)&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Subject: CN=*.rancher.mine.com,O=Default Company Ltd,L=Default City,C=CN&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;Issuer: CN=rancher.mine.com,O=Default Company Ltd,L=Default City,C=CN&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;IsCA: false&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;DNS Names: [*.rancher.mine.com rancher.mine.com]&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;IPAddresses: &lt;none&gt;&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;NotBefore: 2020-02-24 10:32:05 +0000 UTC&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;NotAfter: 2020-03-25 10:32:05 +0000 UTC&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;SignatureAlgorithm: SHA256-RSA&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=info msg=&quot;PublicKeyAlgorithm: RSA&quot;time=&quot;2020-02-28T06:04:22Z&quot; level=fatal msg=&quot;Certificate chain is not complete, please check if all needed intermediate certificates are included in the server certificate (in the correct order) and if the cacerts setting in Rancher either contains the correct CA certificate (in the case of using self signed certificates) or is empty (in the case of using a certificate signed by a recognized CA). Certificate information is displayed above. error: Get https://rancher.mine.com: x509: certificate signed by unknown authority&quot;</code></pre><p>出现了x509问题，但是查询证书信息已经按照上面的教程导入了。如下：</p><pre><code>$ kubectl get secrets -n cattle-systemNAME                            TYPE                                  DATA   AGEcattle-credentials-67b70c2      Opaque                                2      3d3hcattle-token-r5q62              kubernetes.io/service-account-token   3      3d3hdefault-token-hd9nl             kubernetes.io/service-account-token   3      3d23hrancher-token-2kqn9             kubernetes.io/service-account-token   3      3d23hsh.helm.release.v1.rancher.v1   helm.sh/release.v1                    1      3d23htls-rancher                     kubernetes.io/tls                     2      3d22htls-rancher-ingress             kubernetes.io/tls                     2      3d19h$ kubectl describe secret tls-rancher-ingress -n cattle-systemName:         tls-rancher-ingressNamespace:    cattle-systemLabels:       &lt;none&gt;Annotations:  &lt;none&gt;Type:  kubernetes.io/tlsData====tls.key:  1675 bytestls.crt:  1334 bytes</code></pre><p>临时不影响使用，rancher可以照常进行访问。但是在两天后，整个系统挂掉了。rancher页面在访问的时候报错了，所有rancher相关的docker镜像失败了。</p><p>尝试使用网上的方案解决，在命令行执行以下命令：</p><pre><code>$ kubectl  -n cattle-system \patch deployments cattle-cluster-agent --patch &#39;{    &quot;spec&quot;: {        &quot;template&quot;: {            &quot;spec&quot;: {                &quot;hostAliases&quot;: [                    {                        &quot;hostnames&quot;:                        [                            &quot;rancher.mine.com&quot;                        ],                            &quot;ip&quot;: &quot;192.168.238.240&quot;                    }                ]            }        }    }}&#39;$ kubectl -n cattle-system \patch  daemonsets cattle-node-agent --patch &#39;{    &quot;spec&quot;: {        &quot;template&quot;: {            &quot;spec&quot;: {                &quot;hostAliases&quot;: [                    {                        &quot;hostnames&quot;:                        [                            &quot;rancher.mine.com&quot;                        ],                            &quot;ip&quot;: &quot;192.168.238.240&quot;                    }                ]            }        }    }}&#39;</code></pre><p>执行过后，agent相关的pod依旧在重启。</p><p>目前怀疑是证书的问题，现在通过重新签发证书，再进行操作。</p><p><strong>解决方案：</strong></p><ol><li><p>重新签发证书  </p><p> $ docker pull superseb/omgwtfssl</p><p> // 开始签发证书，将证书存储在/home/centos/nginx/new_cert路径中<br> $ docker run -v /home/centos/nginx/new_cert:/certs -e CA_SUBJECT=”mine-root-CA” -e CA_EXPIRE=”1825” -e SSL_EXPIRE=”365” -e SSL_SUBJECT=”rancher.mine.com” -e SSL_DNS=”rancher.mine.com” -e SILENT=”true” superseb/omgwtfssl</p><p> // 查看生成的证书信息<br> $ ls<br> ca-key.pem    ca.pem     ca.srl    cert.pem    key.csr    key.pem     openssl.cnf    secret.yaml</p><p> // 证书变更<br> $ cp ca.pem cacert.pem</p></li></ol><p>证书生成后，需要使用的是cacert.pem、cert.pem和key.pem这三个证书。</p><ol start="2"><li><p>清理rancher安装，使用新的清理脚本，参考自：<a href="https://github.com/theAkito/rancher-helpers/blob/master/scripts/cleanup_rancher.sh" target="_blank" rel="noopener">https://github.com/theAkito/rancher-helpers/blob/master/scripts/cleanup_rancher.sh</a></p><p> $ vim clean-all.sh<br> // 在该文件中输入下面的信息</p><pre><code> #!/bin/bash ######################################################################### # Copyright (C) 2019-2020 Akito &lt;the@akito.ooo&gt;                         # #                                                                       # # This program is free software: you can redistribute it and/or modify  # # it under the terms of the GNU General Public License as published by  # # the Free Software Foundation, either version 3 of the License, or     # # (at your option) any later version.                                   # #                                                                       # # This program is distributed in the hope that it will be useful,       # # but WITHOUT ANY WARRANTY; without even the implied warranty of        # # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the          # # GNU General Public License for more details.                          # #                                                                       # # You should have received a copy of the GNU General Public License     # # along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.  # ######################################################################### # Based on # https://github.com/rancher/rancher/issues/19882#issuecomment-501056386 ## Cleans up Rancher with Kubernetes *entirely*. ## After running this script you are able to ## set up Rancher entirely from scratch ## without any complaints about remains ## from the previous installation. ## Run as root! ## Only works reliably with GNU Bash. ## Expects `systemd` on the host. #################################   Boilerplate of the Boilerplate   #################################################### # Coloured Echoes                                                                                                       # function red_echo      { echo -e &quot;\033[31m$@\033[0m&quot;;   }                                                               # function green_echo    { echo -e &quot;\033[32m$@\033[0m&quot;;   }                                                               # function yellow_echo   { echo -e &quot;\033[33m$@\033[0m&quot;;   }                                                               # function white_echo    { echo -e &quot;\033[1;37m$@\033[0m&quot;; }                                                               # # Coloured Printfs                                                                                                      # function red_printf    { printf &quot;\033[31m$@\033[0m&quot;;    }                                                               # function green_printf  { printf &quot;\033[32m$@\033[0m&quot;;    }                                                               # function yellow_printf { printf &quot;\033[33m$@\033[0m&quot;;    }                                                               # function white_printf  { printf &quot;\033[1;37m$@\033[0m&quot;;  }                                                               # # Debugging Outputs                                                                                                     # function white_brackets { local args=&quot;$@&quot;; white_printf &quot;[&quot;; printf &quot;${args}&quot;; white_printf &quot;]&quot;; }                      # function echoInfo   { local args=&quot;$@&quot;; white_brackets $(green_printf &quot;INFO&quot;) &amp;&amp; echo &quot; ${args}&quot;; }                      # function echoWarn   { local args=&quot;$@&quot;;  echo &quot;$(white_brackets &quot;$(yellow_printf &quot;WARN&quot;)&quot; &amp;&amp; echo &quot; ${args}&quot;;)&quot; 1&gt;&amp;2; }  # function echoError  { local args=&quot;$@&quot;; echo &quot;$(white_brackets &quot;$(red_printf    &quot;ERROR&quot;)&quot; &amp;&amp; echo &quot; ${args}&quot;;)&quot; 1&gt;&amp;2; }  # # Silences commands&#39; STDOUT as well as STDERR.                                                                          # function silence { local args=&quot;$@&quot;; ${args} &amp;&gt;/dev/null; }                                                              # # Check your privilege.                                                                                                 # function checkPriv { if [[ &quot;$EUID&quot; != 0 ]]; then echoError &quot;Please run me as root.&quot;; exit 1; fi;  }                     # # Returns 0 if script is sourced, returns 1 if script is run in a subshell.                                             # function checkSrc { (return 0 2&gt;/dev/null); if [[ &quot;$?&quot; == 0 ]]; then return 0; else return 1; fi; }                     # # Prints directory the script is run from. Useful for local imports of BASH modules.                                    # # This only works if this function is defined in the actual script. So copy pasting is needed.                          # function whereAmI { printf &quot;$( cd &quot;$( dirname &quot;${BASH_SOURCE[0]}&quot; )&quot; &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )&quot;;   }                     # # Alternatively, this alias works in the sourcing script, but you need to enable alias expansion.                       # alias whereIsMe=&#39;printf &quot;$( cd &quot;$( dirname &quot;${BASH_SOURCE[0]}&quot; )&quot; &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )&quot;&#39;                            # ######################################################################################################################### function containerd_restart { silence &quot;systemctl restart containerd&quot;; } function rmMetaDB { silence &quot;rm -f /var/lib/containerd/io.containerd.metadata.v1.bolt/meta.db&quot;; } function docker_start { silence &quot;systemctl start docker&quot;; } function docker_stop { silence &quot;systemctl stop docker&quot;; } function finish_line { white_printf &quot;OK\n&quot;; } function checkSys { ## Makes sure that script is not accidentally run on wrong target system. ## Exits if Docker is not installed. silence &quot;docker version&quot; if [[ $? == 0 ]]; then     echoInfo &quot;Docker exists.&quot;     return 0 else     echoError &quot;Docker not installed/running on system. Exiting.&quot;     exit 1 fi } function docker_restart { docker_stop echoInfo &quot;Restarting Docker...&quot; sleep 10; docker_start if [[ $? == 0 ]]; then     echoInfo &quot;Docker restarted!&quot;     return 0 else     echoError &quot;Docker restart failed!&quot;     white_printf &quot;Manually stop docker then wait 20 seconds and start it again.\n&quot;     exit 1 fi } function rmContainers { ## Removes ALL containers. silence &quot;docker rm -f $(docker ps -aq)&quot; &amp;&amp; \ echoInfo &quot;Successfully removed all Docker containers.&quot; || \ echoInfo &quot;No Docker containers exist! Skipping.&quot; } function rmVolumes { ## Removes ALL volumes. silence &quot;docker volume rm $(docker volume ls -q)&quot; &amp;&amp; \ echoInfo &quot;Successfully removed all Docker volumes.&quot; || \ echoInfo &quot;No Docker volumes exist! Skipping.&quot; } function rmLocs { ## Removes all Rancher and Kubernetes related folders. declare -a FOLDERS FOLDERS=( &quot;/etc/ceph&quot; \             &quot;/etc/cni&quot; \             &quot;/etc/kubernetes&quot; \             &quot;/opt/cni&quot; \             &quot;/opt/rke&quot; \             &quot;/run/secrets/kubernetes.io&quot; \             &quot;/run/calico&quot; \             &quot;/run/flannel&quot; \             &quot;/var/lib/calico&quot; \             &quot;/var/lib/etcd&quot; \             &quot;/var/lib/cni&quot; \             &quot;/var/lib/kubelet&quot; \             &quot;/var/lib/rancher/rke/log&quot; \             &quot;/var/log/containers&quot; \             &quot;/var/log/pods&quot; \             &quot;/var/run/calico&quot; \         ) for loc in &quot;${FOLDERS[@]}&quot;; do     if [ -d ${loc} ]; then     timeout 15s rm -fr ${loc} || \         { \         echoError   &quot;Timed out while trying to remove ${loc}.&quot;         yellow_echo &quot;Run \&quot;rm -fr ${loc}\&quot; manually.&quot;         exit 2         }     echoInfo &quot;${loc} successfully deleted.&quot;     else     echoInfo &quot;${loc} not found! Skipping.&quot;     fi done ## Removes Rancher installation from default installation directory. local rancher_loc=&quot;/opt/rancher&quot; if [ -d ${rancher_loc} ]; then     silence &quot;rm -fr /opt/rancher&quot; &amp;&amp; \     echoInfo &quot;Rancher successfully removed from ${rancher_loc}.&quot; || \     echoError &quot;Rancher could not be removed from ${rancher_loc}!&quot; else     echoInfo &quot;Rancher not found in ${rancher_loc}! Skipping.&quot; fi } function cleanFirewall { ## Removes Firewall entries related to Rancher or Kubernetes. IPTABLES=&quot;/sbin/iptables&quot; cat /proc/net/ip_tables_names | while read table; do     silence &quot;$IPTABLES -t $table -L -n&quot; | while read c chain rest; do         if test &quot;X$c&quot; = &quot;XChain&quot; ; then         silence &quot;$IPTABLES -t $table -F $chain&quot;         fi     done     silence &quot;$IPTABLES -t $table -X&quot; done echoInfo &quot;Firewall rules cleared.&quot; } function rmDevs { ## Unmounts all Rancher and Kubernetes related virtual devices and volumes. fail_mount=false fail_pvc=false local -a mount_list=( $(mount | grep tmpfs | grep &#39;/var/lib/kubelet&#39; | awk &#39;{ print $3 }&#39;) ) for mount in &quot;${mount_list[@]}&quot; /var/lib/kubelet /var/lib/rancher; do     silence &quot;umount -f ${mount}&quot;     if [[ $? ]]; then     echoInfo  &quot;${mount} successfully unmounted.&quot;     else     echoError &quot;${mount} could not be unmounted.&quot;     fi done ## Unmounts all Persistent Volume Claims, forcefully. local -a pvc_list=( $(mount | grep &#39;/var/lib/kubelet/pods&#39; | awk &#39;{ print $3 }&#39;) ) for pvc in &quot;${pvc_list[@]}&quot;; do     silence &quot;umount -f ${pvc}&quot;     if [[ $? ]]; then     echoInfo  &quot;$(printf ${pvc} | cut -c 1-45)... successfully unmounted.&quot;     else     echoError &quot;${pvc} could not be unmounted.&quot;     fi done } function fazit { ## Checks for fail flags set during other processes and ## outputs a summary of possible errors. if   [[  $fail_mount == false ]] &amp;&amp; [[  $fail_pvc == false ]]; then     : elif [[  $fail_mount == true ]] || [[  $fail_pvc == true ]]; then     echoError &quot;Failed to unmount some volumes.&quot; fi } ############################################ ############################################ # Checks if user running the script is root. checkPriv # Makes sure that script is not accidentally run on wrong target system. # Exits if Docker is not installed. checkSys # Removes ALL containers. rmContainers # Removes ALL volumes. rmVolumes # Unmounts all Rancher and Kubernetes related virtual devices and volumes. rmDevs # Removes all Rancher and Kubernetes related folders. # Removes Rancher installation from default installation directory. rmLocs # Removes metadata database. rmMetaDB # Removes Firewall entries related to Rancher or Kubernetes. cleanFirewall # Restarts services, to apply previous removals. containerd_restart # Slowed down Docker restart. Needs a pause, because else it complains about &quot;too quick&quot; restarts. docker_restart # Checks for fail flags set during other processes and outputs a summary of possible errors. fazit # Process finished. finish_line</code></pre><p> // :wq对文件进行保存</p><p> $ sudo chmod +x clean-all.sh</p><p> // 执行清理操作，一定使用sudo提权<br> $ sudo ./clean-all.sh  </p></li><li><p>重新安装，制定证书信息 </p></li></ol><p>清理完成后，重新使用rke安装k8s，配置文件cluster.yml不变。可以拷贝到其它文件夹进行操作</p><pre><code>$ mkdir new_rancher$ cd new_rancher/// 拷贝到该文件夹$ cp /home/centos/cluster/cluster.yml ./// 执行配置文件$ rke up --config ./cluster.yml</code></pre><p>当安装完成后，操作参考之前的步骤即可。验证安装后，等待10分钟左右，开始安装rancher集群。</p><pre><code>// 转到生成证书的文件夹$ cd /home/centos/nginx/new_cert// 创建namespace$ kubectl create ns cattle-system// 创建证书信息$ kubectl -n cattle-system create secret tls tls-rancher-ingress --cert=./cert.pem --key=./key.pem// 创建生成证书的根证书信息$ kubectl -n cattle-system create secret generic tls-ca --from-file=cacerts.pem// 执行新的安装方式，多了一个--set privateCA=true$ helm install rancher rancher-stable/rancher\      --namespace cattle-system \      --set hostname=rancher.mine.com \      --set ingress.tls.source=secret \      --set privateCA=true</code></pre><p>这样就可以了。</p><ol start="4"><li>nginx证书更换 </li></ol><p>需要将已有的证书拷贝到之前nginx证书所在的目录信息。</p><pre><code>$ sudo cp cert.pem /etc/nginx/cert/$ sudo cp key.pem /etc/nginx/cert/// 修改配置文件信息$ sudo vim /etc/nginx/nginx.conf// 找到ssl_certificate开头的内容进行修改ssl_certificate cert/cert.pem;ssl_certificate_key cert/key.pem;// :wq保存退出// 检测配置文件修改$ sudo nginx -t// 重启nginx$ sudo systemctl restart nginx </code></pre><ol start="5"><li><p>重新访问</p><p> // 使用curl进行访问<br> $ curl -L -k rancher.mine.com</p><p> // 输出json信息，已经可以了</p></li><li><p>重新设置环境变量</p></li></ol><p>由于配置文件的变更，需要更改kube_config_cluster.yml的新路径。由于cluster.yml有变更，生成了新的kube_config_cluster.yml文件。</p><pre><code>$ vim ~/.bashrc// 将原来// export KUBECONFIG=/home/centos/cluster/kube_config_cluster.conf// 更改为export KUBECONFIG=/home/centos/cluster/new_rancher/kube_config_cluster.conf// :wq保存退出</code></pre><p>后续使用kubectl进行管理即可。</p><p>二、<strong>关于cattle-cluster-agent处于CrashLoopBackOff的问题</strong></p><pre><code>$ kubectl get pods -n cattle-systemNAME                                    READY   STATUS             RESTARTS   AGEcattle-cluster-agent-85f4fcb5db-pwlgh   0/1     CrashLoopBackOff   23         147mcattle-node-agent-2mt2z                 1/1     Running            0          147mcattle-node-agent-c29zr                 1/1     Running            0          147mcattle-node-agent-dzc7z                 1/1     Running            0          147mrancher-7745c8fdbb-2x576                1/1     Running            2          164mrancher-7745c8fdbb-kpdz2                1/1     Running            2          164mrancher-7745c8fdbb-w2vxz                1/1     Running            0          164m$ kubedescribe pod cattle-cluster-agent-85f4fcb5db-pwlghwlgh -n cattle-systemName:         cattle-cluster-agent-85f4fcb5db-pwlghNamespace:    cattle-systemPriority:     0Node:         192.168.123.232/192.168.123.232Start Time:   Tue, 03 Mar 2020 14:58:49 +0800Labels:       app=cattle-cluster-agent            pod-template-hash=85f4fcb5dbAnnotations:  cni.projectcalico.org/podIP: 10.42.2.5/32Status:       RunningIP:           10.42.2.5IPs:IP:           10.42.2.5Controlled By:  ReplicaSet/cattle-cluster-agent-85f4fcb5dbContainers:cluster-register:    Container ID:   docker://b6caca843298a2544fac374e28bbb8f9e1d901c7cad4b7b4915b40608ef1260b    Image:          rancher/rancher-agent:v2.3.5    Image ID:       docker-pullable://rancher/rancher-agent@sha256:e6aa36cca0d3ce9fea180add5b620baabd823fb34f9d100b7a8d3eb734392c37    Port:           &lt;none&gt;    Host Port:      &lt;none&gt;    State:          Waiting    Reason:       CrashLoopBackOff    Last State:     Terminated    Reason:       Error    Exit Code:    1    Started:      Tue, 03 Mar 2020 17:29:35 +0800    Finished:     Tue, 03 Mar 2020 17:31:45 +0800    Ready:          False    Restart Count:  24    Environment:    CATTLE_SERVER:       https://rancher.icpcloud.com    CATTLE_CA_CHECKSUM:  903483819f88d325eb730ee3017172cac4370a13b7bb1579b7a6373da2defc1e    CATTLE_CLUSTER:      true    CATTLE_K8S_MANAGED:  true    Mounts:    /cattle-credentials from cattle-credentials (ro)    /var/run/secrets/kubernetes.io/serviceaccount from cattle-token-zrzb4 (ro)Conditions:Type              StatusInitialized       TrueReady             FalseContainersReady   FalsePodScheduled      TrueVolumes:cattle-credentials:    Type:        Secret (a volume populated by a Secret)    SecretName:  cattle-credentials-687d480    Optional:    falsecattle-token-zrzb4:    Type:        Secret (a volume populated by a Secret)    SecretName:  cattle-token-zrzb4    Optional:    falseQoS Class:       BestEffortNode-Selectors:  &lt;none&gt;Tolerations:Events:Type     Reason   Age                    From                  Message----     ------   ----                   ----                  -------Warning  BackOff  3m4s (x444 over 147m)  kubelet, 192.168.123.232  Back-off restarting failed container$ ssh -p 15555 centos@192.168.123.232Last login: Tue Mar  3 16:04:54 2020 from k8s-rke-node-start$ curl -k https://rancher.icpcloud.com/pingpong$ kubectl logs -f cattle-cluster-agent-85f4fcb5db-pwlgh -n cattle-systemINFO: Environment: CATTLE_ADDRESS=10.42.2.5 CATTLE_CA_CHECKSUM=903483819f88d325eb730ee3017172cac4370a13b7bb1579b7a6373da2defc1e CATTLE_CLUSTER=true CATTLE_INTERNAL_ADDRESS= CATTLE_K8S_MANAGED=true CATTLE_NODE_NAME=cattle-cluster-agent-85f4fcb5db-pwlgh CATTLE_SERVER=https://rancher.icpcloud.comINFO: Using resolv.conf: nameserver 10.43.0.10 search cattle-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5ERROR: https://rancher.icpcloud.com/ping is not accessible (Failed to connect to rancher.icpcloud.com port 443: Connection timed out)</code></pre><p><strong>解决方案：</strong></p><p>利用kubectl edit命令进行修改：</p><pre><code>$ kubectl edit deploy/cattle-cluster-agent -n cattle-system// 在出现的文本操作环境中，找到dnsPolicy: ClusterFirst选项，作如下修改dnsPolicy: Default// :wq保存退出// 查看状态$ kubectl get pods -n cattle-systemNAME                                    READY   STATUS        RESTARTS   AGEcattle-cluster-agent-6c5485c4fc-vd6bm   1/1     Running       0          8scattle-cluster-agent-85f4fcb5db-pwlgh   1/1     Terminating   26         166m</code></pre><p>参考自：<a href="https://github.com/rancher/rancher/issues/16454#issuecomment-544621587" target="_blank" rel="noopener">https://github.com/rancher/rancher/issues/16454#issuecomment-544621587</a></p><p>三、关于kubernetes通过rke工具进行扩容的操作</p><p>这里提前准备两台服务器，一台是192.168.123.233，另一台是192.168.123.234。登录192.168.123.240服务器，然后执行下命令：</p><pre><code>// 转到部署文件所在的位置$ cd ~/cluster/new_rancher// 对配置文件进行备份$ cp cluster.yml cluster.yml.bak// 执行下更新功能$ rke --debug up --config cluster.yml --update-only</code></pre><p>执行成功后，最终提示：Finished building Kubernetes cluster successfully，说明已经完成。</p><p>随后修改cluster.yml文件，添加以下内容：</p><pre><code>- address: 192.168.123.233  port: &quot;15555&quot;  internal_address: &quot;&quot;  role:  - worker  hostname_override: &quot;&quot;  user: centos  docker_socket: /var/run/docker.sock  ssh_key: &quot;&quot;  ssh_key_path: ~/.ssh/id_rsa  ssh_cert: &quot;&quot;  ssh_cert_path: &quot;&quot;  labels: {}  taints: []- address: 192.168.123.234  port: &quot;15555&quot;  internal_address: &quot;&quot;  role:  - worker  hostname_override: &quot;&quot;  user: centos  docker_socket: /var/run/docker.sock  ssh_key: &quot;&quot;  ssh_key_path: ~/.ssh/id_rsa  ssh_cert: &quot;&quot;  ssh_cert_path: &quot;&quot;  labels: {}  taints: []</code></pre><p>追加在192.168.123.241的配置信息之后，修改完成后保存退出，重新执行下面的命令：</p><pre><code>$ rke --debug up --config cluster.yml --update-only</code></pre><p>最终提示：Finished building Kubernetes cluster successfully，说明已经完成。</p><p>在一开始，不执行更新命令，直接修改cluster.yml，出现各种各样的问题，例如：</p><pre><code>1. Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?2. docker: Error response from daemon: Conflict. The container name &quot;rke-log-linker&quot; is already in use3. 节点连接不上</code></pre><p>所以一定使用这个操作顺序进行增加节点！</p><h2 id="番外，清理脚本内容"><a href="#番外，清理脚本内容" class="headerlink" title="番外，清理脚本内容"></a>番外，清理脚本内容</h2><pre><code>kubeadm resetifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/</code></pre><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul><li>官网安装文档：<a href="https://rancher.com/docs/rancher/v2.x/en/installation/" target="_blank" rel="noopener">https://rancher.com/docs/rancher/v2.x/en/installation/</a></li><li>新版本发布：<a href="https://www.cnblogs.com/rancherlabs/p/11956279.html" target="_blank" rel="noopener">https://www.cnblogs.com/rancherlabs/p/11956279.html</a></li><li>rancher中文社区：<a href="https://forums.cnrancher.com/" target="_blank" rel="noopener">https://forums.cnrancher.com/</a></li><li>部署文章：<a href="https://www.dazhuanlan.com/2019/10/25/5db2df23374e5/" target="_blank" rel="noopener">https://www.dazhuanlan.com/2019/10/25/5db2df23374e5/</a></li><li>部署文章：<a href="https://blog.51cto.com/bilibili/2440304" target="_blank" rel="noopener">https://blog.51cto.com/bilibili/2440304</a></li><li>部署文章：<a href="http://www.eryajf.net/2723.html" target="_blank" rel="noopener">http://www.eryajf.net/2723.html</a></li><li>部署文章：<a href="https://fs.tn/post/PmaL-uIiQ/" target="_blank" rel="noopener">https://fs.tn/post/PmaL-uIiQ/</a></li><li>k8s部署文章：<a href="https://www.pocketdigi.com/2019-09/install-k8s-in-china.html" target="_blank" rel="noopener">https://www.pocketdigi.com/2019-09/install-k8s-in-china.html</a></li><li>常见问题：<a href="http://blog.ponycool.com/archives/94.html" target="_blank" rel="noopener">http://blog.ponycool.com/archives/94.html</a></li><li>常见问题：<a href="https://blog.csdn.net/isea533/article/details/98172030" target="_blank" rel="noopener">https://blog.csdn.net/isea533/article/details/98172030</a></li><li>视频教程：<a href="https://space.bilibili.com/430496045?from=search&amp;seid=8060734016441754889" target="_blank" rel="noopener">https://space.bilibili.com/430496045?from=search&amp;seid=8060734016441754889</a></li><li>私有证书签发：<a href="https://linkscue.com/posts/2019-06-26-ca-and-self-signed-certificates/" target="_blank" rel="noopener">https://linkscue.com/posts/2019-06-26-ca-and-self-signed-certificates/</a></li><li>nginx证书安装：<a href="https://www.jianshu.com/p/02c25d0dd451" target="_blank" rel="noopener">https://www.jianshu.com/p/02c25d0dd451</a></li><li>付费文章，解决证书问题：<a href="https://blog.csdn.net/wxb880114/article/details/102787872" target="_blank" rel="noopener">https://blog.csdn.net/wxb880114/article/details/102787872</a></li><li>docker方式签发证书：<a href="https://medium.com/@superseb/zero-to-rancher-2-x-single-install-using-created-self-signed-certificates-in-5-minutes-5f9fe11fceb0" target="_blank" rel="noopener">https://medium.com/@superseb/zero-to-rancher-2-x-single-install-using-created-self-signed-certificates-in-5-minutes-5f9fe11fceb0</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s方案规划</title>
      <link href="/2020/02/12/k8s-fang-an-gui-hua/"/>
      <url>/2020/02/12/k8s-fang-an-gui-hua/</url>
      
        <content type="html"><![CDATA[<h1 id="k8s方案规划"><a href="#k8s方案规划" class="headerlink" title="k8s方案规划"></a>k8s方案规划</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>随着我们微服务开发的深入，从微服务的单一jar包部署转到了单一镜像部署模式，初步实现了CI/CD自动化流水线部署模式，能够极大的满足我们目前的开发方式。但是随着微服务数量的增加，运维的难度也在不断升高，bug的排除，请求的监控需要一条较长的链路。顺着链路去排查问题，带来的时间成本显著上升。目前的部署模式是以单一docker镜像或者docker-compose实现简单的服务编排，目前还无法做到自动化的集群化部署，不能很好的做到单个微服务部署多套通过负载均衡控制访问。在微服务的熔断、限流等层面尚未有很好的解决方式，仅凭硬编码解决是不够的，硬编码后还需要将服务推送的服务器上替代和生效，影响目前的系统运行方式，同样的无法做到不停机升级。在服务编排上，我们目前只能通过API Gateway来实现微服务层面的服务编排，功能有限</p><p>针对我们目前遇到的这些问题，引入容器编排的功能，对目前容器化部署策略进行管理上的提升。容器编排的框架有很多，例如Docker Swam、Kubernetes、Fleet、Mesos，各有特点，也各有应用场景。根据我们本阶段的规划，考虑到运维平台的搭建和运作，结合目前已有的部署方式，选择Kubernetes进行搭建。</p><h3 id="背景和历史"><a href="#背景和历史" class="headerlink" title="背景和历史"></a>背景和历史</h3><p>在Docker 作为高级容器引擎快速发展的同时，Google也开始将自身在容器技术及集群方面的积累贡献出来。在Google内部，容器技术已经应用了很多年，Borg系统运行管理着成千上万的容器应用，在它的支持下，无论是谷歌搜索、Gmail还是谷歌地图，可以轻而易举地从庞大的数据中心中获取技术资源来支撑服务运行。</p><p>Borg是集群的管理器，在它的系统中，运行着众多集群，而每个集群可由成千上万的服务器联接组成，Borg每时每刻都在处理来自众多应用程序所提交的成百上千的Job, 对这些Job进行接收、调度、启动、停止、重启和监控。正如Borg论文中所说，Borg提供了3大好处:</p><p>1）隐藏资源管理和错误处理，用户仅需要关注应用的开发。</p><p>2) 服务高可用、高可靠。</p><p>3) 可将负载运行在由成千上万的机器联合而成的集群中。</p><p>作为Google的竞争技术优势，Borg理所当然的被视为商业秘密隐藏起来，但当Tiwtter的工程师精心打造出属于自己的Borg系统（Mesos）时， Google也审时度势地推出了来源于自身技术理论的新的开源工具。</p><p>2014年6月，谷歌云计算专家埃里克·布鲁尔（Eric Brewer）在旧金山的发布会为这款新的开源工具揭牌，它的名字Kubernetes在希腊语中意思是船长或领航员，这也恰好与它在容器集群管理中的作用吻合，即作为装载了集装箱（Container）的众多货船的指挥者，负担着全局调度和运行监控的职责。</p><p>虽然Google推出Kubernetes的目的之一是推广其周边的计算引擎（Google Compute Engine）和谷歌应用引擎（Google App Engine）。但Kubernetes的出现能让更多的互联网企业可以享受到连接众多计算机成为集群资源池的好处。</p><p>Kubernetes对计算资源进行了更高层次的抽象，通过将容器进行细致的组合，将最终的应用服务交给用户。Kubernetes在模型建立之初就考虑了容器跨机连接的要求，支持多种网络解决方案，同时在Service层次构建集群范围的SDN网络。其目的是将服务发现和负载均衡放置到容器可达的范围，这种透明的方式便利了各个服务间的通信，并为微服务架构的实践提供了平台基础。而在Pod层次上，作为Kubernetes可操作的最小对象，其特征更是对微服务架构的原生支持。</p><p>Kubernetes项目来源于Borg，可以说是集结了Borg设计思想的精华，并且吸收了Borg系统中的经验和教训。</p><p>Kubernetes作为容器集群管理工具，于2015年7月22日迭代到 v 1.0并正式对外公布，这意味着这个开源容器编排系统可以正式在生产环境使用。与此同时，谷歌联合Linux基金会及其他合作伙伴共同成立了CNCF基金会( Cloud Native Computing Foundation)，并将Kuberentes 作为首个编入CNCF管理体系的开源项目，助力容器技术生态的发展进步。Kubernetes项目凝结了Google过去十年间在生产环境的经验和教训，从Borg的多任务Alloc资源块到Kubernetes的多副本Pod，从Borg的Cell集群管理，到Kubernetes设计理念中的联邦集群，在Docker等高级引擎带动容器技术兴起和大众化的同时，为容器集群管理提供独了到见解和新思路。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>可移植: 支持公有云，私有云，混合云，多重云（multi-cloud）</li><li>可扩展: 模块化, 插件化, 可挂载, 可组合</li><li>自动化: 自动部署，自动重启，自动复制，自动伸缩/扩展</li></ul><p>因为容器小而快，一个应用可以被打包进一个容器映像。正是应用与容器镜像间一对一的关系解锁了容器的很多优点：</p><ul><li>在build或者release 的阶段（而非部署阶段）可以创建不变的容器镜像，因为每个应用都用和其他的应用栈相组合，也不依赖于生产环境基础设施。这使得从研发到生产过程中可以采用持续的环境。</li><li>容器比虚机要更加透明，这更便于监控和管理。尤其是因为窗口的进程的生命周期是被基础设施直接管理而不是被容器中的进程管理器隐藏起来管理。</li><li>因为一个容器包含一个应用，这让对容器的管理等同于对应用部署的管理。</li></ul><p>Kubernetes一个核心的特点就是自动化，能够自主的管理容器来保证云平台中的容器按照用户的期望状态运行着（比如用户想让apache一直运行，用户不需要关心怎么去做，Kubernetes会自动去监控，然后去重启，新建，总之，让apache一直提供服务），管理员可以加载一个微型服务，让规划器来找到合适的位置，同时，Kubernetes也系统提升工具以及人性化方面，让用户能够方便的部署自己的应用（就像canary deployments）。</p><p>现在Kubernetes着重于不间断的服务状态（比如web服务器或者缓存服务器）和原生云平台应用（Nosql）,在不久的将来会支持各种生产云平台中的各种服务，例如，分批，工作流，以及传统数据库。</p><h3 id="建设层面–PaaS层"><a href="#建设层面–PaaS层" class="headerlink" title="建设层面–PaaS层"></a>建设层面–PaaS层</h3><ol><li><p>PaaS是面向应用的核心平台。</p></li><li><p>从功能定义和核心价值分为三个层次：</p><p>1）自动化获取资源进行部署；</p><p>2）提供标准化的编程框架和服务来帮助应用开发和运行实现自动化；</p><p>3）无需感知底层资源的应用自动化运维（包括配置、升级、伸缩等等）。</p></li></ol><p>结合我们目前的情况，我们需要兼容不同的云平台体系——公有云、私有云、混合云，需要面向针对微服务的调度优化，需要自动化的运维功能，需要逐步加入服务网格的功能特性，需要应对大规模服务部署方式，需要扩展AIOps的功能特性。通过对PaaS层进行建设，为微服务的编排提供基础，保障运维效率，保障服务运行，能够支持各类部署方式——灰度部署、蓝绿部署、滚动部署。</p><h2 id="为什么使用kubernetes"><a href="#为什么使用kubernetes" class="headerlink" title="为什么使用kubernetes"></a>为什么使用kubernetes</h2><h3 id="痛点"><a href="#痛点" class="headerlink" title="痛点"></a>痛点</h3><ul><li>单一服务单一镜像，未添加到服务集群，无法进行弹性伸缩、A/B测试、流量切换操作</li><li>自动化程度不足，需要人工干预的点太多</li><li>微服务的熔断、限流、降级完成不足，尚需要编码部署之后进行生效</li><li>部署方式限制，灰度部署、蓝绿部署、滚动部署尚未建设，部署难度过大</li><li>微服务过多，管理不直观，操作复杂</li><li>监控体系不足，日志采集、运行参数采集、异常告警尚不到位，仅仅建设了全链路监控体系</li></ul><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p>Kubernetes可以满足很多运行环境中应用的通用的需求，比如：</p><ul><li>进程协同，利用复合应用保证应用和容器一对一的模型。</li><li>存储系统挂载</li><li>分发密钥</li><li>应用健康检测</li><li>应用实例复制</li><li>水平自动扩展</li><li>命名和发现</li><li>负载均衡</li><li>滚动更新</li><li>资源监控</li><li>日志访问</li><li>自检和调试</li><li>识别和认证</li></ul><p>这为PaaS提供了IaaS层的便利，提供了基础设施提供者间的可移植性。</p><p>但是Kubernetes不是一个传统的，包罗一切的PaaS系统。我们保留用户的选择，这一点非常重要。</p><ul><li><p>Kubernetes不限制支持应用的种类。它不限制应用框架，或者支持的运行时语言，也不去区分 “apps” 或者“services”。 Kubernetes致力于支持不同负载应用，包括有状态、无状态、数据处理类型的应用。只要这个应用可以在容器里运行，那么它就可以在Kubernetes上很多地运行。</p></li><li><p>Kubernetes不提供中间键（如message buses），数据处理框架（如Spark），数据库(如Mysql)，或者集群存储系统(如Ceph)。但这些应用都可以运行于Kubernetes。</p></li><li><p>Kubernetes没有一个点击即可用的应用市场。</p></li><li><p>Kubernetes不部署源码不编译应用。持续集成的 (CI)工作流方面，不同的用户有不同的需求和偏好，因此，我们提供分层的 CI工作流，但并不定义它应该怎么做。</p></li><li><p>Kubernetes允许用户选择自己的日志、监控和报警系统。</p></li><li><p>Kubernetes不提供可理解的应用配置语言(e.g., jsonnet)。</p></li><li><p>Kubernetes不提供或者任何综合的机器配置，维护，管理或者自愈系统。</p></li></ul><p>另一方面，大量的Paas系统都可以运行在Kubernetes上，比如Openshift, Deis, 和Gondor。你可以构建自己的Paas平台，集成CI。因为Kubernetes运行在应用而非硬件层面，它提供了普通的Paas平台提供的一些通用功能，比如部署，扩展，负载均衡，日志，监控等。然而，Kubernetes并非一个庞然大物，这些功能老师可选的。另外，Kubernetes不仅仅是一个“编排系统”；它消弥了编排的需要。“编排”的定义是指执行一个预定的工作流：先做A，之后B，然后C。相反地，Kubernetes是由一系列独立的、可组合的驱使当前状态走向预想状态的控制进程组成的。怎么样从A到C并不重要：达到目的就好。当然也是需要中心控制的；方法更像排舞的过程。这让这个系统更加好用更加强大、健壮、 有弹性且可扩展。</p><p>Kubernetes 是云原生（Cloud Native Computing）哲学的体现，通过容器技术和抽象的 IaaS 接口，屏蔽了底层基础设施的细节和差异，可实现多环境部署并可以在多环境之间灵活迁移；这样一方面可以实现跨域、多环境的高可用多活灾备，另一方面帮助用户不必被某个云厂商、底层环境所绑定。</p><h3 id="风险"><a href="#风险" class="headerlink" title="风险"></a>风险</h3><p>使用k8s最大的风险是，学习成本。当前学习成本过高，概念多，不容易理解，短期内掌握不太现实。</p><p>其次问题在于，服务器资源问题，测试环境可以简单进行搭建，目前无法预知大规模部署时，对容量的规划方案。</p><p>最后，kubernetes不是银弹，虽然天然契合微服务，但是目前无法预知部署时的问题。</p><p>另外，目前尚不知道是否需要针对微服务开发的过程进行变更，例如针对Dockerfile的重写、各类请求地址的变更，有不小的变动。</p><p>针对私有化部署，如何准备部署环境，也是一个不小的考验。</p><h3 id="规避风险"><a href="#规避风险" class="headerlink" title="规避风险"></a>规避风险</h3><ul><li>针对学习成本问题，建议拆分学习，优先使用起来，直接进入实战，引入外部资源进行指导培训。</li><li>针对服务器资源的问题，需要进行协调。</li><li>针对部署时问题，优先考虑一键部署和简化部署的方式，在不影响主体功能的情况下进行部署。</li><li>针对微服务的开发过程变更，需要同组内成员统一讨论、协商、培训。</li></ul><h2 id="实现什么"><a href="#实现什么" class="headerlink" title="实现什么"></a>实现什么</h2><h3 id="功能特性"><a href="#功能特性" class="headerlink" title="功能特性"></a>功能特性</h3><ul><li>微服务集群化</li><li>资源调度。自动扩容、应用实例复制、水平自动扩展。</li><li>资源隔离。进程隔离，资源划分（开发、运维、测试），权限控制。</li><li>流量控制。流量转移和服务优雅停机。</li><li>数据监控。日志监控、运行数据监控，异常告警。</li><li>易迁移，易扩展。</li><li>服务自动发现和注册。</li><li>部署更新。滚动部署、蓝绿部署、A/B测试。</li><li>负载均衡。</li><li>对接CI/CD，实现自动化部署。</li></ul><p>其它由k8s实现的功能特性需要继续补充。</p><h2 id="如何做"><a href="#如何做" class="headerlink" title="如何做"></a>如何做</h2><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p>直接部署k8s环境，以主从模式进行部署。划定一个三主六从的环境，三主使用keepalived+HAProxy保证高可用。</p><p>实施前提如下：</p><ul><li>服务器的基础镜像，基础工具包，至少要包含docker以及docker-compose，以及k8s相关组件。</li><li>Docker的基础镜像，分为前端和后端的工具镜像。</li><li>测试微服务程序。</li></ul><h3 id="各类方案"><a href="#各类方案" class="headerlink" title="各类方案"></a>各类方案</h3><p>在k8s的基础上，衍生出各类方案，这里以开源方案为主进行选择，主要包含两类：</p><ul><li><p>k8s“简化”：这里简化的意思指可以针对k8s部署时的简单化处理，一键脚本部署。主要包括以下方案：</p><ul><li><a href="https://kuboard.cn" target="_blank" rel="noopener">Kuboard</a>，一款图形化管理界面。</li><li><a href="https://tilt.dev/" target="_blank" rel="noopener">Tilt</a>，主要是为微服务开发提供无压力体验。使用 Tilt 开发微服务允许开发人员在自己的 IDE 中编辑并保存到文件系统，开发人员可在为团队完美配置的环境中处理微服务。定位于本地简化部署环境。</li><li><a href="https://github.com/morningspace/lab-k8s-playground" target="_blank" rel="noopener">lab-k8s-playground</a>，简化的k8s安装方式。</li><li><a href="https://sealyun.com/" target="_blank" rel="noopener">sealyun</a>，专注于kubernetes安装的简化安装工具。</li></ul></li><li><p>k8s扩展：以k8s为基础，扩展出各类实用功能，以商用大厂的开源方案为主。主要包括以下方案：</p><ul><li><a href="https://www.openshift.com/" target="_blank" rel="noopener">OpenShift</a>，RedHat出品基于k8s的工具信息。偏向开发侧。</li><li><a href="https://www.rancher.com/" target="_blank" rel="noopener">Rancher</a>，偏向运维侧的k8s工具，全开源，服务收费，应用较为广泛。</li><li><a href="https://kubesphere.com.cn/zh-CN/" target="_blank" rel="noopener">KubeSphere</a>，以应用为中心的容器平台。</li></ul></li></ul><ul><li><p>开发平台：结合Spring Cloud进行开发的平台，以开发为主。主要包括以下方案</p><ul><li><a href="https://github.com/Qihoo360/wayne" target="_blank" rel="noopener">wayne</a>，360开源的开发工具。</li><li><a href="https://www.rainbond.com/" target="_blank" rel="noopener">rainbond</a></li><li><a href="http://choerodon.io/zh/" target="_blank" rel="noopener">Choerodon猪齿鱼</a></li><li><a href="https://github.com/Tencent/bk-bcs-saas" target="_blank" rel="noopener">bk-bcs-saas</a></li></ul></li></ul><p>目前以k8s简化和扩展中进行选择，对于开发平台，我们并不选择进行集成，深度集成会破坏我们目前的开发方式以及现有微服务的迁移。</p><h3 id="方案对比"><a href="#方案对比" class="headerlink" title="方案对比"></a>方案对比</h3><p>如果以简化安装的角度来看，在lab-k8s-playground和sealyun选择一个进行安装k8s即可。</p><p>如果以扩展平台的思路来看，基于已安装的k8s进行使用，三者均可以选择，不过KubeSphere开源版本功能缺失较多。</p><h3 id="权衡和选择"><a href="#权衡和选择" class="headerlink" title="权衡和选择"></a>权衡和选择</h3><p>优先安装完成k8s部署，然后选择OpenShift或者Rancher进行使用。</p><p>基于以上选择进行二次开发扩展，添加自己的功能。顺便结合后续运维平台的内容，进行确定扩展的功能。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><p><a href="http://www.sohu.com/a/199018805_671228" target="_blank" rel="noopener">http://www.sohu.com/a/199018805_671228</a></p></li><li><p><a href="https://segmentfault.com/a/1190000014405563" target="_blank" rel="noopener">https://segmentfault.com/a/1190000014405563</a></p></li><li><p><a href="https://www.zhihu.com/question/279119001/answer/465434344" target="_blank" rel="noopener">https://www.zhihu.com/question/279119001/answer/465434344</a></p></li><li><p>厂商产品选择：<a href="https://github.com/leizhnxp/learning/wiki/k8s-PaaS-compare-with-Rancher" target="_blank" rel="noopener">https://github.com/leizhnxp/learning/wiki/k8s-PaaS-compare-with-Rancher</a></p></li><li><p>rancher优劣：<a href="https://blog.csdn.net/hxpjava1/article/details/79325516" target="_blank" rel="noopener">https://blog.csdn.net/hxpjava1/article/details/79325516</a></p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《微服务架构设计模式》读书笔记</title>
      <link href="/2020/02/06/wei-fu-wu-jia-gou-she-ji-mo-shi-du-shu-bi-ji/"/>
      <url>/2020/02/06/wei-fu-wu-jia-gou-she-ji-mo-shi-du-shu-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="《微服务架构设计模式》读书笔记"><a href="#《微服务架构设计模式》读书笔记" class="headerlink" title="《微服务架构设计模式》读书笔记"></a>《微服务架构设计模式》读书笔记</h1><p>从单体架构到微服务，演化路线和注意事项，如何一步步重构单体架构到微服务架构，包含以下内容：</p><ul><li>单体地狱</li><li>服务拆分策略</li><li>通信模式</li><li>数据一致性</li><li>领域驱动设计（DDD）</li><li>事件溯源</li><li>API组合、CQRS</li><li>网关和外部请求</li><li>微服务测试转变</li><li>到达生产环境之前–安全性、外部化可配置、服务可观测</li><li>部署模式</li><li>从单体到微服务的重构</li></ul><p>要结合我们当前的开发模式、团队协作模式进行思考，考虑做的好与不好的地方，顺便指出需要补充的知识体系。</p><h2 id="番外"><a href="#番外" class="headerlink" title="番外"></a>番外</h2><p>我始终不赞同直接使用微服务架构模式来开发一个从0开始的软件系统，受限于个人认知和对后端开发的认知，从零开始，就要先去解决有无的问题，我认为此时是可以直接走单体架构的。如果扯到时下很火的中台概念，系统是基于中台的基础设施进行开发，那无可厚非，有业务积累和沉淀，自然可借鉴可拆分的内容就更多了。</p><p>我们现在从零开始开发，只有跨语言的技术积累，以及业务逻辑的积累，但并不是同一技术栈的实现方式，自然转换时会存在偏差。我个人坚持先实现业务功能，而不是立刻去做微服务的开发，当能够满足需求后再考虑架构演进，考虑周边设施。</p><p>如果一定要去实践运行，我的思考是，前期在人员不足、资源不足的情况下，优先考虑把微服务做大，先去划分较大功能范围且有明确边界的微服务，颗粒度要大，但是边界要清晰。然后由此进行开发，尽量维持三人一个微服务的维护团队，尽量让所有人均能承担前端和后端的开发任务！</p><h2 id="单体地狱"><a href="#单体地狱" class="headerlink" title="单体地狱"></a>单体地狱</h2><p>存在问题-&gt;解决方式：微服务架构-&gt;微服务架构的好坏-&gt;描述模式-&gt;流程和组织如何变更</p><ol><li><p>不是银弹</p></li><li><p>对微服务架构的认知普及，统一团队思想，确定演进路线。但是要注意团队变小后，团队内部如何进行自治，如何变被动为主动。</p></li><li><p>DevOps理念的引入，CI\CD外加自动部署</p></li><li><p>运维保障的规划，可观测、可衡量、可追溯</p></li><li><p>架构模式和模式描述语言</p></li></ol><h2 id="服务拆分策略"><a href="#服务拆分策略" class="headerlink" title="服务拆分策略"></a>服务拆分策略</h2><p>架构-&gt;如何定义</p><ol><li><p>非功能性因素：可维护、可测试、可部署、可扩展</p></li><li><p>分层与六边形架构描述</p></li><li><p>如何定义：定义系统操作、定义服务、定义服务API和协作方式。</p></li></ol><p><strong>补充内容：</strong>这里需要补充面向对象设计和UML描述的相关知识</p><ol start="4"><li>分解模式：业务能力、领域驱动DDD</li></ol><p><strong>补充内容：</strong>这里需要补充领域驱动设计（DDD）的相关知识，补充分布式架构的相关知识</p><h2 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h2><p>进程间通信-&gt;通信方式(REST、gRPC、消息队列)-&gt;异步消息提高可用性</p><ol><li>交互方式：一对一、一对多、同步、异步</li></ol><p>一对一：请求/响应、异步请求/响应、单向通知<br>一对多：发布/订阅、发布/异步响应</p><ol start="2"><li><p>API演化：语义化版本及其兼容策略、消息格式确定（文本、二进制）</p></li><li><p>同步远程过程调用模式：REST模式、gRPC模式、断路器、服务注册和发现</p></li><li><p>异步消息模式：消息队列、发布订阅、消息代理、并发和重复消息处理、事务性消息处理</p></li></ol><p><strong>补充内容：</strong>这里需要补充消息队列的相关知识</p><h2 id="Saga事务模型"><a href="#Saga事务模型" class="headerlink" title="Saga事务模型"></a>Saga事务模型</h2><p>微服务事务管理-&gt;Saga模型设计（协同式、编排式）-&gt;解决隔离问题-&gt;设计案例</p><ol><li><p>事务管理：数据一致性、补偿事务回滚操作</p></li><li><p>协调模式：协同式、编排式。基于消息队列的协调机制、ACD事务模型带来的隔离性问题。</p></li><li><p>解决隔离问题：ACD缺乏I、丢失更新、脏读、模糊或不可重复读</p></li></ol><p>对策：语义锁、交换式更新、悲观视图、重读值、版本文件、业务风险评级</p><h2 id="业务逻辑设计"><a href="#业务逻辑设计" class="headerlink" title="业务逻辑设计"></a>业务逻辑设计</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>持续集成实践大纲</title>
      <link href="/2020/01/31/chi-xu-ji-cheng-shi-jian-da-gang/"/>
      <url>/2020/01/31/chi-xu-ji-cheng-shi-jian-da-gang/</url>
      
        <content type="html"><![CDATA[<h1 id="持续集成实践"><a href="#持续集成实践" class="headerlink" title="持续集成实践"></a>持续集成实践</h1><h2 id="当前进度"><a href="#当前进度" class="headerlink" title="当前进度"></a>当前进度</h2><ul><li>未完成触发规则的内容，jenkins设置和gitlab中webhook设置</li><li>未完成镜像环境变量的传递</li><li>jenkins插件列表需要重新整理</li><li>未完成Dockerfile的自定义编写以及环境变量的使用</li></ul><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><ul><li>尽量进行一步一操作的方式进行编写，截图并进行标注。</li><li>示例项目为<a href="https://github.com/callicoder/spring-security-react-ant-design-polls-app" target="_blank" rel="noopener">spring-security-react-ant-design-polls-app</a>。将该项目上传到gitlab中，修改数据库连接信息，即可进行搭建。</li></ul><h2 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h2><ol><li>采用CentOS 7.6服务器操作系统，要求以<em>Basic Server</em>的方式进行安装。目前服务器地址为：10.0.11.11，ssh端口号为15555 。</li><li>提前安装好docker以及docker-compose，要求安装的docker-ce版本为19.03.2，要求安装的docker-compose版本为1.24.1。</li><li>Gitlab版本为：12.2.5</li><li>Jenkins的war版本为：2.194</li><li>sonarqube版本为：7.9.1-community</li><li>镜像仓库–registry版本为：registry-2</li><li>两个数据库支持，需要MySQL 5.7以及Postgresql 10的支持。</li></ol><h2 id="各部分作用"><a href="#各部分作用" class="headerlink" title="各部分作用"></a>各部分作用</h2><p>我们在持续集成的过程中选择以下工具，以下是其作用列表：</p><table><thead><tr><th>服务名称</th><th>服务版本</th><th>作用</th></tr></thead><tbody><tr><td>gitlab</td><td>12.2.5</td><td>代码托管，git版本控制的远程仓库</td></tr><tr><td>Jenkins</td><td>2.194</td><td>自动构建工具，CI/CD的核心</td></tr><tr><td>sonarqube</td><td>7.9.1-community</td><td>代码质量管理</td></tr><tr><td>registry</td><td>2</td><td>镜像托管仓库，所有微服务打包后的镜像及其版本管理</td></tr></tbody></table><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>下面所有的安装，均基于docker进行安装。</p><ul><li><p>数据库安装</p><pre><code>  // MySQL数据库的启动  $ docker pull mysql:5.7  $ docker run --name mysql:5.7 --restart always -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 \              -v /home/vagrant/mysql5.7/data:/var/lib/mysql -d mysql:5.7  // PostgreSQL启动  $ docker pull postgres:10  $ docker run --restart=always --name postgres -e POSTGRES_USER=sonar \          -e POSTGRES_PASSWORD=sonar -d postgres:10</code></pre></li><li><p>Gitlab安装</p><pre><code>  // 1. 拉取gitlab的镜像  $ docker pull gitlab/gitlab-ce:latest  // 2. 启动gitlab  $ docker run -d \      --restart=always \      --hostname ${HOST_NAME} \      -p 8443:443 -p 8181:80 -p 15555:22 \      --name gitlab \      -v ${GITLAB_DIR}/config:/etc/gitlab \      -v ${GITLAB_DIR}/logs:/var/log/gitlab \      -v ${GITLAB_DIR}/data:/var/opt/gitlab \      gitlab/gitlab-ce:latest</code></pre></li><li><p>jenkins安装</p><pre><code>  // 预先安装好maven，并指定maven路径为/home/centos/maven/apache-maven-3.6.2  // 拉取docker镜像  $ docker pull jenkinsci/blueocean  // 运行docker镜像  $ docker run --restart=always --name jenkins --user=root -d -p 0.0.0.0:8282:8080 \              -p 50000:50000 \               -v /etc/localtime:/etc/localtime:ro \              -v /root/data/maven:/home/centos/maven/apache-maven-3.6.2  \              -v jenkins-data:/var/jenkins_home \              -v /var/run/docker.sock:/var/run/docker.sock    \              jenkinsci/blueocean</code></pre></li><li><p>sonarqube安装</p><pre><code>  // 拉取docker镜像  $ docker pull sonarqube:7.9.1-community  // 运行sonarqube镜像  $ docker run --name sonarqube --link postgres -e SONARQUBE_JDBC_URL=jdbc:postgresql://postgres:5432/sonar -p 9000:9000 -d sonarqube:7.9.1-community</code></pre></li><li><p>docker镜像仓库–registry安装</p><pre><code>  // 拉取docker镜像  $ docker pull registry:2  // 运行registry镜像  $ docker run --name registry --link postgres -e SONARQUBE_JDBC_URL=jdbc:postgresql://postgres:5432/sonar -p 9000:9000 -d registry:2</code></pre></li><li><p>开放对外的端口</p></li></ul><p>通过操作firewall-cmd来开放对应的端口，操作请查看<a href="/2020/01/31/linux-ri-chang-yun-wei-cao-zuo/" title="Linux日常运维操作">Linux日常运维操作</a>。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><ul><li><strong>docker配置</strong></li></ul><p>我们的目标是，将所有的服务通过docker镜像的方式提供对外服务。因此需要提供对docker镜像进行管理的仓库。这里启动镜像仓库后，需要对本机的docker进行设置，指定其需要的连接的镜像仓库。其次要允许该服务器的docker开启远程访问，以便于拉取docker镜像。</p><ol><li>docker开启远程访问</li></ol><p>通过ssh登录服务器，进行以下操作：</p><pre><code>$ sudo vim /usr/lib/systemd/system/docker.service// 修改ExecStart所在行，添加 -H tcp://0.0.0.0:2375 内容ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H fd:// --containerd=/run/containerd/containerd.sock// :wq 将文档进行保存// 重新加载docker配置// 1，加载docker守护线程$ sudo systemctl daemon-reload // 2，重启docker$ sudo systemctl restart docker</code></pre><p>这样docker服务就可以进行远程访问了。可以<a href="https://blog.csdn.net/u012946310/article/details/82315302" target="_blank" rel="noopener">参考该文章进行更改</a>。</p><p>由于我们在之前的镜像启动时，均添加了<em>–restart=always</em>配置项，所以在重启docker服务的时候，所有正在运行的docker镜像均可以自动拉起。</p><ol start="2"><li>docker配置远程仓库</li></ol><p>由于这里没有配置ssh秘钥，存在一定的风险，使用时请屏蔽该服务的外部访问请求。如果不配置ssh密钥登录，将会出现以下错误：</p><pre><code>Error response from daemon: Get https://10.0.11.11:5000/v2/: http: server gave HTTP response to HTTPS client</code></pre><p>配置如下：</p><pre><code>$ sudo vim /etc/docker/daemon.json    // 在文档中输入以下信息    {         &quot;insecure-registries&quot;: [&quot;10.0.11.11:5000&quot;]    }// :wq进行保存// 重启docker服务$ sudo systemctl restart docker</code></pre><p>带ssh密钥版本<a href="https://juejin.im/post/5bbbf72c5188255c554710ca" target="_blank" rel="noopener">可参考该文章配置</a>。如果是外网访问的话，建议使用域名并配合HTTPS证书进行配置。</p><p>这样针对docker服务的配置就到此为止了。</p><ul><li><strong>gitlab基础配置</strong></li></ul><p>当gitlab启动后，访问<a href="http://10.0.11.11:8181" target="_blank" rel="noopener">http://10.0.11.11:8181</a>即可进入。</p><ol start="0"><li><p>设置管理员密码</p><p> <img src="./images/gitlab/%E8%AE%BE%E7%BD%AE%E7%AE%A1%E7%90%86%E5%91%98%E5%AF%86%E7%A0%81.png" alt="设置管理员密码"></p></li></ol><p>设置后管理员密码后，就进入登录页面，输入用户名 root 和刚才设置的密码就进入了 gitlab 的控制台。如下图所示:<br>    <img src="./images/gitlab/gitlab%E5%88%9D%E5%A7%8B%E9%A1%B5%E9%9D%A2.png" alt="gitlab初始页面"></p><ol><li>创建组</li></ol><p>gitlab 里面有三类对象：组（ group）、项目（ project）和用户 （people）。<br>为了方便管理，我们应该基于组来创建项目。一个项目就是一个 git 的仓库。基于组创建项目 ，然后将用户设置合适的权限后加入到组里面。这样用户就有了组里面所有项目的对应权限。</p><p>点击 “Create a group” 链接，如下图所示创建一个“健康医疗开发组” 的组<br>    <img src="./images/gitlab/%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%E7%BB%84.png" alt="创建用户组"></p><ol start="2"><li>创建用户</li></ol><p>点击 “Add people” 链接，如下图所示创建一个 “yanggch” 的用户。如下图：<br>    <img src="./images/gitlab/%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7.png" alt="创建用户"></p><p>因为还没有配置好邮件服务，所以还不能发送用户初始化密码的邮件。我们需要编辑用户，手动设置一个密码。如下图所示。如果用户忘记了密码，充值密码也可以在这里进行。<br>    <img src="./images/gitlab/%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7-%E8%AE%BE%E7%BD%AE%E5%AF%86%E7%A0%81.png" alt="创建用户-设置密码"></p><ol start="3"><li>将用户加入组</li></ol><p>为了方便管理，需要将用户加入到对应的组里面。如下图所示，在组管理界面中，点击组的名称，进入组用户设置界面。将刚才创建的用户 “yanggch” 加入到组 “健康医疗开发组”中，并且给他设置为 “Master” 角色。只有 “Master” 或者 “Owner” 角色才能推送 git 的更新。<br>    <img src="./images/gitlab/%E7%94%A8%E6%88%B7%E5%8A%A0%E5%85%A5%E7%BB%84.png" alt="用户加入组"></p><ol start="4"><li>创建项目</li></ol><p>增加 gitlab 组的时候，为了让项目让组里面的人都能访问，注意要将项目建立在组之下。如下图所示，在“健康医疗开发组”之下建立了 “redis_util” 的项目。<br>    <img src="./images/gitlab/%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE.png" alt="创建项目"></p><ol start="5"><li>ssh密钥配置</li></ol><p>针对该项目需要配置jenkins拉取的ssh密钥，以方便后续从jenkins中拉取该项目。首先配置本机的ssh访问，开启命令行，进行操作：</p><pre><code>// 转换到ssh目录下,进入 ssh key 存储目录cd ~/.ssh// 执行下面的命令生成ssh 访问的密钥信息ssh-keygen -t rsa// 全部回车后，会生成id_rsa.pub公钥文件</code></pre><p>这时候需要将公钥文件id_rsa.pub内的信息拷贝到gitlab中，以文本方式打开，添加到。这样我们需要登录至gitlab，点击用户信息中的settings选项，如下图：<br>    <img src="./images/gitlab/%E4%B8%AA%E4%BA%BA%E8%AE%BE%E7%BD%AE%E8%BF%9B%E5%85%A5.png" alt="个人设置进入"></p><p>进入个人配置中心后，选择SSH Keys，填入密钥信息，如下图：<br>    <img src="./images/gitlab/%E5%A1%AB%E5%86%99%E5%AF%86%E9%92%A5.png" alt="填写密钥"><br>完成后即可进行保存，点击Add key，保存该密钥。</p><p>然后配置docker的ssh访问，登录目标服务器，进入docker镜像中，操作如下：</p><pre><code>// 进入docker镜像$ docker exec -it b979 /bin/bash// 生成ssh密钥$ ssh-keygen// 全部回车// 查看ssh公钥信息$ cat ~/.ssh/id_rsa.pub</code></pre><p>这时拷贝输出的信息，添加到gitlab的配置中即可，操作可参考上面的步骤。</p><ol start="6"><li>创建我们自己的项目</li></ol><p>将我们的测试项目拉取到本地</p><pre><code>// 先将目标项目拉取到本地git clone https://github.com/callicoder/spring-security-react-ant-design-polls-app.git// 然后更改项目中polling-app-server/src/main/resources/目录下的application.properties文件，修改数据库连接地址，更改为自己的即可// 提交修改git commit -am &quot;修改数据库连接信息&quot;</code></pre><p>然后在gitlab中我们创建的Group下，创建项目Test_CI-CD，并赋予权限。项目地址为：<a href="http://10.0.11.11:8181/lisongyang/test_ci-cd.git" target="_blank" rel="noopener">http://10.0.11.11:8181/lisongyang/test_ci-cd.git</a></p><p>最终我们把我们创建的Test_CI-CD拉取到本地，将原来的spring-security-react-ant-design-polls-app中的文件覆盖到Test_CI-CD中，进行提交即可，最终效果如下：</p><p><img src="./images/gitlab/%E6%9C%80%E7%BB%88%E5%88%9B%E5%BB%BA%E7%9A%84%E9%A1%B9%E7%9B%AE.png" alt="最终创建的项目"></p><ul><li><strong>sonar基础配置</strong></li></ul><p>sonar启动后，访问<a href="http://http://10.0.11.11:9000/，进入sonarqube的管理页面，用户名和密码均为admin。" target="_blank" rel="noopener">http://http://10.0.11.11:9000/，进入sonarqube的管理页面，用户名和密码均为admin。</a></p><ol start="0"><li>访问token设置</li></ol><p>登录后点击右上角加号右侧的用户信息，在弹出框中点击My Account选项，进入管理页面如下：<br><img src="./images/sonar/sonar%E7%94%A8%E6%88%B7%E9%85%8D%E7%BD%AE.png" alt="sonar用户配置"></p><p><img src="./images/sonar/sonar%E7%94%A8%E6%88%B7%E9%85%8D%E7%BD%AE%E8%AF%A6%E6%83%85.png" alt="sonar用户配置详情"></p><p>点击Security选项，看到有个Generate Tokens的输入框。如下图：<br><img src="./images/sonar/sonar%E7%94%9F%E6%88%90%E5%85%A8%E5%B1%80%E8%AE%BF%E9%97%AE%E7%9A%84token.png" alt="sonar生成全局访问的token"></p><p>在输入框中输入test，点击Generate按钮，生成一个全局访问的token信息，如下图：<br><img src="./images/sonar/sonar%E4%B8%ADtoken%E4%BF%A1%E6%81%AF%E7%94%9F%E6%88%90.png" alt="sonar中token信息生成"></p><p>先记录下这个token信息，为：b3cd96221a2f1723123dbca4a5bdd34eb5a3e339。后续会用到。</p><ol><li>新建项目</li></ol><p>点击右上角的加号，弹出Create new project选项，创建一个新项目。如下图：<br><img src="./images/sonar/sonar%E6%96%B0%E5%BB%BA%E9%A1%B9%E7%9B%AE.png" alt="sonar新建项目"></p><p>输入Project key和Display name的信息，到下一个创建页面，如下图：</p><p><img src="./images/sonar/sonar%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%E5%85%B7%E4%BD%93%E4%BF%A1%E6%81%AF.png" alt="sonar创建项目具体信息"></p><p>在创建的时候，我们在第一个Provide a token输入框中，需要选择Use existing token，将刚才生成的token信息输入，如下图：</p><p><img src="./images/sonar/sonar%E8%BE%93%E5%85%A5token%E4%BF%A1%E6%81%AF.png" alt="sonar输入token信息"></p><p>点击continue，就会出现如何去运行该项目，以达到将代码分析数据传入到sonar平台的效果。如下图：</p><p><img src="./images/sonar/sonar%E6%96%B0%E5%BB%BAjava%E9%A1%B9%E7%9B%AE.png" alt="sonar新建java项目"></p><p>选择Java和Maven选项，就出现了下面通过maven执行代码测试的命令。这样我们就可以将该命令应用在jenkins中。</p><ol start="2"><li>构建脚本</li></ol><p>针对构建脚本需要做部分优化，需要添加两个新的选项才行，要指定sources和java.binaries的目录地址，如下图：</p><pre><code>mvn sonar:sonar \    -Dsonar.projectKey=Test-CI-CD \    -Dsonar.host.url=http://10.0.11.11:9000 \    -Dsonar.login=jenkins    \    -Dsonar.login=d3830ce5b21ca809290798ed7f093dd3f4396edf \    -Dsonar.java.binaries=./polling-app-server/target/ </code></pre><p>一定要指定<strong>sonar.java.binaries</strong>属性信息，否则jenkins构建时会报错。在构建自己的项目时，需要调整为自己项目对应的目录结构信息。</p><ul><li><strong>jenkins基础配置</strong></li></ul><ol start="0"><li><p>初始配置</p><ul><li><p>初始的配置步骤</p><p>当docker启动jenkins后，通过访问<a href="http://10.0.11.11:9090/，进入jenkins的初始化设置。首先需要对jenkins进行解锁，如下图所示：" target="_blank" rel="noopener">http://10.0.11.11:9090/，进入jenkins的初始化设置。首先需要对jenkins进行解锁，如下图所示：</a><br><img src="./images/jenkins/jenkins%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AF%86%E7%A0%81.png" alt="jenkins初始化密码"></p><p>需要回到服务器中，进入jenkins正在运行的docker镜像中。操作如下：</p><p>  // 进入jenkins所在镜像<br>  $ docker exec -it b979 /bin/bash</p><p>  // 查看初始化的密码信息<br>  bash4.4# cat /var/jenkins_home/secrets/initialAdminPassword</p><p>将输出的信息复制，粘贴到输入框中，再点击右下角的”Continue”按钮。</p><p>解锁后，进入Customize Jenkins页面，如下图：<br><img src="./images/jenkins/jenkins%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6.png" alt="jenkins初始化安装插件"></p><p>如果是离线模式安装，Jenkins会提醒“This Jenkins instance appears to be offline”。我们先点击“Skip Plugin Installations”跳过插件安装。<br>如果是在线模式安装的话，可以选择跳过该部分，后续根据需要安装插件。</p><p>也可以选择Install suggested plugins进行安装。但是插件安装过程很慢，需要很长时间，且存在安装不成功的情况。如下图：<br><img src="./images/jenkins/jenkins%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6%E4%BF%A1%E6%81%AF.png" alt="jenkins安装插件信息"></p><p>下一步开始设置管理员信息，如下图：<br><img src="./images/jenkins/jenkins%E7%AE%A1%E7%90%86%E5%91%98%E4%BF%A1%E6%81%AF%E5%A1%AB%E5%86%99.png" alt="jenkins管理员信息填写"></p><p>填写管理员信息后，点击右下角Save and Finish按钮，即可进入jenkins主界面，如下图：<br><img src="./images/jenkins/jenkins%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AE%8C%E6%AF%95.png" alt="jenkins初始化完毕"></p></li></ul></li><li><p>jenkins安装插件</p><p> 进入jenkins主页面，点击左侧Manage Jenkins，再转到Manage Plugins界面，操作如下图：<br><img src="./images/jenkins/jenkins%E7%AE%A1%E7%90%86%E9%A1%B5%E9%9D%A2%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6.png" alt="jenkins管理页面安装插件"><br> 最终转到插件安装界面如下：<br><img src="./images/jenkins/jenkins%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85Available%E9%A1%B5%E9%9D%A2.png" alt="jenkins插件安装Available页面"></p><p> 点击切换到Available页面下，选择插件安装，可以在右上角输入框中输入插件名称进行查找。插件列表如下：</p><ul><li><p>maven相关</p><ul><li>maven integration plugin – maven构建</li><li>Pipeline Maven Integration Plugin</li></ul></li><li><p>sonar相关</p><ul><li>Sonar Quality Gates Plugin</li></ul></li><li><p>触发构建相关</p><ul><li>Generic Webhook Trigger</li></ul></li><li><p>pipeline相关</p></li><li><p>gitlab相关</p><ul><li>Git plugin</li><li>Git client plugin</li><li>Gitlab Plugin – 从Gitlab拉取</li></ul></li><li><p>docker相关</p><ul><li>Docker Compose Build Step Plugin</li><li>Docker Pipeline</li><li>Docker plugin</li><li>docker-build-step – docker分步执行命令</li></ul></li><li><p>ssh执行相关</p><ul><li>Publish Over SSH – 通过ssh执行远程命令</li><li>SSH plugin</li><li>SSH Slaves plugin</li><li>SSH Agent Plugin</li></ul></li><li><p>nodejs相关</p><ul><li>NodeJS Plugin</li><li>nvm-wrapper – 利用该插件管理和安装nodejs版本</li></ul></li><li><p>邮件相关</p><ul><li>Email Extension</li><li>Email Extension Template</li></ul></li><li><p>企业微信通知</p><ul><li>Qy Wechat Notification Plugin</li></ul></li><li><p>其它</p><ul><li>Workspace Cleanup Plugin – 清理工作空间</li><li>Webhook Step Plugin – 执行设置webhook，方便构建</li><li>Localization:Chinese (Simplified) – 设置中语言包插件</li><li>Timestamper – 设置构建后显示时间进度的问题</li></ul><p>安装插件时，可能需要多次重启jenkins服务。这里不需要重启jenkins镜像，当一批插件安装完成时，可以选择自动重启，也可以选择安装完成所有的插件再重启。插件安装图示如下：<br><img src="./images/jenkins/jenkins%E9%80%89%E6%8B%A9%E6%8F%92%E4%BB%B6%E5%90%8E%E7%9A%84%E5%AE%89%E8%A3%85%E9%A1%B5%E9%9D%A2.png" alt="jenkins选择插件后的安装页面"></p></li></ul></li><li><p>jenkins安装工具后的配置</p><p> 首先开启Configure System的配置，也就是系统全局配置，方式如下图：</p><p> <img src="./images/jenkins/jenkins%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE.png" alt="jenkins系统配置"></p><p> 然后设置以下的两个选项：</p><ul><li><p>ssh远程连接服务器设置</p><p>找到SSH remote hosts部分，如下图：<br><img src="./images/jenkins/jenkins_ssh_remote_hosts.png" alt="jenkins设置ssh部分"></p><p>点击最下面的Add按钮，添加一个服务器配置，填入以下信息。如下图：<br><img src="./images/jenkins/jenkins%E6%B7%BB%E5%8A%A0ssh%E8%BF%9E%E6%8E%A5.png" alt="jenkins添加ssh连接"></p><p>在Credentials选项中，点击后边的add选项，添加秘钥信息，如下图：<br><img src="./images/jenkins/jenkins%E6%B7%BB%E5%8A%A0ssh%E5%AF%86%E9%92%A5%E4%BF%A1%E6%81%AF.png" alt="jenkins添加ssh密钥信息"></p><p>填写username为centos，Password为123456，ID为centos登录，点击add即可添加远程连接的密钥。如下图：<br><img src="./images/jenkins/jenkins%E5%AF%86%E9%92%A5%E9%80%89%E6%8B%A9.png" alt="jenkins密钥选择"></p><p>点击选择密钥后，即可进行连接测试，点击Check Connection，测试服务器的连接信息，如下图：<br><img src="./images/jenkins/jenkins%E6%B5%8B%E8%AF%95%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9E%E6%8E%A5.png" alt="jenkins测试服务器连接"></p><p>最后点击页面最下方的灰色Apply按钮，即可保存成功，留在该页面继续进行配置。如果点击蓝色Save按钮，同样可以保存成功，但是会退出该页面，回到Dashboard首页。</p></li><li><p>docker远程仓库配置</p><p>同样在改配置页面上，找到Docker Builder模块，配置Docker URL信息，填入前面配置的docker中的tcp连接信息，如下图：<br><img src="./images/jenkins/jenkins%E4%B8%ADdockerURL%E9%85%8D%E7%BD%AE.png" alt="jenkins中dockerURL配置"></p><p>点击TestConnection，测试远程仓库的连接信息。若成功则点击Apply按钮进行保存。</p><p>这部分完成了以后，需要转到Global Tool Configuration选项中，跳转方式如下：<br><img src="./images/jenkins/jenkins%E5%85%A8%E5%B1%80%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE.png" alt="jenkins全局工具配置"></p><p>然后设置以下的两个选项：</p></li><li><p>jdk配置–镜像内配置和安装配置<br>找到JDK模块，点击JDK installations按钮，展开添加jdk的信息，首先添加本地docker镜像内的jdk信息，输入如下图：<br><img src="./images/jenkins/jenkins%E5%AE%89%E8%A3%85%E6%9C%AC%E5%9C%B0JDK.png" alt="jenkins安装本地JDK"></p><!-- 暂时不添加OracleJDK的安装如果要添加Oracle的JDK安装信息，需要点击Add JDK，出现新的JDK安装信息。勾选Install automatically，自动在jdk不存在的情况下进行安装。点击Add Installer，如下图：![jenkinsjdk选项](./images/jenkins/jenkinsjdk选项.png) --></li><li><p>maven配置<br>找到Maven模块，点击Maven installations按钮，展开添加Maven的信息，点击Add Maven添加，如下图：<br><img src="./images/jenkins/jenkins%E4%B8%AD%E7%9A%84Maven%E6%B7%BB%E5%8A%A0.png" alt="jenkins中的Maven添加"></p><p>填写Maven的名称，勾选Install automatically，在下面选择Install from Apache中的Version选项，选择3.6.3版本。点击最下面的Apply即可。</p></li><li><p>nodejs配置信息<br>由于使用nvm管理nodejs的安装，因此对其不进行配置，在前端项目构建的时候体现。</p></li><li><p>sonar服务<br>由于我们使用maven执行sonarqube相关的命令，所以在此可以不设置sonarqube服务的内容。</p></li><li><p>sonarqube scanner<br>由于我们使用maven执行sonarqube相关的命令，所以在此可以不设置sonarqube scanner的内容。</p></li></ul></li><li><p>jenkins授权配置–密钥管理</p><p> 密钥添加的界面可以查看<strong>ssh远程连接服务器设置</strong>界面的设置，添加密钥信息。添加密钥时主要有以下类型，如下图：<br> <img src="./images/jenkins/jenkins%E5%AF%86%E9%92%A5%E7%B1%BB%E5%9E%8B.png" alt="jenkins密钥类型"></p><p> 常用的除了Username with password，还有Secret text。如果需要使用HTTPS的话，需要添加证书信息。</p></li><li><p>jenkins管理配置文件</p><p> 在项目中，使用了nexus作为maven的管理仓库，所以需要设置到私服的连接上。因此需要对maven的配置文件进行调整。转到Manage Jenkins页面，点击Manage files，如下图：<br> <img src="./images/jenkins/jenkins%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86.png" alt="jenkins配置文件管理"></p><p> <img src="./images/jenkins/jenkins%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86.png" alt="jenkins文件管理"></p><p> 然后可以点击Add a new Config，新增一个Maven的settings.xml，勾选Maven settings.xml，点击Submit即可添加。如下图：<br> <img src="./images/jenkins/jenkins%E6%96%B0%E5%A2%9E%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.png" alt="jenkins新增配置文件"></p><p> 添加后，即可进行编辑，如下图：<br> <img src="./images/jenkins/jenkins%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91.png" alt="jenkins文本编辑"></p><p> 需要按照当前平台的<a href="./files/settings.xml">配置文件</a>进行设定，将提供的配置文件内容复制到输入框中，编辑后点击Submit进行保存。</p></li></ol><h2 id="搭建全流程"><a href="#搭建全流程" class="headerlink" title="搭建全流程"></a>搭建全流程</h2><h3 id="jenkins创建jobs"><a href="#jenkins创建jobs" class="headerlink" title="jenkins创建jobs"></a>jenkins创建jobs</h3><p>现在开始构建Test_CI-CD项目，由于这是个前后端分离的项目，需要分别从前端和后端构建。文件夹polling-app-client内为前端项目，文件夹polling-app-server内为后端项目，需要注意路径的问题。</p><ul><li><p>后端构建案例</p><ul><li><p>创建新的job<br>  创建新的job，点击左上角的New Item选项，进入创建页面：<br>  <img src="./images/jenkins/jenkins%E5%88%9B%E5%BB%BA%E6%96%B0%E7%9A%84job.png" alt="jenkins创建新的job"></p><p>  <img src="./images/jenkins/jenkins%E4%B8%ADjob%E5%88%9B%E5%BB%BA%E9%A1%B5%E9%9D%A2.png" alt="jenkins中job创建页面"></p><p>  在Enter an item name中输入名称信息，点击选择下面的Maven project项目，点击OK即可创建项目，如下图：<br>  <img src="./images/jenkins/jenkins%E4%B8%ADjob%E5%88%9B%E5%BB%BA%E9%A1%B5%E9%9D%A2.png" alt="jenkins中job创建页面"></p><p>  <img src="./images/jenkins/jenkins%E5%88%9B%E5%BB%BA%E8%BE%93%E5%85%A5%E5%90%8D%E7%A7%B0maven.png" alt="jenkins创建输入名称maven"></p><p>  也可以从之前的项目进行复制，不要点选Maven project，直接在最下面的Copy from中输入要复制的项目，点击选择自动提示的项目名称，就会自动将已有项目的配置信息添加到新项目中了，点击OK即可创建之前已有项目的副本。如下图：<br>  <img src="./images/jenkins/jenkins%E4%BB%8E%E5%B7%B2%E6%9C%89%E9%A1%B9%E7%9B%AE%E5%A4%8D%E5%88%B6.png" alt="jenkins从已有项目复制"></p></li><li><p>构建之前的设置</p><ul><li><p>通用配置设置–指定JDK信息<br>在后端项目构建时，需要添加之前我们设定的jdk信息，转到General模块。如下图：<br><img src="./images/jenkins/jenkins%E9%A1%B9%E7%9B%AE%E7%9A%84%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE.png" alt="jenkins项目的通用配置"></p><p>需要勾选Execute concurrent builds if necessary，然后在弹出的JDK一栏选择我们添加的JDK名称，这里选择jenkins的docker镜像中的JDK。默认指定的是(System)中的JDK信息。</p><p>其它的可以填写Description内容。点击Apply进行保存，然后继续设置。</p></li><li><p>拉取项目代码<br>转到Source Code Mangement模块，添加项目的gitlab地址信息。首先勾选Git，弹出Git的配置信息，开始填写Repository URL，如下图：<br><img src="./images/jenkins/jenkins%E4%B8%ADjob%E6%B7%BB%E5%8A%A0repository%E4%BF%A1%E6%81%AF.png" alt="jenkins中job添加repository信息"></p><p>填写Repository URL，选择之前添加的Credentials认证信息。如果认证信息未添加，可以点击下拉框右边的Add按钮添加。如下图：<br><img src="./images/jenkins/jenkins%E4%B8%ADjob%E6%B7%BB%E5%8A%A0repository%E4%BF%A1%E6%81%AF-gitlab%E5%9C%B0%E5%9D%80.png" alt="jenkins中job添加repository信息-gitlab地址"></p><p>然后输入Branches to build中的要构建的分支信息，这里以Develop分支为例，如下图：<br><img src="./images/jenkins/jenkins%E6%B7%BB%E5%8A%A0%E5%88%86%E6%94%AF%E4%BF%A1%E6%81%AF.png" alt="jenkins添加分支信息"></p><p>点击Apply进行保存，然后继续设置。</p></li><li><p>构建触发策略<br>转到Build Triggers模块，这里我们有多种构建方式可选，例如Build periodically（定时策略构建）、Build when a change is pushed to GitLab. （有push操作就构建）、Trigger builds remotely（远程连接触发构建）、Gitlab Merge Requests Builder（merge操作后构建）等等。这里先介绍最简单的定时任务构建。<br>现在我们选择定时任务构建，选择Build periodically，展示出Schedule输入框，如下图：<br><img src="./images/jenkins/jenkins%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E8%AE%BE%E7%BD%AE.png" alt="jenkins定时任务设置"><br>输入以下信息：</p><pre><code>  H H/8 * * *</code></pre><p>表示每隔8小时构建一次，这里定时任务的编写可以<a href="https://blog.csdn.net/ZZY1078689276/article/details/77520441" target="_blank" rel="noopener">参考该文章进行</a>。 点击Apply进行保存，然后继续设置。</p></li><li><p>构建环境<br>转到Build Environment模块，后端构建的时候对其不做设置，前端构建时对其设置。</p></li><li><p>前置步骤<br>转到Pre Steps模块。在构建之前，我们需要将代码提交到sonarqube进行代码审查，这里使用maven的命令方式进行提交。选择Add pre-build step项，如下图：<br><img src="./images/jenkins/jenkins%E9%80%89%E6%8B%A9%E6%89%A7%E8%A1%8Cshell%E5%91%BD%E4%BB%A4.png" alt="jenkins选择执行shell命令"></p><p>点击选择Execute shell，会添加Execute shell的输入信息。在Command的输入框中填写以下信息：</p><pre><code>  cd ./polling-app-server &amp;&amp; mvn sonar:sonar \          -Dsonar.projectKey=Test-CI-CD \          -Dsonar.host.url=http://10.0.11.11:9000 \          -Dsonar.login=jenkins    \          -Dsonar.login=d3830ce5b21ca809290798ed7f093dd3f4396edf \          -Dsonar.java.binaries=$WORKSPACE/polling-app-server/target/</code></pre><p>需要切换到后端代码的文件夹中，执行maven的sonar扫描操作。将代码推送到sonarqube中，进行代码自动化审查，出具相应的代码审查结果。当项目开始自动化构建时，将会先执行前置步骤的操作。代码审查结果如下图：<br><img src="./images/sonar/sonar%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E5%AE%A1%E6%9F%A5%E7%BB%93%E6%9E%9C.png" alt="sonar代码自动审查结果"></p><p>针对jenkins中的设置，点击Apply进行保存，然后继续设置。</p></li><li><p>构建<br>转到Build模块，选择使用maven构建需要确定pom文件地址和maven执行的命令信息。首先在Root POM输入框中填入pom.xml文件所在地址，然后在Goals and options输入框中填入要执行的命令，如下图：<br><img src="./images/jenkins/jenkins%E4%B8%ADmaven%E6%9E%84%E5%BB%BA%E8%B7%AF%E5%BE%84%E5%92%8C%E5%91%BD%E4%BB%A4.png" alt="jenkins中maven构建路径和命令"></p><p>随后点击Advanced…选项，展开的内容拉到最后，有个Maven Validation Level的标签，在它下面的Settings file中，下拉选择Provided settings.xml，在随后弹出的Provided Settings 下拉框中选择Settings文件信息，添加maven的配置文件。如下图：<br><img src="./images/jenkins/jenkins%E9%80%89%E6%8B%A9maven%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.png" alt="jenkins选择maven的配置文件"></p><p>这样就设置完成了Maven的配置文件。点击Apply进行保存，然后继续设置。</p></li><li><p>构建后处理步骤<br>转到Post Steps模块，点选Run only if build succeeds，选择只有在构建成功后执行下面的操作。点击Add post-build step，选择Execute Docker command，添加docker的操作步骤。如下图：<br><img src="./images/jenkins/jenkins%E9%80%89%E6%8B%A9docker-command.png" alt="jenkins选择docker-command"></p><p>然后进行下面的操作，首先我们要对已经构建过产出的jar包构建docker镜像。在新增的Execute Docker command中，在Docker command部分，下拉选择Create/build image选项，如下图：<br><img src="./images/jenkins/jenkins%E5%88%9B%E5%BB%BAdockerimage.png" alt="jenkins创建dockerimage"></p><p>然后天禧Build context folder，指定Dockerfile路径，对要产出的镜像信息指定tag，如下图：<br><img src="./images/jenkins/jenkins%E6%8C%87%E5%AE%9Adockerfile%E8%B7%AF%E5%BE%84.png" alt="jenkins指定dockerfile路径"></p><p>随后新增Execute Docker command，在Docker command部分，下拉选择Push image，然后填写要推送到镜像仓库的镜像名称。选择Docker registry URL，输入框中填写镜像地址即可，如下图：<br><img src="./images/jenkins/jenkins%E8%AE%BE%E7%BD%AE%E9%95%9C%E5%83%8Fpush.png" alt="jenkins设置镜像push"></p><p>当镜像已经push到镜像仓库时，需要删除当前服务器上已经构建的镜像，因此需要添加新的Execute Docker command，从Docker command中选择Remove image选项，填入要删除的镜像名称，如下图：<br><img src="./images/jenkins/jenkins%E5%88%A0%E9%99%A4%E6%9C%AC%E5%9C%B0%E6%9E%84%E5%BB%BA%E7%9A%84%E9%95%9C%E5%83%8F.png" alt="jenkins删除本地构建的镜像"></p><p>这样针对Docker的操作就完成了，点击最下面的Apply按钮，即可进行保存。</p><p>最后我们需要将docker镜像指定在目标服务器上执行，点击Add post-build step按钮，选择Execute shell script on remote host using ssh，如下图：<br><img src="./images/jenkins/jenkins%E8%AE%BE%E7%BD%AE%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%E9%80%89%E9%A1%B9.png" alt="jenkins设置远程执行选项"></p><p>在SSH site部分选择前面设置的服务器ssh连接信息，在Command输入以下命令：</p><pre><code>  #--------------------------------------------------------------------------  # 判断是否存在镜像  docker ps -a | grep -w polling-app-server &amp;&gt; /dev/null  # 如果存在先停止运行并删除镜像  if [$? -eq 0]  then      echo &quot;polling-app-server is exsited!!&quot;      docker stop `docker ps -a | grep -w polling-app-server | awk &#39;{print $1}&#39;`      docker rm `docker ps -a | grep -w polling-app-server | awk &#39;{print $1}&#39;`  fi  echo &quot;pull the polling-app-server image&quot;  docker pull 10.0.11.11:5000/polling-app-server:$BUILD_NUMBER  docker run -d -p 19900:80 10.0.11.11:5000/polling-app-server:$BUILD_NUMBER</code></pre><p>表示在远程服务器上，执行下面的脚本信息。将已有的服务进行停机，随后用新的docker镜像替代原来运行的docker镜像，达到自动部署的效果。点击最下面的Apply按钮，即可进行保存。</p></li><li><p>构建后行为<br>前面所有的构建步骤完成后，转到最后一步Post-build Actions中。点击Add post-build action，然后选择企业微信通知，即可创建针对企业微信的通知内容。<br>需要在企业微信中创建机器人，首先创建群聊，完成后右击群聊框，弹出创建群机器人的选项，按照图示操作，如下图：<br><img src="./images/jenkins/jenkins%E7%BE%A4%E6%9C%BA%E5%99%A8%E4%BA%BA.png" alt="jenkins群机器人"></p><p><img src="./images/jenkins/jenkins%E5%88%9B%E5%BB%BA%E7%BE%A4%E6%9C%BA%E5%99%A8%E4%BA%BA2.png" alt="jenkins创建群机器人"></p><p><img src="./images/jenkins/jenkins%E7%82%B9%E5%87%BB%E5%88%9B%E5%BB%BA%E7%BE%A4%E6%9C%BA%E5%99%A8%E4%BA%BA.png" alt="jenkins点击创建群机器人"></p><p><img src="./images/jenkins/jenkins%E5%88%9B%E5%BB%BA%E7%BE%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%90%8D%E7%A7%B0.png" alt="jenkins创建群机器人名称"></p><p><img src="./images/jenkins/jenkins%E6%B7%BB%E5%8A%A0%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%8C%E6%88%90.png" alt="jenkins添加机器人完成"></p><p>添加完成后需要复制Webhook地址信息，如下：</p><pre><code>  https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=293181db-f077-427f-879e-332e883a6198</code></pre><p>这时，回到jenkins中，在添加的企业微信通知模块中，填写Webhook地址，将上面的地址信息复制粘贴到该位置，这样就能够监控构建时的构建状态。点击最下面的Apply按钮，即可进行保存。</p></li></ul><p>由此所有的都已经配置完毕，下面开始后端项目构建！<br>点击Save保存所有的内容，回到项目页面，如下图：<br><img src="./images/jenkins/jenkins%E5%BC%80%E5%A7%8B%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA.png" alt="jenkins开始项目构建"></p><p>点击右边的Build Now，即可手动触发构建。或者等待定时任务在特定的时间节点进行构建。点击后将会在左下方的Build History中生成一条构建信息，如下图：</p><p><img src="./images/jenkins/jenkins%E8%BF%9B%E5%85%A5%E6%9E%84%E5%BB%BA%E4%B9%8B%E5%89%8D.png" alt="jenkins进入构建之前"></p><p>随后点击序号，进入该构建内，如下图：</p><p><img src="./images/jenkins/jenkins%E8%BF%9B%E5%85%A5%E6%9E%84%E5%BB%BA%E5%86%85.png" alt="jenkins进入构建内"></p><p>可以点击Console Output，查看构建日志，如下图：</p><p><img src="./images/jenkins/jenkins%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97.png" alt="jenkins构建日志"></p><p>最后在构建开始时，会在企业微信进行通知，在构建完成后，也会在企业微信进行通知。构建出错时，也会在企业微信进行通知，如下图：</p><p><img src="./images/jenkins/jenkins%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%9E%84%E5%BB%BA%E7%BB%93%E6%9E%9C.png" alt="jenkins企业微信构建结果"></p><p>这样我们就完成了后端构建的内容。</p></li></ul></li><li><p>前端构建案例</p><ul><li><p>创建前端项目</p><p>  下面我们需要先创建一个前端项目，点击New Item，填写item name信息，选择Freestyle project，点击OK开始进行配置。如下图：<br>  <img src="./images/jenkins/jenkins%E5%88%9B%E5%BB%BA%E5%89%8D%E7%AB%AF%E9%A1%B9%E7%9B%AE.png" alt="jenkins创建前端项目"></p></li><li><p>通用设置</p><p>  转到General模块，在构建前端项目的时候，不需要设置JDK的相关信息，在通用设置中可以保持默认设置。</p></li><li><p>代码管理</p><p>  转到Source Code Management模块，点击选择Git，弹出Git的弹出框。由于这里前端和后端项目均在同一个仓库中，因此填写内容和上面后端构建时相同，不再赘述，如下图：<br>  <img src="./images/jenkins/jenkins%E5%89%8D%E7%AB%AF%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%E4%BB%93%E5%BA%93.png" alt="jenkins前端项目代码仓库"></p></li><li><p>构建触发内容</p><p>  转到Build Triggers模块，设置同上面后端的设置相同，使用定时构建的方式进行构建。如下图：<br>  <img src="./images/jenkins/jenkins%E5%89%8D%E7%AB%AF%E5%AE%9A%E6%97%B6%E6%9E%84%E5%BB%BA.png" alt="jenkins前端定时构建"></p></li><li><p>构建环境设置</p><p>  在构建环境设置，需要指定nodejs的版本，这里利用之前安装的nvm-wrapper插件进行管理nodejs版本。点击勾选Run the build in an NVM managed environment，弹出下面的内容，如下图：<br>  <img src="./images/jenkins/jenkins%E5%89%8D%E7%AB%AFNVM%E6%9C%AA%E8%AE%BE%E7%BD%AE.png" alt="jenkins前端NVM未设置"></p><p>  然后填写nodejs版本，指定NVM Install URL、NVM_DIR installation dir。设置NVM安装路径的时候，一定保证文件夹路径存在，并且具备相应的权限信息。填写完成后，如下表：<br>  <img src="./images/jenkins/jenkins%E5%89%8D%E7%AB%AFNVM%E8%AE%BE%E7%BD%AE.png" alt="jenkins前端NVM设置"></p><p>  这样设置完成，在构建的前置，检测nvm是否安装、nodejs是否安装，如果未安装会优先安装该程序。点击最下面的Apply按钮，即可进行保存。</p></li><li><p>构建部分</p><p>  转到Build模块。这里针对前端构建，通过对前端的React代码（Vue操作类同）进行打包，生成docker镜像，放到服务器上对外提供前端访问。首先要进行前端代码的打包，点击Add build step，选择Execute shell，添加代码执行的窗口，插入以下代码：</p><pre><code>      cd ./polling-app-client      npm --registry https://registry.npm.taobao.org install -g yarn // 指定使用淘宝的npm镜像      yarn --registry https://registry.npm.taobao.org install      yarn run build  </code></pre><p>  插入完成后，如下图：<br>  <img src="./images/jenkins/jenkins%E6%8F%92%E5%85%A5nodejs%E6%89%93%E5%8C%85%E4%BB%A3%E7%A0%81.png" alt="jenkins插入nodejs打包代码"></p><p>  然后进行docker镜像的构建、推送仓库、删除镜像、目标服务器执行镜像，操作类同后端构建的操作，最终结果如下：<br>  <img src="./images/jenkins/jenkins%E5%89%8D%E7%AB%AF%E6%B7%BB%E5%8A%A0docker%E6%93%8D%E4%BD%9C.png" alt="jenkins前端添加docker操作"></p><p>  这样关于构建的设置已经完成了。点击最下面的Apply按钮，即可进行保存。</p></li><li><p>构建之后</p><p>  转到Post-build Actions，创建企业微信的通知操作，同后端构建的配置信息，不再赘述。</p><p>上述构建配置完成后，点击Save存储，转到项目构建页面。<br>点击Build Now，开始即时构建。其它查看信息同后端构建项目相同。</p></li></ul></li><li><p>最终测试</p></li><li><p>对于开发侧的影响</p><ul><li><p>Dockerfile编写</p><ul><li><p>编写后端Dockerfile<br>示例项目中已经编写了Dockerfile，见项目中的Dockerfile文件，内容如下：</p><pre><code>  #### Stage 1: Build the application  FROM openjdk:8-jdk-alpine as build  # Set the current working directory inside the image  WORKDIR /app  # Copy maven executable to the image  COPY mvnw .  COPY .mvn .mvn  # Copy the pom.xml file  COPY pom.xml .  # Build all the dependencies in preparation to go offline.   # This is a separate step so the dependencies will be cached unless   # the pom.xml file has changed.  RUN ./mvnw dependency:go-offline -B  # Copy the project source  COPY src src  # Package the application  RUN ./mvnw package -DskipTests  RUN mkdir -p target/dependency &amp;&amp; (cd target/dependency; jar -xf ../*.jar)  #### Stage 2: A minimal docker image with command to run the app   FROM openjdk:8-jre-alpine  ARG DEPENDENCY=/app/target/dependency  # Copy project dependencies from the build stage  COPY --from=build ${DEPENDENCY}/BOOT-INF/lib /app/lib  COPY --from=build ${DEPENDENCY}/META-INF /app/META-INF  COPY --from=build ${DEPENDENCY}/BOOT-INF/classes /app  ENTRYPOINT [&quot;java&quot;,&quot;-cp&quot;,&quot;app:app/lib/*&quot;,&quot;com.example.polls.PollsApplication&quot;]</code></pre><p>但是针对自己的服务，只需要构建运行时状态，不需要构建mvn的设置。通用的简化版的Dockerfile的配置如下（以注册中心为例）：</p><pre><code>  # FROM XX---指定基础镜像  FROM openjdk:8-jre-alpine  # WORKDIR 设置当前工作目录   WORKDIR /app  # 复制jar包所在地址 到 目标地址  --将jar包拷贝到镜像系统的相应地址  COPY ./target/test-serverregister-0.0.1-SNAPSHOT.jar /app  # 设置运行的加入点  ENTRYPOINT [&quot;java&quot;,&quot;-Dspring.profiles.active=peer0&quot;,&quot;-jar&quot;,&quot;test-serverregister-0.0.1-SNAPSHOT.jar&quot;]</code></pre></li><li><p>编写前端Dockerfile<br>  示例项目中前端已经编写了Dockerfile，见项目中的Dockerfile文件，内容如下：</p><pre><code>  #### Stage 1: Build the react application  FROM node:12.4.0-alpine as build  # Configure the main working directory inside the docker image.   # This is the base directory used in any further RUN, COPY, and ENTRYPOINT   # commands.  WORKDIR /app  # Copy the package.json as well as the package-lock.json and install   # the dependencies. This is a separate step so the dependencies   # will be cached unless changes to one of those two files   # are made.  COPY package.json package-lock.json ./  RUN npm install  # Copy the main application  COPY . ./  # Arguments  ARG REACT_APP_API_BASE_URL  ENV REACT_APP_API_BASE_URL=${REACT_APP_API_BASE_URL}  # Build the application  RUN npm run build  #### Stage 2: Serve the React application from Nginx   FROM nginx:1.17.0-alpine  # Copy the react build from Stage 1  COPY --from=build /app/build /var/www  # Copy our custom nginx config  COPY nginx.conf /etc/nginx/nginx.conf  # Expose port 3000 to the Docker host, so we can access it   # from the outside.  EXPOSE 80  ENTRYPOINT [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]</code></pre><p>但是针对自己的服务，只需要构建运行时状态，不需要构建mvn的设置。通用的简化版的Dockerfile的配置如下：</p><pre><code>  #### Serve the Vue application from Nginx   FROM nginx:1.17.0-alpine  # Copy the react dist from Stage 1  COPY dist/ /var/www  # Copy our custom nginx config  COPY nginx/ /etc/nginx/  # Expose port 3000 to the Docker host, so we can access it   # from the outside.  EXPOSE 80  ENTRYPOINT [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]</code></pre></li></ul></li></ul></li></ul><pre><code>* 配置信息动态化--nacos配置中心配合修改基础配置</code></pre><h3 id="jenkins执行jobs"><a href="#jenkins执行jobs" class="headerlink" title="jenkins执行jobs"></a>jenkins执行jobs</h3><ul><li><p>触发策略</p><ul><li>定时触发</li><li>提交触发</li><li>URL触发</li></ul></li><li><p>日志输出和查看</p></li><li><p>pipeline构建与传统的流程构建</p></li></ul><h2 id="后续规划"><a href="#后续规划" class="headerlink" title="后续规划"></a>后续规划</h2><ol><li>各微服务部署方式和部署体系</li><li>k8s体系–服务编排与治理</li></ol><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol><li><p>服务器上的docker设置，问题：<br> Error response from daemon: Get <a href="https://10.0.11.11:5000/v2/" target="_blank" rel="noopener">https://10.0.11.11:5000/v2/</a>: http: server gave HTTP response to HTTPS client</p><p> 需要添加<em>insecure-registry</em>设定，添加完成后需要重启docker服务。详情需要见：<strong>2. docker配置远程仓库</strong></p></li><li><p>关于jenkins的备份策略</p><p> thinBackup的使用。</p></li><li><p>玄学问题：jenkins重启后jobs丢失</p></li><li><p>Dockerfile的编写</p></li><li><p>jenkins中的各配置项含义解析</p></li><li><p>获取jenkins镜像的root权限</p></li></ol><p>执行命令：</p><pre><code>    docker exec -u 0 -it cbbc bash</code></pre><p>即可以root用户登录docker镜像。</p><ol start="7"><li><p>镜像时钟同步问题</p><pre><code> // 先以root用户登录到docker镜像中 $ docker exec -u 0 -it cbbc bash // 然后添加链接信息 bash-4.2# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  // 查看日期信息 bash-4.2# date  2017年 09月 20日 星期三 20:54:35 CST // 输出日期正常</code></pre></li><li><p>gitlab的备份和恢复</p></li></ol><p>见<a href="/2020/01/18/gitlab-bei-fen/" title="gitlab备份">gitlab备份</a></p><ol start="9"><li><p>docker镜像构建原则</p><ol><li>减少镜像层数，尽量把一些功能上面的统一命令合并到一起去执行</li><li>注意清理镜像构建的中间产物，比如一些安装包安装完成后及时清理</li><li>注意优化网络请求，用一些网络比较好的开源镜像站点，在构建过程中节约时间，减少失败率</li><li>尽量去使用构建缓存，尽量把一些不变的层级或者变更较少的层级放在前面，因为不变的东西是可以进行缓存的</li><li>多阶段进行镜像构建，将我们镜像制作的目的做一个明确，把我们的构建和真正的一些产物做分离，构建就用构建的镜像去做，最终产物就打最终产物的镜像</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> CI/CD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux DevOps CI/CD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux日常运维操作</title>
      <link href="/2020/01/31/linux-ri-chang-yun-wei-cao-zuo/"/>
      <url>/2020/01/31/linux-ri-chang-yun-wei-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux系统初始化以及部署"><a href="#Linux系统初始化以及部署" class="headerlink" title="Linux系统初始化以及部署"></a>Linux系统初始化以及部署</h1><h2 id="系统版本"><a href="#系统版本" class="headerlink" title="系统版本"></a>系统版本</h2><ol><li>系统版本：CentOS 7.6</li><li>内核版本：3.10.0-957.27.2.el7.x86_64</li><li>虚拟机信息：Vmware Exsi 6.5</li></ol><h2 id="当前进度"><a href="#当前进度" class="headerlink" title="当前进度"></a>当前进度</h2><ul><li>未完成消息队列的编写</li><li>未完成Redis集群搭建的编写</li><li>未完成mongodb集群搭建的编写</li></ul><h2 id="系统初始化"><a href="#系统初始化" class="headerlink" title="系统初始化"></a>系统初始化</h2><p>首先进行镜像的安装，使用CentOS 7.6的镜像进行安装，采用最小化安装的方式（无图形化界面、不联网更新）。安装完成后进行系统初始化之前，确保机器可以配置外网连接，如果需要内网使用，请选择基本系统安装方式（安装方式说明见附录）。</p><p>注意：</p><pre><code>1. “#” 代表在root用户下执行命令，“$” 代表在常规用户下执行命令。2. 如果处于“$”常规用户下，可以使用sudo命令进行提权操作。3. 所有命令使用man查看帮助，例如 man ls 查看ls命令的使用方式。某些命令还可以使用--help项查看帮助信息，例如git --help，查看git命令的相关使用方式。4. 文中所有的服务器ip均为假ip信息，需要根据自己的需要进行更换。</code></pre><h3 id="0-0-查看基础配置信息"><a href="#0-0-查看基础配置信息" class="headerlink" title="0.0 查看基础配置信息"></a>0.0 查看基础配置信息</h3><ul><li><p>cpu查看</p></li><li><p>内存查看 </p></li><li><p>硬盘查看</p></li><li><p>系统信息获取</p></li></ul><h3 id="0-网络配置"><a href="#0-网络配置" class="headerlink" title="0. 网络配置"></a>0. 网络配置</h3><ul><li><p>设置公网ip或者是内网可访问的地址</p><pre><code>  // 1. 先查看机器的网卡信息  # ip addr  1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000  ...  2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000  ...  // 2. 修改网卡的配置文件，对应上方的ens-192  # vi /etc/sysconfig/network-scripts/ifcfg-ens192   // 按照以下配置修改  TYPE=Ethernet  PROXY_METHOD=none  BROWSER_ONLY=no  BOOTPROTO=static                // 配置为静态ip  DEFROUTE=yes  IPV4_FAILURE_FATAL=no  IPV6INIT=yes  IPV6_AUTOCONF=yes  IPV6_DEFROUTE=yes  IPV6_FAILURE_FATAL=no  IPV6_ADDR_GEN_MODE=stable-privacy  NAME=ens192  UUID=1df6ac77-3294-42c8-b2af-fd81ce6bb5fc  DEVICE=ens192  ONBOOT=yes                      // 配置开机启动  IPADDR=10.0.11.156              // 配置ip地址  NETMASK=255.255.255.0           // 配置子网掩码  GATEWAY=10.0.11.254             // 配置网关  DNS=202.102.152.3               // 配置DNS信息  ZONE=public  // 3. 重启网络服务  # systemctl restart network  // 4. 查看修改结果  # ip addr  1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000  ...  2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000  link/ether 00:0c:29:73:f8:de brd ff:ff:ff:ff:ff:ff  inet 10.0.11.156/24 brd 10.0.11.255 scope global noprefixroute ens192  valid_lft forever preferred_lft forever  // 此时表示网络设置完成</code></pre></li></ul><p>如果网络依旧不能使用，例如ping baidu.com不通，但是ping 114.114.114.114成功，这时需要设置DNS服务器地址。</p><pre><code>    // 编辑/etc/resolv.conf，如果是新安装的机器，这个文件的内容可能为空    # vim /etc/resolv.conf    // 添加下面的内容：    nameserver 114.114.114.114 // (电信的DNS)    nameserver 8.8.8.8 //（googel的DNS）    nameserver 1.1.1.1    nameserver 223.5.5.5 // 阿里的公共DNS    nameserver 223.6.6.6    nameserver 202.102.152.3 // 山东网通的dns    // :wq保存文件    // 重启network服务    # systemctl restart network</code></pre><h3 id="1-软件源配置"><a href="#1-软件源配置" class="headerlink" title="1. 软件源配置"></a>1. 软件源配置</h3><ul><li><p>更新源</p><pre><code>  // 0. 转到软件路径下  # cd /etc/yum.repos.d/  // 1. 备份一下当前的repo文件  # mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak  // 2. 下载yum源，目前这里使用163的  # wget http://mirrors.163.com/.help/CentOS7-Base-163.repo  // 可以使用aliyun的yum源，不需要执行下面修改名称的操作  # wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo  // 3. 执行替换操作  # mv CentOS7-Base-163.repo CentOS7-Base.repo  // 4. 将服务器上的软件包信息 现在本地缓存,以提高 搜索 安装软件的速度  # yum makecache  // 等待更新完成即可进行更新系统操作  // 5. 安装epel源---常用工具安装必备  # yum install -y epel-release  // 6. 安装sysstat---排查工具  # yum install -y sysstat  // 7. 安装httpd---添加压测命令ab  # yum install httpd-tools -y</code></pre></li><li><p>更新系统</p><pre><code>  // 使用update命令更新  // 总的来说更新（update）和升级（upgrade）是相同的，除了事实上 升级 = 更新 + 更新时进行废弃处理。  # yum update &amp;&amp; yum upgrade  // 注意：为了避免更新内核造成启动不稳定的问题，update时尽量避免更新内核相关组件，如下：  # yum update --exclude=kernel*  // 或者直接修改/etc/yum.conf文件，追加  # cat /etc/yum.conf &gt;&gt; EOF &lt;   exclude=kernel*  exclude=centos-release*</code></pre></li><li><p>包管理工具常用命令</p><pre><code>  // 搜索软件包  # yum search 软件包     // 安装软件包  # yum install 软件包    // 移除软件包  # yum remove 软件包  //更新系统  # yum update  </code></pre></li><li><p>常用工具安装</p><pre><code>  // 1. 安装 Wget  # yum install -y wget  // 2. 安装git  # yum install -y git  // 3. 安装unzip  # yum install -y unzip  // 4. 网络工具箱  # yum install -y net-tools   // 5. curl工具  # yum install -y curl  // 6. vim编辑器，比vi工具更现代  # yum install -y vim   // 7. telnet工具  # yum install -y telnet   // 8. 安装pstree工具集  // psmisc包含三个帮助管理/proc目录的程序。  // fuser 显示使用指定文件或者文件系统的进程的PID。  // killall 杀死某个名字的进程，它向运行指定命令的所有进程发出信号。  // pstree 树型显示当前运行的进程。  # yum install -y psmisc  // 9. 安装更直观的htop来替代top命令  # yum install -y htop  // 10. 安装sshpass用于shell脚本中传递密码信息  # yum install -y sshpass  // 11. 安装tree实现文件夹下批量展示  # yum install -y tree  // 12. 安装jq可以对json数据进行分片、过滤、映射和转换，  # yum install -y jq  // 13. 安装yum工具  # yum install -y yum-utils  // 14. jenkins所在机器需要安装  # yum install -y autoconf automake libtool nasm libpng libpng-devel  // 15. patch工具  # yum install -y patch gcc pcre pcre-devel zlib zlib-devel openssl-devel openssl</code></pre><p>  注意：“-y”代表同意安装该程序，无需在安装时确定</p><p>  一条命令安装如下：</p><pre><code>  # yum install -y wget git unzip net-tools curl vim telnet psmisc  tree sshpass jq yum-utils</code></pre></li></ul><h3 id="2-用户配置"><a href="#2-用户配置" class="headerlink" title="2. 用户配置"></a>2. 用户配置</h3><ul><li><p>添加用户</p><pre><code>  // 创建用户的命令, -m表示创建home目录信息，centos表示常规使用的用户  # useradd -m centos   // 设定密码  # passwd centos  // 创建用户的命令, -m表示创建home目录信息，ftpuser表示常规使用的用户  # useradd -m ftpuser   // 设定密码  # passwd ftpuser</code></pre></li></ul><ul><li><p>用户权限（非测试环境不允许启用root权限）</p><pre><code>  // 使用vim修改文件  # vim /etc/sudoers  ##  ## The COMMANDS section may have other options added to it.  ##  ## Allow root to run any commands anywhere  root    ALL=(ALL)       ALL  #centos  ALL=NOPASSWD:/usr/libexec/openssh/sftp-server  centos  ALL=(ALL)       ALL    # 添加该行信息</code></pre></li><li><p>修改主机名称</p><pre><code>  // 变更服务器hostname  # hostnamectl set-hostname your-own-machine-name</code></pre></li><li><p><strong>注意</strong>：一定不要在生产环境启用root权限，原则上不允许在线上环境切换到root用户执行命令。</p></li></ul><h3 id="3-时钟同步（可访问外网的情况下启用）"><a href="#3-时钟同步（可访问外网的情况下启用）" class="headerlink" title="3. 时钟同步（可访问外网的情况下启用）"></a>3. 时钟同步（可访问外网的情况下启用）</h3><p>使用ntpdate+crontab来完成时钟同步。</p><pre><code>    // 0. 查看时钟是否正确    $ date    // 1. 首先安装ntpdate    # yum install -y ntpdate    // 2. 调用ntpdate命令进行时钟修正，确保该命令可用    # ntpdate -u pool.ntp.org    // 3. 最后，集成定时任务    # crontab -e    // 类vim编辑页面，输入i添加下面的内容，表示每10分钟执行一次定时任务，同步时钟信息。    */10 * * * * /usr/sbin/ntpdate pool.ntp.org &gt; /dev/null 2&gt;&amp;1    // 点击Esc按钮，退出编辑，输入:wq进行保存。此时定时任务已经设置完成。    // 4. 重启定时任务服务    # systemctl restart crond    // 5. 查看定时任务执行情况，参考下面命令：    tail -f /var/log/cron</code></pre><p>参考链接：<a href="https://www.cnblogs.com/frankdeng/p/9005691.html" target="_blank" rel="noopener">https://www.cnblogs.com/frankdeng/p/9005691.html</a></p><h3 id="4-防火墙设置（默认不允许关闭）"><a href="#4-防火墙设置（默认不允许关闭）" class="headerlink" title="4. 防火墙设置（默认不允许关闭）"></a>4. 防火墙设置（默认不允许关闭）</h3><p>此处的防火墙为CentOS系统默认的防火墙服务firewalld，使用firewall-cmd来进行设置端口的开放和关闭。系统初始使用时，默认只开放21（ftp服务）、22（ssh）、80（http web服务）、443（https）等端口。</p><pre><code>    // 1. 首先查看firewalld服务是否已经打开    # systemctl status firewalld    // 2. 查看目前已选的区信息    # firewall-cmd --get-default-zone     // 一般已启用的区信息为public    // 3. 查看目前已经开放的端口信息    # firewall-cmd --list-all     // 或者使用下面的命令    # firewall-cmd --zone=public --list-ports    // 4. 设置要开放的端口，--permanent代表永久生效    # firewall-cmd --zone=public --add-port=5000/tcp --permanent    // 5. 设置要开放的端口段，例如19000-19999    # firewall-cmd --zone=public --add-port=4990-4999/tcp --permanent    // 6. 关闭已经设置的端口    # firewall-cmd --zone=public --remove-port=5000/tcp --permanent    // 7. 重新加载，当设置完端口后，使用该命令生效    # firewall-cmd --reload    // 8. 重启firewalld服务    # systemctl restart firewalld    // 9. 禁用firewalld服务    # systemctl disable firewalld    // 10. 启用firewalld服务    # systemctl start firewalld</code></pre><h3 id="5-ftp服务设置（可选）"><a href="#5-ftp服务设置（可选）" class="headerlink" title="5. ftp服务设置（可选）"></a>5. ftp服务设置（可选）</h3><pre><code>    // 使用yum安装vsftpd    # yum install -y vsftpd     // 启动 FTP 服务    # systemctl start vsftpd    // 对ftp服务进行配置，配置信息位于/etc/vsftpd/目录下    # vim /etc/vsftpd/vsftpd.conf    // 配置文件中找到下面两行    // 禁用匿名用户  12 YES 改为NO    anonymous_enable=NO    // 禁止切换根目录 101 行 删除#    chroot_local_user=YES     // 更换登录端口    listen=NO    listen_port=2231    // :wq保存配置信息    // 重启服务    # systemctl restart vsftpd    // 防火墙开启2231端口对外访问    # firewall-cmd --zone=public --add-port=2231/tcp --permanent    # firewall-cmd --reload    // 添加ftp用户    # useradd ftpuser    // 设置密码，如果需要自己设定请替换引号内的信息    # echo &quot;testftp&quot; | passwd ftpuser --stdin    // 限制ftpuser只能通过ftp服务访问    # usermod -s /sbin/nologin ftpuser    // 为用户分配主目录    // /home/ftpuser/ 为主目录, 该目录不可上传文件    // /home/ftpuser/pub 文件只能上传到该目录下（目前设定的是，直接传输到/home/ftpuser/目录下）    // 创建目录结构    # mkdir -p /home/ftpuser/pub    // 设置访问权限    # chmod a-w /home/ftpuser &amp;&amp; chmod 777 -R /home/ftpuser/pub    // 设置为用户主目录    # usermod -d /home/ftpuser ftpuser    // 重启下ftp服务    # systemctl restart vsftpd这样ftp服务就设置完成了，可以使用[WinSCP](https://winscp.net/eng/docs/lang:chs)登录或者使用[FileZilla](https://filezilla-project.org/)登录。</code></pre><h3 id="6-ssh服务设置"><a href="#6-ssh服务设置" class="headerlink" title="6. ssh服务设置"></a>6. ssh服务设置</h3><pre><code>    // 首先，确保服务器已安装openssh-server，命令如下：    // 1. 判断ssh服务已安装    # yum list installed | grep openssh-server    // 如果有以下输出，说明已经安装相关组件    openssh-server.x86_64                7.4p1-21.el7                   @base      // 2. 若未安装，首先安装openssh-server    # yum install -y openssh-server    // 3. 找到sshd的配置文件    # cd /etc/ssh    # vim ./sshd_config    // 3.1 修改登录端口为15555, 允许任意ip远程登录，如果是内网机器，则不要设置任意ip远程登录    Port 15555    ListenAddress 0.0.0.0    ListenAddress ::    // 3.2 设置不允许root用户远程登录    PermitRootLogin no    // 3.3 开启使用用户名密码来作为连接验证    PasswordAuthentication yes    // 3.4 加速ssh连接速度    GSSAPIAuthentication no    // 如果这句没有需要自己添加，如果被注释请放开注释    UseDNS no    // 设置完成后, :wq 退出编辑    // 4. 重启ssh服务，重启网络，并设置开机启动sshd，依次执行下面的命令    # systemctl restart sshd    # systemctl restart network    # systemctl enable sshd    // 5. 防火墙开启访问信息    # firewall-cmd --zone=public --add-port=15555/tcp --permanent    # firewall-cmd --reload这样ssh服务就设置完毕了，我们在前面已经创建了**centos**用户，因此在客户机上尝试远程连接服务器。以Windows系统为例，使用[Cmder](https://cmder.net/)作为终端工具。打开Cmder终端工具，输入以下信息：    ssh -p 15555 centos@10.0.11.11其中**-p*表示指定端口号，*centos*就是我们在前面设置的用户名，**10.0.11.11*需要替换为你自己的真实ip地址。在出现输入密码的位置，输入用户对应的密码信息，回车即可登录服务器。另外，在重启sssh服务的时候，可能会报错，如下：    # systemctl restart sshd    Job for sshd.service failed because the control process exited with error code. See &quot;systemctl status sshd.service&quot; and &quot;journalctl -xe&quot; for details.    // 执行后无法启动，查看报错信息    # journalctl -xe    error: Bind to port 522 on 0.0.0.0 failed: Permission denied.    error: Bind to port 522 on :: failed: Permission denied.    fatal: Cannot bind any address.    // 当端口设置完成后，需要在selinux设置允许该端口作为ssh的连接端口    // 首先安装管理工具    # yum install -y policycoreutils-python    // 开放该端口使用    # semanage port -a -t ssh_port_t -p tcp 522    // 防火墙开启访问信息    # firewall-cmd --zone=public --add-port=552/tcp --permanent    # firewall-cmd --reload上边552代表你在/etc/ssh/sshd_config中设置的端口号。参考地址：https://wildwolf.name/centos-7-how-to-change-ssh-port/</code></pre><h3 id="7-sshd进程提升优先级"><a href="#7-sshd进程提升优先级" class="headerlink" title="7. sshd进程提升优先级"></a>7. sshd进程提升优先级</h3><p>建议使用 <strong>nice</strong> 命令或者<strong>renice</strong>命令将 sshd 进程的优先级调高，这样当系统内存紧张时，还能勉强登陆服务器进行调试，然后分析故障。操作如下：</p><pre><code>// 找到该进程信息，注意查找真正的进程，根据/usr/sbin/sshd -D获取$ ps -aux | grep sshd// 展示下面的信息root      2567  0.0  0.0 158936  5684 ?        Ss   08:34   0:00 sshd: centos [priv]centos    2601  0.0  0.0 158936  2452 ?        S    08:35   0:00 sshd: centos@pts/0root      2994  0.0  0.0 158936  5684 ?        Ss   08:51   0:00 sshd: centos [priv]centos    3000  0.0  0.0 158936  2452 ?        S    08:51   0:00 sshd: centos@pts/1centos    4474  0.0  0.0 112712   964 pts/1    S+   09:36   0:00 grep --color=auto sshdroot     23773  0.0  0.0 112920   460 ?        Ss   Mar11   0:00 /usr/sbin/sshd -D// 查看进程优先级$  ps ax -o pid,nice,comm | grep sshd 2567   0 sshd 2601   0 sshd 2994   0 sshd 3000   0 sshd23773   0 sshd// 修改第一个进程的id信息，将优先级提升到最高// 修改的是/usr/sbin/sshd -D启动命令所在的进程$ sudo renice -20 -p 23773// 查看进程优先级$  ps ax -o pid,nice,comm | grep sshd 2567   0 sshd 2601   0 sshd 2994   0 sshd 3000   0 sshd23773 -20 sshd// 退出后重新登录，再查看$ ps aux | grep sshdroot      2567  0.0  0.0 158936  5684 ?        Ss   08:34   0:00 sshd: centos [priv]centos    2601  0.0  0.0 158936  2452 ?        S    08:35   0:00 sshd: centos@pts/0root      4944  0.5  0.0 158936  5684 ?        S&lt;s  09:40   0:00 sshd: centos [priv]centos    4959  0.0  0.0 158936  2300 ?        S&lt;   09:41   0:00 sshd: centos@pts/1centos    4997  0.0  0.0 112716   960 pts/1    S&lt;+  09:41   0:00 grep --color=auto sshdroot     23773  0.0  0.0 112920   460 ?        S&lt;s  Mar11   0:00 /usr/sbin/sshd -D$ ps ax -o pid,nice,comm | grep sshd2567   0 sshd2601   0 sshd4944 -20 sshd4959 -20 sshd23773 -20 sshd</code></pre><p>可以看到的是sshd进程中，出现<strong>S&lt;s</strong>，”&lt;”代表高优先级进程标志，说明测试成功。</p><p>参考链接：</p><ul><li>关于进程相关命令的说明：<a href="https://www.cnblogs.com/alongdidi/p/linux_process.html" target="_blank" rel="noopener">https://www.cnblogs.com/alongdidi/p/linux_process.html</a></li></ul><h3 id="8-内核升级和优化（非必选）"><a href="#8-内核升级和优化（非必选）" class="headerlink" title="8.内核升级和优化（非必选）"></a>8.内核升级和优化（非必选）</h3><p>以内核升级到4.19版本为例，进行操作，需要在root用户下操作。</p><p>首先更新并重启机器，需要排除内核信息</p><pre><code># yum update -y --exclude=kernel* &amp;&amp; reboot</code></pre><p>在生产环境中必须进行内核升级，如果需要安装docker，强烈建议将CentOS 7的内核版本升级到4.18+。 这里使用4.19版本</p><pre><code># cd /root# wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm# wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm</code></pre><p>可以先下载后再上传到服务器上，如果有多个服务器，可以使用下面的示例命令，传输到各个服务器上。</p><pre><code>// 将内核文件传输到各个机器上# for i in k8s-master-02 k8s-master-03 k8s-node-01 k8s-node-02; do scp kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm $i:/root/ ;done</code></pre><p>开始安装内核信息，在服务器上执行以下命令，如下：</p><pre><code># cd /root/ &amp;&amp; yum localinstall kernel-ml* -y</code></pre><p>安装完成后，需要更改内核启动顺序，以便启动时自动选择最新的内核版本信息。</p><pre><code># grub2-set-default 0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfg# grubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;</code></pre><p>最后检查内核版本信息。</p><pre><code># grubby --default-kernel// 输出以下版本信息/boot/vmlinuz-4.19.12-1.el7.elrepo.x86_64</code></pre><p>所有节点重启，再检查内核信息。</p><h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><p>前提信息：区分使用原生安装还是docker安装</p><ol><li><p>基础设施</p><ul><li><p>JDK安装–OpenJDK、OracleJDK</p><p>  首先介绍OracleJDK的安装，生产环境中使用的是OracleJDK，版本为1.8.0_202。预设前提是，已经将OracleJDK的安装包下载完毕了。第一步需要判断服务器上是否安装过jdk，执行下面的命令：</p><pre><code>  # yum list installed | grep openjdk</code></pre><p>  如果已经安装过jdk，例如镜像内已经含有openjdk，需要先移除已安装的jdk。执行下面的命令：</p><pre><code>  # yum remove java-1.8.0-openjdk</code></pre><p>  第二步，通过ftp的方式将OracleJDK安装文件传入服务器，可以使用图形化工具，例如<a href="https://winscp.net/eng/index.php" target="_blank" rel="noopener">WinSCP</a>、<a href="https://filezilla-project.org/" target="_blank" rel="noopener">FileZilla</a>，这里使用命令行的形式传输，打开Cmder工具，输入以下命令：</p><pre><code>  scp -P 15555 jdk-8u202-linux-x64.tar.gz ftpuser@10.0.11.11:/home/ftpuser</code></pre><p>  这样就将OracleJDK的安装包传入到服务器了。第三部开始解压，并创建相关目录。</p><pre><code>  // 将安装文件解压  $ tar -zxvf jdk-8u202-linux-x64.tar.gz  // 创建安装目录  $ sudo mkdir /usr/local/java/  // 将解压后的文件夹拷贝到安装目录  $ sudo cp -r jdk1.8.0_202/ /usr/local/java/</code></pre><p>  第四步，开始配置环境变量，用vim打开/etc/profile，进行设置</p><pre><code>  $ sudo vim /etc/profile  // 在文档末尾添加  # JAVA env  JAVA_HOME=/usr/local/java/  JRE_HOME=$JAVA_HOME/jre  CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib  PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin  export JAVA_HOME JRE_HOME CLASS_PATH PATH  // 最后输入:wq保存文档  // 使环境变量生效  $ sudo source /etc/profile  // 添加软链接  $ sudo ln -s /usr/local/java/jdk1.8.0_202/bin/java /usr/bin/java  $ sudo ln -s /usr/local/java/jdk1.8.0_202/bin/javac /usr/bin/javac</code></pre><p>  这样OracleJDK就安装完成了，可以执行以下命令进行检查。</p><pre><code>  $ java -version  $ javac -version</code></pre><p>  如果均能输出java的版本信息，说明安装成功。</p><p>  如果是要安装openjdk的话，直接使用下面的命令安装即可：</p><pre><code>  $ sudo yum install -y java-1.8.0-openjdk.x86_64</code></pre><p>  如果是多版本jdk存在，使用alternatives命令进行管理和切换。<a href="https://blog.csdn.net/waplys/article/details/98478247" target="_blank" rel="noopener">参考下面的链接</a>。</p></li></ul></li></ol><ul><li><p>docker、docker-compose安装</p><p>  安装的docker版本为19.0.3，安装的docker-compose版本为1.24.1。依据下面的命令来安装docker：</p><pre><code>  // 确定自己服务器的内核版本  # uname -r</code></pre><p>  确定</p><pre><code>  // 更新当前的软件源  # yum update  // 卸载旧版本（如果以前安装过旧版本），并清除之前的docker存档(如果需要的话)  # yum remove docker docker-common docker-selinux docker-engine  # rm -r /var/lib/docker  // 安装需要的软件包依赖  # yum install -y yum-utils \              device-mapper-persistent-data \              lvm2  // 添加repo信息  # yum-config-manager \          --add-repo \          https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  // 更新软件源缓存  # yum makecache fast  // 查找软件源中的docker版本  # yum list docker-ce.x86_64 --showduplicates | sort -r  // 安装最新的稳定版本的docker或者安装指定版本的docker，执行一条命令即可  # yum install -y docker-ce  // 配置docker服务以及设置开机启动  # systemctl start docker  # systemctl enable docker  // 拉取hello-world镜像测试docker命令是否可用  # docker pull hello-world  # docker run hello-world</code></pre><p>  如果能够正常输出hello-world，则说明docker安装成功。</p><p>  安装完成后，需要配置当前用户在不需要root权限的情况下可以正常使用docker的各项命令。例如我们添加centos用户到docker可执行的组中。配置命令如下：</p><pre><code>  $ sudo groupadd docker  $ sudo usermod -aG docker ${USER}  (或者执行 sudo gpasswd -a ${USER} docker)  $ sudo systemctl restart docker  // 建议这时候退出重新登录下，断开ssh连接或者重启下服务器  // 否则可能在执行docker pull命令的时候报错：Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/images/create?fromImage=hello-world&amp;tag=latest: dial unix /var/run/docker.sock: connect: permission denied  # reboot  // 这时候也可以尝试，退出ssh，重新使用ssh登录服务器  // 如果不重启服务器，可以执行下面的命令解决  $ sudo setfacl --modify user:centos:rw /var/run/docker.sock  // 更新用户组  $ newgrp docker</code></pre><p>  这样docker就全部安装完成了。可以在用户centos环境下，测试</p><pre><code>  $ docker run hello-world</code></pre><p>  如果可以正常输出hello-world，则说明成功。</p><p>  另外，可能因为某些原因无意间执行了yum update或者apt-get -y upgrade;导致Docker版本升级。为了避免此类问题发生，建议在安装好Docker后对Docker软件进行锁定，防止Docker意外更新。</p></li></ul><pre><code>    // 安装yum-plugin-versionlock插件    # yum install yum-plugin-versionlock -y    // 锁定软件包    # yum versionlock add docker-ce docker-ce-cli    // 查看已锁定的软件包    # yum versionlock list    // 解锁指定的软件包    # yum versionlock delete &lt;软件包名称&gt;    // 解锁所有的软件包    # yum versionlock clear需要注意的是，centos系统的内核版本也尽可能进行锁定，防止yum update执行时，被更新。最后，安装docker-compose，确保前面已经安装好docker了！    $ sudo curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose    $ sudo chmod +x /usr/local/bin/docker-compose       $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</code></pre><ol start="2"><li>数据库</li></ol><ul><li><p>MySQL安装</p><p>  MySQL版本选择5.7.28。分别演示使用rpm安装和docker安装。安装之前，首先需要检查系统中是否存在MariaDB，并删除MariaDB。</p><pre><code>  # yum list installed | grep mariadb  mariadb-libs-x86_64                  ........................................  // 删除MariaDB  # yum remove -y mariadb*</code></pre><p>  由于是测试环境，所以先使用docker安装MySQL镜像。</p><pre><code>  // 拉取docker镜像  $ docker pull mysql:5.7.28  // 启动docker镜像  $ docker run --restart=always --name mysql5.7 -v /data/mysql5.7-data:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456  -d mysql:5.7.28</code></pre><p>  当MySQL的docker镜像启动后，登录的用户名默认为root，密码设定为123456。这样启动后，需要打开3306端口，允许外部连接。执行如下：</p><pre><code>  # firewall-cmd --zone=public --add-port=15555/tcp --permanent  # firewall-cmd --reload</code></pre><p>  这样就可以通过<a href="https://www.navicat.com.cn/" target="_blank" rel="noopener">Navicat</a>等工具访问MySQL数据库了。</p><p>  如果选择使用软件源的方式安装，需要进行以下操作：</p><pre><code>  // 下载MySQL的YUM源  $ wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm  // 安装 MySQL 的 YUM 源  #  rpm -ivh mysql57-community-release-el7-11.noarch.rpm  // 检查 MySQL 的 YUM 源是否安装成功  # yum repolist enabled | grep &quot;mysql.*-community.*&quot;  // 加入缓存信息  # yum makecache fast  // 如果结果中出现mysql57-community/x86_64的信息，说明安装成功  // 查看MySQL版本  # yum repolist all | grep mysql  // 安装MySQL，这样安装并不是安装5.7.25版本，有可能是5.7.31这样的版本  # yum install -y mysql-community-server   // 启动MySQL服务  # systemctl start mysqld  // 远程访问 MySQL，需要开放 3306 端口：  # firewall-cmd --permanent --zone=public --add-port=3306/tcp  # firewall-cmd --permanent --zone=public --add-port=3306/udp  # firewall-cmd --reload</code></pre><p>  由此MySQL就安装完成了，这时候需要对MySQL进行连接。</p><pre><code>  $ mysql -u root -p </code></pre></li></ul><p>注意事项:</p><ol start="0"><li>关于MySQL在安装后修改初始化密码出现的问题解决：</li></ol><pre><code>$ mysql -u root -pEnter password:Welcome to the MySQL monitor.  Commands end with ; or \g.Your MySQL connection id is 4Server version: 5.7.25Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.mysql&gt; UPDATE mysql.user SET authentication_string=PASSWORD(&#39;htMySQL789&#39;) where USER=&#39;root&#39;;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.mysql&gt; show databases;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.mysql&gt; ALTER USER USER() IDENTIFIED BY &#39;htMySQL789&#39;;ERROR 1819 (HY000): Your password does not satisfy the current policy requirementsmysql&gt; SHOW VARIABLES LIKE &#39;validate_password%&#39;;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.mysql&gt; set global validate_password_policy=LOW;Query OK, 0 rows affected (0.00 sec)mysql&gt; ALTER USER USER() IDENTIFIED BY &#39;htMySQL789&#39;;Query OK, 0 rows affected (0.00 sec)mysql&gt; exitBye</code></pre><ol><li><p>针对rpm+yum安装，刚安装的 MySQL 是没有密码的，这时如果出现：</p><p> ERROR 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: NO)，解决如下：</p><pre><code> // ① 停止 MySQL 服务 # systemctl stop mysqld  // ② 以不检查权限的方式启动 MySQL # mysqld --user=root --skip-grant-tables &amp; // ③ 再次输入  # mysql -u root 或者 mysql // ④ 更新密码： // MySQL 5.7 以下版本： mysql&gt; UPDATE mysql.user SET Password=PASSWORD(&#39;123456&#39;) where USER=&#39;root&#39;; // MySQL 5.7 版本： mysql&gt; UPDATE mysql.user SET authentication_string=PASSWORD(&#39;123456&#39;) where USER=&#39;root&#39;; // ⑤ 刷新，使之前的修改生效： mysql&gt; flush privileges; // ⑥ 退出： mysql&gt; exit;</code></pre></li></ol><p>设置完之后，输入 mysql -u root -p，这时输入刚设置的密码，就可以登进数据库了。</p><ol start="2"><li>针对rpm+yum安装，需要设置允许远程访问</li></ol><p>默认情况下 MySQL 是不允许远程连接的，所以在 Java 项目或者 MySQLWorkbench 等数据库连接工具连接服务器上的 MySQL 服务的时候会报 “Host ‘x.x.x.x’ is not allowed to connect to this MySQL server”。可以通过下面的设置解决。详细可以参考之前写的一篇文章 XXX is not allowed to connect to this MySQL server。</p><pre><code>// 授权操作, &#39;0&#39;需要改为之前设置的MySQL密码mysql&gt; grant all privileges on *.* to root@&quot;%&quot; identified by &#39;0&#39;;mysql&gt; flush privileges;</code></pre><ol start="3"><li>错误信息：ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.</li></ol><p>首先登录MySQL，然后进行修改密码的操作，常见于第一次登录的时候，如下：</p><pre><code>$ mysql -u root -p Enter password:mysql&gt;  set global validate_password_policy=0;mysql&gt;  alter user &#39;root&#39;@&#39;localhost&#39; identified by &#39;jy@MySQL246&#39;;mysql&gt;  flush privileges;</code></pre><ul><li><p>MyCat安装以及高可用</p></li><li><p>MongoDB安装</p></li></ul><p>通过docker的方式来安装MongoDB测试服务器，版本为3.4。操作如下:</p><pre><code>// 拉取docker镜像$ docker pull mongo:3.4// 运行镜像$ docker run --name mongod -p 27017:27017 -d mongo:3.4 --auth// 查看已有的镜像信息$ docker ps -a // 进入mongoDB设置登录用户信息$ docker exec -it &lt;镜像的md5信息&gt; mongo admin// 在下面的&gt;后填写以下命令创建用户&gt; db.createUser({user:&quot;root&quot;,pwd:&quot;root&quot;,roles:[{role:&#39;root&#39;,db:&#39;admin&#39;}]})</code></pre><p>这时MongoDB就部署完成了。</p><p>如果是通过软件源安装，这里配置的是4.2版本的mongodb。步骤如下：</p><pre><code>// 添加软件源$ sudo vim /etc/yum.repos.d/mongodb-org-4.2.repo// 文件中输入以下信息[mongodb-org-4.2]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.2/x86_64/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-4.2.asc// :wq 保存文件// 更新源信息$  sudo yum update -y// 开始安装mongodb$  sudo yum install -y mongodb-org// 启动已经安装的mongo服务$ sudo systemctl start mongod$ sudo systemctl enable mongod// 针对mongodb进行设置// 1. 进入mongodb的shell操作空间$ mongo// 2. 切换数据库&gt; use admin// 3. 添加管理员用户&gt; db.createUser({         user: &quot;admin&quot;,          customData：{description:&quot;superuser&quot;},        pwd: &quot;admin&quot;,          roles: [ { role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; } ]      })  // 4. 退出，以管理员身份重新登录&gt; quit()$ mongo -u admin -p --authenticationDatabase admin// 输入密码// 5. 使用指定的库&gt; use db001// 6. 对需要操作的库进行授权&gt; db.createUser({        user:&quot;user001&quot;,        pwd:&quot;123456&quot;,        roles:[            {role:&quot;readWrite&quot;,db:&quot;db001&quot;},            &#39;read&#39;// 对其他数据库有只读权限，对db001、db002是读写权限        ]    })// 6. 退出&gt; quit()</code></pre><p>这样用户权限就授权完毕了，现在需要对mongodb的配置文件进行修改。</p><pre><code>// 修改配置文件$ vim /etc/mongod.conf    # mongod.conf    # for documentation of all options, see:    #   http://docs.mongodb.org/manual/reference/configuration-options/    # where to write logging data.    systemLog:        destination: file        logAppend: true        path: /var/log/mongodb/mongod.log    journal:        enabled: true    #  engine:    #  wiredTiger:    # how the process runs    processManagement:        fork: true  # fork and run in background        pidFilePath: /var/run/mongodb/mongod.pid  # location of pidfile        timeZoneInfo: /usr/share/zoneinfo    # network interfaces    net:        port: 27017        # 修改为所有ip均可访问        bindIp: 0.0.0.0  # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.    # 添加授权登录的配置信息    security:        authorization: enabled// :wq 保存退出// 这时重启mongodb服务  $  sudo systemctl restart mongod</code></pre><p>这样配置就完成了，随后通过NoSQLBooster for MongoDB工具或者Studio 3T工具进行连接。需要注意的是在javaweb开发时，在配置文件中写入完整的连接字符串，如下：</p><pre><code>data:    mongodb:        uri: mongodb://mongo_ex:123456@10.0.11.12:27017/test-visualmodel  </code></pre><ul><li>MongoDB集群搭建</li></ul><ol start="3"><li>缓存Redis</li></ol><ul><li>Redis单体安装</li></ul><p>Redis版本选择Redis 4.0以上版本，因为4.0以上版本可以支持官方的cluster集群模式构建缓存集群，这里使用4.0.14版本。安装步骤如下：</p><pre><code>// 下载Redis安装包$ wget http://download.redis.io/releases/redis-4.0.14.tar.gz// 解压$ tar -zxvf redis-4.0.14.tar.gz// 进入文件夹准备安装$ cd redis-4.0.14// 安装依赖包$ sudo yum -y install gcc gcc-c++ kernel-devel// 执行编译$ make// 进行安装，确定安装目录到/usr/local/redis$ make PREFIX=/usr/local/redis install// copy配置文件redis.conf到/usr/local/redis目录下$ cp redis.conf /usr/local/redis// 修改Redis的配置文件$ vim /usr/local/redis/redis.conf    # 修改一下配置    # redis以守护进程的方式运行    # no表示不以守护进程的方式运行(会占用一个终端)      daemonize yes    # 客户端闲置多长时间后断开连接，默认为0关闭此功能      timeout 300    # 设置redis日志级别，默认级别：notice                        loglevel verbose    # 设置日志文件的输出方式,如果以守护进程的方式运行redis 默认:&quot;&quot;     # 并且日志输出设置为stdout,那么日志信息就输出到/dev/null里面去了     logfile stdout    # 设置密码授权    requirepass &lt;设置密码&gt;    # 监听ip，允许外网连接    bind 0.0.0.0// 输入:wq保存并退出编辑// 配置环境变量# vim /etc/profile// 在文件末尾进行追加export REDIS_HOME=/usr/local/redisexport PATH=$PATH:$REDIS_HOME/bin// 输入:wq保存并退出编辑// 使环境变量生效# source /etc/profile// 需要配置开机启动# cd /etc/init.d# vim redis    #!/bin/bash    #chkconfig: 2345 80 90    # Simple Redis init.d script conceived to work on Linux systems    # as it does use of the /proc filesystem.    PATH=/usr/local/bin:/sbin:/usr/bin:/bin    REDISPORT=6379    EXEC=/usr/local/redis/bin/redis-server    REDIS_CLI=/usr/local/redis/bin/redis-cli    PIDFILE=/var/run/redis.pid    CONF=&quot;/usr/local/redis/etc/redis.conf&quot;    case &quot;$1&quot; in        start)            if [ -f $PIDFILE ]            then                    echo &quot;$PIDFILE exists, process is already running or crashed&quot;            else                    echo &quot;Starting Redis server...&quot;                    $EXEC $CONF            fi            if [ &quot;$?&quot;=&quot;0&quot; ]             then                echo &quot;Redis is running...&quot;            fi            ;;        stop)            if [ ! -f $PIDFILE ]            then                    echo &quot;$PIDFILE does not exist, process is not running&quot;            else                    PID=$(cat $PIDFILE)                    echo &quot;Stopping ...&quot;                    $REDIS_CLI -p $REDISPORT SHUTDOWN                    while [ -x ${PIDFILE} ]                do                        echo &quot;Waiting for Redis to shutdown ...&quot;                        sleep 1                    done                    echo &quot;Redis stopped&quot;            fi            ;;        restart|force-reload)            ${0} stop            ${0} start            ;;        *)        echo &quot;Usage: /etc/init.d/redis {start|stop|restart|force-reload}&quot; &gt;&amp;2        exit 1    esac// 输入:wq保存并退出编辑// 给脚本增加运行权限# chmod +x /etc/init.d/redis// 查看服务列表# chkconfig --list// 添加服务# chkconfig --add redis// 配置启动级别# chkconfig --level 2345 redis on// 使配置生效# systemctl daemon-reload// 启动测试# systemctl start redis   #或者 /etc/init.d/redis start  # systemctl stop redis   #或者 /etc/init.d/redis stop// 添加redis开放端口# firewall-cmd --permanent --zone=public --add-port=6379/tcp# firewall-cmd --permanent --zone=public --add-port=6379/udp# firewall-cmd --reload</code></pre><ul><li>Redis集群–哨兵模式搭建</li></ul><p>（进程内缓存使用–caffine–框架进行开发，在编码时使用）</p><ol start="4"><li><p>消息队列</p><ul><li>RocketMQ搭建</li></ul></li><li><p>依赖管理，仓库和镜像</p></li></ol><p>请参考<a href="/2020/01/31/chi-xu-ji-cheng-shi-jian-da-gang/" title="持续集成实践大纲">持续集成实践大纲</a></p><ol start="6"><li>CI/CD工具链</li></ol><p>请参考<a href="/2020/01/31/chi-xu-ji-cheng-shi-jian-da-gang/" title="持续集成实践大纲">持续集成实践大纲</a></p><ol start="7"><li>Zookeeper</li></ol><ul><li>Zookeeper安装和集群搭建</li></ul><ul><li><p>使用安装文件进行发布</p></li><li><p>所在目录：/home/centos/zookeeper-3.4.14</p></li><li><p>发布地址：<br>  10.0.11.11:21810<br>  10.0.11.12:21810<br>  10.0.11.13:21810</p></li><li><p>相关命令：</p></li></ul><ol><li>配置环境变量</li></ol><ul><li><p>在每一台部署了zookeeper的服务器上</p><pre><code>  #修改环境变量文件  vi /etc/profile  #增加以下内容  export ZOOKEEPER_HOME=/usr/zookeeper/zookeeper-3.4.11  export PATH=$ZOOKEEPER_HOME/bin:$PATH  #使环境变量生效  source /etc/profile  #查看配置结果  echo $ZOOKEEPER_HOME</code></pre></li></ul><ol start="2"><li>配置节点标识</li></ol><pre><code>    #针对zk01，也就是11服务器：    echo &quot;1&quot; &gt; /zookeeper/data/myid    #针对zk02，也就是12服务器：    echo &quot;2&quot; &gt; /zookeeper/data/myid    #针对zk03，也就是13服务器：    echo &quot;3&quot; &gt; /zookeeper/data/myid</code></pre><ol start="3"><li>配置文件修改</li></ol><ul><li><p>在每一台部署了zookeeper的服务器上</p><pre><code>  #进入ZooKeeper配置目录  cd $ZOOKEEPER_HOME/conf  #新建配置文件  vim zoo.cfg  #写入以下内容并保存  tickTime=2000  initLimit=10  syncLimit=5  dataDir=/home/centos/zookeeper-3.4.14/data  dataLogDir=/home/centos/zookeeper-3.4.14/logs  clientPort=21810  server.1=10.0.11.11:28880:38880  server.2=10.0.11.12:28880:38880  server.3=10.0.11.13:28880:38880        </code></pre></li></ul><ol start="4"><li>开放端口信息</li></ol><ul><li><p>在每一台部署了zookeeper的服务器上</p><pre><code>  sudo firewall-cmd --add-port=21810/tcp --permanent  sudo firewall-cmd --add-port=28880/tcp --permanent  sudo firewall-cmd --add-port=38880/tcp --permanent  sudo firewall-cmd --reload</code></pre></li></ul><ol start="5"><li>启动</li></ol><ul><li><p>在每一台部署了zookeeper的服务器上</p><pre><code>  #进入ZooKeeper bin目录  cd $ZOOKEEPER_HOME/bin  #启动  nohup sh zkServer.sh start &amp;</code></pre></li></ul><ul><li>配置方式：使用ZooInspector访问zookeeper服务并进行配置</li></ul><ul><li><a href="https://juejin.im/post/5ba879ce6fb9a05d16588802" target="_blank" rel="noopener">参考地址</a></li></ul><ol start="8"><li>ELK体系</li></ol><p>请参考<a href="/2020/01/15/elk-dan-ji-ri-zhi-xi-tong-da-jian/" title="ELK单机日志系统搭建">ELK单机日志系统搭建</a></p><ol start="9"><li>Docker+kubernetes体系</li></ol><ol start="10"><li>API管理工具–YApi安装</li></ol><p>启动方式，先启动MongoDB，再通过pm2 启动yapi项目</p><p>请参考<a href="http://yapi.demo.qunar.com/" target="_blank" rel="noopener">YApi官方</a>安装方式。</p><ol start="11"><li>JIRA/Confluence的安装部署</li></ol><ul><li><p>前提：需要安装docker和docker-compose，下载docker镜像包</p></li><li><p>首先启动数据库</p><pre><code>  docker pull mysql:5.7  # 启动jira的数据库  docker run --name mysql-jira --restart always -p 46001:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=jira -e MYSQL_USER=jira -e MYSQL_PASSWORD=jira -d mysql:5.7 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci  # 针对jira的数据库设置  vim /etc/mysql/conf.d/mysql.cnf  添加下面的内容：  [mysql]  default-character-set=utf8  [client]  default-character-set=uft8  # 启动confluence的数据库  docker run --name mysql-confluence --restart always -p 46003:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=confluence -e MYSQL_USER=confluence -e MYSQL_PASSWORD=confluence -d mysql:5.7 --character-set-server=utf8 --collation-server=utf8_bin  # 针对confluence的数据库的设置  vim /etc/mysql/conf.d/mysql.cnf  添加下面的内容：  [mysql]  default-character-set=utf8  [client]  default-character-set=utf8  [mysqld]  character-set-server=utf8  collation-server=utf8_bin</code></pre></li><li><p>下载需要的jira和Confluence镜像信息</p><pre><code>  git clone https://github.com/zhangguanzhang/Dockerfile.git  cd ./Dockerfile/atlassian-jira</code></pre></li></ul><ul><li><p>构建jira镜像</p><pre><code>  # 不要忘记后面的“.”，表示当前目录下进行构建  docker build -t ht-jira:v7 .   # 等待一段时间进行构建  # 查看已经构建完成的镜像  docker images   REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE</code></pre><p>ht-jira                                 v1                  c4a747622b73        45 seconds ago      532MB</p><pre><code>  # 启动镜像  docker run --restart always --detach --link mysql-jira:mysql --publish 46002:8080 ht-jira:v7</code></pre></li><li><p>构建confluence镜像</p><pre><code>  # 不要忘记后面的“.”，表示当前目录下进行构建  docker build -t ht-confluence:v2 .   # 等待一段时间进行构建  # 查看已经构建完成的镜像  docker images   REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE</code></pre><p>ht-confluence                      v2                  75d3834d330f        27 minutes ago      785MB</p><pre><code>  # 启动镜像  docker run --restart always --detach --link mysql-confluence:mysql --publish 46004:8080 ht-confluence:v2</code></pre></li><li><p>访问地址</p></li></ul><p>jira：10.0.11.11:46002<br>confluence：10.0.11.12.46004</p><ul><li><p>参考地址：</p><pre><code>  * https://zhangguanzhang.github.io/2019/02/19/jira-confluence/#jira%E6%95%B0%E6%8D%AE%E5%BA%93%E9%85%8D%E7%BD%AE  * https://blog.csdn.net/weixin_38229356/article/details/84875205  * https://github.com/zhangguanzhang/Dockerfile.git  * confluence文档：https://www.cwiki.us/pages/viewpage.action?pageId=917513  * jira文档：https://www.cwiki.us/pages/viewpage.action?pageId=2393502</code></pre></li></ul><h2 id="关于服务器内核自动升级问题的解决"><a href="#关于服务器内核自动升级问题的解决" class="headerlink" title="关于服务器内核自动升级问题的解决"></a>关于服务器内核自动升级问题的解决</h2><p>由于部分机器进行了</p><pre><code>sudo yum update</code></pre><p>操作，导致内核信息进行了升级，导致有一些机器在启动的时候出现了黑屏和无法启动的情况。</p><p>由于在启动时，默认选择了新升级的内核进行了启动，导致报错。如下：</p><p><img src="%E6%96%B0%E5%86%85%E6%A0%B8%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99.png" alt></p><p>目前可用的稳定版内核为：3.10.0-1062.12.1.el7.x86_64，或者低于该版本的内核信息。</p><p>以192.168.229.221机器为例进行查看和修改，首先登陆192.168.229.221，查看已安装的内核信息：</p><pre><code>$ sudo rpm -qa |grep kernelkernel-tools-libs-3.10.0-1127.10.1.el7.x86_64kernel-tools-3.10.0-1127.10.1.el7.x86_64abrt-addon-kerneloops-2.1.11-57.el7.centos.x86_64kernel-3.10.0-1127.10.1.el7.x86_64kernel-3.10.0-862.el7.x86_64kernel-3.10.0-1062.12.1.el7.x86_64$ sudo rpm -e kernel.x86_64error: &quot;kernel.x86_64&quot; specifies multiple packages:  kernel-3.10.0-862.el7.x86_64  kernel-3.10.0-1062.12.1.el7.x86_64  kernel-3.10.0-1127.10.1.el7.x86_64</code></pre><p>然后查看系统可用内核，查看启动时可选择的内核信息：</p><pre><code>$ sudo cat /boot/grub2/grub.cfg |grep menuentryif [ x&quot;${feature_menuentry_id}&quot; = xy ]; then  menuentry_id_option=&quot;--id&quot;  menuentry_id_option=&quot;&quot;export menuentry_id_optionmenuentry &#39;CentOS Linux (3.10.0-1127.10.1.el7.x86_64) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-3.10.0-862.el7.x86_64-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {menuentry &#39;CentOS Linux (3.10.0-1062.12.1.el7.x86_64) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-3.10.0-862.el7.x86_64-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {menuentry &#39;CentOS Linux (3.10.0-862.el7.x86_64) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-3.10.0-862.el7.x86_64-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {menuentry &#39;CentOS Linux (0-rescue-ffe45569e61149c0a79c77c7b38c8e33) 7 (Core)&#39; --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option &#39;gnulinux-0-rescue-ffe45569e61149c0a79c77c7b38c8e33-advanced-f67e74a9-a85f-48b9-b521-32bdb355ab80&#39; {</code></pre><p>下一步查看当前内核信息：</p><pre><code>$ uname -aLinux dev-k8s-node01 3.10.0-1127.10.1.el7.x86_64 #1 SMP Wed Jun 3 14:28:03 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</code></pre><p>目前默认的就是这个3.10.0-1127的内核进行启动的，可能会存在问题，这时候需要修改开机时的默认使用的内核。</p><pre><code>$ sudo grub2-set-default  &#39;CentOS Linux (3.10.0-1062.12.1.el7.x86_64) 7 (Core)&#39;$ sudo grub2-editenv listsaved_entry=CentOS Linux (3.10.0-1062.12.1.el7.x86_64) 7 (Core)</code></pre><p>修改完成后，进行如下操作：</p><ol><li>删除存在启动问题内核</li></ol><pre><code>$ sudo rpm -qa | grep kernelkernel-tools-libs-3.10.0-1127.10.1.el7.x86_64kernel-tools-3.10.0-1127.10.1.el7.x86_64abrt-addon-kerneloops-2.1.11-57.el7.centos.x86_64kernel-3.10.0-1127.10.1.el7.x86_64kernel-3.10.0-862.el7.x86_64kernel-3.10.0-1062.12.1.el7.x86_64$ sudo yum remove kernel-3.10.0-1127.10.1.el7.x86_64Loaded plugins: fastestmirror, langpacks, versionlockSkipping the running kernel: kernel-3.10.0-1127.10.1.el7.x86_64No Packages marked for removal </code></pre><p>删除内核时出现上述错误，是因为当前内核正在使用，需要机器进行重启后才能删除。</p><ol start="2"><li>禁用内核自动更新</li></ol><pre><code>// 复制保留原来的配置文件$ sudo cp /etc/yum.conf /etc/yum.conf.bak$ sudo vim /etc/yum.conf// 在[main]的最后添加 exclude=kernel*exclude=kernel*// :wq保存退出</code></pre><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol><li>硬盘扩容</li></ol><p>参考地址：<a href="http://ttlop.com/2016/11/29/Centos-7-LVM-%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/" target="_blank" rel="noopener">参考链接</a></p><ol start="2"><li><p>修改运行中的docker配置文件</p><p> 首先停止所有的容器</p><pre><code> $ sudo docker stop $(docker ps -a | awk &#39;{ print $1}&#39; | tail -n +2)</code></pre><p> 然后停止docker服务</p><pre><code> $ sudo systemctl stop docker</code></pre><p> 备份容器的配置文件（需要管理员权限）</p><pre><code> # cd /var/lib/docker/containers/de9c6501cdd3(容器编号) # cp hostconfig.json hostconfig.json.bak # cp config.v2.json config.v2.json.bak</code></pre><p> 修改配置文件，例如</p><pre><code> # vim config.v2.json</code></pre><p> 重启docker服务</p><pre><code> $ sudo systemctl start docker</code></pre><p> 重启docker镜像</p><pre><code> $ sudo docker start $(docker ps -a | awk &#39;{ print $1}&#39; | tail -n +2</code></pre></li><li><p>CentOS 7 安装方式选择？</p></li></ol><ul><li><p>Desktop ：基本的桌面系统，包括常用的桌面软件，如文档查看工具。</p></li><li><p>Minimal Desktop：基本的桌面系统，包含的软件更少。</p></li><li><p>Minimal：基本的系统，不含有任何可选的软件包。</p></li><li><p>Basic Server ：安装的基本系统的平台支持，不包含桌面。</p></li><li><p>Database Server：基本系统平台，加上MySQL和PostgreSQL数据库，无桌面。</p></li><li><p>Web Server：基本系统平台，加上PHP，Web server，还有MySQL和PostgreSQL数据库的客户端，无桌面。</p></li><li><p>Virtual Host：基本系统加虚拟平台。</p></li><li><p>Software Development Workstation：包含软件包较多，基本系统，虚拟化平台，桌面环境，开发工具。</p></li></ul><p>而安装Linux基本是用来构建服务器的，所以基本上选择Basic Server即可。如果业务环境无法连接外网，尽量选择比minimal安装更高级别的安装方式，例如Software Development Workstation方式。</p><ol start="4"><li>选取Linux版本原则</li></ol><p>尽量选择LTS（长期支持版）版本，5年期支持，社区有保障。第二选择就是选择次新版本，例如当前发行版本为CentOS 7.6，可以选择CentOS 7.4版本。尽量不要选择CentOS 6系列，因为长期维护支持马上到期。</p><p>对照表：</p><table><thead><tr><th>发行版本</th><th>全力支持到期</th><th>维护支持到期</th></tr></thead><tbody><tr><td>CentOS 6</td><td>2016</td><td>2020-11</td></tr><tr><td>CentOS 7</td><td>2019</td><td>2024-06</td></tr></tbody></table><ol start="5"><li>利用scp命令上传下载文件夹</li></ol><p>使用scp命令，远程上传下载文件/文件夹<br>1、从服务器下载文件<br>scp username@servername:/path/filename /local/path<br>例如: scp <a href="mailto:ubuntu@117.50.20.56" target="_blank" rel="noopener">ubuntu@117.50.20.56</a>:/ygf/data/data.txt /desktop/ygf   把117.50.20.56上的/ygf/data/data.txt 的文件下载到/desktop/ygf目录中</p><p>2、上传本地文件到服务器<br>scp /local/path/local_filename username@servername:/path<br>例如: scp /ygf/learning/deeplearning.doc  <a href="mailto:ubuntu@117.50.20.56" target="_blank" rel="noopener">ubuntu@117.50.20.56</a>:/ygf/learning    把本机/ygf/learning/目录下的deeplearning.doc文件上传到117.50.20.56这台服务器上的/ygf/learning目录中</p><p>3、从服务器下载整个目录<br>scp -r username@servername:/path /path<br>例如: scp  -r  <a href="mailto:ubuntu@117.50.20.56" target="_blank" rel="noopener">ubuntu@117.50.20.56</a>:/home/ygf/data  /local/local_dir    “-r”命令是文件夹目录，把当前/home/ygf/data目录下所有文件下载到本地/local/local_dir目录中</p><p>4、上传目录到服务器<br>scp  -r  /path  username@servername:/path<br>例如: scp -r  /ygf/test  <a href="mailto:ubuntu@117.50.20.56" target="_blank" rel="noopener">ubuntu@117.50.20.56</a>:/ygf/tx     “-r”命令是文件夹目录，把当前/ygf/test目录下所有文件上传到服务器的/ygf/tx/目录中</p><p>参考自：<a href="https://www.cnblogs.com/tectal/p/9478326.html" target="_blank" rel="noopener">https://www.cnblogs.com/tectal/p/9478326.html</a></p><ol start="6"><li>关于shell脚本编写问题</li></ol><p>执行 shell 脚本 \r 问题解决<br>一看应该是 windows 的回车换行跟 linux 换行差异，百度了一下，的确有很多类似问题，在 windows 下编辑 shell 文件，输入的回车是 “\r\n” ，导致在 linux 下执行 shell 脚本时报这个 \r 的错。<br>怎么办？想办法解决。<br>让开发或测试自己想办法转化或者在 linux 环境编辑这个 build.sh 很明显不现实，在服务器安装 dos2unix 来转化 build.sh 我又要在主从节点分别重新构建镜像，麻烦。想了想用替换的方式看能否实现，百度一下，使用 sed -i ‘s/\r//‘ 来处理 build.sh 后再执行，即 sed -i ‘s/\r//‘ build.sh &amp;&amp; bash build.sh，完美解决问题，大功告成！！！</p><pre><code>$ sed -i &#39;s/\r//&#39; init.sh$ sed -i &#39;s/\r//&#39; ssh-keygen-send.sh</code></pre><ol start="7"><li>使用yumdownloader对依赖库进行下载</li></ol><pre><code>// 创建下载的文件夹$ sudo mkdir -p /opt/download-package/// 如果未安装yum-utils，可以先安装$ sudo yum install yum-utils -y// 查看 yum-utils 软件包有没有 yumdownloader，如果有输出代表可用$ sudo rpm -ql yum-utils | grep yumdownloader/usr/bin/yumdownloader/usr/share/man/man1/yumdownloader.1.gz// 下载，--resolve：下载依赖，--destdir：指定下载目录， --donwloadonly：只下载不更新$ sudo yumdownloader java-1.8.0-openjdk.x86_64 --resolve --destdir=/opt/download-package/ --donwloadonly</code></pre><p><strong>注意：</strong>如果openjdk已经安装了，容易导致下载时无法下载相关的依赖信息。</p><ol start="8"><li>编辑/etc/profile文件导致命令not found的问题</li></ol><pre><code>cat </code></pre><p>重新配置export PATH信息，中午整理</p><ol start="9"><li>查看要安装的依赖信息版本</li></ol><pre><code>$ sudo yum list available docker-ce --showduplicates</code></pre><ol start="10"><li>alternatives使用</li></ol><pre><code>// --display// 看到一个命令的所有可选命令# update-alternatives --display java  // --config// 该参数用于给某个命令选择一个link值，相当于在可用值之中进行切换// 列出系统中的所有可用值# update-alternatives --config java // --install// 该参数用于添加一个命令的link值，相当于添加一个可用值，其中slave非常有用。# update-alternatives –install /usr/bin/java java /usr/local/jre1.6.0_20/bin/javac 100  # update-alternatives –install /usr/bin/java java /usr/local/jre1.6.0_20/bin/javac 100 –slave /usr/bin/javac javac /usr/local/jre1.6.0_20/bin/javac  // --remove// 该参数用于删除一个命令的link值，其附带的slave也将一起删除。# update-alternatives –remove java /usr/local/jre1.6.0_20/bin/java  </code></pre><ol start="11"><li>openjdk的卸载与安装(rpm本地包)</li></ol><p>打开rpm解压后的文件夹。</p><p>卸载方式：</p><pre><code>// ./*.rpm包是指所有的jdk依赖包# rpm -e --nodeps ./*.rpm// 或者使用# yum autoremove ./*.rpm# yum autoremove java-1.8.0-openjdk*</code></pre><p>安装方式：</p><pre><code>// ./*.rpm包是指所有的jdk依赖包# rpm -ivh ./*.rpm --nodeps --force// 或者使用# yum localinstall ./*.rpm</code></pre>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps, Linux 运维手册 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/01/28/hello-world/"/>
      <url>/2020/01/28/hello-world/</url>
      
        <content type="html"><![CDATA[<h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>gitlab备份</title>
      <link href="/2020/01/18/gitlab-bei-fen/"/>
      <url>/2020/01/18/gitlab-bei-fen/</url>
      
        <content type="html"><![CDATA[<h1 id="关于gitlab的备份和导入"><a href="#关于gitlab的备份和导入" class="headerlink" title="关于gitlab的备份和导入"></a>关于gitlab的备份和导入</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li><p>操作环境：CentOS 7.6，Docker version 19.03.5</p></li><li><p>待备份对象：192.168.1.199:8181下的gitlab，备份该gitlab镜像</p></li><li><p>启动时已经指定了镜像名称gitlab</p></li><li><p>备份后目标恢复的服务器：192.168.1.198</p></li></ul><h2 id="docker中gitlab-的备份"><a href="#docker中gitlab-的备份" class="headerlink" title="docker中gitlab 的备份"></a>docker中gitlab 的备份</h2><p>首先登录192.168.1.199服务器，找到gitlab所在容器，对该容器的当前状态进行备份。</p><pre><code>    // 找到gitlab在运行的容器信息    $ docker ps -a | grep gitlab     ffce609db97c        gitlab/gitlab-ce:latest     &quot;/assets/wrapper&quot;        2 months ago        Up 11 days (healthy)       0.0.0.0:2222-&gt;22/tcp, 0.0.0.0:8181-&gt;80/tcp, 0.0.0.0:8443-&gt;443/tcp   gitlab    // 进入该容器    $ docker exec -it ffce /bin/bash    // 执行内部gitlab自身的备份命令    root@10:/#  gitlab-rake gitlab:backup:create    // 输入exit退出容器    // 将容器中以及备份的内容拷贝出来    $ docker cp gitlab:/var/opt/gitlab/backups/1579400282_2020_01_19_12.2.5_gitlab_backup.tar .    // 执行备份容器的命令    // 提交保存旧容器（容器名：gitlab）    $ docker commit gitlab gitlab/gitlab-ce:12.2.5    // 从旧docker导出gitlab容器镜像    $ docker export gitlab &gt; gitlab-ce-12.2.5.img</code></pre><p>完成后，已经存在docker容器的备份信息，以及gitlab的备份信息了。随后将其拷贝出来，准备到恢复环境中运行。</p><h2 id="gitlab恢复"><a href="#gitlab恢复" class="headerlink" title="gitlab恢复"></a>gitlab恢复</h2><p>首先通过ftp将文件传输到192.168.1.198服务器上，然后进行恢复操作。</p><pre><code>    // 导入镜像    $ docker import - gitlab/gitlab-ce:12.2.5 &lt; gitlab-ce-12.2.5.img    // 查看镜像是否被导入    $ docker images    REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE    gitlab/gitlab-ce    12.2.5              a574a1869b14        3 hours ago         1.75GB    // 创建文件夹    $ mkdir /home/centos/gitlab_all/logs    $ mkdir /home/centos/gitlab_all/data    $ mkdir /home/centos/gitlab_all/config    // 启动导入的镜像    $ docker run  \        -d \        -h gitlab \        -p 2222:22 \        -p 8181:80 \        -p 8443:443 \        -v /home/centos/gitlab_all/config:/etc/gitlab \        -v /home/centos/gitlab_all/logs:/var/log/gitlab \        -v /home/centos/gitlab_all/data:/var/opt/gitlab \        --restart always \        --name gitlab \        gitlab/gitlab-ce:12.2.5  /assets/wrapper    // 复制之前的备份信息放入backup目录    // 停止gitlab服务    $ docker stop gitlab    // 修改用户和用户组权限    $ sudo chown root.root 1579400282_2020_01_19_12.2.5_gitlab_backup.tar    // 拷贝文件    $ cp 1579400282_2020_01_19_12.2.5_gitlab_backup.tar /home/centos/gitlab_all/data/backups/    // 启动gitlab    $ docker start gitlab    // 进入gitlab容器开始执行恢复命令    $ docker exec -it gitlab /bin/bash    // 停止数据服务    root@10:/# gitlab-ctl stop unicorn    root@10:/# gitlab-ctl stop sidekiq    // 检查关闭状态    root@10:/# gitlab-ctl status    // 所属用户组转换    // 不做转换容易导致权限问题，无法进行备份    root@10:/# chown git.root /var/opt/gitlab/backups/1579400282_2020_01_19_12.2.5_gitlab_backup.tar    // 开始进行数据恢复    root@10:/# gitlab-rake gitlab:backup:restore BACKUP=1579400282_2020_01_19_12.2.5    // 恢复主要分两部分，第一部分是数据库的变更，需要确定是否删除之前的数据库表    // 第二部分是仓库的变更    // 重启gitlab    root@10:/# gitlab-ctl restart    // 验证是否恢复成功    root@10:/# gitlab-rake gitlab:check SANITIZE=true    // 输入exit退出</code></pre><p>这时访问192.168.1.198:8181，可以访问到新的gitlab，用之前的用户名密码进行登录即可。但是可能之前的ssh_key会不可用。</p><h2 id="补充：使用rpm的方式安装gitlab"><a href="#补充：使用rpm的方式安装gitlab" class="headerlink" title="补充：使用rpm的方式安装gitlab"></a>补充：使用rpm的方式安装gitlab</h2><pre><code>$ wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-12.2.5-ce.0.el7.x86_64.rpm$ sudo yum install -y policycoreutils-python$ sudo rpm -i gitlab-ce-12.2.5-ce.0.el7.x86_64.rpmwarning: gitlab-ce-12.2.5-ce.0.el7.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID f27eab47: NOKEYIt looks like GitLab has not been configured yet; skipping the upgrade script.       *.                  *.      ***                 ***     *****               *****    .******             *******    ********            ********   ,,,,,,,,,***********,,,,,,,,,  ,,,,,,,,,,,*********,,,,,,,,,,,  .,,,,,,,,,,,*******,,,,,,,,,,,,      ,,,,,,,,,*****,,,,,,,,,.         ,,,,,,,****,,,,,,            .,,,***,,,,                ,*,.     _______ __  __          __    / ____(_) /_/ /   ____ _/ /_   / / __/ / __/ /   / __ `/ __ \  / /_/ / / /_/ /___/ /_/ / /_/ /  \____/_/\__/_____/\__,_/_.___/Thank you for installing GitLab!GitLab was unable to detect a valid hostname for your instance.Please configure a URL for your GitLab instance by setting `external_url`configuration in /etc/gitlab/gitlab.rb file.Then, you can start your GitLab instance by running the following command:  sudo gitlab-ctl reconfigureFor a comprehensive list of configuration options please see the Omnibus GitLab readmehttps://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.md$ sudo vim /etc/gitlab/gitlab.rb// 修改external-url如下：external_url &#39;http://192.168.1.198:8181&#39;// 配置刷新$ sudo gitlab-ctl reconfigure// 重启$ sudo gitlab-ctl restart</code></pre><p>启动完成后，访问<a href="http://192.168.1.198:8181，重新设置管理员密码，即可进行登录！" target="_blank" rel="noopener">http://192.168.1.198:8181，重新设置管理员密码，即可进行登录！</a></p><h2 id="gitlab-502-问题排查"><a href="#gitlab-502-问题排查" class="headerlink" title="gitlab 502 问题排查"></a>gitlab 502 问题排查</h2><p>安装好GitLab，开启服务，发现有502错误.</p><p>排查措施如下：</p><!-- 1.找到/var/log/gitlab/nginx中的错误日志文件，发现有如下错误/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket failed (2: No such file or directory)，然后用 nc命令创建了这个socket文件，最终权限设为 srwxrwxrwx，用户和组设置为git:git，但发现这个方法行不通。 --><ol><li>跑到GitLab的官网去寻找解决办法，<a href="https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.md" target="_blank" rel="noopener">https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.md</a><br>ctrl+f 502 找到官方教程中说502出现的问题</li></ol><pre><code>Note that on a single-core server it may take up to a minute to restart Unicorn and Sidekiq. Your GitLab instance will give a 502 error until Unicorn is up again.It is also possible to start, stop or restart individual components.sudo gitlab-ctl restart sidekiq Unicorn supports zero-downtime reloads. These can be triggered as follows:sudo gitlab-ctl hup unicorn Note that you cannot use a Unicorn reload to update the Ruby runtime.</code></pre><p>尝试用上面两个命令解决，发现没有用。但这时不断的输入gitlab-ctl status,发现unicorn的pid一直在变大。而其他几个服务的pid没有变化。</p><ol start="2"><li>这时差不多找到了问题的所在了，应该就是unicorn的问题。然后看官方教程，可以使用gitlab-ctl tail unicorn 来跟踪unicorn的状态，这时候悲催的发现原来时8080端口被占用了</li></ol><pre><code>E, [2015-02-11T17:27:57.818492 #26687] ERROR -- : adding listener failed addr=127.0.0.1:8080 (in use)E, [2015-02-11T17:27:57.818621 #26687] ERROR -- : retrying in 0.5 seconds (4 tries left)E, [2015-02-11T17:27:58.318902 #26687] ERROR -- : adding listener failed addr=127.0.0.1:8080 (in use)E, [2015-02-11T17:27:58.318998 #26687] ERROR -- : retrying in 0.5 seconds (3 tries left)E, [2015-02-11T17:27:58.819309 #26687] ERROR -- : adding listener failed addr=127.0.0.1:8080 (in use)E, [2015-02-11T17:27:58.819423 #26687] ERROR -- : retrying in 0.5 seconds (2 tries left)E, [2015-02-11T17:27:59.319954 #26687] ERROR -- : adding listener failed addr=127.0.0.1:8080 (in use)E, [2015-02-11T17:27:59.320076 #26687] ERROR -- : retrying in 0.5 seconds (1 tries left)</code></pre><ol start="3"><li>终于发现了问题的所在。这时候的选择就变成了是把原来8080端口的服务给杀了还是将unicorn的端口换一个呢。这个就看自己的具体需求了。我这边是将unicorn端口换成了9090，方法如下：</li></ol><pre><code># sudo vim /etc/gitlab/gitlab.rb// 修改unicorn运行端口unicorn[&#39;port&#39;] = 9090</code></pre><p>修改完成后执行reconfigure和restart命令如下：</p><pre><code># gitlab-ctl reconfigure# sudo gitlab-ctl restart</code></pre><h2 id="定时备份并发送备份文件到服务器"><a href="#定时备份并发送备份文件到服务器" class="headerlink" title="定时备份并发送备份文件到服务器"></a>定时备份并发送备份文件到服务器</h2><p>脚本信息编写如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#!/bin/bash</span><span class="token comment" spellcheck="true"># 参数信息</span>remote_user<span class="token operator">=</span><span class="token string">'root'</span>remote_host<span class="token operator">=</span><span class="token string">'192.168.1.62'</span>remote_passwd<span class="token operator">=</span><span class="token string">'123456'</span><span class="token comment" spellcheck="true"># 备份docker中的gitlab信息</span>docker <span class="token function">exec</span> -t gitlab gitlab-rake gitlab:backup:create<span class="token comment" spellcheck="true"># 判断文件是否存在</span>bak_file_tar<span class="token operator">=</span><span class="token variable"><span class="token variable">`</span><span class="token function">ls</span> /home/centos/gitlab/data/backups/ <span class="token operator">|</span> <span class="token function">grep</span> gitlab_backup<span class="token variable">`</span></span><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token operator">!</span> -f <span class="token string">"<span class="token variable">$bak_file_tar</span>"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"备份文件 <span class="token variable">$bak_file_tar</span> 存在，开始传输"</span>    sshpass -p <span class="token variable">$remote_passwd</span> <span class="token function">scp</span> -r /home/centos/gitlab/data/backups/ <span class="token variable">$remote_user@</span><span class="token variable">$remote_host</span>:/home/backup/gitlab/    <span class="token comment" spellcheck="true"># sshpass -p $remote_passwd scp -r /home/centos/gitlab/data/backups $remote_user@$remote_host:~/</span>    <span class="token keyword">echo</span> <span class="token string">"=============传输完毕================="</span><span class="token keyword">fi</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要是在docker中进行备份，由于我们的gitlab做了外部路径的映射，可以在外部路径直接传输备份文件。</p><p>在操作之前需要先安装sshpass工具，安装完成后需要先执行以下scp命令连接一下陌生的主机，获取主机指纹信息，获取完成后sshpass工具才能正常运行，如下：</p><pre><code># yum install sshpass -y# scp workflow.txt root@192.168.1.62:~/The authenticity of host &#39;192.168.1.62 (192.168.1.62)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:rFFMBntJrMa/7ljJbRngqNmqhA/eFuuEZ2qmPqRh34k.ECDSA key fingerprint is MD5:59:df:64:7c:48:3c:af:67:96:55:54:93:f1:5c:cb:c5.Are you sure you want to continue connecting (yes/no)?yesWarning: Permanently added &#39;192.168.1.62&#39; (ECDSA) to the list of known hosts.root@192.168.1.62&#39;s password:// ctrl+c终止执行// 执行上面编写的脚本# sh +x send.sh</code></pre><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>文件权限问题，所属</li></ul><p>必须将文件所有者修改，同容器所在卷所有者信息一致。否则容易导致恢复不成功！</p><ul><li>gitlab停机问题</li></ul><p>不停机就往里复制备份文件，容易出现不可知的问题。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul><li><p><a href="https://www.jianshu.com/p/e7c056d273b6" target="_blank" rel="noopener">https://www.jianshu.com/p/e7c056d273b6</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/56108334" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/56108334</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git gitlab DevOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELK单机日志系统搭建</title>
      <link href="/2020/01/15/elk-dan-ji-ri-zhi-xi-tong-da-jian/"/>
      <url>/2020/01/15/elk-dan-ji-ri-zhi-xi-tong-da-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="ELK单机日志系统搭建"><a href="#ELK单机日志系统搭建" class="headerlink" title="ELK单机日志系统搭建"></a>ELK单机日志系统搭建</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul><li>背景</li></ul><p>日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。</p><p>通常，日志被分散的储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。</p><p>集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。</p><p>开源实时日志分析ELK平台能够完美的解决我们上述的问题，ELK由ElasticSearch、Logstash和Kiabana三个开源工具组成。官方网站：<a href="https://www.elastic.co/products" target="_blank" rel="noopener">https://www.elastic.co/products</a></p><p>ELK Stack 是软件集合 Elasticsearch、Logstash、Kibana 的简称，由这三个软件及其相关的组件可以打造大规模日志实时处理系统。</p><p>Elasticsearch 是一个基于 Lucene 的、支持全文索引的分布式存储和索引引擎，主要负责将日志索引并存储起来，方便业务方检索查询。</p><p>Logstash 是一个日志收集、过滤、转发的中间件，主要负责将各条业务线的各类日志统一收集、过滤后，转发给 Elasticsearch 进行下一步处理。</p><p>Kibana 是一个可视化工具，主要负责查询 Elasticsearch 的数据并以可视化的方式展现给业务方，比如各类饼图、直方图、区域图等。</p><p>filebeat 是一个日志收集工具，从单机日志收集后发送到Logstash，或者直接写入ES中。</p><p>所谓“大规模”，指的是 ELK Stack 组成的系统以一种水平扩展的方式支持每天收集、过滤、索引和存储 TB 规模以上的各类日志。</p><ul><li>特点</li></ul><p>一个完整的集中式日志系统，需要包含以下几个主要特点：</p><p>1）收集－能够采集多种来源的日志数据</p><p>2）传输－能够稳定的把日志数据传输到中央系统</p><p>3）存储－如何存储日志数据</p><p>4）分析－可以支持 UI 分析</p><p>5）警告－能够提供错误报告，监控机制</p><p>而ELK则提供了一整套解决方案，并且都是开源软件，之间互相配合使用，完美衔接，高效的满足了很多场合的应用。是目前主流的一种日志系统。</p><ul><li>架构</li></ul><p>上图是 ELK Stack 实际应用中典型的一种架构，其中 filebeat 在具体的业务机器上，通过实时的方式获取增量的日志，并转发到 Kafka 消息系统暂存。</p><p>Kafka 以高吞吐量的特征，作为一个消息系统的角色，接收从 filebeat 收集转发过来的日志，通常以集群的形式提供服务。</p><p>然后，Logstash 从 Kafka 中获取日志，并通过 Input-Filter-Output 三个阶段的处理，更改或过滤日志，最终输出我们感兴趣的数据。通常，根据 Kafka 集群上分区(Partition)的数量，1:1 确定 Logstash 实例的数量，组成 Consumer Group 进行日志消费。</p><p>最后，Elasticsearch 存储并索引 Logstash 转发过来的数据，并通过 Kibana 查询和可视化展示，达到实时分析日志的目的。</p><p>Elasticsearch/Kibana 还可以通过安装 x-pack 插件实现扩展功能，比如监控 Elasticsearch 集群状态、数据访问授权等。</p><p>示意图如下：</p><p><img src="%E6%9E%B6%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="架构示意图"></p><h2 id="案例介绍"><a href="#案例介绍" class="headerlink" title="案例介绍"></a>案例介绍</h2><p>在一台服务器上安装nginx，保持默认配置，通过访问nginx来产生日志信息，然后用filebeat进行收集，传输到ElasticSearch，再到kibana里面进行日志展示。</p><h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><ul><li>安装方式</li></ul><p>单机安装</p><ul><li>安装版本</li></ul><ol><li><p>elasticsearch-6.2.2.tar.gz  </p></li><li><p>kibana-6.2.2-linux-x86_64.tar.gz  </p></li><li><p>logstash-6.2.2.tar.gz</p></li><li><p>filebeat-6.2.2-linux-x86_64.tar.gz</p></li></ol><p><strong>注意：</strong>以6.2.2版本进行验证，后续会选择切换到ELK 7版本的相关安装包。</p><ul><li><p>环境要求</p><ul><li><p>操作系统<br>CentOS 7.6</p></li><li><p>Java环境<br>JDK-1.8.0_202版本，已安装配置。</p></li><li><p>IP地址</p></li></ul><p>  192.168.188.174 ELK部署机器</p><p>  192.168.188.176 Nginx、filebeat部署机器</p><ul><li><p>占用端口</p><p>elasticsearch：9200、9300</p><p>kibana：5601</p><p>logstash：5044</p></li><li><p>其它</p><p>操作用户为centos，需要使用sudo开启root权限。</p></li></ul></li><li><p>基础环境配置</p><ol><li><p>关闭防火墙（可不进行，开放相关端口即可）</p><pre><code> // 关闭防火墙，防止重启 $ sudo systemctl stop firewalld $ sudo systemctl disable firewalld // 关闭selinux $ setenforce 0 $ sed -i &#39;/SELINUX/s/enforcing/disabled/&#39; /etc/selinux/config</code></pre></li><li><p>内核优化</p><pre><code> $ sudo vim /etc/security/limits.conf // 在文件最后添加以下内容 * soft nofile 65537 * hard nofile 65537 * soft nproc 65537 * hard nproc 65537 // 查看文档内容是否已经修改 $ egrep -v &quot;^$|#&quot; /etc/security/limits.conf // 注意：修改了/etc/security/limits.conf ，必须要重启，才能生效。也可以在最后配置完成后进行重启。 //  $ sudo vim /etc/security/limits.d/20-nproc.conf // 修改以下内容，已有则修改，没有则需要添加 * soft nproc 4096 $ sudo vim /etc/sysctl.conf // #添加以下内容 vm.max_map_count = 262144 net.core.somaxconn=65535 net.ipv4.ip_forward = 1 // 执行sysctl -p使其生效 $ sudo sysctl -p // 生效后需要重启network服务 $ sudo systemctl restart network</code></pre></li><li><p>JDK安装–参考<a href="/2020/01/31/linux-ri-chang-yun-wei-cao-zuo/" title="Linux日常运维操作">Linux日常运维操作</a></p></li></ol></li></ul><h2 id="ElasticSearch的安装配置"><a href="#ElasticSearch的安装配置" class="headerlink" title="ElasticSearch的安装配置"></a>ElasticSearch的安装配置</h2><p>首先登陆192.168.188.174服务器进行操作。</p><pre><code>    // 0、开放相关端口    $ sudo firewall-cmd --permanent --zone=public --add-port=5044/tcp    $ sudo firewall-cmd --permanent --zone=public --add-port=9200/tcp    $ sudo firewall-cmd --permanent --zone=public --add-port=9300/tcp    $ sudo firewall-cmd --permanent --zone=public --add-port=5601/tcp    // 1、创建持久化目录及logs目录    $ mkdir -p /home/centos/elasticsearch/{data,logs}    // 2、下载elasticsearch软件包    $ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.2.tar.gz    // 3、解压并重命名    $ tar zxf elasticsearch-6.2.2.tar.gz    $ sudo mv elasticsearch-6.2.2 /usr/local/elasticsearch    // 4、修改elasticsearch.yml配置文件，文件内容如下    $ sudo vim /usr/local/elasticsearch/config/elasticsearch.yml        node.name: localhost        path.data: /home/centos/elasticsearch/data        path.logs: /home/centos/elasticsearch/logs        network.host: 0.0.0.0        http.port: 9200    // 5、创建elk用户并授权    // (这条命令可以不执行)    $ sudo chown -R centos.centos /home/centos/elasticsearch/*    $ sudo chown -R centos.centos /usr/local/elasticsearch    // 6、启动es服务（第一次先测试好在加-d后台启动）    $ /usr/local/elasticsearch/bin/elasticsearch    // 7、后台启动es服务    $ nohup /usr/local/elasticsearch/bin/elasticsearch -d &amp;</code></pre><p>Elasticsearch常用命令</p><pre><code>    curl -XDELETE &#39;http://host.IP.address:9200/logstash-*&#39; 删除索引(后面为索引名称)    curl -XGET &#39;host.IP.address:9200/_cat/health?v&amp;pretty&#39; 查看集群状态    curl -XGET &#39;host.IP.address:9200/_cat/indices?v&amp;pretty&#39; 查看索引</code></pre><p>Elasticsearch安装X-Pack插件（后续补充）</p><h2 id="kibana的安装配置"><a href="#kibana的安装配置" class="headerlink" title="kibana的安装配置"></a>kibana的安装配置</h2><pre><code>    // 1、下载Kibana软件包    $ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.2.2-linux-x86_64.tar.gz    // 2、解压并重命名    $ tar zxf kibana-6.2.2-linux-x86_64.tar.gz    $ sudo mv kibana-6.2.2-linux-x86_64 /usr/local/kibana    $ sudo chown -R centos.centos /usr/local/kibana    // 3、编辑kibana.yml配置文件    $ sudo vim /usr/local/kibana/config/kibana.yml        server.port: 5601        server.host: &quot;192.168.0.97&quot;        elasticsearch.url: &quot;http://192.168.0.97:9200&quot;    // 4、启动Kibana服务（采用后台启动）    //#前台启动    $ /usr/local/kibana/bin/kibana    // 检查运行情况    // #后台启动    $ nohup /usr/local/kibana/bin/kibana &amp;    // 温馨提示：可以先前台启动查看日志，正常之后在后台启动。</code></pre><p>安装完成后，可以在浏览器访问<a href="http://192.168.188.174:5601/，打开kibana页面。" target="_blank" rel="noopener">http://192.168.188.174:5601/，打开kibana页面。</a></p><h2 id="Nginx的安装配置"><a href="#Nginx的安装配置" class="headerlink" title="Nginx的安装配置"></a>Nginx的安装配置</h2><p>由于192.168.188.176服务器上已经安装了nginx，因此不在赘述。具体安装详细见<a href>参考地址</a></p><p>主要是针对nginx进行配置，请登录192.168.188.176服务器，操作如下：</p><pre><code>    // 打开nginx所在目录    $ cd /usr/local/nginx    // 编辑配置文件    $ sudo vim ./conf/nginx.conf    // 在http { 下边添加    log_format main &#39;{&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&#39;        &#39;&quot;host&quot;:&quot;$server_addr&quot;,&#39;        &#39;&quot;clientip&quot;:&quot;$remote_addr&quot;,&#39;        &#39;&quot;remote_user&quot;:&quot;$remote_user&quot;,&#39;        &#39;&quot;request&quot;:&quot;$request&quot;,&#39;        &#39;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,&#39;        &#39;&quot;size&quot;:$body_bytes_sent,&#39;        &#39;&quot;responsetime&quot;:$request_time,&#39;        &#39;&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&#39;        &#39;&quot;upstreamhost&quot;:&quot;$upstream_addr&quot;,&#39;        &#39;&quot;http_host&quot;:&quot;$host&quot;,&#39;        &#39;&quot;requesturi&quot;:&quot;$request_uri&quot;,&#39;        &#39;&quot;url&quot;:&quot;$uri&quot;,&#39;        &#39;&quot;domain&quot;:&quot;$host&quot;,&#39;        &#39;&quot;xff&quot;:&quot;$http_x_forwarded_for&quot;,&#39;        &#39;&quot;referer&quot;:&quot;$http_referer&quot;,&#39;        &#39;&quot;status&quot;:&quot;$status&quot;}&#39;;    access_log  logs/access.log  main;    // :wq 进行保存    // 重新reload    $ sudo ./sbin/nginx -s reload</code></pre><p>访问<a href="http://192.168.188.176" target="_blank" rel="noopener">http://192.168.188.176</a> 查看日志信息，这时候可以在/usr/local/nginx/logs目录下看到access.log日志了。真正对其收集的配置，可以参考filebeat安装的内容。</p><h2 id="filebeat安装配置"><a href="#filebeat安装配置" class="headerlink" title="filebeat安装配置"></a>filebeat安装配置</h2><p>继续在176服务器上进行操作，如下：</p><pre><code>    // 1、下载软件包    $ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.2.2-linux-x86_64.tar.gz    // 2、解压并重命名    $ tar -zxvf filebeat-6.2.2-linux-x86_64.tar.gz    $ sudo mv filebeat-6.2.2-linux-x86_64 /usr/local/filebeat    // 3、编辑logstash.yml配置文件，添加以下内容    $ sudo vim /usr/local/filebeat/filebeat.yml    # 开启日志模式    - type: log      # Change to true to enable this prospector configuration.      enabled: true      # Paths that should be crawled and fetched. Glob based paths.      paths:        #- /var/log/*.log        # 添加日志路径信息        - /usr/local/nginx/logs/*.log    。。。。。。      ### Multiline options        # Mutiline can be used for log messages spanning multiple lines. This is common        # for Java Stack Traces or C-Line Continuation        # The regexp Pattern that has to be matched. The example pattern matches all lines starting #with [        # 配置匹配模式         multiline.pattern: ^\[        # Defines if the pattern set under pattern should be negated or not. Default is false.        #multiline.negate: false        # Match can be set to &quot;after&quot; or &quot;before&quot;. It is used to define if lines should be append to # a pattern        # that was (not) matched before or after or as long as a pattern is not matched based on #negate.        # Note: After is the equivalent to previous and before is the equivalent to to next in #Logstash        # 配置匹配模式         multiline.match: after        #============================= Filebeat modules ===============================        filebeat.config.modules:        # Glob pattern for configuration loading        path: ${path.config}/modules.d/*.yml        # Set to true to enable config reloading          reload.enabled: false        # Period on which files under path should be checked for changes          reload.period: 10s        。。。。。。。        #----------------------------- Logstash output --------------------------------        output.logstash:            # The Logstash hosts            # 配置Logstash地址            hosts: [&quot;192.168.188.174:5044&quot;]            # Optional SSL. By default is off.            # List of root certificates for HTTPS server verifications            #ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;]            # Certificate for SSL client authentication            #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;            # Client Certificate Key            #ssl.key: &quot;/etc/pki/client/cert.key&quot;    // 4. 测试开启    $ ./filebeat -e -c filebeat.yml -d &quot;publish&quot;    // 5. 后台启动运行    $ nohup ./filebeat -e -c filebeat.yml -d &quot;publish&quot; &amp;</code></pre><p>配置完成后，这时再去访问176服务器的nginx服务，进行访问，看到filebeat已经开始上传数据了。下一步开始配置Logstash，接收这些上传的数据信息。</p><h2 id="Logstash的安装配置"><a href="#Logstash的安装配置" class="headerlink" title="Logstash的安装配置"></a>Logstash的安装配置</h2><pre><code>    // 1、下载软件包    $ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.2.tar.gz    // 2、解压并重命名    $ tar zxf logstash-6.2.2.tar.gz    $ sudo mv logstash-6.2.2 /usr/local/logstash    // 3、编辑logstash.yml配置文件，添加以下内容    $ sudo vim /usr/local/logstash/config/logstash.yml    config.reload.automatic: true    config.reload.interval: 10s    // 4、创建Nginx配置文件    $ sudo mkdir /usr/local/logstash/conf    $ sudo vim /usr/local/logstash/conf/nginx.conf    input {        file {            path =&gt; &quot;/usr/local/nginx/logs/access.log&quot;            start_position =&gt; &quot;beginning&quot;        }        beats {            port =&gt; 5044        }    }    filter {    }    output {        elasticsearch {            hosts =&gt; [&quot;192.168.188.174:9200&quot;]            #index =&gt; &quot;nginx_access-%{+YYYY.MM.dd}&quot;        }    }    // 5. 测试启动    $ /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/nginx.conf    // 6. 后台启动    $ nohup /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/nginx.conf &amp;    // 7. 检查配置文件是否正常    $ /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/nginx.conf -t</code></pre><p>温馨提示，一定要确保logs目录centos有权限写入，建议我们在启动elk之前在执行一次命令</p><pre><code>    $ sudo chown -R centos.centos /usr/local/logstash</code></pre><p>请确保logstash中的file文件有读取权限，否则无法在ES中创建索引！</p><h2 id="通过kibana进行配置查看"><a href="#通过kibana进行配置查看" class="headerlink" title="通过kibana进行配置查看"></a>通过kibana进行配置查看</h2><ol><li><p>配置elasticsearch</p></li><li><p>配置日志展示</p></li></ol><p><img src="./kibana0.png" alt="kibana初始"></p><p><img src="./kibana1.png" alt="kibana初始"></p><p><img src="./kibana2.png" alt="kibana初始"></p><p><img src="./kibana3.png" alt="kibana初始"></p><p><img src="./kibana4.png" alt="kibana初始"></p><h2 id="结合Logback进行微服务日志的实践"><a href="#结合Logback进行微服务日志的实践" class="headerlink" title="结合Logback进行微服务日志的实践"></a>结合Logback进行微服务日志的实践</h2><p>以数据字典微服务为例，进行Logback的配置。</p><p>首先是依赖相关信息，在pom.xml中配置Logback依赖，如下：</p><pre><code>      &lt;!--    logback依赖，以及Logstash插件    引入slf4j进行打印     --&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.slf4j&lt;/groupId&gt;        &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;        &lt;version&gt;1.7.26&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt;        &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt;        &lt;version&gt;6.2&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;        &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;        &lt;version&gt;1.2.3&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;        &lt;artifactId&gt;logback-core&lt;/artifactId&gt;        &lt;version&gt;1.2.3&lt;/version&gt;    &lt;/dependency&gt;</code></pre><p>随后，在src/main/resources/目录下，创建logback.xml配置文件，进入如下配置：</p><pre><code>        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;        &lt;!--        configuration根节点配置项：        scan:是否自动加载，默认为true。        scanPeriod:监听修改时间间隔，默认一分钟。        debug: 是否查看logback运行状态，默认true。        必须在配置文件中指定的配置项：        logging.levelfile=        logstash.ip_port=192.168.188.174:5000        如果不使用Logstash，需要将Logstash的设置进行注释        --&gt;        &lt;configuration debug=&quot;true&quot; scan=&quot;true&quot; scanPeriod=&quot;30 seconds&quot;&gt;            &lt;property resource=&quot;application.properties&quot;&gt;&lt;/property&gt;            &lt;!--            定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径            LOG_HOME：日志存储位置            logging.levelfile=            LOG_LEVEL：日志输出级别            logging.all.level=            LOG_SERVICE_NAME：日志所在的微服务名称，指定为服务名称            --&gt;            &lt;property name=&quot;LOG_HOME&quot; value=&quot;logs/&quot; /&gt;            &lt;property name=&quot;LOG_LEVEL&quot; value=&quot;INFO&quot; /&gt;            &lt;property name=&quot;LOG_SERVICE_NAME&quot; value=&quot;test-data-dic&quot; /&gt;            &lt;!--            控制台输出 ， 对应root标签中的ref引用信息            --&gt;            &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;                &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt;                    &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt;                    &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50}:%L - %msg  %n&lt;/pattern&gt;                    &lt;charset&gt;UTF-8&lt;/charset&gt;                &lt;/encoder&gt;            &lt;/appender&gt;            &lt;!--            日志文件输出，按照每天生成日志文件            线上日志文件配置            --&gt;            &lt;appender name=&quot;SERVER_FILE&quot;  class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;                &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;                    &lt;!--日志文件输出的文件名，由于微服务是部署在容器内，所以对使用位置进行限定--&gt;                    &lt;FileNamePattern&gt;/var/logs/app_logs/${LOG_HOME}/${LOG_SERVICE_NAME}-%d{yyyy-MM-dd}.log&lt;/FileNamePattern&gt;                    &lt;!--日志文件保留天数--&gt;                    &lt;MaxHistory&gt;30&lt;/MaxHistory&gt;                    &lt;!-- 日志文件总共最大为10G --&gt;                    &lt;totalSizeCap&gt;10GB&lt;/totalSizeCap&gt;                &lt;/rollingPolicy&gt;                &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt;                    &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt;                    &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt;                    &lt;charset&gt;UTF-8&lt;/charset&gt;                &lt;/encoder&gt;                &lt;!--日志文件最大的大小--&gt;                &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt;                    &lt;MaxFileSize&gt;10MB&lt;/MaxFileSize&gt;                &lt;/triggeringPolicy&gt;                &lt;!-- 设置日志级别，貌似只能打印一个级别的日志信息 --&gt;        &lt;!--        &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;--&gt;        &lt;!--            &lt;level&gt;INFO&lt;/level&gt;--&gt;        &lt;!--            &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;--&gt;        &lt;!--            &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;--&gt;        &lt;!--        &lt;/filter&gt;--&gt;            &lt;/appender&gt;            &lt;!--            日志输出到Logstash            需要配置Logstash地址信息            logstash.ip_port=            若不配置，需要            --&gt;        &lt;!--    &lt;appender name=&quot;logstash&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt;--&gt;        &lt;!--        &lt;destination&gt;${logstash.ip_port}&lt;/destination&gt;--&gt;        &lt;!--        &lt;encoder charset=&quot;UTF-8&quot; class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot; /&gt;--&gt;        &lt;!--        &lt;queueSize&gt;1048576&lt;/queueSize&gt;--&gt;        &lt;!--        &lt;keepAliveDuration&gt;5 minutes&lt;/keepAliveDuration&gt;--&gt;        &lt;!--        &amp;lt;!&amp;ndash;&lt;customFields&gt;{&quot;application-name&quot;:&quot;data-repo-interface&quot;}&lt;/customFields&gt;&amp;ndash;&amp;gt;--&gt;        &lt;!--        &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;--&gt;        &lt;!--            &lt;level&gt;INFO&lt;/level&gt;--&gt;        &lt;!--        &lt;/filter&gt;--&gt;        &lt;!--        &lt;filter class=&quot;ch.qos.logback.core.filter.EvaluatorFilter&quot;&gt;--&gt;        &lt;!--            &lt;evaluator&gt; &amp;lt;!&amp;ndash; 默认为 ch.qos.logback.classic.boolex.JaninoEventEvaluator &amp;ndash;&amp;gt;--&gt;        &lt;!--                &lt;expression&gt;return message.contains(&quot;billing&quot;);&lt;/expression&gt;--&gt;        &lt;!--            &lt;/evaluator&gt;--&gt;        &lt;!--            &lt;OnMatch&gt;ACCEPT&lt;/OnMatch&gt;--&gt;        &lt;!--            &lt;OnMismatch&gt;DENY&lt;/OnMismatch&gt;--&gt;        &lt;!--        &lt;/filter&gt;--&gt;        &lt;!--    &lt;/appender&gt;--&gt;            &lt;!--        日志输出级别，可以在配置文件中指定        ****注意*****        如果需要在本地进行调试，请注释&lt;appender-ref ref=&quot;FILE&quot; /&gt;该选项！        appender-ref指定了日志输出方式，这里指定了到控制台和文件        level 是日志输出的级别，additivity表示是否在控制台打印该日志。        --&gt;        &lt;!--    &lt;logger name=&quot;elk_logger&quot; level=&quot;INFO&quot; additivity=&quot;false&quot;&gt;--&gt;        &lt;!--        &lt;appender-ref ref=&quot;logstash&quot;/&gt;--&gt;        &lt;!--    &lt;/logger&gt;--&gt;            &lt;root level=&quot;INFO&quot;&gt;                &lt;appender-ref ref=&quot;STDOUT&quot; /&gt;                &lt;appender-ref ref=&quot;SERVER_FILE&quot; /&gt;        &lt;!--        &lt;appender-ref ref=&quot;logstash&quot; /&gt;--&gt;            &lt;/root&gt;        &lt;/configuration&gt;</code></pre><p>配置完成后，在服务的Controller中，添加<strong>log.info(“测试日志”);</strong>代码，以此进行测试。</p><p>编写完成后，对服务所要部署的服务器部署<a href="https://github.com/AliyunContainerService/log-pilot" target="_blank" rel="noopener">log-pilot</a>工具，直接将日志信息写入ElasticSearch中。转到微服务集群所在的服务器192.168.232.179，执行命令如下：</p><pre><code>    // 拉取logpilot镜像    $ docker pull registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.5-filebeat    // 交互式启动    $ docker run --rm -it     \        -v /var/run/docker.sock:/var/run/docker.sock \        -v /etc/localtime:/etc/localtime   \        -v /:/host:ro   \        --cap-add SYS_ADMIN   \        -e FILEBEAT_OUTPUT=elasticsearch \        -e ELASTICSEARCH_HOST=192.168.188.174    \         -e ELASTICSEARCH_PORT=9200  \          registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.5-filebeat    // 以守护进程的方式后台启动    $ docker run -d  --restart=always   \        -v /var/run/docker.sock:/var/run/docker.sock \        -v /etc/localtime:/etc/localtime   \        -v /:/host:ro   \        --cap-add SYS_ADMIN   \        --privileged    \        -e FILEBEAT_OUTPUT=elasticsearch \        -e ELASTICSEARCH_HOST=192.168.188.174    \         -e ELASTICSEARCH_PORT=9200  \          registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.5-filebeat</code></pre><p>启动后，将会看到各类日志信息在控制台上输出。随后回到192.168.188.174服务器，配置Logstash配置文件，操作如下：</p><pre><code>    // 转到之前使用的conf文件夹    $ vim log-pilot.conf    // 输入以下内容    input {        beats {                port =&gt; 5044        }    }    filter {    }    output {        elasticsearch {                hosts =&gt; [&quot;192.168.188.174:9200&quot;]                #index =&gt; &quot;nginx_access-%{+YYYY.MM.dd}&quot;        }    }    // :wq保存退出    // 启动Logstash进行查看    $ nohup /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/log-pilot.conf &amp;</code></pre><p>这时候所有的服务都已经启动了，可以访问kibana，重新添加index pattern，进行日志查看即可。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p>后续需要针对ELK搭建集群内容，进行集群化管理。</p></li><li><p>如何根据应用场景选择要采集的日志信息？</p></li></ol><p>以业务场景为原则！</p><p>日志内容复杂多样，如何去收集有价值的日志是我们重点关注的。日志的价值其实是取决于业务操作的，不同的业务场景下相同类型的日志的价值会截然不同。</p><p>根据以往的业务实践，结合企业级的一些业务需求，我们选定关注以下几类日志。</p><p>• 跟踪日志【trace.log】 Server引擎的调试日志，用于系统维护人员定位系统运行问题使用。</p><p>• 系统日志【system.log】 大粒度的引擎运行的入口、出口的日志，用于调用栈分析，可以进行性能分析使用。</p><p>• 部署日志【deploy.log】 记录系统启动、停止、构件包部署、集群通知等信息的日志。</p><p>• 引擎日志【engine.log】 细粒度的引擎运行日志，可以打印上下文数据，用于定位业务问题。  </p><p>• 构件包日志【contribution.log】 构件包记录的业务日志（使用基础构件库的日志输出API写日志）</p><p>通过以上几种日志，我们可以在分析问题时明确我们要查找的位置，通过分类缩小查找范围提高效率。</p><h2 id="注意事项："><a href="#注意事项：" class="headerlink" title="注意事项："></a>注意事项：</h2><ol><li><p>针对/etc/security/limits.conf的修改，需要使其生效，目前是使用重启的方式生效的</p></li><li><p>在nginx安装完成后，在目标服务器安装filebeat，复制到/usr/local/filebeat，修改文件夹权限，chown转centos，配置文件并启动。核心解决方式就是注意文件夹所属用户以及权限信息。</p></li></ol><h2 id="参考文章："><a href="#参考文章：" class="headerlink" title="参考文章："></a>参考文章：</h2><p><a href="https://www.yangxingzhen.com/5439.html" target="_blank" rel="noopener">https://www.yangxingzhen.com/5439.html</a></p>]]></content>
      
      
      <categories>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ELK Linux运维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DevOps介绍</title>
      <link href="/2020/01/12/devops-jie-shao/"/>
      <url>/2020/01/12/devops-jie-shao/</url>
      
        <content type="html"><![CDATA[<h1 id="DevOps-ppt讲解"><a href="#DevOps-ppt讲解" class="headerlink" title="DevOps ppt讲解"></a>DevOps ppt讲解</h1><ol><li>理念</li></ol><ul><li><p>是什么？并没有明确的定义！</p><ul><li>字面意思：Develop+Operations，开发运维一体化</li><li>深层次含义：DevOps集文化理念、实践和工具于一身。可以提高组织交付应用程序和服务的能力。与使用传统软件和基础设施管理流程相比，能够帮助组织更快的发展和改进产品。这种速度使组织更好的服务其客户，并在市场上高效的参与竞争。（AWS）</li><li>通俗来讲：为了帮助研发团队在保持质量的前提下提高交付效率的方法和方法论都隶属于 DevOps 的范畴。</li><li>管理层面：全新的管理理念，打破职责屏障，减少研发、测试、运维的矛盾，统一思想，从业务需求出发，向着同一个目标前进</li></ul></li><li><p>为什么？</p></li></ul><p>企业内部的效率提升和研发成本的控制，保证高质量的同时减少发布流程的总耗时。</p><p>着眼在质量+效率上</p><p>继承自敏捷开发</p><ul><li>核心理念</li></ul><p>DevOps的核心实践理念统称为CALMS：文化（Culture）、自动化（Automation）、精益（Lean）、度量（Measurement） 、共享（Share）。</p><p>文化：统一思想，精简组织架构，强调协作和融合，标准化体系建设。</p><p>自动化：自动化工具的引入，灵活部署，解放双手，聚焦业务实现。</p><p>精益：代码质量、接口质量的提升，对于问题的快速响应和快速解决，事故复盘和学习。</p><p>度量：系统监控以及告警</p><p>共享：共同学习，共同成长，允许试错</p><ol start="2"><li>场景</li></ol><p>将DevOps定义为一种强调通过高速、小型化和迭代步骤的应用程序开发、部署和反馈环方式，提供更主动的响应来满足客户需求的方法。它的特点是业务文化上的转变，将开发团队和运营团队作为一个团队，专注于实现业务价值。</p><p>特别是微服务场景下，天然适合DevOps理念。</p><p>强调合作性，运维与开发相互配合，协同工作。</p><ol start="3"><li>应用</li></ol><p>开发云有项目管理、配置管理、代码检查、编译构建、测试、部署、发布、流水线这几大服务。以华为为例。</p><p>开源的应用工具链信息：</p><ul><li>版本控制&amp;协作开发：GitHub、GitLab、BitBucket、SubVersion、Coding、Bazaar</li><li>自动化构建和测试:Apache Ant、Maven 、Selenium、PyUnit、QUnit、JMeter、Gradle、PHPUnit</li><li>持续集成&amp;交付:Jenkins、Capistrano、BuildBot、Fabric、Tinderbox、Travis CI、flow.ci Continuum、LuntBuild、CruiseControl、Integrity、Gump、Go</li><li>容器平台: Docker、Rocket、Ubuntu（LXC）、第三方厂商如（AWS/阿里云）</li><li>配置管理：Chef、Puppet、CFengine、Bash、Rudder、Powershell、RunDeck、Saltstack、Ansible</li><li>微服务平台：OpenShift、Cloud Foundry、Kubernetes、Mesosphere</li><li>服务开通：Puppet、Docker Swarm、Vagrant、Powershell、OpenStack Heat</li><li>日志管理：Logstash、CollectD、StatsD</li><li>监控，警告&amp;分析：Nagios、Ganglia、Sensu、zabbix、ICINGA、Graphite、Kibana</li></ul><p>重中之重：标准化体系的建设。</p><ol start="4"><li>困难/痛点</li></ol><ul><li>既有项目的复杂依赖</li></ul><p>已有单体项目难以拆分的问题</p><ul><li><p>自动化程度不足</p><ul><li>持续构建</li><li>版本控制</li><li>代码质量</li></ul></li><li><p>项目交付与交付质量</p></li></ul><p>交付速度和交付质量的平衡，利用持续集成来保障。</p><ul><li>多运行环境的管理</li></ul><p>多套运行环境，开发环境、测试环境、灰度环境、正式环境等等，各环境要求不同，配置不同，如何统一纳入管理，是问题。</p><ul><li>开发人员身兼多职的问题</li></ul><p>全才和专才的矛盾，全才干专才的事情，专才往全才转换，都是一种人才资源的浪费。</p><ul><li>对公司：摸着石头过河</li></ul><p>各公司要求不一致，发展程度、业务要求不一致，如何找到适合自己的方式。</p><ol start="5"><li>误区</li></ol><ul><li>DevOps是银弹</li></ul><p>没有银弹</p><ul><li>站在运维角度看开发/站在开发角度看运维</li></ul><p>都不对，应该站在架构或者更高的位置看，站在全局的位置看。</p><ul><li>强调开发人员/运维人员的全栈能力？</li></ul><p>强调的是协作和融合，发挥个人长处即可，打破协作的壁垒。</p><ul><li>对开发：一定要敏捷？</li></ul><p>如果团队实行敏捷开发的方式，承接敏捷开发，引入DevOps是正确的思路；如果不是敏捷开发的方式，那么可以利用DevOps引入自动化流水线。</p><ol start="6"><li>总结</li></ol><p>DevOps是一种工程模式，本质上是一种分工，通过对开发、运维、测试，配管等角色职责的分工，实现工程效率最大化，进而满足业务的需求。</p><p>DevOps的核心是角色的分工，而不是组织架构变化，垂直化的组织架构不代表可以实现DevOps所需要的分工模式，横向的组织架构也不代表传统的分工模式。</p><p>DevOps的目标是工程效率最大化，它本身也只是一种方法论，是为了实现工程效率最大化的目标而存在的。</p><p>DevOps应该是一种有价值的试错，通过实践形成满足我们团队的开发运维模式。</p><ol start="7"><li>参考文章</li></ol><ul><li><p>具体实践：<a href="https://library.prof.wang/handbook/h/hdbk-MWnS99ThmLVDi7U5mVFrB9#toc314" target="_blank" rel="noopener">https://library.prof.wang/handbook/h/hdbk-MWnS99ThmLVDi7U5mVFrB9#toc314</a></p></li><li><p>理念：<a href="https://www.jianshu.com/p/645bb1283a77" target="_blank" rel="noopener">https://www.jianshu.com/p/645bb1283a77</a></p></li><li><p>腾讯实践：<a href="https://cloud.tencent.com/developer/article/1084454" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1084454</a></p></li><li><p>工具集：<a href="https://www.cnblogs.com/SanMaoSpace/p/10110703.html" target="_blank" rel="noopener">https://www.cnblogs.com/SanMaoSpace/p/10110703.html</a></p></li><li><p>初学者指南：<a href="https://zhuanlan.zhihu.com/p/36905891" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36905891</a></p></li><li><p>传统行业落地指南：<a href="https://dbaplus.cn/news-134-2068-1.html" target="_blank" rel="noopener">https://dbaplus.cn/news-134-2068-1.html</a></p></li><li><p><a href="ram14026-01-devops-for-dummies-3rd-ibm-editon-simplified-chinese_RAM14026CNZH.pdf">IBM 的DevOps手册</a></p></li><li><p>极客时间《DevOps实战笔记》</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>运维平台方案预研</title>
      <link href="/2020/01/05/yun-wei-ping-tai-fang-an-yu-yan/"/>
      <url>/2020/01/05/yun-wei-ping-tai-fang-an-yu-yan/</url>
      
        <content type="html"><![CDATA[<h1 id="关于运维平台搭建方案"><a href="#关于运维平台搭建方案" class="headerlink" title="关于运维平台搭建方案"></a>关于运维平台搭建方案</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>根据目前微服务平台的需求，主要是针对DevOps的迫切需求，结合目前的CI/CD平台，引入新的运维平台，来统一进行微服务以及服务器的运维工作。</p><h2 id="当前情况"><a href="#当前情况" class="headerlink" title="当前情况"></a>当前情况</h2><ul><li>已有CI/CD平台</li><li>缺少统一的日志管理平台，未利用ELK进行搭建</li><li>针对公有云、私有云、混合云的管理，目前没有合适的工具</li><li>运维流程没有进行建设，未制定运维规则</li><li>未针对服务器以及容器，添加运维告警的内容</li><li>需要以产品思维面向运维内容</li></ul><h2 id="需求列表及其说明"><a href="#需求列表及其说明" class="headerlink" title="需求列表及其说明"></a>需求列表及其说明</h2><ul><li>用户管理和权限管理</li></ul><p>结合平台目前已有的内容进行实现，开发和运维相关人员不做区分，实践DevOps理念。服务器访问的权限控制，例如验证码的引入。</p><ul><li>工作流程以及自定义表单信息</li></ul><p>结合平台目前已有的内容进行实现，主要体现在工单信息提交，问题提交处理信息。</p><ul><li>资产管理–CMDB</li></ul><p>对接各类公有云平台，例如阿里云、腾讯云、华为云等；针对私有云平台，执行动态扩容缩容，增删机器设备等操作。</p><ul><li>任务管理和任务调度，定时执行</li></ul><p>定时的任务执行平台，对定时任务的管理，对定时任务执行的管理，对其执行结果的管理。</p><ul><li>CI/CD接入，持续集成</li></ul><p>针对持续集成平台的管理，接入jenkins体系。对构建的管理，对构建结果的管理，根据构建返回的信息，定位错误并报警</p><ul><li>数据管理，SQL执行管理</li></ul><p>数据库资源的管理，例如MySQL、postgresql、MongoDB、Redis。针对SQL部分，慢SQL查询，SQL执行审计。</p><ul><li>工单系统</li></ul><p>常见资源申请工单提交，测试提交流程，需求/bug的修复流程以及修复确认。</p><ul><li>文件管理</li></ul><p>整个文件上传的集群构建，大文件管理，文件分发到各个服务器。</p><ul><li>知识库信息</li></ul><p>团队知识管理，知识积累，形成技术周报并分发到每个开发人员。</p><ul><li>监控系统</li></ul><p>针对服务器的监控，针对docker镜像的监控运维，容错和避免单点故障，平台预警。针对各类数据库、缓存、消息队列等进行监控。</p><ul><li>堡垒机，WebShell，操作审计</li></ul><p>提供内外网交互的渠道，直接在线输入命令信息，针对各个服务器的操作，进行审计。</p><ul><li>配置管理</li></ul><p>例如Nginx配置管理和下发，Redis的配置管理和下发，多用于支持配置信息的组件。区别于微服务的配置中心，这里配置管理不对微服务镜像配置。</p><ul><li>镜像管理，应用管理</li></ul><p>利用k8s集群，管理镜像信息。</p><ul><li>微服务基础平台信息</li></ul><p>配置中心、注册中心、全链路监控信息，针对目前的痛点而言，要配置在线的参数传入。</p><ul><li>安全审计</li></ul><p>系统安全、混沌工程、接口安全监控、非法调用等信息</p><ul><li>API制定</li></ul><p>对外提供API的调用</p><h2 id="风险点"><a href="#风险点" class="headerlink" title="风险点"></a>风险点</h2><ol><li>依托开源项目进行开发，优先引入各模块管理页面信息。</li><li>需求需要进行边界梳理。</li><li>可能会出现跨语言的情况，例如使用Python或者Rust进行开发，尽量寻找java相关的平台。</li></ol><h2 id="开源平台"><a href="#开源平台" class="headerlink" title="开源平台"></a>开源平台</h2><ol><li><p>针对k8s管理，使用<a href="https://kubesphere.com.cn/" target="_blank" rel="noopener">kubesphere</a></p></li><li><p>针对运维平台，可参考的项目如下：</p><ul><li>蓝鲸智云 <a href="https://github.com/Tencent/bk-bcs-saas" target="_blank" rel="noopener">https://github.com/Tencent/bk-bcs-saas</a></li><li>Rainbond <a href="https://www.rainbond.com/" target="_blank" rel="noopener">https://www.rainbond.com/</a></li><li>猪齿鱼 <a href="http://choerodon.io/zh/" target="_blank" rel="noopener">http://choerodon.io/zh/</a></li><li>CoDo  <a href="http://www.opendevops.cn" target="_blank" rel="noopener">www.opendevops.cn</a></li><li>Hygieia <a href="https://github.com/Hygieia/Hygieia" target="_blank" rel="noopener">https://github.com/Hygieia/Hygieia</a></li><li>女娲 <a href="http://nvwa-io.com/" target="_blank" rel="noopener">http://nvwa-io.com/</a></li><li>adminset <a href="https://github.com/guohongze/adminset" target="_blank" rel="noopener">https://github.com/guohongze/adminset</a></li><li>super-devops <a href="https://github.com/wl4g/super-devops" target="_blank" rel="noopener">https://github.com/wl4g/super-devops</a></li><li>devops-x <a href="https://github.com/unixhot/devops-x" target="_blank" rel="noopener">https://github.com/unixhot/devops-x</a></li><li>super-cloudops <a href="https://github.com/wl4g/super-cloudops" target="_blank" rel="noopener">https://github.com/wl4g/super-cloudops</a></li><li>Jpom <a href="https://gitee.com/keepbx/Jpom" target="_blank" rel="noopener">https://gitee.com/keepbx/Jpom</a></li><li>bee-apm <a href="https://gitee.com/beetle082/bee-apm" target="_blank" rel="noopener">https://gitee.com/beetle082/bee-apm</a></li></ul></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>针对微服务的后续开发，应提升运维保障的高度，因为服务分拆之后，对比之前单机的运维方式，需要更深层次的运维管理，不仅是水平扩展的问题，在垂直层面进行微服务的拆分和部署的时候，更需要运维层面进行保障。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://juejin.im/entry/5b568dc4e51d451964625b8c" target="_blank" rel="noopener">https://juejin.im/entry/5b568dc4e51d451964625b8c</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY1MzcyOQ==&amp;mid=2247483682&amp;idx=1&amp;sn=aa393af4a869db5ec402da4722cf681f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzU5MDY1MzcyOQ==&amp;mid=2247483682&amp;idx=1&amp;sn=aa393af4a869db5ec402da4722cf681f&amp;scene=21#wechat_redirect</a></li><li><a href="https://mp.weixin.qq.com/s/HwOi-ARTvvNjGTWrDmZIkQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/HwOi-ARTvvNjGTWrDmZIkQ</a></li><li><a href="https://github.com/Hygieia/Hygieia" target="_blank" rel="noopener">https://github.com/Hygieia/Hygieia</a></li><li><a href="https://blog.csdn.net/wowotuo/article/details/79293323" target="_blank" rel="noopener">https://blog.csdn.net/wowotuo/article/details/79293323</a></li><li><a href="https://www.cosmoplat.com/pc/community/topicDetail?id=2304922" target="_blank" rel="noopener">https://www.cosmoplat.com/pc/community/topicDetail?id=2304922</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开源项目研究对比</title>
      <link href="/2019/09/05/kai-yuan-xiang-mu-yan-jiu-dui-bi/"/>
      <url>/2019/09/05/kai-yuan-xiang-mu-yan-jiu-dui-bi/</url>
      
        <content type="html"><![CDATA[<h1 id="当前已封装的微服务框架对比"><a href="#当前已封装的微服务框架对比" class="headerlink" title="当前已封装的微服务框架对比"></a>当前已封装的微服务框架对比</h1><h2 id="开发层面–微服务架构"><a href="#开发层面–微服务架构" class="headerlink" title="开发层面–微服务架构"></a>开发层面–微服务架构</h2><h3 id="1-项目基本信息"><a href="#1-项目基本信息" class="headerlink" title="1. 项目基本信息"></a>1. 项目基本信息</h3><table><thead><tr><th>名称</th><th>地址</th><th>star数</th><th>最近维护日期</th><th>是否可演示</th><th>演示地址</th><th>技术栈</th><th>issue总数</th><th>issue解决数</th><th>开源协议</th></tr></thead><tbody><tr><td>zhangxd1989/spring-boot-cloud</td><td><a href="https://github.com/zhangxd1989/spring-boot-cloud" target="_blank" rel="noopener">https://github.com/zhangxd1989/spring-boot-cloud</a></td><td>1761</td><td>2年前</td><td>无演示</td><td>-</td><td>SpringBoot1.x</td><td>26</td><td>5</td><td>无</td></tr><tr><td>cloudframeworks-springcloud/user-guide-springcloud</td><td><a href="https://github.com/cloudframeworks-springcloud/user-guide-springcloud/blob/master/README_CN.md" target="_blank" rel="noopener">https://github.com/cloudframeworks-springcloud/user-guide-springcloud/blob/master/README_CN.md</a></td><td>1254</td><td>2018年4月</td><td>无演示</td><td>-</td><td>SpringBoot2.x</td><td>12</td><td>4</td><td>APACHE LICENSE 2.0</td></tr><tr><td>mxdldev/spring-cloud-flycloud</td><td><a href="https://github.com/mxdldev/spring-cloud-flycloud" target="_blank" rel="noopener">https://github.com/mxdldev/spring-cloud-flycloud</a></td><td>1161</td><td>2019年8月</td><td>无演示</td><td>-</td><td>SpringBoot1.x</td><td>3</td><td>2</td><td>APACHE LICENSE 2.0</td></tr><tr><td>zhoutaoo/SpringCloud</td><td><a href="https://github.com/zhoutaoo/SpringCloud" target="_blank" rel="noopener">https://github.com/zhoutaoo/SpringCloud</a></td><td>1152</td><td>2019年8月</td><td>无演示，正在开发</td><td>-</td><td>SpringBoot2.x</td><td>30</td><td>22</td><td>Apache License 2.0</td></tr><tr><td>chillzhuang/SpringBlade</td><td><a href="https://github.com/chillzhuang/SpringBlade" target="_blank" rel="noopener">https://github.com/chillzhuang/SpringBlade</a></td><td>768</td><td>2019年8月</td><td>有演示</td><td>Sword演示地址：<a href="https://sword.bladex.vip" target="_blank" rel="noopener">https://sword.bladex.vip</a> / Saber演示地址：<a href="https://saber.bladex.vip" target="_blank" rel="noopener">https://saber.bladex.vip</a></td><td>SpringBoot2.x</td><td>2</td><td>2</td><td>Apache License 2.0</td></tr><tr><td>417511458/jbone</td><td><a href="https://github.com/417511458/jbone" target="_blank" rel="noopener">https://github.com/417511458/jbone</a></td><td>763</td><td>2019年8月</td><td>无演示</td><td>-</td><td>SpringBoot2.x</td><td>11</td><td>5</td><td>Apache License 2.0</td></tr><tr><td>The Sun / Cloud-Platform</td><td><a href="https://gitee.com/geek_qi/cloud-platform" target="_blank" rel="noopener">https://gitee.com/geek_qi/cloud-platform</a> （github也有地址）</td><td>3286(github) / 9.7k(gitee)</td><td>2019年7月</td><td>SpringBoot2.x</td><td>189</td><td>184</td><td>Apache License 2.0</td><td></td><td></td></tr><tr><td>pig4cloud.com / pig</td><td><a href="https://gitee.com/log4j/pig" target="_blank" rel="noopener">https://gitee.com/log4j/pig</a></td><td>8.3k(gitee)</td><td>2019年8月（最近）</td><td>有演示</td><td><a href="http://pigx.pig4cloud.com/#/login" target="_blank" rel="noopener">http://pigx.pig4cloud.com/#/login</a></td><td>SpringBoot2.x</td><td>212</td><td>209</td><td>LGPL</td></tr><tr><td>@Sun / JeeSpringCloud</td><td><a href="https://gitee.com/JeeHuangBingGui/jeeSpringCloud" target="_blank" rel="noopener">https://gitee.com/JeeHuangBingGui/jeeSpringCloud</a></td><td>5.6k(gitee)</td><td>2019年8月（最近）</td><td>有演示</td><td><a href="http://bknfdnl.hn3.mofasuidao.cn/admin/login" target="_blank" rel="noopener">http://bknfdnl.hn3.mofasuidao.cn/admin/login</a></td><td>SpringBoot2.x</td><td>48</td><td>1</td><td>MIT</td></tr><tr><td>zhuan / zscat-me</td><td><a href="https://gitee.com/catshen/zscat_sw" target="_blank" rel="noopener">https://gitee.com/catshen/zscat_sw</a></td><td>2.8k（gitee）</td><td>2019年8月（最近）</td><td>有演示</td><td><a href="http://www.yjlive.cn:8090/#/login" target="_blank" rel="noopener">http://www.yjlive.cn:8090/#/login</a></td><td>SpringBoot2.x</td><td>6</td><td>0</td><td>Apache License 2.0</td></tr><tr><td>zlt2000/microservices-platform</td><td><a href="https://github.com/zlt2000/microservices-platform/blob/master/LICENSE" target="_blank" rel="noopener">https://github.com/zlt2000/microservices-platform/blob/master/LICENSE</a></td><td>680</td><td>2019年8月</td><td>有演示</td><td><a href="http://mp.zlt2000.cn" target="_blank" rel="noopener">http://mp.zlt2000.cn</a></td><td>SpringBoot2.x</td><td>2</td><td>1</td><td>Apache License 2.0</td></tr><tr><td>open-capacity-platform</td><td><a href="https://gitee.com/owenwangwen/open-capacity-platform" target="_blank" rel="noopener">https://gitee.com/owenwangwen/open-capacity-platform</a></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>注意：</p><ol><li>issue总数和issue解决数相对不准确，参考时需要降低这两项的权重</li><li>Cloud-Platform项目在GitHub上的star数量有造假的嫌疑</li></ol><h3 id="2-项目功能支持-技术栈"><a href="#2-项目功能支持-技术栈" class="headerlink" title="2. 项目功能支持/技术栈"></a>2. 项目功能支持/技术栈</h3><table><thead><tr><th>名称</th><th>服务注册和发现</th><th>服务网关</th><th>服务治理(调用、限流、熔断)</th><th>配置管理</th><th>链路跟踪</th><th>日志监控</th><th>缓存中间件</th><th>消息队列</th><th>多租户</th><th>认证管理</th><th>其它支持</th></tr></thead><tbody><tr><td>zhangxd1989/spring-boot-cloud</td><td>Eureka</td><td>Zuul</td><td>Hystrix - 熔断器 / Feign - 调用</td><td>Spring Cloud Config</td><td>zipkin</td><td>Spring Cloud Sleuth（二次封装）</td><td>-</td><td>rabbitmq / Spring Cloud Bus - 事件、消息总线</td><td>不支持</td><td>OAuth2 / Spring Cloud OAuth2</td><td>数据流操作封装Spring Cloud Stream、Ribbon - 提供云端负载均衡</td></tr><tr><td>cloudframeworks-springcloud/user-guide-springcloud</td><td>Eureka</td><td>Zuul</td><td>Hystrix - 熔断器 / Turbine - 聚合器</td><td>Spring Cloud Config</td><td>包含</td><td>通过EFKA（elasticsearch fluentd kibana kafka）进行日志收集和展示</td><td>-</td><td>RabbitMQ（服务容错机制使用）</td><td>不支持</td><td>提供基本认证服务</td><td>业务无关性、配置详细、文档丰富</td></tr><tr><td>mxdldev/spring-cloud-flycloud</td><td>Eureka</td><td>Zuul</td><td>Hystrix</td><td>Spring Cloud Config</td><td>zipkin</td><td>Turbine聚合监控服务</td><td>-</td><td>不支持</td><td>不支持</td><td>OAuth2+JWT / Spring Cloud OAuth2</td><td></td></tr><tr><td>zhoutaoo/SpringCloud</td><td>Eureka</td><td>SpringCloud Gateway</td><td>Hystrix / OpenFeign</td><td>Appollo</td><td>SkyWalking</td><td>ES + Kibana、Zipkin / kibana</td><td>redis</td><td>rabbitmq</td><td>不支持</td><td>Spring Security OAuth2</td><td>支持分库分表、支持操作审计、基础模块未完成项目较多</td></tr><tr><td>chillzhuang/SpringBlade</td><td>Nacos</td><td>Zuul</td><td>Hystrix - 熔断器 / Sentinel - 限流/  Feign - 调用</td><td>Nacos</td><td>SkyWalking</td><td>ELK</td><td>redis</td><td>未知</td><td>支持</td><td>Oauth2+JWT</td><td>Ribbon - 提供云端负载均衡，完善的监控体系、开发手册付费</td></tr><tr><td>417511458/jbone</td><td>Eureka</td><td>Zuul</td><td>Hystrix - 熔断器 /  Feign - 调用</td><td>未知</td><td>zipkin</td><td>未知</td><td>Redis / Spring Data Redis</td><td>rabbitmq</td><td>不支持</td><td>Shiro / Spring Validator /Hibernate Validator</td><td>管理侧前端页面重写、 有jvm监控</td></tr><tr><td>The Sun / Cloud-Platform</td><td>Nacos / Consul</td><td>Zuul</td><td>Sentinel - 限流 / Feign - 调用</td><td>支持</td><td>支持</td><td>ELK + logstash</td><td>redis</td><td>未知</td><td>支持</td><td>不支持</td><td>Oauth2+JWT</td></tr><tr><td>pig4cloud.com / pig</td><td>Eureka</td><td>SpringCloud Gateway</td><td>Hystrix</td><td>支持</td><td>zipkin</td><td>未知</td><td>未知</td><td>未知</td><td>不支持</td><td>Oauth2</td><td>含有代码生成模块、商用需要付费</td></tr><tr><td>@Sun / JeeSpringCloud</td><td></td><td></td><td></td><td></td><td></td><td></td><td>ActiveMQ</td><td></td><td></td><td>Shiro + Oauth2</td><td>需捐献领取企业版本、资料不全无法查看、演示后台示例详细</td></tr><tr><td>zhuan / zscat-me</td><td>Nacos</td><td>Zuul</td><td>Sentinel - 限流 / 支持熔断 / Feign - 调用</td><td>支持</td><td>SkyWalking</td><td>ELK / Kibana / Logstash</td><td>redis</td><td>RocketMQ</td><td>不支持</td><td>Oauth2 + JWT</td><td>完整商城项目、支持监控预警、支持数据库读写分离</td></tr><tr><td>zlt2000/microservices-platform</td><td>Nacos</td><td>Zuul</td><td>Sentinel - 限流 / 支持熔断 / Feign - 调用</td><td>支持</td><td>支持</td><td>支持</td><td>Redis</td><td>RocketMQ</td><td>支持</td><td>基于RBAC、jwt和oauth2的无状态统一权限认证</td><td>文档部分付费、监控数据全面完善</td></tr><tr><td>open-capacity-platform</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="3-项目环境要求"><a href="#3-项目环境要求" class="headerlink" title="3. 项目环境要求"></a>3. 项目环境要求</h3><table><thead><tr><th>名称</th><th>容器化</th><th>依赖管理</th><th>前端支持</th><th>java要求</th></tr></thead><tbody><tr><td>zhangxd1989/spring-boot-cloud</td><td>支持docker</td><td>maven</td><td>无，需开发</td><td>java8</td></tr><tr><td>cloudframeworks-springcloud/user-guide-springcloud</td><td>Docker+Kubernetes</td><td>maven</td><td>无，需开发</td><td>java8</td></tr><tr><td>mxdldev/spring-cloud-flycloud</td><td>需自行配置</td><td>gradle</td><td>无，需开发</td><td>java8</td></tr><tr><td>zhoutaoo/SpringCloud</td><td>支持docker</td><td>maven</td><td>有，可参考</td><td>java8</td></tr><tr><td>chillzhuang/SpringBlade</td><td>支持docker、k8s</td><td>maven</td><td>有，可参考，支持React和Vue</td><td>java8</td></tr><tr><td>417511458/jbone</td><td>需自行配置</td><td>maven</td><td>有，可参考</td><td>java11</td></tr><tr><td>The Sun / Cloud-Platform</td><td>支持docker</td><td>maven</td><td>有，支持Vue</td><td>java8</td></tr><tr><td>pig4cloud.com / pig</td><td>支持docker</td><td>maven</td><td>有，支持vue，pigx-ui</td><td>java8</td></tr><tr><td>@Sun / JeeSpringCloud</td><td>需自行配置</td><td>maven</td><td>有， 可参考，案例翔实</td><td>java8</td></tr><tr><td>zhuan / zscat-me</td><td>需自行配置</td><td>maven</td><td>有，可参考</td><td>java8</td></tr><tr><td>zlt2000/microservices-platform</td><td>支持，需自行配置</td><td>maven</td><td>有，可参考</td><td>java8</td></tr><tr><td>open-capacity-platform</td><td></td><td></td><td></td><td></td></tr></tbody></table><h2 id="筛选条件"><a href="#筛选条件" class="headerlink" title="筛选条件"></a>筛选条件</h2><p>针对GitHub：star数超过500</p><p>针对Gitee：star数超过2000</p><p>特别：stylefeng / Guns 项目需要商业授权，对微服务支持一般，故放弃</p><h1 id="番外：行业内微服务平台架构建议"><a href="#番外：行业内微服务平台架构建议" class="headerlink" title="番外：行业内微服务平台架构建议"></a>番外：行业内微服务平台架构建议</h1><h2 id="没有银弹"><a href="#没有银弹" class="headerlink" title="没有银弹"></a>没有银弹</h2><p>希望各位已经认真评估了已有平台的优势和劣势，切勿跟风。要深知微服务并不会解决所有问题，在解决当下所存在矛盾的同时，又引入了新的矛盾。如何评估新的矛盾对于当前系统拆分微服务的影响，并且如何找到克制这些矛盾的做法，才能更好的认识微服务带来的价值。</p><h2 id="对自己的正确认识"><a href="#对自己的正确认识" class="headerlink" title="对自己的正确认识"></a>对自己的正确认识</h2><p>深知自己公司、企业对于微服务是否真的需要，评估时请通盘考虑自己的需求。对于已有系统，需要认识到是否真的到了拆分微服务的时候，是否真的理解拆分微服务带来的各种边际效益以及其引入的问题；对于尚未开发的系统，临时只停留在纸面规划上，需要考虑是先解决有无还是解决系统演进。</p><p>根据我们实施微服务的经验，我们认为直接使用微服务进行开发成本高于单体应用的开发，况且存在各种人力资源、线上服务器资源以及开发人员素质参差不齐的矛盾，也存在团队内部磨合、流程不正规的问题，在前期都是需要去纠正的问题。</p><h2 id="对业务的充分理解"><a href="#对业务的充分理解" class="headerlink" title="对业务的充分理解"></a>对业务的充分理解</h2><p>是否对自己公司的业务了如执掌？是否对于各个流程、各个模块之间的关系了如指掌？能否在业务层面划分清晰的层次？能否对于当前的发展正确认识，了解到如何面向业务进行妥协？能否在纸面上体现所有的业务信息并且有准确的描述？</p><h2 id="对于业务演进路线的判断"><a href="#对于业务演进路线的判断" class="headerlink" title="对于业务演进路线的判断"></a>对于业务演进路线的判断</h2><p>能否对于自己业务的成长有充分的判断依据，并且能够提前预见业务成长对于整个平台架构的影响？</p><p>我们目前实行的方案是以10倍容量进行设计，以3至4倍的容量进行实施，留有较大余量满足业务成长的需求。</p><h2 id="对于实施微服务过程的理解"><a href="#对于实施微服务过程的理解" class="headerlink" title="对于实施微服务过程的理解"></a>对于实施微服务过程的理解</h2><p>是接受立马看到效果还是接受较长时间内才能看到整体的成长？能否接受引入微服务开发带来的开发成本、运维成本的上升？能否接受对于硬件资源、云服务资源的高度需求？根据业务量选择是否上云。</p><h2 id="妥协"><a href="#妥协" class="headerlink" title="妥协"></a>妥协</h2><p>无论任何架构、任何软件在一定时期内必须做到妥协，即满足现有业务，略微超前思考，预先留有业务扩展空间。在权衡业务和技术、人员和团队、刚需和低级需求的矛盾中找到平衡，能接受一定程度的不良产出，换取长足的业务进步。</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java SpringCloud 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开发环境以及版本约定</title>
      <link href="/2019/09/01/kai-fa-huan-jing-yi-ji-ban-ben-yue-ding/"/>
      <url>/2019/09/01/kai-fa-huan-jing-yi-ji-ban-ben-yue-ding/</url>
      
        <content type="html"><![CDATA[<h1 id="开发环境以及版本约定"><a href="#开发环境以及版本约定" class="headerlink" title="开发环境以及版本约定"></a>开发环境以及版本约定</h1><h2 id="1-日常开发"><a href="#1-日常开发" class="headerlink" title="1. 日常开发"></a>1. 日常开发</h2><p>1.1 环境：java8还是更高版本？jdk1.8的商用限制？</p><p>1.2 IDE：Intellij Idea还是Eclipse，版本用最新吗？</p><p>1.3 构建工具：maven还是gradle？用maven3以上版本？gradle是否用最新？</p><p>1.4 私有仓库建设，接上用maven如何搭建nexus私服？同步策略？</p><p>1.5 应用容器：日常测试用jetty？war包用Tomcat？Tomcat8版本？jetty版本？</p><p>1.6 数据库：PostgreSQL还是MySQL？各自版本？MySQL是5.7还是8？集群搭建方案？读写分离？后端数据库框架的变更？</p><p>1.7 前端：npm版本？nodejs版本？Vue版本？React版本？EcmaScript用2016版本？是否需要使用框架，例如ElementUI或者是AntDesign这样的？前端的发布和管理？前后端交互方式，是HTTP还是其它？原型设计和易用性交互？前端工程化？依赖管理/包管理？（前端单独整理）</p><p>1.8 浏览器兼容：IE 6、7、8问题？Chrome版本问题？FireFox版本问题？</p><p>1.9 后端开发（每一项都需要列举具体框架版本）</p><pre><code>前提：SpringBoot具体版本？Spring Cloud具体版本？1.9.1 服务注册和发现Eureka还是Dubbo？zookeeper、etcd还是consul？Nacos？服务发现：客户端发现还是服务端发现？服务注册：自注册还是第三方注册方式？1.9.2 网关Zuul还是Nginx Plus？选择Spring Cloud Gateway？Linkerd还是Envoy,还有UnderTow？1.9.3 服务调用RestTemplate还是FeignClient?Ribbon和Feign的对比与选择？1.9.5 服务限流RateLimiter还是使用sentinel？限流算法选择？是固定、滑动时间窗口限流还是令牌桶、漏桶限流？1.9.6 服务熔断Hystrix（Turbine收集Hystrix指标）1.9.7 配置管理Spring Cloud Config还是Apollo还是Nacos？1.9.8 链路跟踪Skywalking还是Spring Cloud Sleuth+Zipkin？是否考虑Pinpoint？1.9.9 消息队列ActiveMQ，RabbitMQ，RocketMQ，还是kafka？1.9.10 缓存及其配置分布式缓存？使用Redis sentinel搭建还是使用微博的CacheService？DCache？1.9.11 日志监控ELK+Kibana+Logstash？EFKA（elasticsearch fluentd kibana kafka）体系？是否需要Turbine收集Hystrix指标?日志的存储策略，包括级别、提取的有效信息、存储时间、备份策略等。TICK体系（https://blog.csdn.net/lin_credible/article/details/60579738）日志审计策略1.9.12 权限管理用户权限：基于RBAC、JWT、OAuth2的无状态权限管理体系文档权限：用户访问控制，区域控制，权限继承-&gt;模型继承</code></pre><p>1.10 数据库解决方案</p><pre><code>1.10.1 数据库分库分表方案Sharding横向扩展、ScaleOut/ScaleUp（垂直切分/水平拆分）、分表分区、分表分库？分布式事务问题？拆分维度、事务问题、跨库跨表join，额外的数据管理负担、多库结果集合并？是否考虑atomikos+jta解决分布式事务问题？框架权衡：基于代理方式的有MySQL Proxy和Amoeba， 基于Hibernate框架的是Hibernate Shards，基于jdbc的有当当sharding-jdbc， 基于mybatis的类似maven插件式的有蘑菇街的蘑菇街TSharding，其它例如MySQL Fabric、TDDL（阿里）、Cobar（阿里）、Altas（269）、Heisenberg（百度）、Vitess（youtube）。1.10.2 数据库读写分离方案主从复制，一写多读？Master故障后的选举策略？读库和写库的同步策略？微服务缓存架构是否跟适合支撑大数据量、高并发、高可用要求（见1.11缓存部分问题）？如果面临大量写数据的问题，例如大BOM的写入，又该如何权衡？引擎选择 MyISAM vs InnoDB？框架权衡：Maxsacle + KeepAlived lvs？1.10.3 数据库高可用方案同城双活、异地容灾、两地三中心策略？数据同步机制？数据恢复机制？1.10.4 数据库中间件MyCat？其它选择？1.10.5 文件数据库/非关系型数据库</code></pre><p>1.11 缓存策略与缓存集群</p><pre><code>1.11.1 缓存框架分布式缓存Redis、Memcache？程序侧Ehcache、Guava Cache？1.11.2 缓存策略什么内容入缓存？过期策略？脏数据处理？是否固化？更新缓存的策略：Cache aside、 Read through、 Write through、 Write behind caching，用哪一种？与数据库的协同？1.11.3 缓存集群Cluster模式还是哨兵模式？单机多应用伪集群（测试）模式还是多机集群（生产）部署模式？集群内的同步策略？</code></pre><p>1.12 文件服务</p><pre><code>1.12.1 文件集群小文件存储和大文件存储是否分离？几个FS的权衡和选择：GFS、HDFS、Ceph、Lustre、MogileFS、mooseFS、FastDFS、TFS、GridFS1.12.2 OSS安全性-文件验证、可执行代码问题文件访问控制，禁止文件存储位置的可执行权限，验证文件格式信息？1.12.3 大文件存储和传输1.12.4 </code></pre><p>1.15 搜索引擎</p><pre><code>数据检索、结果缓存、分词存储、</code></pre><p>1.16 多语言支持</p><p>1.17 任务调度中心</p><p>1.13 访问</p><pre><code>1.13.1 负载均衡Nginx/OpenResty + Keepalived？网关侧控制？内部接口调用的负载均衡？1.13.2 边缘加速CDN服务、DNS服务？1.13.3 分布式部署</code></pre><p>1.14 自动化</p><p>自动构建和测试用例，测试用例覆盖度？</p><p>git + gitlab + jenkins + sonar自动构建和单元测试平台。</p><p>1.14 开发时，操作系统选择：开发层面统一使用Win10还是Win7？是否允许其它操作系统？</p><h2 id="2-线上部署和运维"><a href="#2-线上部署和运维" class="headerlink" title="2. 线上部署和运维"></a>2. 线上部署和运维</h2><p>2.1 服务器选择及其配置</p><p>CentOS7？具体版本确定？大致的服务器数量、配置、功能分配、位置选择？私有镜像制作，针对不同使用场景加入我们自身的安全策略！</p><p>2.2 容器化</p><p>选择docker还是选择其它容器？是Kubernetes还是Mosos+marathon？具体版本？</p><p>2.3 服务器实例/集群管理</p><p>（暂时未想好）</p><p>2.4 监控平台</p><p>Zabbix+Grafana？Kibana？日志展示？全链路监控展示？</p><p>如何将展示信息具象化、简洁化？</p><p>2.5 异常告警</p><p>告警方式？邮件、企业微信、钉钉？</p><p>紧急响应的人员如何安排？针对紧急上线的修改如何管理？</p><p>2.6 平台安全性</p><p>SQL注入？跨站攻击？渗透测试？</p><h2 id="3-日常培训策略"><a href="#3-日常培训策略" class="headerlink" title="3. 日常培训策略"></a>3. 日常培训策略</h2><p>3.1 规约与制度</p><p>编码规范、SQL规范或者最佳实践、CodeReview</p><p>议事制度、协作方式、冲突裁定</p><p>3.2 开发方式</p><p>有限开放？封闭？</p><p>3.3 安全</p><p>HTTPS TLS2.1，接口数据加密与解密</p><p>3.4 最佳实践</p><p>设计模式、重构</p><p>3.5 研发管理</p><p>bug管理、功能日常进度、测试情况 – 禅道？Coplan？</p><h2 id="4-具体业务落地"><a href="#4-具体业务落地" class="headerlink" title="4. 具体业务落地"></a>4. 具体业务落地</h2><p>4.1 用户中心</p><p>多租户服务能力、权限控制、单点登录–RBAC、JWT、OAuth2的无状态权限管理体系</p><p>4.2 BOM系统</p><p>4.3 工作流</p><p>4.4 图文档管理</p><p>Activiti、JBoss JBPM 6.5、JFlow 6.0（国产）、FixFlow 5.0</p><p>4.5 自定义表单</p><p>4.6 BI数据统计（非重点）</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java SpringCloud 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zabbix+Grafana运维监控平台搭建</title>
      <link href="/2019/08/15/zabbix-grafana-yun-wei-jian-kong-ping-tai-da-jian/"/>
      <url>/2019/08/15/zabbix-grafana-yun-wei-jian-kong-ping-tai-da-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="zabbix-grafana进行服务监控和参数图形化展示"><a href="#zabbix-grafana进行服务监控和参数图形化展示" class="headerlink" title="zabbix+grafana进行服务监控和参数图形化展示"></a>zabbix+grafana进行服务监控和参数图形化展示</h1><h2 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p>CentOS7.6，内核版本3.10，承载Nginx服务和Zabbix+Grafana服务。需要注意的是，Docker 目前支持 CentOS 7 及以后的版本，内核要求至少为 3.10。</p><h3 id="WinServer"><a href="#WinServer" class="headerlink" title="WinServer"></a>WinServer</h3><ul><li><p>四台IIS服务<br>WinServer2012</p></li><li><p>四台SQLServer服务<br>WinServer2016 + SQLServer2016</p></li><li><p>一台FileServer<br>WinServer2016</p></li></ul><h3 id="监控版本说明"><a href="#监控版本说明" class="headerlink" title="监控版本说明"></a>监控版本说明</h3><ul><li>zabbix 4.2</li><li>grafana 6.1</li></ul><h2 id="监控信息–利用zabbix进行服务监控"><a href="#监控信息–利用zabbix进行服务监控" class="headerlink" title="监控信息–利用zabbix进行服务监控"></a>监控信息–利用zabbix进行服务监控</h2><ul><li><p>zabbix安装和部署–利用docker进行安装部署</p><ul><li><p>zabbix版本：<br>zabbix server和zabbix agent版本均采用v4.2.1版本的程序进行安装部署。</p></li><li><p>CentOS安装docker</p><p>  注意：Docker 需要用到 <code>centos-extra</code> 这个源，如果您关闭了，需要重启启用，可以参考 <a href="https://wiki.centos.org/AdditionalResources/Repositories" target="_blank" rel="noopener">Available Repositories for CentOS</a>。</p><ul><li><p>卸载旧版本</p><pre><code>$ sudo yum remove docker \       docker-client \       docker-client-latest \       docker-common \       docker-latest \       docker-latest-logrotate \    docker-logrotate \        docker-engine</code></pre><p> 旧版本的内容在 /var/lib/docker 下，目录中的镜像(images), 容器(containers), 存储卷(volumes), 和 网络配置（networks）都可以保留。</p><p> Docker CE 包，目前的包名为 docker-ce。Docker现在分为两个版本，Docker CE和Docker EE<br> 其中Docker CE为开源版，Docker EE为企业版</p><ul><li><p>安装准备<br> 为了方便添加软件源，支持 devicemapper 存储类型，安装如下软件包</p><pre><code>$ sudo yum update$ sudo yum install -y yum-utils \      device-mapper-persistent-data \      lvm2</code></pre></li><li><p>添加 yum 软件源<br>添加 Docker 稳定版本的 yum 软件源</p><pre><code>$ sudo yum-config-manager \      --add-repo \      https://download.docker.com/linux/centos/docker-ce.repo</code></pre><ul><li>安装 Docker<br>更新一下 yum 软件源的缓存，并安装 Docker。<pre><code>$ sudo yum update$ sudo yum install docker-ce</code></pre>如果弹出 GPG key 的接收提示，请确认是否为 <strong>060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35</strong>，如果是，可以接受并继续安装。<br>至此，Docker 已经安装完成了，Docker 服务是没有启动的，操作系统里的 docker 组被创建，但是没有用户在这个组里。</li></ul><p><strong>注意：</strong><br>默认的 docker 组是没有用户的（也就是说需要使用 sudo 才能使用 docker 命令）。<br>您可以将用户添加到 docker 组中（此用户就可以直接使用 docker 命令了）。<br>加入 docker 用户组命令:</p><pre><code>$ sudo usermod -aG docker USER_NAME</code></pre><p>用户组更新信息后，重新登录系统生效。</p><ul><li><p>安装指定版本<br>如果想安装指定版本的 Docker，可以查看一下版本并安装。</p><pre><code>$ yum list docker-ce --showduplicates | sort -rdocker-ce.x86_64  3:18.09.1-3.el7                     docker-ce-stabledocker-ce.x86_64  3:18.09.0-3.el7                     docker-ce-stabledocker-ce.x86_64  18.06.1.ce-3.el7                    docker-ce-stabledocker-ce.x86_64  18.06.0.ce-3.el7                    docker-ce-stable</code></pre><p>可以指定版本安装,版本号可以忽略 <em>:</em> 和 _el7_，如 docker-ce-18.09.1</p><pre><code>$ sudo yum install docker-ce-&lt;VERSION STRING&gt;</code></pre><p>至此，指定版本的 Docker 也安装完成，同样，操作系统内 docker 服务没有启动，只创建了 docker 组，而且组里没有用户。</p></li></ul></li></ul><ul><li>启动docker<br>如果想添加到开机启动<pre><code>$ sudo systemctl enable docker</code></pre>启动 docker 服务<pre><code>$ sudo systemctl start docker</code></pre></li><li>验证安装<br>验证 Docker CE 安装是否正确，可以运行 hello-world 镜像<pre><code>$ sudo docker run hello-world</code></pre></li><li>更新和卸载Docker CE<pre><code># 更新 Docker CE$ sudo yum update docker-ce# 卸载 Docker CE$ sudo yum remove docker-ce</code></pre></li><li>删除本地文件<br>注意，docker 的本地文件，包括镜像(images), 容器(containers), 存储卷(volumes)等，都需要手工删除。默认目录存储在 /var/lib/docker。<pre><code>$ sudo rm -rf /var/lib/docker</code></pre></li></ul></li></ul></li><li><p>通过docker安装zabbix服务<br>  首先需要把镜像下载下来，分为以下四个</p><ul><li><p>mysql</p></li><li><p>zabbix-java-gateway</p></li><li><p>zabbix-server-mysql</p></li><li><p>zabbix-web-apache-mysql</p><pre><code># 下载镜像$sudo docker pull mysql:5.7$sudo docker pull zabbix/zabbix-java-gateway:centos-4.2-latest$sudo docker pull zabbix/zabbix-server-mysql:centos-4.2-latest$sudo docker pull zabbix/zabbix-web-apache-mysql:centos-4.2-latest</code></pre><p>然后挨个进行启动</p><pre><code># mysql$ sudo docker run --name mysql-server -t \      -e MYSQL_DATABASE=&quot;zabbix&quot; \      -e MYSQL_USER=&quot;zabbix&quot; \      -e MYSQL_PASSWORD=&quot;zabbix&quot; \      -e MYSQL_ROOT_PASSWORD=&quot;zabbix&quot; \      -p 127.0.0.1:3306:3306 \      -d mysql:5.7 \      --character-set-server=utf8 \ --collation-server=utf8_bin # zabbix-java-gateway$sudo docker run --name zabbix-java-gateway -t \      -d zabbix/zabbix-java-gateway:latest# zabbix-server-mysql$sudo docker run --name zabbix-server-mysql -t \      --link mysql-server:mysql \      -e DB_SERVER_HOST=&quot;mysql-server&quot; \      -e MYSQL_DATABASE=&quot;zabbix&quot; \      -e MYSQL_USER=&quot;zabbix&quot; \      -e MYSQL_PASSWORD=&quot;zabbix&quot; \      -e MYSQL_ROOT_PASSWORD=&quot;zabbix&quot; \      -p 10051:10051 \      -d \      zabbix/zabbix-server-mysql:centos-4.2-latest# zabbix-web-apache-mysql$sudo docker run --name zabbix-web-apache-mysql -t \  --link mysql-server:mysql \  --link zabbix-server-mysql:zabbix-server \  -e DB_SERVER_HOST=&quot;mysql-server&quot; \  -e MYSQL_DATABASE=&quot;zabbix&quot; \  -e MYSQL_USER=&quot;zabbix&quot; \  -e MYSQL_PASSWORD=&quot;zabbix&quot; \  -e MYSQL_ROOT_PASSWORD=&quot;zabbix&quot; \  -e PHP_TZ=&quot;Asia/Shanghai&quot; \  -p 80:8088 \     # 前一个端口为docker镜像端口，后一个端口为服务器本地端口,映射时  -d \  zabbix/zabbix-web-apache-mysql:centos-4.2-latest</code></pre></li></ul><p>  <strong>注意：</strong></p><ol><li>需要注意端口号占用的问题， 灵活调整</li><li>按顺序进行启动</li></ol></li></ul></li></ul><p>至此，zabbix服务端就部署完毕了，可以通过浏览器访问localhost:8088进行访问了，输入用户名Admin，密码zabbix，即可进入。</p><ul><li><p>补充：Docker镜像开机启动命令，在所有命令后面添加<strong>–restart=always</strong></p></li><li><p>补充：Docker镜像添加<em>–restart=always</em>后，对于zabbix-web-apache-mysql所在镜像，需要及时检查其启动状态。如果一直处在restart状态，需要停止运行、rm，重新启动</p></li><li><p>补充：关于时钟问题，当服务器时钟不准确时，需要去掉”-e PHP_TZ=”Asia/Shanghai” “的配置选项！</p></li><li><p>补充：Docker常用命令</p></li></ul><pre><code># 查看当前系统 Docker 信息 docker info# 拉取 docker 镜像 docker pull image_name# 从 Docker hub 上下载某个镜像 docker pull centos:latest# 查看宿主机上的镜像，Docker 镜像保存在 / var/lib/docker 目录下:docker imagesREPOSITORY                      TAG                 IMAGE ID            CREATED             SIZEmysql                           5.7                 1b30b36ae96a        8 days ago          372MBzabbix/zabbix-web-nginx-mysql   centos-4.0-latest   8be5f91b2fa1        3 weeks ago         415MBzabbix/zabbix-server-mysql      centos-4.0-latest   8e5becf45c4e        3 weeks ago         326MB# 删除镜像 docker rmi image_name/image_iddocker rmi zabbix/zabbix-web-nginx-mysql:centos-4.0-latest 或者 docker rmi 8be5f91b2fa1# 查看当前有哪些容器正在运行 docker ps# 查看所有容器 docker ps -aCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS                       PORTS                           NAMESb30307ad65be        zabbix/zabbix-web-nginx-mysql:centos-4.0-latest   &quot;docker-entrypoint.sh&quot;   7 days ago          Exited (255) 8 minutes ago   443/tcp, 0.0.0.0:8080-&gt;80/tcp   zabbix-web-nginx-mysql0ad822cd52b7        zabbix/zabbix-server-mysql:centos-4.0-latest      &quot;docker-entrypoint.sh&quot;   7 days ago          Exited (255) 8 minutes ago   0.0.0.0:10051-&gt;10051/tcp        zabbix-server-mysqld01c89a112f7        mysql:5.7                                         &quot;docker-entrypoint.s…&quot;   7 days ago          Exited (255) 8 minutes ago   3306/tcp, 33060/tcp             mysql-server# 启动、停止、重启容器命令：docker start container_name/container_iddocker stop container_name/container_iddocker restart container_name/container_id# 后台启动一个容器后，如果想进入到这个容器，可以使用 attachdocker attach container_name/container_id# 删除容器 docker rm container_name/container_id# 查看容器日志 docker logs -f container_name/container_id# 查看容器 IP 地址 docker inspect container_name/container_id# 进入容器 docker exec -it container_name/container_id bash# 从 Docker 容器与宿主机相互传输文件 [root@localhost tmp]# docker cp --helpUsage:    docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-    docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATHCopy files/folders between a container and the local filesystemdocker cp zabbix_config.sql mysql-server:/tmpdocker cp mysql-server:/tmp/zabbix_config.sql /tmp# 批量删除所有已经退出的容器 docker rm -v $(docker ps -aq -f status=exited)# docker help[root@centos7lined1 wangao]# docker helpUsage:    docker [OPTIONS] COMMANDA self-sufficient runtime for containersOptions:      --config string      Location of client config files (default &quot;/root/.docker&quot;)  -D, --debug              Enable debug mode  -H, --host list          Daemon socket(s) to connect to  -l, --log-level string   Set the logging level (&quot;debug&quot;|&quot;info&quot;|&quot;warn&quot;|&quot;error&quot;|&quot;fatal&quot;) (default &quot;info&quot;)      --tls                Use TLS; implied by --tlsverify      --tlscacert string   Trust certs signed only by this CA (default &quot;/root/.docker/ca.pem&quot;)      --tlscert string     Path to TLS certificate file (default &quot;/root/.docker/cert.pem&quot;)      --tlskey string      Path to TLS key file (default &quot;/root/.docker/key.pem&quot;)      --tlsverify          Use TLS and verify the remote  -v, --version            Print version information and quitManagement Commands:  config      Manage Docker configs  container   Manage containers  image       Manage images  network     Manage networks  node        Manage Swarm nodes  plugin      Manage plugins  secret      Manage Docker secrets  service     Manage services  stack       Manage Docker stacks  swarm       Manage Swarm  system      Manage Docker  trust       Manage trust on Docker images  volume      Manage volumesCommands:  attach      Attach local standard input, output, and error streams to a running container  build       Build an image from a Dockerfile  commit      Create a new image from a container&#39;s changes  cp          Copy files/folders between a container and the local filesystem  create      Create a new container  diff        Inspect changes to files or directories on a container&#39;s filesystem  events      Get real time events from the server  exec        Run a command in a running container  export      Export a container&#39;s filesystem as a tar archive  history     Show the history of an image  images      List images  import      Import the contents from a tarball to create a filesystem image  info        Display system-wide information  inspect     Return low-level information on Docker objects  kill        Kill one or more running containers  load        Load an image from a tar archive or STDIN  login       Log in to a Docker registry  logout      Log out from a Docker registry  logs        Fetch the logs of a container  pause       Pause all processes within one or more containers  port        List port mappings or a specific mapping for the container  ps          List containers  pull        Pull an image or a repository from a registry  push        Push an image or a repository to a registry  rename      Rename a container  restart     Restart one or more containers  rm          Remove one or more containers  rmi         Remove one or more images  run         Run a command in a new container  save        Save one or more images to a tar archive (streamed to STDOUT by default)  search      Search the Docker Hub for images  start       Start one or more stopped containers  stats       Display a live stream of container(s) resource usage statistics  stop        Stop one or more running containers  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE  top         Display the running processes of a container  unpause     Unpause all processes within one or more containers  update      Update configuration of one or more containers  version     Show the Docker version information  wait        Block until one or more containers stop, then print their exit codes</code></pre><ul><li><p>zabbix agent安装</p><ul><li><p>LVS</p><ol><li><p>直接安装，从软件源安装二进制包：</p><pre><code># 安装客户端$ sudo yum install zabbix-agent</code></pre><p>当客户端找不到的时候，需要先进行添加安装的rpm</p><pre><code>rpm -ivh http://repo.zabbix.com/zabbix/4.2/rhel/7/x86_64/zabbix-agent-4.2.0-1.el7.x86_64.rpm</code></pre><p>安装完成之后，进行启动：</p><pre><code>$ sudo service zabbix-agent start#或者执行以下命令：$ sudo systemctl start zabbix-agent# 停止、重启、查看状态，则需要执行以下命令：$ sudo service zabbix-agent stop$ sudo service zabbix-agent restart$ sudo service zabbix-agent status</code></pre><p>如果你的zabbix-agent和zabbix-server安装在同一台机器上，就不需要做额外配置，否则就需要进行服务器日志等的配置，配置参考win端的zabbix agent配置。</p><p>但是注意，如果不做配置的话，需要将你zabbix所在的linux服务端名称配置为默认的Zabbix Server。</p></li><li><p>编译安装(暂时不写)</p></li></ol></li><li><p>WinServer</p><ol><li><p>解压包安装</p><p>先从<a href="https://www.zabbix.com/download_agents#tab:42" target="_blank" rel="noopener">下载地址</a>下载windows的解压包，我们这里设定解压路径为<em>C:\ZabbixAgent</em>。下载图片中标记的客户端。</p><p>然后我们需要将其配置为系统服务并进行启动：</p><p>```shell</p><p>// cmd需要以管理员身份运行<br>// 安装<br>C:\ZabbixAgent\bin\zabbix_agentd.exe -c c:\ZabbixAgent\conf\zabbix_agentd.win.conf -i<br>// 启动<br>C:\ZabbixAgent\bin\zabbix_agentd.exe -c C:\ZabbixAgent\conf\zabbix_agentd.win.conf -s</p><p>// 停止<br>C:\ZabbixAgent\bin\zabbix_agentd.exe -c C:\ZabbixAgent\conf\zabbix_agentd.win.conf -x<br>// 卸载<br>C:\ZabbixAgent\bin\zabbix_agentd.exe -c C:\ZabbixAgent\conf\zabbix_agentd.win.conf -d   </p><p>// 命令解析</p></li></ol><p>  -c ：指定配置文件所有位置<br>  -i ：安装客户端<br>  -s ：启动客户端<br>  -x ：停止客户端<br>  -d ：卸载客户端</p><pre><code>  2. 安装包安装(暂时不写)操作完成之后需要从任务管理器里面查看是否存在ZabbixAgent服务，如果存在说明启动成功。</code></pre></li></ul></li><li><p>数据监控</p><ul><li><p>针对win server</p><p>如果只是针对WinServer进行监控，不需要对Zabbix agent进行脚本的配置更改。</p><ol><li><p>Zabbix agent端进行配置</p><p>首先找到我们解压的ZabbixAgent所在路径，为<strong>C:\ZabbixAgent\</strong>，然后找到conf文件夹下的zabbix_agentd.win.conf文件，进行修改：</p><pre><code>#日志文件存储位置LogFile=c:\ZabbixAgent\zabbix_agentd.log#zabbix主控端ip地址，也就是Server的地址Server=192.168.1.132#本机名，也可以在cmd下使用hostname命令获得，可以进行自定义Hostname=WinServer#zabbix主控端ip地址ServerActive=192.168.1.132</code></pre><p>然后安装再重启即可。</p></li><li><p>导入WinServer监控模板</p><p>由于Zabbix中自带WinServer监控模板，因此无需导入。</p></li><li><p>创建WinServer主机</p><p>步骤如下：</p><p><img src="zabbix-%E5%88%9B%E5%BB%BA%E4%B8%BB%E6%9C%BA.PNG" alt="zabbix-创建主机.PNG"></p><p><img src="zabbix-winServer%E9%85%8D%E7%BD%AE%E5%90%8D%E7%A7%B0.PNG" alt="zabbix-创建主机.PNG"></p><p><img src="zabbix-winserver-%E9%80%89%E6%8B%A9%E6%A8%A1%E6%9D%BF.PNG" alt="zabbix-创建主机.PNG"></p><p><img src="zabbix-WinServer-%E6%B7%BB%E5%8A%A0%E6%A8%A1%E6%9D%BF.PNG" alt="zabbix-创建主机.PNG"></p></li><li><p>查看上报数据<br>这样通过监测-&gt;图形，选择自己要监控的主机和监控数据类型，即可看到监控数据上报。如下图所示：</p><p><img src="zabbix-winserver%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE.PNG" alt="zabbix-创建主机.PNG"></p></li><li><p>参数指标详解</p></li></ol></li><li><p>针对linux server</p><p>如果只是针对WinServer进行监控，不需要对Zabbix agent进行脚本的配置更改。</p><ol><li><p>Zabbix agent端进行配置</p><ul><li><p>安装Zabbix Agent</p><p>注意：需要在VPS管理页面开放10051端口的访问。</p></li></ul></li><li><p>导入Linux监控模板</p></li><li><p>创建Linux主机</p></li><li><p>查看上报的数据</p></li><li><p>参数指标详解</p></li></ol><ul><li><p>补充：针对linux server遇到：failed to accept an incoming connection: connection from “one-ip” rejected, allowed hosts:“two-ip”的问题，需要：</p><p>  针对配置文件中的Server字段，多配置几个ip，示例如下：Server=127.0.0.1,192.168.100.10,172.73.1.3</p></li></ul></li><li><p>针对nginx监控</p><p>针对Nginx的监控，需要开启Nginx监控功能，然后对Zabbix agent端添加脚本并修改配置文件，然后进行导入模板的操作。Nginx部分接<br><a href="https://www.zybuluo.com/lsyAndroid/note/1466869" target="_blank" rel="noopener">上一篇文章</a></p><ol><li><p>配置文件开启监控功能</p><p>切换到nginx安装路径的conf文件夹下，例如/usr/local/nginx/conf目录，然后进行操作：</p><pre><code># 创建配置文件$ sudo touch nginx-status.conf$ vim nginx-status.confserver { listen 8090; # 内网访问端口 server_name abc.test.com; location /nginx_stat {     # 开启状态可访问     stub_status on;     access_log off; }}# 在nginx.conf文件中引用该文件$ sudo vim nginx.conf......include nginx-status.conf;# 检查配置文件没有问题$ sudo nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful# 重启nignx$ sudo nginx -s reload# 检查之后，可以通过curl来访问测试$  curl http://127.0.0.1:8090/nginx_statActive connections: 2server accepts handled requests 55342 55342 74613Reading: 0 Writing: 1 Waiting: 1</code></pre><p>参数指标解释：</p><p>Active connections –当前活跃的连接数量  </p><p>server accepts handled requests — 总共处理了756072922个连接 , 成功创建 756072922次握手, 总共处理了1136799890个请求</p><p>reading — 读取客户端的连接数.</p><p>writing — 响应数据到客户端的数量</p><p>waiting — 开启 keep-alive 的情况下,这个值等于 active – (reading+writing), 意思就是 Nginx 已经处理完正在等候下一次请求指令的驻留连接.</p><p>这样就配置完成了Nginx的监控状态。</p></li><li><p>配置Nginx监控执行脚本</p><p>在Nginx安装目录下，添加scripts文件夹，存放我们的监控脚本。</p><pre><code>$ sudo vim scripts/nginx-performance.sh#!/bin/bashHOST=&quot;139.159.199.40&quot;     #这里的地址要写自己的PORT=&quot;8090&quot;               #端口号和配置文件中的nginx_statfunction ping { num=$(/sbin/pidof nginx |wc -l)}function active { num=$(/usr/bin/curl -s &quot;http://$HOST:$PORT/nginx_stat&quot; |grep &#39;Active&#39; |awk &#39;{print $NF}&#39;)}function reading { num=$(/usr/bin/curl -s &quot;http://$HOST:$PORT/nginx_stat&quot; |grep &#39;Reading&#39; |awk &#39;{print $2}&#39;)}function writing { num=$(/usr/bin/curl -s &quot;http://$HOST:$PORT/nginx_stat&quot; |grep &#39;Writing&#39; |awk &#39;{print $4}&#39;)}function waiting { num=$(/usr/bin/curl -s &quot;http://$HOST:$PORT/nginx_stat&quot; |grep &#39;Waiting&#39; |awk &#39;{print $6}&#39;)}function accepts { num=$(/usr/bin/curl -s &quot;http://$HOST:$PORT/nginx_stat&quot; |awk NR==3 |awk &#39;{print $1}&#39;)}function handled { num=$(/usr/bin/curl -s &quot;http://$HOST:$PORT/nginx_stat&quot; |awk NR==3 |awk &#39;{print $2}&#39;)}function requests { num=$(/usr/bin/curl -s &quot;http://$HOST:$PORT/nginx_stat&quot; |awk NR==3 |awk &#39;{print $3}&#39;)}$1echo ${num:-0}</code></pre><p>脚本很简单，分表对应上节讲的各项指标。</p></li><li><p>Zabbix agent进行配置</p><p>配置文件配置如下：</p><pre><code>$ sudo vim /etc/zabbix/zabbix_agentd.conf# 守护进程文件PidFile=/var/run/zabbix/zabbix_agentd.pid# 日志文件LogFile=/var/log/zabbix/zabbix_agentd.log# 日志文件初始大小LogFileSize=0# zabbix server所在ip地址Server=127.0.0.1 # 更换为你自己的地址# 同上配置即可ServerActive=127.0.0.1# 主机名称，和你在zabbix中配置的主机名称一致，否则无法进行监控数据的采集Hostname=HT-LVS-Nginx-0001# 配置文件Include=/etc/zabbix/zabbix_agentd.d/*.conf# 用户自定义参数，传入脚本用UnsafeUserParameters=1# 脚本配置，*,*以前是你在zabbix服务端进行配置的参数名称，*， *后是你要执行的脚本以及传入的参数UserParameter=nginx.status[*],/usr/local/nginx/scripts/nginx-performance.sh $1</code></pre><p>配置完成后，需要重启zabbix-agent进程</p><pre><code>$ sudo systemctl restart zabbix_agentd# 或者执行下面的代码$ sudo /usr/sbin/zabbix_agentd -c /etc/zabbix/zabbix_agentd.conf</code></pre><p>这样就可以进行对Nginx的监控数据采集了。</p><p>此时可以在Server端执行:</p><pre><code># 在zabbix服务端（server）进行测试。$ zabbix_get -s 10.253.17.20 -p 10050 -k &quot;nginx[reading]&quot;0</code></pre></li><li><p>导入Nginx监控模板</p><p><a href="zabbix_monitor_nginx_template_ttlsa_com">下载模板</a>，然后进行导入。</p><p>首先，登录zabbix界面，依次点击：配置（configuration）—模板（template）—导入（import）</p><p><img src="zabbix_nginx%E6%A8%A1%E6%9D%BF%E5%AF%BC%E5%85%A5.png" alt></p><p>其次配置主机，注意这里配置的主机名称为：<strong>HT-LVS-Nginx-0001</strong>，然后是添加模板</p><p><img src="zabbix_nginx%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA.png" alt></p><p>最后，查看nginx监控的最新数据：监控中—图形—选择相应的监控类型。</p><p><img src="zabbix_nginx_%E7%9B%91%E6%8E%A71.png" alt></p><p><img src="zabbix_nginx-%E5%8F%82%E6%95%B0.png" alt></p></li><li><p>参数指标详解</p><table><thead><tr><th>名称</th><th>描述</th><th>指标类型</th></tr></thead><tbody><tr><td>Accepts（接受）</td><td>NGINX 所接受的客户端连接数</td><td>资源: 功能</td></tr><tr><td>Handled（已处理）</td><td>成功的客户端连接数</td><td>资源: 功能</td></tr><tr><td>Active（活跃）</td><td>当前活跃的客户端连接数</td><td>资源: 功能</td></tr><tr><td>Dropped（已丢弃，计算得出）</td><td>丢弃的连接数（接受 - 已处理）</td><td>工作：错误*</td></tr><tr><td>Requests（请求数）</td><td>客户端请求数</td><td>工作：吞吐量</td></tr></tbody></table></li></ol></li><li><p>针对SQL Server监控</p><ol><li><p>导入SQLServer配置模板</p><p>SQLServer模板很多，推荐使用<a href="https://share.zabbix.com/template-windows-sql-server" target="_blank" rel="noopener">这个模板</a>，下载后解压先把Template文件夹下面的<strong>Template - Windows LLD MSSQL - 32.xml</strong>的模板导入到zabbix中即可。<br>注意模板是葡萄牙语的，如果有能力请更换为英语或者简体中文。</p><p>汉化文章在此：<a href="https://blog.51cto.com/ygqygq2/2073139" target="_blank" rel="noopener">https://blog.51cto.com/ygqygq2/2073139</a></p></li><li><p>配置SQLServer监控执行脚本</p><p>在SQLServer所在机器安装zabbix agent，这点操作同WinServer，安装完成并配置服务进行启动。<br>然后再WinServer中配置监控执行脚本。在ZabbixAgent安装路径下，创建<strong>scripts</strong>文件夹，将上面下载的<strong>discovery.mssql.server.ps1</strong>拷贝到该文件夹下面。最后需要把<strong>discovery.mssql.server.conf</strong>这个配置文件拷贝到ZabbixAgent安装目录下的conf文件夹中，并且修改同级目录下的</p></li></ol><p>  <strong>zabbix_agentd.win.conf</strong>文件，如下:</p><pre><code>  LogFile=c:\ZabbixAgent\zabbix_agentd.log  Server=127.0.0.1  # 配置你自己的ZabbixServer所在地址  ServerActive=127.0.0.1 # 配置你自己的ZabbixServer所在地址  Hostname=HT-WinServer2016-db-0002-SQLServer-25  Timeout=30  Include=C:\ZabbixAgent\conf\discovery.mssql.server.conf # 引用这个配置文件  UnsafeUserParameters=1</code></pre><p>  discovery.mssql.server.conf文件内容如下：</p><pre><code>  # 要注意修改discovery.mssql.server.ps1脚本所在路径  UserParameter=discovery.mssql.databases,powershell.exe -noprofile -executionpolicy bypass -File C:\ZabbixAgent\scripts\discovery.mssql.server.ps1 JSONDB  UserParameter=discovery.mssql.jobs,powershell.exe -noprofile -executionpolicy bypass -File C:\ZabbixAgent\scripts\discovery.mssql.server.ps1 JSONJOB  UserParameter=discovery.mssql.data[*],powershell.exe -noprofile -executionpolicy bypass -File C:\ZabbixAgent\scripts\discovery.mssql.server.ps1 $1 &quot;$2&quot;</code></pre><ol start="3"><li><p>创建SQLServer主机</p><p>创建SQLServer所在主机，并且配置我们刚刚导入的监控模板。</p></li><li><p>查看上报的数据</p><p>上报数据示例如下图：</p><p><img src="SQLServer%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE.png" alt></p></li><li><p>参数指标详解</p></li></ol></li></ul></li></ul><p><strong>注意：</strong></p><ol><li>agent部分所在服务器要开放10050和10051端口</li><li>agent部分所在服务器的配置文件内的<strong>Hostname</strong>，同创建主机时<strong>主机名称</strong>一致，如果不一致，无法看到主机的监控数据</li></ol><ul><li><p>配置报警</p><ul><li><p>邮件</p><p>实现邮件报警有两种思路，一个是通过平台的报警媒介配置实现，另一个是通过通过配置可执行脚本来发送邮件，这里演示前一种，因为后一种在Linux端配置的时候，<strong>遇到了邮件发送超时的问题</strong>，无法配置成功。</p><ul><li><p>配置报警媒介类型</p><p>首先通过下面路径前往：管理-&gt;报警媒介类型-&gt;创建媒体类型，到以下页面:</p><p><img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A61.PNG" alt="zabbix配置邮件报警1"></p><p><img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A62.PNG" alt="zabbix配置邮件报警2"></p></li></ul><p>  <strong>注意：</strong></p><ol><li><p>在创建报警媒介类型时，建议使用企业邮箱进行创建，不建议使用普通的QQ邮箱或者163邮箱进行，个人原因在配置的时候这两个邮箱均不可用，浪费了大量时间，特此订正。我自己用的是腾讯企业邮。</p></li><li><p>配置邮件报警后，回到前一个页面，可以点击测试，测试邮件是否能够发送成功。</p><p>然后再切换到选项页面，如下图设置：</p><p><img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A63.PNG" alt="zabbix配置邮件报警3"></p><p>设置完成后点击添加/更新即可。这样配置完成了媒介类型。</p></li></ol></li><li><p>将媒介类型绑定用户</p><p>  切换到用户页面，可以先创建用户，也可以在原有用户上进行添加。根据下图操作，切换到Admin用户所在页面：</p><p>  <img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A64.PNG" alt="zabbix配置邮件报警4"></p><p>  <img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A65.PNG" alt="zabbix配置邮件报警5"></p><p>  切换到报警媒介页面，点击添加。这里可以添加多个收件人。<strong>添加完成后需要返回点击更新，以免添加无效！</strong>下面为添加页面：</p><p>  <img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A66.PNG" alt="zabbix配置邮件报警6"></p></li><li><p>添加报警动作</p><p>  切换到配置-&gt;动作-&gt;创建动作页面</p><p>  <img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A67.PNG" alt="zabbix配置邮件报警7"></p><p>  <img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A68.PNG" alt="zabbix配置邮件报警8"></p><p>  添加名称，然后添加触发器：</p><p>  <img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A69.PNG" alt="zabbix配置邮件报警9"></p><p>  通过选择群组和模板，选择要报警的类别，在这里选择<strong>Zabbix agent on Template OS Linux is unreachable for 5 minutes</strong>这一项，不要忘了点击“新的出发条件”一栏，下面的<strong>添加</strong>按钮。将监控项添加到里面。</p><ul><li><p>配置邮件内容-操作、恢复操作、更新操作<br>以操作为例，进行添加，如下图点击<strong>新的</strong>超链接：</p><p><img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A610.PNG" alt="zabbix配置邮件报警10"></p><p>到达：</p><p><img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A611.PNG" alt="zabbix配置邮件报警11"></p><p>在上面页面中，添加<strong>发送到用户群组</strong>和<strong>发送到用户</strong>，这里发送到Admin用户下配置的邮箱。恢复操作和更新操作与上面雷同，不再说明。这里可以编辑默认标题和消息内容，更改邮件发送的内容格式。</p></li><li><p>测试<br>前提是你要测试的VPS已经在系统中添加并且配置了和前面页面选择模板时选择的相同的模板。这样在你的VPS中关闭ZabbixAgent系统服务，5分钟左右就会收到邮件，类似如下：</p><p><img src="zabbix%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A612.PNG" alt="zabbix配置邮件报警12"></p><p>至此，配置完成。</p></li></ul></li><li><p>企业微信</p></li><li><p>钉钉</p></li></ul></li></ul><h2 id="grafana抓取zabbix数据"><a href="#grafana抓取zabbix数据" class="headerlink" title="grafana抓取zabbix数据"></a>grafana抓取zabbix数据</h2><p>注意：这里操作均在root用户下操作。</p><ul><li><p>安装 </p><ul><li><p>安装Grafana</p><pre><code>yum install -y initscripts fontconfig    rpm -ivh ./package/grafana-4.0.2-1481203731.x86_64.rpm   yum install -y fontconfig    yum install -y freetype*    yum install -y urw-fonts    #显示安装的文件    rpm -qc grafana    #二进制文件 /usr/sbin/grafana-server    #服务管理脚本 /etc/init.d/grafana-server    #安装默认文件 /etc/sysconfig/grafana-server    #配置文件 /etc/grafana/grafana.ini</code></pre><p>这样安装完成后就可以启动了，启动的命令如下：</p><pre><code>systemctl enable grafana-serversystemctl start grafana-server   这样grafana在开机时会自动启动</code></pre></li><li><p>安装插件</p><ul><li><p>zabbix插件</p><pre><code>#下载模板等信息    cd ~    git clone https://github.com/hqh546020152/grafana.git    cd grafana#获取可用插件列表    grafana-cli plugins list-remote#安装Zabbix插件    grafana-cli plugins install alexanderzobnin-zabbix-app#安装硬盘监控插件,用于danyi.json模板   # grafana-cli plugins install grafana-piechart-panel    # 重启服务生效systemctl restart grafana-server</code></pre></li></ul></li><li><p>安装图库（饼图、折线图、仪表盘等）</p></li></ul></li><li><p>Grafana和Zabbix联通</p></li></ul><ul><li>在浏览器地址栏输入 <a href="http://IP:3000就可以看到Grafana的登陆页面了。输入默认的用户名admin" target="_blank" rel="noopener">http://IP:3000就可以看到Grafana的登陆页面了。输入默认的用户名admin</a> 密码admin登陆<br>点击左侧的齿轮按钮，弹出选项框中，选择Data Sourses进入Data Source子页面，点击add data source进行添加。</li></ul><p><img src="Grafana%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE%E6%BA%90.PNG" alt="Grafana添加数据源"></p><ul><li>Type下拉框中选择Zabbix</li></ul><p><img src="Grafana%E6%B7%BB%E5%8A%A0zabbix%E6%95%B0%E6%8D%AE%E6%BA%90.PNG" alt="Grafana添加zabbix数据源"></p><ul><li>Name 可以自由发挥~~</li><li>HTTP–&gt;Url 填入<a href="http://zabbix-server-ip/zabbix/api_jsonrpc.php" target="_blank" rel="noopener">http://zabbix-server-ip/zabbix/api_jsonrpc.php</a> 这里填入的是Zabbix API接口 （例如：<a href="http://10.0.93.126:8181/api_jsonrpc.php）" target="_blank" rel="noopener">http://10.0.93.126:8181/api_jsonrpc.php）</a></li><li>Http–&gt;Access 选择 Server(default) 使用直接访问的方式</li><li>Zabbix API details–&gt;User 填入Admin</li><li>Zabbix API details–&gt;Password 填入 zabbix</li><li>点Save&amp;Test按钮保存并且测试API配置是否正确。出现：Zabbix API version: 4.2.1 信息，说明配置成功</li></ul><p><img src="Grafana%E9%85%8D%E7%BD%AEZabbix%E6%95%B0%E6%8D%AE%E6%BA%90.PNG" alt="Grafana配置Zabbix数据源"></p><ul><li>导入模板</li></ul><p>以LVS监控为例子来进行讲解，首先我们先导入Linux监控的模板，从<a href="https://grafana.com/dashboards" target="_blank" rel="noopener">这里</a>挑选模板进行下载。由于我们使用zabbix做数据监控，这样我们可以使用<a href="https://grafana.com/dashboards/8677" target="_blank" rel="noopener">Dashboard Servers Linux</a>模板进行，如图所示：</p><p><img src="grafana%E4%B8%8B%E8%BD%BD%E6%A8%A1%E6%9D%BF.PNG" alt="grafana下载模板"></p><p>然后复制ID，转到我们添加模板的页面，如图所示：<br><img src="grafana%E5%AF%BC%E5%85%A5%E6%A8%A1%E6%9D%BF.PNG" alt="grafana导入模板"></p><p>将ID粘贴到Grafana.com Dashboard这个下的输入框中，点击下边的load按钮，即可进入模板数据源绑定页面，如下所示：<br><img src="grafana%E5%AF%BC%E5%85%A5%E6%A8%A1%E6%9D%BF%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%BA%90.PNG" alt="grafana导入模板配置数据源"></p><p>在数据源绑定部分，我们选择zabbix数据源，然后稍加修改，就可以导入到grafana中了。如下图所示：<br><img src="grafana%E5%AF%BC%E5%85%A5%E6%A8%A1%E6%9D%BF-%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%BA%90%E5%AE%8C%E6%88%90.PNG" alt="grafana导入模板-配置数据源完成"></p><p>这样我们需要修改一些地方，才能使模板工作。因为我目前只有一台Linux服务器，所以我们在这个模板中固定下服务器的参数即可。首先去掉预设的参数，操作如下：</p><p><img src="grafana%E6%A8%A1%E6%9D%BF%E4%BF%AE%E6%94%B9.PNG" alt="grafana模板修改"></p><p><img src="grafana%E5%88%A0%E9%99%A4%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E5%8F%82%E6%95%B0.PNG" alt="grafana删除自定义的参数"></p><p>修改完成之后点击Save进行保存。然后我们回到dashboard页面，随便点开一个图表，进入图标的设计页面，如图：</p><p><img src="grafana%E5%9B%BE%E8%A1%A8%E9%85%8D%E7%BD%AE%E8%BF%9B%E5%85%A5%E4%B9%8B%E5%89%8D.PNG" alt="grafana图表配置进入之前"></p><p>最后，我们对这个图表进行调整，第一步，修改Group和Host数据为下面的内容：</p><p><img src="grafana-%E4%BF%AE%E6%94%B9Group-Host.PNG" alt="grafana-修改Group-Host"></p><p>第二步，数据出现后，我们修改图表名称，切换到下一个页面中，在如下图所示</p><p><img src="grafana%E4%BF%AE%E6%94%B9%E5%9B%BE%E8%A1%A8%E5%90%8D%E5%AD%97.PNG" alt="grafana修改图表名字"></p><p>这样这个图表就修复完成了。</p><p>最后的最后，经过调整，我们会得到这样的图表：</p><p><img src="grafana%E6%9C%80%E7%BB%88%E6%95%88%E6%9E%9C.PNG" alt="grafana最终效果"></p><ul><li><p>自定义创建图表</p></li><li><p>添加分组信息</p></li></ul><p>针对多台相同类型的服务器进行统一管理</p><ul><li><p>数据绑定和处理</p></li><li><p>自带报警（对zabbix无效）</p></li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul><li><p><a href="https://qizhanming.com/blog/2019/01/25/how-to-install-docker-ce-on-centos-7" target="_blank" rel="noopener">CentOS 7 下 yum 安装 Docker CE</a></p></li><li><p><a href="https://wsgzao.github.io/post/zabbix-docker/" target="_blank" rel="noopener">使用 Docker 安装 Zabbix 实践</a></p></li><li><p><a href="https://wsgzao.github.io/post/docker/" target="_blank" rel="noopener">Docker 从入门到实践</a></p></li><li><p><a href="http://www.ttlsa.com/zabbix/zabbix-monitor-nginx-performance/" target="_blank" rel="noopener">Zabbix监控nginx性能</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps Zabbix Grafana </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Objective-C编码规范</title>
      <link href="/2019/08/10/objective-c-bian-ma-gui-fan/"/>
      <url>/2019/08/10/objective-c-bian-ma-gui-fan/</url>
      
        <content type="html"><![CDATA[<h1 id="Objective-C编码规范"><a href="#Objective-C编码规范" class="headerlink" title="Objective-C编码规范"></a>Objective-C编码规范</h1><p>Objective-C编码规范，内容来自苹果、谷歌的文档翻译，<a href="https://github.com/boai" target="_blank" rel="noopener">『博爱』</a>的编码经验和对其它资料的总结。</p><p>##概要<br>Objective-C是一门面向对象的动态编程语言，主要用于编写iOS和Mac应用程序。关于Objective-C的编码规范，苹果和谷歌都已经有很好的总结：</p><ul><li><a href="https://developer.apple.com/library/mac/documentation/Cocoa/Conceptual/CodingGuidelines/CodingGuidelines.html" target="_blank" rel="noopener">Apple Coding Guidelines for Cocoa</a></li><li><a href="https://google-styleguide.googlecode.com/svn/trunk/objcguide.xml?showone=Line_Length#Line_Length" target="_blank" rel="noopener">Google Objective-C Style Guide</a></li></ul><p>本文主要整合了对上述文档的翻译、作者自己的编程经验和其他的相关资料，为公司总结出一份通用的编码规范。</p><p>##代码格式</p><p>###使用空格而不是制表符Tab</p><p>不要在工程里使用Tab键，使用空格来进行缩进。在<code>Xcode &gt; Preferences &gt; Text Editing</code>将Tab和自动缩进都设置为<strong>4</strong>个空格。（_Google的标准是使用两个空格来缩进，但这里还是推荐使用Xcode默认的设置。_）</p><p>###每一行的最大长度</p><p>同样的，在<code>Xcode &gt; Preferences &gt; Text Editing &gt; Page guide at column:</code>中将最大行长设置为<strong>80</strong>，过长的一行代码将会导致可读性问题。</p><p>###函数的书写</p><p>一个典型的Objective-C函数应该是这样的：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">- (void)writeVideoFrameWithData:(NSData *)frameData timeStamp:(int)timeStamp {    ...}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>在<code>-</code>和<code>(void)</code>之间应该有一个空格，第一个大括号<code>{</code>的位置在函数所在行的末尾，同样应该有一个空格。（_我司的C语言规范要求是第一个大括号单独占一行，但考虑到OC较长的函数名和苹果SDK代码的风格，还是将大括号放在行末。_）</p><p>如果一个函数有特别多的参数或者名称很长，应该将其按照<code>:</code>来对齐分行显示：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">-(id)initWithModel:(IPCModle)model       ConnectType:(IPCConnectType)connectType        Resolution:(IPCResolution)resolution          AuthName:(NSString *)authName          Password:(NSString *)password               MAC:(NSString *)mac              AzIp:(NSString *)az_ip             AzDns:(NSString *)az_dns             Token:(NSString *)token             Email:(NSString *)email          Delegate:(id<IPCConnectHandlerDelegate>)delegate;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在分行时，如果第一段名称过短，后续名称可以以Tab的长度（4个空格）为单位进行缩进：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">- (void)short:(GTMFoo *)theFoo        longKeyword:(NSRect)theRect  evenLongerKeyword:(float)theInterval              error:(NSError **)theError {    ...}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###函数调用</p><p>函数调用的格式和书写差不多，可以按照函数的长短来选择写在一行或者分成多行：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//写在一行[myObject doFooWith:arg1 name:arg2 error:arg3];//分行写，按照':'对齐[myObject doFooWith:arg1               name:arg2              error:arg3];//第一段名称过短的话后续可以进行缩进[myObj short:arg1          longKeyword:arg2    evenLongerKeyword:arg3                error:arg4];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以下写法是错误的：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//错误，要么写在一行，要么全部分行[myObject doFooWith:arg1 name:arg2              error:arg3];[myObject doFooWith:arg1               name:arg2 error:arg3];//错误，按照':'来对齐，而不是关键字[myObject doFooWith:arg1          name:arg2          error:arg3];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###@public和@private标记符</p><p>@public和@private标记符应该以<strong>一个空格</strong>来进行缩进：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">@interface MyClass : NSObject { @public  ... @private  ...}@end<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###协议（Protocols）</p><p>在书写协议的时候注意用<code>&lt;&gt;</code>括起来的协议和类型名之间是没有空格的，比如<code>IPCConnectHandler()&lt;IPCPreconnectorDelegate&gt;</code>,这个规则适用所有书写协议的地方，包括函数声明、类声明、实例变量等等：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">@interface MyProtocoledClass : NSObject<NSWindowDelegate> { @private    id<MyFancyDelegate> _delegate;}- (void)setDelegate:(id<MyFancyDelegate>)aDelegate;@end<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###闭包（Blocks）</p><p>根据block的长度，有不同的书写规则：</p><ul><li>较短的block可以写在一行内。</li><li>如果分行显示的话，block的右括号<code>}</code>应该和调用block那行代码的第一个非空字符对齐。</li><li>block内的代码采用<strong>4个空格</strong>的缩进。</li><li>如果block过于庞大，应该单独声明成一个变量来使用。</li><li><code>^</code>和<code>(</code>之间，<code>^</code>和<code>{</code>之间都没有空格，参数列表的右括号<code>)</code>和<code>{</code>之间有一个空格。</li></ul><pre class="line-numbers language-objective-c"><code class="language-objective-c">//较短的block写在一行内[operation setCompletionBlock:^{ [self onOperationDone]; }];//分行书写的block，内部使用4空格缩进[operation setCompletionBlock:^{    [self.delegate newDataAvailable];}];//使用C语言API调用的block遵循同样的书写规则dispatch_async(_fileIOQueue, ^{    NSString* path = [self sessionFilePath];    if (path) {      // ...    }});//较长的block关键字可以缩进后在新行书写，注意block的右括号'}'和调用block那行代码的第一个非空字符对齐[[SessionService sharedService]    loadWindowWithCompletionBlock:^(SessionWindow *window) {        if (window) {          [self windowDidLoad:window];        } else {          [self errorLoadingWindow];        }    }];//较长的block参数列表同样可以缩进后在新行书写[[SessionService sharedService]    loadWindowWithCompletionBlock:        ^(SessionWindow *window) {            if (window) {              [self windowDidLoad:window];            } else {              [self errorLoadingWindow];            }        }];//庞大的block应该单独定义成变量使用void (^largeBlock)(void) = ^{    // ...};[_operationQueue addOperationWithBlock:largeBlock];//在一个调用中使用多个block，注意到他们不是像函数那样通过':'对齐的，而是同时进行了4个空格的缩进[myObject doSomethingWith:arg1    firstBlock:^(Foo *a) {        // ...    }    secondBlock:^(Bar *b) {        // ...    }];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###数据结构的语法糖</p><p>应该使用可读性更好的语法糖来构造<code>NSArray</code>，<code>NSDictionary</code>等数据结构，避免使用冗长的<code>alloc</code>,<code>init</code>方法。</p><p>如果构造代码写在一行，需要在括号两端留有一个空格，使得被构造的元素于与构造语法区分开来：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，在语法糖的"[]"或者"{}"两端留有空格NSArray *array = @[ [foo description], @"Another String", [bar description] ];NSDictionary *dict = @{ NSForegroundColorAttributeName : [NSColor redColor] };//不正确，不留有空格降低了可读性NSArray* array = @[[foo description], [bar description]];NSDictionary* dict = @{NSForegroundColorAttributeName: [NSColor redColor]};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果构造代码不写在一行内，构造元素需要使用<strong>两个空格</strong>来进行缩进，右括号<code>]</code>或者<code>}</code>写在新的一行，并且与调用语法糖那行代码的第一个非空字符对齐：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">NSArray *array = @[  @"This",  @"is",  @"an",  @"array"];NSDictionary *dictionary = @{  NSFontAttributeName : [NSFont fontWithName:@"Helvetica-Bold" size:12],  NSForegroundColorAttributeName : fontColor};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>构造字典时，字典的Key和Value与中间的冒号<code>:</code>都要留有一个空格，多行书写时，也可以将Value对齐：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，冒号':'前后留有一个空格NSDictionary *option1 = @{  NSFontAttributeName : [NSFont fontWithName:@"Helvetica-Bold" size:12],  NSForegroundColorAttributeName : fontColor};//正确，按照Value来对齐NSDictionary *option2 = @{  NSFontAttributeName :            [NSFont fontWithName:@"Arial" size:12],  NSForegroundColorAttributeName : fontColor};//错误，冒号前应该有一个空格NSDictionary *wrong = @{  AKey:       @"b",  BLongerKey: @"c",};//错误，每一个元素要么单独成为一行，要么全部写在一行内NSDictionary *alsoWrong= @{ AKey : @"a",                            BLongerKey : @"b" };//错误，在冒号前只能有一个空格，冒号后才可以考虑按照Value对齐NSDictionary *stillWrong = @{  AKey       : @"b",  BLongerKey : @"c",};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>##命名规范</p><p>###基本原则</p><p>####清晰</p><p>命名应该尽可能的清晰和简洁，但在Objective-C中，清晰比简洁更重要。由于Xcode强大的自动补全功能，我们不必担心名称过长的问题。</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//清晰insertObject:atIndex://不清晰，insert的对象类型和at的位置属性没有说明insert:at://清晰removeObjectAtIndex://不清晰，remove的对象类型没有说明，参数的作用没有说明remove:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不要使用单词的简写，拼写出完整的单词：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//清晰destinationSelectionsetBackgroundColor://不清晰，不要使用简写destSelsetBkgdColor:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然而，有部分单词简写在Objective-C编码过程中是非常常用的，以至于成为了一种规范，这些简写可以在代码中直接使用，下面列举了部分：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">alloc   == Allocate                    max    == Maximumalt     == Alternate                min    == Minimumapp     == Application                msg    == Messagecalc    == Calculate                nib    == Interface Builder archivedealloc == Deallocate                pboard == Pasteboardfunc    == Function                    rect   == Rectanglehoriz   == Horizontal                Rep    == Representation (used in class name such as NSBitmapImageRep).info    == Information                temp   == Temporaryinit    == Initialize                vert   == Verticalint     == Integer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>命名方法或者函数时要避免歧义</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//有歧义，是返回sendPort还是send一个Port？sendPort//有歧义，是返回一个名字属性的值还是display一个name的动作？displayName<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>####一致性</p><p>整个工程的命名风格要保持一致性，最好和苹果SDK的代码保持统一。不同类中完成相似功能的方法应该叫一样的名字，比如我们总是用<code>count</code>来返回集合的个数，不能在A类中使用<code>count</code>而在B类中使用<code>getNumber</code>。</p><p>###使用前缀</p><p>如果代码需要打包成Framework给别的工程使用，或者工程项目非常庞大，需要拆分成不同的模块，使用命名前缀是非常有用的。</p><ul><li><p>前缀由大写的字母缩写组成，比如Cocoa中<code>NS</code>前缀代表Founation框架中的类，<code>IB</code>则代表Interface Builder框架。</p></li><li><p>可以在为类、协议、函数、常量以及typedef宏命名的时候使用前缀，但注意<strong>不要</strong>为成员变量或者方法使用前缀，因为他们本身就包含在类的命名空间内。</p></li><li><p>命名前缀的时候不要和苹果SDK框架冲突。</p></li></ul><p>###命名类和协议（Class&amp;Protocol）</p><p>类名以大写字母开头，应该包含一个<em>名词</em>来表示它代表的对象类型，同时可以加上必要的前缀，比如<code>NSString</code>, <code>NSDate</code>, <code>NSScanner</code>, <code>NSApplication</code>等等。</p><p>而协议名称应该清晰地表示它所执行的行为，而且要和类名区别开来，所以通常使用<code>ing</code>词尾来命名一个协议，比如<code>NSCopying</code>,<code>NSLocking</code>。</p><p>有些协议本身包含了很多不相关的功能，主要用来为某一特定类服务，这时候可以直接用类名来命名这个协议，比如<code>NSObject</code>协议，它包含了id对象在生存周期内的一系列方法。</p><p>###命名头文件（Headers）</p><p>源码的头文件名应该清晰地暗示它的功能和包含的内容：</p><ul><li><p>如果头文件内只定义了单个类或者协议，直接用类名或者协议名来命名头文件，比如<code>NSLocale.h</code>定义了<code>NSLocale</code>类。</p></li><li><p>如果头文件内定义了一系列的类、协议、类别，使用其中最主要的类名来命名头文件，比如<code>NSString.h</code>定义了<code>NSString</code>和<code>NSMutableString</code>。</p></li><li><p>每一个Framework都应该有一个和框架同名的头文件，包含了框架中所有公共类头文件的引用，比如<code>Foundation.h</code></p></li><li><p>Framework中有时候会实现在别的框架中类的类别扩展，这样的文件通常使用<code>被扩展的框架名</code>+<code>Additions</code>的方式来命名，比如<code>NSBundleAdditions.h</code>。</p></li></ul><p>###命名方法（Methods）</p><p>Objective-C的方法名通常都比较长，这是为了让程序有更好地可读性，按苹果的说法<em>“好的方法名应当可以以一个句子的形式朗读出来”</em>。</p><p>方法一般以小写字母打头，每一个后续的单词首字母大写，方法名中不应该有标点符号（<em>包括下划线</em>），有两个例外：</p><ul><li>可以用一些通用的大写字母缩写打头方法，比如<code>PDF</code>,<code>TIFF</code>等。</li><li>可以用带下划线的前缀来命名私有方法或者类别中的方法。</li></ul><p>如果方法表示让对象执行一个动作，使用动词打头来命名，注意不要使用<code>do</code>，<code>does</code>这种多余的关键字，动词本身的暗示就足够了：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//动词打头的方法表示让对象执行一个动作- (void)invokeWithTarget:(id)target;- (void)selectTabViewItem:(NSTabViewItem *)tabViewItem;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>如果方法是为了获取对象的一个属性值，直接用属性名称来命名这个方法，注意不要添加<code>get</code>或者其他的动词前缀：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，使用属性名来命名方法- (NSSize)cellSize;//错误，添加了多余的动词前缀- (NSSize)calcCellSize;- (NSSize)getCellSize;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于有多个参数的方法，务必在每一个参数前都添加关键词，关键词应当清晰说明参数的作用：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，保证每个参数都有关键词修饰- (void)sendAction:(SEL)aSelector toObject:(id)anObject forAllCells:(BOOL)flag;//错误，遗漏关键词- (void)sendAction:(SEL)aSelector :(id)anObject :(BOOL)flag;//正确- (id)viewWithTag:(NSInteger)aTag;//错误，关键词的作用不清晰- (id)taggedView:(int)aTag;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不要用<code>and</code>来连接两个参数，通常<code>and</code>用来表示方法执行了两个相对独立的操作（<em>从设计上来说，这时候应该拆分成两个独立的方法</em>）：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//错误，不要使用"and"来连接参数- (int)runModalForDirectory:(NSString *)path andFile:(NSString *)name andTypes:(NSArray *)fileTypes;//正确，使用"and"来表示两个相对独立的操作- (BOOL)openFile:(NSString *)fullPath withApplication:(NSString *)appName andDeactivate:(BOOL)flag;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>方法的参数命名也有一些需要注意的地方:</p><ul><li>和方法名类似，参数的第一个字母小写，后面的每一个单词首字母大写</li><li>不要再方法名中使用类似<code>pointer</code>,<code>ptr</code>这样的字眼去表示指针，参数本身的类型足以说明</li><li>不要使用只有一两个字母的参数名</li><li>不要使用简写，拼出完整的单词</li></ul><p>下面列举了一些常用参数名：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">...action:(SEL)aSelector...alignment:(int)mode...atIndex:(int)index...content:(NSRect)aRect...doubleValue:(double)aDouble...floatValue:(float)aFloat...font:(NSFont *)fontObj...frame:(NSRect)frameRect...intValue:(int)anInt...keyEquivalent:(NSString *)charCode...length:(int)numBytes...point:(NSPoint)aPoint...stringValue:(NSString *)aString...tag:(int)anInt...target:(id)anObject...title:(NSString *)aString<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###存取方法（Accessor Methods）</p><p>存取方法是指用来获取和设置类属性值的方法，属性的不同类型，对应着不同的存取方法规范：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//属性是一个名词时的存取方法范式- (type)noun;- (void)setNoun:(type)aNoun;//栗子- (NSString *)title;- (void)setTitle:(NSString *)aTitle;//属性是一个形容词时存取方法的范式- (BOOL)isAdjective;- (void)setAdjective:(BOOL)flag;//栗子- (BOOL)isEditable;- (void)setEditable:(BOOL)flag;//属性是一个动词时存取方法的范式- (BOOL)verbObject;- (void)setVerbObject:(BOOL)flag;//栗子- (BOOL)showsAlpha;- (void)setShowsAlpha:(BOOL)flag;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>命名存取方法时不要将动词转化为被动形式来使用：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确- (void)setAcceptsGlyphInfo:(BOOL)flag;- (BOOL)acceptsGlyphInfo;//错误，不要使用动词的被动形式- (void)setGlyphInfoAccepted:(BOOL)flag;- (BOOL)glyphInfoAccepted;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以使用<code>can</code>,<code>should</code>,<code>will</code>等词来协助表达存取方法的意思，但不要使用<code>do</code>,和<code>does</code>：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确- (void)setCanHide:(BOOL)flag;- (BOOL)canHide;- (void)setShouldCloseDocument:(BOOL)flag;- (BOOL)shouldCloseDocument;//错误，不要使用"do"或者"does"- (void)setDoesAcceptGlyphInfo:(BOOL)flag;- (BOOL)doesAcceptGlyphInfo;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为什么Objective-C中不适用<code>get</code>前缀来表示属性获取方法？因为<code>get</code>在Objective-C中通常只用来表示从函数指针返回值的函数：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//三个参数都是作为函数的返回值来使用的，这样的函数名可以使用"get"前缀- (void)getLineDash:(float *)pattern count:(int *)count phase:(float *)phase;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>###命名委托（Delegate）</p><p>当特定的事件发生时，对象会触发它注册的委托方法。委托是Objective-C中常用的传递消息的方式。委托有它固定的命名范式。</p><p>一个委托方法的第一个参数是触发它的对象，第一个关键词是触发对象的类名，除非委托方法只有一个名为<code>sender</code>的参数：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//第一个关键词为触发委托的类名- (BOOL)tableView:(NSTableView *)tableView shouldSelectRow:(int)row;- (BOOL)application:(NSApplication *)sender openFile:(NSString *)filename;//当只有一个"sender"参数时可以省略类名- (BOOL)applicationOpenUntitledFile:(NSApplication *)sender;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>根据委托方法触发的时机和目的，使用<code>should</code>,<code>will</code>,<code>did</code>等关键词</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">- (void)browserDidScroll:(NSBrowser *)sender;- (NSUndoManager *)windowWillReturnUndoManager:(NSWindow *)window;、- (BOOL)windowShouldClose:(id)sender;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###集合操作类方法（Collection Methods）</p><p>有些对象管理着一系列其它对象或者元素的集合，需要使用类似“增删查改”的方法来对集合进行操作，这些方法的命名范式一般为：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//集合操作范式- (void)addElement:(elementType)anObj;- (void)removeElement:(elementType)anObj;- (NSArray *)elements;//栗子- (void)addLayoutManager:(NSLayoutManager *)obj;- (void)removeLayoutManager:(NSLayoutManager *)obj;- (NSArray *)layoutManagers;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意，如果返回的集合是无序的，使用<code>NSSet</code>来代替<code>NSArray</code>。如果需要将元素插入到特定的位置，使用类似于这样的命名：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">- (void)insertLayoutManager:(NSLayoutManager *)obj atIndex:(int)index;- (void)removeLayoutManagerAtIndex:(int)index;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果管理的集合元素中有指向管理对象的指针，要设置成<code>weak</code>类型以防止引用循环。</p><p>下面是SDK中<code>NSWindow</code>类的集合操作方法：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">- (void)addChildWindow:(NSWindow *)childWin ordered:(NSWindowOrderingMode)place;- (void)removeChildWindow:(NSWindow *)childWin;- (NSArray *)childWindows;- (NSWindow *)parentWindow;- (void)setParentWindow:(NSWindow *)window;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###命名函数（Functions）</p><p>在很多场合仍然需要用到函数，比如说如果一个对象是一个单例，那么应该使用函数来代替类方法执行相关操作。</p><p>函数的命名和方法有一些不同，主要是：</p><ul><li>函数名称一般带有缩写前缀，表示方法所在的框架。</li><li>前缀后的单词以“驼峰”表示法显示，第一个单词首字母大写。</li></ul><p>函数名的第一个单词通常是一个动词，表示方法执行的操作：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">NSHighlightRectNSDeallocateObject<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果函数返回其参数的某个属性，省略动词：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">unsigned int NSEventMaskFromType(NSEventType type)float NSHeight(NSRect aRect)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果函数通过指针参数来返回值，需要在函数名中使用<code>Get</code>：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">const char *NSGetSizeAndAlignment(const char *typePtr, unsigned int *sizep, unsigned int *alignp)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>函数的返回类型是BOOL时的命名：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">BOOL NSDecimalIsNotANumber(const NSDecimal *decimal)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>###命名属性和实例变量（Properties&amp;Instance Variables）</p><p>属性和对象的存取方法相关联，属性的第一个字母小写，后续单词首字母大写，不必添加前缀。属性按功能命名成名词或者动词：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//名词属性@property (strong) NSString *title;//动词属性@property (assign) BOOL showsAlpha;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>属性也可以命名成形容词，这时候通常会指定一个带有<code>is</code>前缀的get方法来提高可读性：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">@property (assign, getter=isEditable) BOOL editable;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>命名实例变量，在变量名前加上<code>_</code>前缀（<em>有些有历史的代码会将<code>_</code>放在后面</em>），其它和命名属性一样：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">@implementation MyClass {    BOOL _showsTitle;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>一般来说，类需要对使用者隐藏数据存储的细节，所以不要将实例方法定义成公共可访问的接口，可以使用<code>@private</code>，<code>@protected</code>前缀。</p><p><em>按苹果的说法，不建议在除了<code>init</code>和<code>dealloc</code>方法以外的地方直接访问实例变量，但很多人认为直接访问会让代码更加清晰可读，只在需要计算或者执行操作的时候才使用存取方法访问，我就是这种习惯，所以这里不作要求。</em></p><p>###命名常量（Constants）</p><p>如果要定义一组相关的常量，尽量使用枚举类型（enumerations），枚举类型的命名规则和函数的命名规则相同。<br>建议使用 <code>NS_ENUM</code> 和 <code>NS_OPTIONS</code> 宏来定义枚举类型，参见官方的 <a href="https://developer.apple.com/library/ios/releasenotes/ObjectiveC/ModernizationObjC/AdoptingModernObjective-C/AdoptingModernObjective-C.html" target="_blank" rel="noopener">Adopting Modern Objective-C</a> 一文：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//定义一个枚举typedef NS_ENUM(NSInteger, NSMatrixMode) {    NSRadioModeMatrix,    NSHighlightModeMatrix,    NSListModeMatrix,    NSTrackModeMatrix};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>定义bit map：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">typedef NS_OPTIONS(NSUInteger, NSWindowMask) {    NSBorderlessWindowMask      = 0,    NSTitledWindowMask          = 1 << 0,    NSClosableWindowMask        = 1 << 1,    NSMiniaturizableWindowMask  = 1 << 2,    NSResizableWindowMask       = 1 << 3};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用<code>const</code>定义浮点型或者单个的整数型常量，如果要定义一组相关的整数常量，应该优先使用枚举。常量的命名规范和函数相同：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">const float NSLightGray;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>不要使用<code>#define</code>宏来定义常量，如果是整型常量，尽量使用枚举，浮点型常量，使用<code>const</code>定义。<code>#define</code>通常用来给编译器决定是否编译某块代码，比如常用的：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">#ifdef DEBUG<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意到一般由编译器定义的宏会在前后都有一个<code>__</code>，比如<em><code>__MACH__</code></em>。</p><p>###命名通知（Notifications）</p><p>通知常用于在模块间传递消息，所以通知要尽可能地表示出发生的事件，通知的命名范式是：</p><pre><code>[触发通知的类名] + [Did | Will] + [动作] + Notification</code></pre><p>栗子：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">NSApplicationDidBecomeActiveNotificationNSWindowDidMiniaturizeNotificationNSTextViewDidChangeSelectionNotificationNSColorPanelColorDidChangeNotification<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>##注释</p><p>读没有注释代码的痛苦你我都体会过，好的注释不仅能让人轻松读懂你的程序，还能提升代码的逼格。注意注释是为了让别人看懂，而不是仅仅你自己。</p><p>###文件注释</p><p>每一个文件都<strong>必须</strong>写文件注释，文件注释通常包含</p><ul><li>文件所在模块</li><li>作者信息</li><li>历史版本信息</li><li>版权信息</li><li>文件包含的内容，作用</li></ul><p>一段良好文件注释的栗子：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">/*******************************************************************************    Copyright (C), 2011-2013, Andrew Min Chang    File name:     AMCCommonLib.h    Author:        Andrew Chang (Zhang Min)     E-mail:        LaplaceZhang@126.com    Description:                 This file provide some covenient tool in calling library tools. One can easily include         library headers he wants by declaring the corresponding macros.             I hope this file is not only a header, but also a useful Linux library note.    History:        2012-??-??: On about come date around middle of Year 2012, file created as "commonLib.h"        2012-08-20: Add shared memory library; add message queue.        2012-08-21: Add socket library (local)        2012-08-22: Add math library        2012-08-23: Add socket library (internet)        2012-08-24: Add daemon function        2012-10-10: Change file name as "AMCCommonLib.h"        2012-12-04: Add UDP support in AMC socket library        2013-01-07: Add basic data type such as "sint8_t"        2013-01-18: Add CFG_LIB_STR_NUM.        2013-01-22: Add CFG_LIB_TIMER.        2013-01-22: Remove CFG_LIB_DATA_TYPE because there is already AMCDataTypes.h    Copyright information:             This file was intended to be under GPL protocol. However, I may use this library        in my work as I am an employee. And my company may require me to keep it secret.         Therefore, this file is neither open source nor under GPL control. ********************************************************************************/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em>文件注释的格式通常不作要求，能清晰易读就可以了，但在整个工程中风格要统一。</em></p><p>###代码注释</p><p>好的代码应该是“自解释”（self-documenting）的，但仍然需要详细的注释来说明参数的意义、返回值、功能以及可能的副作用。</p><p>方法、函数、类、协议、类别的定义都需要注释，推荐采用Apple的标准注释风格，好处是可以在引用的地方<code>alt+点击</code>自动弹出注释，非常方便。</p><p>有很多可以自动生成注释格式的插件，推荐使用<a href="https://github.com/onevcat/VVDocumenter-Xcode" target="_blank" rel="noopener">VVDocumenter</a>：</p><p><img src="https://raw.github.com/onevcat/VVDocumenter-Xcode/master/ScreenShot.gif" alt="Screenshot"></p><p>一些良好的注释：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">/** *  Create a new preconnector to replace the old one with given mac address. *  NOTICE: We DO NOT stop the old preconnector, so handle it by yourself. * *  @param type       Connect type the preconnector use. *  @param macAddress Preconnector's mac address. */- (void)refreshConnectorWithConnectType:(IPCConnectType)type  Mac:(NSString *)macAddress;/** *  Stop current preconnecting when application is going to background. */-(void)stopRunning;/** *  Get the COPY of cloud device with a given mac address. * *  @param macAddress Mac address of the device. * *  @return Instance of IPCCloudDevice. */-(IPCCloudDevice *)getCloudDeviceWithMac:(NSString *)macAddress;// A delegate for NSApplication to handle notifications about app// launch and shutdown. Owned by the main app controller.@interface MyAppDelegate : NSObject {  ...}@end<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>协议、委托的注释要明确说明其被触发的条件：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">/** Delegate - Sent when failed to init connection, like p2p failed. */-(void)initConnectionDidFailed:(IPCConnectHandler *)handler;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果在注释中要引用参数名或者方法函数名，使用<code>||</code>将参数或者方法括起来以避免歧义：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">// Sometimes we need |count| to be less than zero.// Remember to call |StringWithoutSpaces("foo bar baz")|<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>定义在头文件里的接口方法、属性必须要有注释！</strong></p><p>##编码风格</p><p>每个人都有自己的编码风格，这里总结了一些比较好的Cocoa编程风格和注意点。</p><p>###不要使用new方法</p><p>尽管很多时候能用<code>new</code>代替<code>alloc init</code>方法，但这可能会导致调试内存时出现不可预料的问题。Cocoa的规范就是使用<code>alloc init</code>方法，使用<code>new</code>会让一些读者困惑。</p><p>###Public API要尽量简洁</p><p>共有接口要设计的简洁，满足核心的功能需求就可以了。不要设计很少会被用到，但是参数极其复杂的API。如果要定义复杂的方法，使用类别或者类扩展。</p><p>####import和#include</p><p><code>#import</code>是Cocoa中常用的引用头文件的方式，它能自动防止重复引用文件，什么时候使用<code>#import</code>，什么时候使用<code>#include</code>呢？</p><ul><li>当引用的是一个Objective-C或者Objective-C++的头文件时，使用<code>#import</code></li><li>当引用的是一个C或者C++的头文件时，使用<code>#include</code>，这时必须要保证被引用的文件提供了保护域（#define guard）。</li></ul><p>栗子：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">#import <Cocoa/Cocoa.h>#include <CoreFoundation/CoreFoundation.h>#import "GTMFoo.h"#include "base/basictypes.h"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>为什么不全部使用<code>#import</code>呢？主要是为了保证代码在不同平台间共享时不出现问题。</p><p>###引用框架的根头文件</p><p>上面提到过，每一个框架都会有一个和框架同名的头文件，它包含了框架内接口的所有引用，在使用框架的时候，应该直接引用这个根头文件，而不是其它子模块的头文件，即使是你只用到了其中的一小部分，编译器会自动完成优化的。</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，引用根头文件#import <Foundation/Foundation.h>//错误，不要单独引用框架内的其它头文件#import <Foundation/NSArray.h>#import <Foundation/NSString.h><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###BOOL的使用</p><p>BOOL在Objective-C中被定义为<code>signed char</code>类型，这意味着一个BOOL类型的变量不仅仅可以表示<code>YES</code>(1)和<code>NO</code>(0)两个值，所以永远<strong>不要</strong>将BOOL类型变量直接和<code>YES</code>比较：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//错误，无法确定|great|的值是否是YES(1)，不要将BOOL值直接与YES比较BOOL great = [foo isGreat];if (great == YES)  // ...be great!//正确BOOL great = [foo isGreat];if (great)  // ...be great!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同样的，也不要将其它类型的值作为BOOL来返回，这种情况下，BOOL变量只会取值的最后一个字节来赋值，这样很可能会取到0（NO）。但是，一些逻辑操作符比如<code>&amp;&amp;</code>,<code>||</code>,<code>!</code>的返回是可以直接赋给BOOL的：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//错误，不要将其它类型转化为BOOL返回- (BOOL)isBold {  return [self fontTraits] & NSFontBoldTrait;}- (BOOL)isValid {  return [self stringValue];}//正确- (BOOL)isBold {  return ([self fontTraits] & NSFontBoldTrait) ? YES : NO;}//正确，逻辑操作符可以直接转化为BOOL- (BOOL)isValid {  return [self stringValue] != nil;}- (BOOL)isEnabled {  return [self isValid] && [self isBold];}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>另外BOOL类型可以和<code>_Bool</code>,<code>bool</code>相互转化，但是<strong>不能</strong>和<code>Boolean</code>转化。</p><p>###使用ARC</p><p>除非想要兼容一些古董级的机器和操作系统，我们没有理由放弃使用ARC。在最新版的Xcode(6.2)中，ARC是自动打开的，所以直接使用就好了。</p><p>###在init和dealloc中不要用存取方法访问实例变量</p><p>当<code>init``dealloc</code>方法被执行时，类的运行时环境不是处于正常状态的，使用存取方法访问变量可能会导致不可预料的结果，因此应当在这两个方法内直接访问实例变量。</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，直接访问实例变量- (instancetype)init {  self = [super init];  if (self) {    _bar = [[NSMutableString alloc] init];  }  return self;}- (void)dealloc {  [_bar release];  [super dealloc];}//错误，不要通过存取方法访问- (instancetype)init {  self = [super init];  if (self) {    self.bar = [NSMutableString string];  }  return self;}- (void)dealloc {  self.bar = nil;  [super dealloc];}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###按照定义的顺序释放资源</p><p>在类或者Controller的生命周期结束时，往往需要做一些扫尾工作，比如释放资源，停止线程等，这些扫尾工作的释放顺序应当与它们的初始化或者定义的顺序保持一致。这样做是为了方便调试时寻找错误，也能防止遗漏。</p><p>###保证NSString在赋值时被复制</p><p><code>NSString</code>非常常用，在它被传递或者赋值时应当保证是以复制（copy）的方式进行的，这样可以防止在不知情的情况下String的值被其它对象修改。</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">- (void)setFoo:(NSString *)aFoo {  _foo = [aFoo copy];}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>###使用NSNumber的语法糖</p><p>使用带有<code>@</code>符号的语法糖来生成NSNumber对象能使代码更简洁：</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">NSNumber *fortyTwo = @42;NSNumber *piOverTwo = @(M_PI / 2);enum {  kMyEnum = 2;};NSNumber *myEnum = @(kMyEnum);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###nil检查</p><p>因为在Objective-C中向nil对象发送命令是不会抛出异常或者导致崩溃的，只是完全的“什么都不干”，所以，只在程序中使用nil来做逻辑上的检查。</p><p>另外，不要使用诸如<code>nil == Object</code>或者<code>Object == nil</code>的形式来判断。</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，直接判断if (!objc) {    ...    }//错误，不要使用nil == Object的形式if (nil == objc) {    ...    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###属性的线程安全</p><p>定义一个属性时，编译器会自动生成线程安全的存取方法（Atomic），但这样会大大降低性能，特别是对于那些需要频繁存取的属性来说，是极大的浪费。所以如果定义的属性不需要线程保护，记得手动添加属性关键字<code>nonatomic</code>来取消编译器的优化。</p><p>###点分语法的使用</p><p>不要用点分语法来调用方法，只用来访问属性。这样是为了防止代码可读性问题。</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">//正确，使用点分语法访问属性NSString *oldName = myObject.name;myObject.name = @"Alice";//错误，不要用点分语法调用方法NSArray *array = [NSArray arrayWithObject:@"hello"];NSUInteger numberOfItems = array.count;array.release;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>###Delegate要使用弱引用</p><p>一个类的Delegate对象通常还引用着类本身，这样很容易造成引用循环的问题，所以类的Delegate属性要设置为弱引用。</p><pre class="line-numbers language-objective-c"><code class="language-objective-c">/** delegate */@property (nonatomic, weak) id <IPCConnectHandlerDelegate> delegate;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> Objective-C </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx负载均衡配置与监控</title>
      <link href="/2019/08/02/nginx-fu-zai-jun-heng-pei-zhi-yu-jian-kong/"/>
      <url>/2019/08/02/nginx-fu-zai-jun-heng-pei-zhi-yu-jian-kong/</url>
      
        <content type="html"><![CDATA[<h1 id="Nginx负载均衡配置与监控"><a href="#Nginx负载均衡配置与监控" class="headerlink" title="Nginx负载均衡配置与监控"></a>Nginx负载均衡配置与监控</h1><h2 id="Nginx环境配置"><a href="#Nginx环境配置" class="headerlink" title="Nginx环境配置"></a>Nginx环境配置</h2><ol><li>操作环境以及配置</li></ol><ul><li>Nginx版本：1.15.11</li><li>Linux系统版本：CentOS 7.6 内核版本：3.10</li><li>VPS配置：16cores 16GB内存 100GB高效云盘</li></ul><ol start="2"><li><p>编译安装</p></li><li><p>1 下载源码</p><pre><code>  #下载  wget http://nginx.org/download/nginx-1.15.11.tar.gz  #解压  tar -xzf nginx-1.15.11.tar.gz  cd nginx-1.15.11</code></pre></li></ol><p>2.2 安装编译环境</p><pre><code>    yum update    yum -y install gcc pcre pcre-devel zlib zlib-devel openssl openssl-devel</code></pre><p>2.3 编译安装<br>其实首次编译安装，我并没有安装模块，后续需要功能模块的时候再添加。</p><pre><code># 添加Nginx组groupadd nginx    # 添加Nginx用户useradd nginx -g nginx -s /sbin/nologin -M# 编译配置./configure --user=nginx \    # 设置用户名--group=nginx \   # 设置用户组--prefix=/usr/local/nginx  \ # 设置安装位置--with-http_stub_status_module \  # 用来监控 Nginx 的当前状态--with-http_ssl_module \ # 打开HTTPS功能--with-http_realip_module \  # 允许改变客户端请求头中X-Real-IP 或 X-Forwarded-For等, 可用于记录客户IP--with-http_gzip_static_module #gzip压缩# 构建和安装make &amp;&amp; make install</code></pre><p>2.4 测试</p><pre><code>/usr/local/nginx/sbin/nginx -V</code></pre><p>输出类似如下：</p><pre><code>nginx version: nginx/1.15.11built by gcc 4.8.5 20190424 (Red Hat 4.8.5-28) (GCC) built with OpenSSL 1.0.2k-fips  26 Jan 2019TLS SNI support enabledconfigure arguments: --user=www --group=www --prefix=/usr/local/nginx --with-http_ssl_module --with-http_stub_status_module --with-threads</code></pre><p>2.5 启动Nginx</p><pre><code>cd /usr/local/nginx./sbin/nginx</code></pre><p>再通过请求访问信息</p><pre><code>curl http://localhost:80</code></pre><p>得到以下输出信息：</p><pre><code>&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;body {    width: 35em;    margin: 0 auto;    font-family: Tahoma, Verdana, Arial, sans-serif;}&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><p>说明Nginx安装完成。然后暂时关闭Nginx服务：</p><pre><code>/usr/local/nginx/sbin/nginx -s stop</code></pre><p>2.6 配置开机自动启动</p><pre><code>vim /etc/init.d/nginx</code></pre><p>输入以下内容：</p><pre><code>#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig:   - 85 15# description:  NGINX is an HTTP(S) server, HTTP(S) reverse \#               proxy and IMAP/POP3 proxy server# processname: nginx# config:      /usr/local/nginx/conf/nginx.conf# config:      /etc/sysconfig/nginx# pidfile:     /usr/local/nginx/logs/nginx.pid# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0nginx=&quot;/usr/local/nginx/sbin/nginx&quot;prog=$(basename $nginx)NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot;[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginxlockfile=/var/lock/subsys/nginxmake_dirs() {# make required directoriesuser=`$nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &#39;s/[^*]*--user=\([^ ]*\).*/\1/g&#39; -`if [ -z &quot;`grep $user /etc/passwd`&quot; ]; thenuseradd -M -s /bin/nologin $userfioptions=`$nginx -V 2&gt;&amp;1 | grep &#39;configure arguments:&#39;`for opt in $options; doif [ `echo $opt | grep &#39;.*-temp-path&#39;` ]; then   value=`echo $opt | cut -d &quot;=&quot; -f 2`   if [ ! -d &quot;$value&quot; ]; then       # echo &quot;creating&quot; $value       mkdir -p $value &amp;&amp; chown -R $user $value   fifidone}start() {[ -x $nginx ] || exit 5[ -f $NGINX_CONF_FILE ] || exit 6make_dirsecho -n $&quot;Starting $prog: &quot;daemon $nginx -c $NGINX_CONF_FILEretval=$?echo[ $retval -eq 0 ] &amp;&amp; touch $lockfilereturn $retval}stop() {echo -n $&quot;Stopping $prog: &quot;killproc $prog -QUITretval=$?echo[ $retval -eq 0 ] &amp;&amp; rm -f $lockfilereturn $retval}restart() {configtest || return $?stopsleep 1start}reload() {configtest || return $?echo -n $&quot;Reloading $prog: &quot;killproc $nginx -HUPRETVAL=$?echo}force_reload() {restart}configtest() {$nginx -t -c $NGINX_CONF_FILE}rh_status() {status $prog}rh_status_q() {rh_status &gt;/dev/null 2&gt;&amp;1}case &quot;$1&quot; in    start)        rh_status_q &amp;&amp; exit 0        $1;;stop)rh_status_q || exit 0$1    ;;    restart|configtest)        $1;;reload)rh_status_q || exit 7$1    ;;    force-reload)        force_reload    ;;    status)        rh_status    ;;    condrestart|try-restart)        rh_status_q || exit 0    ;;*)echo $&quot;Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}&quot;exit 2esac</code></pre><p>赋予脚本可执行权限：</p><pre><code>chmod a+x /etc/init.d/nginx</code></pre><p>将nginx服务加入chkconfig管理列表：</p><pre><code>chkconfig --add /etc/init.d/nginxchkconfig nginx on# 启动systemctl start nginx</code></pre><p>这样nginx就启动了。</p><p>番外：通过systemctl配置系统服务，实现自动重启。参考地址：<a href="https://www.jianshu.com/p/ca5ee5f7075c" target="_blank" rel="noopener">https://www.jianshu.com/p/ca5ee5f7075c</a></p><p>2.7 Nginx常用命令</p><pre><code>/usr/local/nginx/sbin/nginx -s reload # 重新加载/usr/local/nginx/sbin/nginx -t # 检测配置文件是否有错误/usr/local/nginx/sbin/nginx    # 启动nginx/usr/local/nginx/sbin/nginx -s stop # 停止nginx</code></pre><h2 id="配置内容详解"><a href="#配置内容详解" class="headerlink" title="配置内容详解"></a>配置内容详解</h2><pre><code>    ######Nginx配置文件nginx.conf中文详解#####    #定义Nginx运行的用户和用户组    user www www;    #nginx进程数，建议设置为等于CPU总核心数。    worker_processes 8;    #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]    error_log /usr/local/nginx/logs/error.log info;    #进程pid文件    pid /usr/local/nginx/logs/nginx.pid;    #指定进程可以打开的最大描述符：数目    #工作模式与连接数上限    #这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。    #现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。    #这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。    worker_rlimit_nofile 65535;    events    {    #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型    #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。    #补充说明：    #与apache相类，nginx针对不同的操作系统，有不同的事件模型    #A）标准事件模型    #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll    #B）高效事件模型    #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。    #Epoll：使用于Linux内核2.6版本及以后的系统。    #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。    #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。    use epoll;    #单个进程最大连接数（最大连接数=连接数*进程数）    #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。    worker_connections 65535;    #keepalive超时时间。    keepalive_timeout 60;    #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。    #分页大小可以用命令getconf PAGESIZE 取得。    #[root@web001 ~]# getconf PAGESIZE    #4096    #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。    client_header_buffer_size 4k;    #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。    open_file_cache max=65535 inactive=60s;    #这个是指多长时间检查一次缓存的有效信息。    #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息.    open_file_cache_valid 80s;    #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。    #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location  这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态.    open_file_cache_min_uses 1;    #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件时记录cache错误.    open_file_cache_errors on;    }    #设定http服务器，利用它的反向代理功能提供负载均衡支持    http    {    #文件扩展名与文件类型映射表    include mime.types;    #默认文件类型    default_type application/octet-stream;    #默认编码    #charset utf-8;    #服务器名字的hash表大小    #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小.    server_names_hash_bucket_size 128;    #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。    client_header_buffer_size 32k;    #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。    large_client_header_buffers 4 64k;    #设定通过nginx上传文件的大小    client_max_body_size 8m;    #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。    #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。    sendfile on;    #开启目录列表访问，合适下载服务器，默认关闭。    autoindex on;    #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用    tcp_nopush on;    tcp_nodelay on;    #长连接超时时间，单位是秒    keepalive_timeout 120;    #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。    fastcgi_connect_timeout 300;    fastcgi_send_timeout 300;    fastcgi_read_timeout 300;    fastcgi_buffer_size 64k;    fastcgi_buffers 4 64k;    fastcgi_busy_buffers_size 128k;    fastcgi_temp_file_write_size 128k;    #gzip模块设置    gzip on; #开启gzip压缩输出    gzip_min_length 1k;    #最小压缩文件大小    gzip_buffers 4 16k;    #压缩缓冲区    gzip_http_version 1.0;    #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）    gzip_comp_level 2;    #压缩等级    gzip_types text/plain application/x-javascript text/css application/xml application/json;    #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。    gzip_vary on;    #开启限制IP连接数的时候需要使用    #limit_zone crawler $binary_remote_addr 10m;    #负载均衡配置    upstream jh.w3cschool.cn {    #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。    server 192.168.80.121:80 weight=3;    server 192.168.80.122:80 weight=2;    server 192.168.80.123:80 weight=3;    #nginx的upstream目前支持4种方式的分配    #1、轮询（默认）    #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。    #2、weight    #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。    #例如：    #upstream bakend {    #    server 192.168.0.14 weight=10 max_fails=2 fail_timeout=30;    #    server 192.168.0.15 weight=10;    #}    #2、ip_hash    #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。    #例如：    #upstream bakend {    #    ip_hash;    #    server 192.168.0.14:88;    #    server 192.168.0.15:80;    #}    #3、fair（第三方）    #按后端服务器的响应时间来分配请求，响应时间短的优先分配。    #upstream backend {    #    server server1;    #    server server2;    #    fair;    #}    #4、url_hash（第三方）    #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。    #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法    #upstream backend {    #    server squid1:3128;    #    server squid2:3128;    #    hash $request_uri;    #    hash_method crc32;    #}    #tips:    #upstream bakend    #在需要使用负载均衡的server中增加 proxy_pass http://bakend/;    #upstream sessionIdStream {     # 对cookie做hash运算    #set $key &quot;&quot;;    #if ( $http_cookie ~* &quot;ASP.NET_SessionId=(.+?)(?=;|$)&quot;) {    #    set $key $1;    #}    #  hash $key;    #    server 127.0.0.1:9090 down;    #    server 127.0.0.1:8080 weight=2;    #    server 127.0.0.1:6060;    #    server 127.0.0.1:7070 backup;    #}    #此处为对session做hash运算选择所要负载的服务器，在需要使用负载均衡的server中增加 proxy_pass http://sessionIdStream/;    #每个设备的状态设置为:    #1.down表示单前的server暂时不参与负载    #2.weight为weight越大，负载的权重就越大。    #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误    #4.fail_timeout:max_fails次失败后，暂停的时间。    #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。    #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。    #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug    #client_body_temp_path设置记录文件的目录 可以设置最多3层目录    #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡    }    #虚拟主机的配置    server    {    #监听端口    listen 80;    #域名可以有多个，用空格隔开    server_name www.w3cschool.cn w3cschool.cn;    index index.html index.htm index.php;    root /data/www/w3cschool;    #对******进行负载均衡    location ~ .*.(php|php5)?$    {        fastcgi_pass 127.0.0.1:9000;        fastcgi_index index.php;        include fastcgi.conf;    }    #图片缓存时间设置    location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$    {        expires 10d;    }    #JS和CSS缓存时间设置    location ~ .*.(js|css)?$    {        expires 1h;    }    #日志格式设定    #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址；    #$remote_user：用来记录客户端用户名称；    #$time_local： 用来记录访问时间与时区；    #$request： 用来记录请求的url与http协议；    #$status： 用来记录请求状态；成功是200，    #$body_bytes_sent ：记录发送给客户端文件主体内容大小；    #$http_referer：用来记录从那个页面链接访问过来的；    #$http_user_agent：记录客户浏览器的相关信息；    #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。    log_format access &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;    &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;    &#39;&quot;$http_user_agent&quot; $http_x_forwarded_for&#39;;    #定义本虚拟主机的访问日志    access_log  /usr/local/nginx/logs/host.access.log  main;    access_log  /usr/local/nginx/logs/host.access.404.log  log404;    #对 &quot;/&quot; 启用反向代理    location / {        proxy_pass http://127.0.0.1:88;        proxy_redirect off;        proxy_set_header X-Real-IP $remote_addr;        #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        #以下是一些反向代理的配置，可选。        proxy_set_header Host $host;        #允许客户端请求的最大单文件字节数        client_max_body_size 10m;        #缓冲区代理缓冲用户端请求的最大字节数，        #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。        #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误        client_body_buffer_size 128k;        #表示使nginx阻止HTTP应答代码为400或者更高的应答。        proxy_intercept_errors on;        #后端服务器连接的超时时间_发起握手等候响应超时时间        #nginx跟后端服务器连接超时时间(代理连接超时)        proxy_connect_timeout 90;        #后端服务器数据回传时间(代理发送超时)        #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据        proxy_send_timeout 90;        #连接成功后，后端服务器响应时间(代理接收超时)        #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间）        proxy_read_timeout 90;        #设置代理服务器（nginx）保存用户头信息的缓冲区大小        #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小        proxy_buffer_size 4k;        #proxy_buffers缓冲区，网页平均在32k以下的设置        #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k        proxy_buffers 4 32k;        #高负荷下缓冲大小（proxy_buffers*2）        proxy_busy_buffers_size 64k;        #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长        #设定缓存文件夹大小，大于这个值，将从upstream服务器传        proxy_temp_file_write_size 64k;    }    #设定查看Nginx状态的地址    location /NginxStatus {        stub_status on;        access_log on;        auth_basic &quot;NginxStatus&quot;;        auth_basic_user_file confpasswd;        #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。    }    #本地动静分离反向代理配置    #所有jsp的页面均交由tomcat或resin处理    location ~ .(jsp|jspx|do)?$ {        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_pass http://127.0.0.1:8080;    }    #所有静态文件由nginx直接读取不经过tomcat或resin    location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|    pdf|xls|mp3|wma)$    {        expires 15d;     }    location ~ .*.(js|css)?$    {        expires 1h;    }    }    include nginx-resin.conf; # 导入其他server的配置文件    }</code></pre><h2 id="负载均衡策略"><a href="#负载均衡策略" class="headerlink" title="负载均衡策略"></a>负载均衡策略</h2><p>负载均衡的策略可以分为两大类：内置策略 和扩展策略</p><ul><li>内置策略：一般会直接编译进Nginx内核，常用的有轮询、ip hash</li><li>扩展策略：fair、url hash等</li></ul><h3 id="1-普通轮询策略"><a href="#1-普通轮询策略" class="headerlink" title="1. 普通轮询策略"></a>1. 普通轮询策略</h3><p>默认选项，当weight不指定时，各服务器权重相同， 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。</p><pre><code>upstream bakend {server 10.11.0.1;server 10.11.0.2;server 10.11.0.3;}</code></pre><h3 id="2-加权轮询策略"><a href="#2-加权轮询策略" class="headerlink" title="2. 加权轮询策略"></a>2. 加权轮询策略</h3><p>指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。</p><pre><code>upstream backend {server 10.11.0.1 weight=3;server 10.11.0.2 weight=7;}</code></pre><p>权重越高，在被访问的概率越大，如上例，分别是30%，70%。</p><ul><li>轮询流程图</li></ul><p><img src="ngx_wr.png" alt="Nginx轮询流程"></p><p>首先将请求都分给高权重的机器，直到该机器的权值降到了比其他机器低，才开始将请求分给下一个高权重的机器；当所有后端机器都down掉时，nginx会立即将所有机器的标志位清成初始状态，以避免造成所有的机器都处在timeout的状态</p><h3 id="ip-hash方式"><a href="#ip-hash方式" class="headerlink" title="ip hash方式"></a>ip hash方式</h3><p>默认情况下，Nginx 会为你提供轮询作为负载均衡策略。</p><p>采用ip_hash策略解决登录信息丢失，如果客户已经访问了某个服务器，当用户再次访问时，会将该请求通过哈希算法，自动定位到同一个服务器，当然，如果所 hash 到的 server 当前不可用，则请求会被转移到其他server。<br>每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。</p><pre><code>upstream backend {ip_hash;server 10.11.0.1;server 10.11.0.2;}</code></pre><h3 id="fair"><a href="#fair" class="headerlink" title="fair"></a>fair</h3><p>根据后端服务器的响应时间判断负载情况，从中选出负载最轻的机器进行优先分配。</p><pre><code>upstream backend {server 10.11.0.1;server 10.11.0.2;fair;}</code></pre><p>缺点：需要「负载均衡器」不停的去统计每一台后端服务器对请求的处理速度，比如一分钟统计一次，生成一个后端服务器处理速度的排行榜，然后「负载均衡器」根据这个排行榜去转发服务。</p><h3 id="hash"><a href="#hash" class="headerlink" title="hash"></a>hash</h3><p>这里举的例子是针对cookie中的某些项做hash操作，进行负载均衡。</p><pre><code>upstream backend {set $key &quot;&quot;;if ( $http_cookie ~* &quot;ASP.NET_SessionId=(.+?)(?=;|$)&quot;) {set $key $1;}hash $key;server 10.11.0.1;server 10.11.0.2;}</code></pre><p>根据cookie内某些变量的不同，分配到不同的服务器上。在这个例子中，后端回传给浏览器的cookie中带有session信息，可以凭借该session信息进行后端负载均衡的操作，其效果约等同于ip_hash策略。</p><h2 id="负载均衡问题"><a href="#负载均衡问题" class="headerlink" title="负载均衡问题"></a>负载均衡问题</h2><ol><li>Session同步问题<br>先对nginx采取轮询的负载均衡策略，监控请求后向转移到某台服务器的时候，发现请求失败的问题，通过监控日志，看到前后两个步骤的操作，它们对应的请求负载到不同的服务器上，导致错误。解决方案如下：</li></ol><ul><li>利用nginx负载均衡中的ip_hash策略或者对cookie做hash的策略，可以保证session同步，一个session只访问一台服务器，但是这样会导致负载压到单机上，不能进行请求分配。</li><li>对后端业务做无状态化处理，利用token替代session进行请求，用CPU负载换取无状态优势</li><li>session中心化策略，利用Redis做session缓存，对session进行统一管理，但是如果Redis服务器出现问题，也会导致session失效，服务宕机的情况。</li></ul><p>最后我们采用的是Session中心化策略，主要是考虑到整体业务进行改造成本太高。</p><ol start="2"><li>文件传输和存储的问题<br>还是对nginx采取轮询的负载均衡策略，在上传文件时，文件上传到一台服务器，对文件操作的请求却负载到另一台服务器上，导致请求中文件丢失找不到的情况。解决方案如下：</li></ol><ul><li>OSS服务引入，所有文件通过第三方OSS服务进行存储。</li><li>自建文件服务器，利用FastDFS等分布式存储服务进行。</li><li>服务器之间进行文件同步，利用共享文件夹以及共享空间，但存在同步不及时导致后续业务请求无法完成的情况。</li></ul><p>最后我们采用的是自建文件服务器的策略。</p><h2 id="整体部署图"><a href="#整体部署图" class="headerlink" title="整体部署图"></a>整体部署图</h2><p><img src="%E6%95%B4%E4%BD%93%E9%83%A8%E7%BD%B2%E5%9B%BE.png" alt="整体部署图"></p><h2 id="意见和建议–来自阿里云技术交流会议"><a href="#意见和建议–来自阿里云技术交流会议" class="headerlink" title="意见和建议–来自阿里云技术交流会议"></a>意见和建议–来自阿里云技术交流会议</h2><p>阿里云技术交流会议中，通过交流总结了如下内容：</p><ol><li>阿里的经验总结和建设原则：</li></ol><ul><li>代码重构，尽可能拆分。业务层逐步拆分、合理拆分，尽可能拆到最小层级</li><li>尽可能容错。容错机制构建，监控、诊断、预案</li><li>尽可能只保证最终一致性。数据层级，在数据同步方面，追求最终一致性</li><li>尽可能缓存。缓存引入，多层级，拦截分批处理和计算</li><li>尽可能自动化。自动化部署，DevOps引入</li><li>尽可能云化。弹性伸缩，来之扩容，用完释放</li></ul><ol start="2"><li>针对当前我们的产品的建议：</li></ol><ul><li>数据库使用云数据库，不再进行VPS内的数据库搭建。</li><li>前后端分离，前端单独部署，后端接口化</li><li>引入消息队列</li><li>弹性伸缩</li><li>负载均衡</li><li>数据库</li><li>文件服务</li><li>消息队列</li></ul><ol start="3"><li>补充建议：</li></ol><ul><li>针对文件服务，采用OSS+CDN的策略</li><li>单体应用向分布式、容器化方向发展</li><li>架构驱动到数据驱动</li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.cnblogs.com/stulzq/p/9291223.html" target="_blank" rel="noopener">Nginx源码编译安装</a></p><p><a href="https://www.jianshu.com/p/3e2b9964c279" target="_blank" rel="noopener">Nginx配置详解</a></p><p><a href="https://zsr.github.io/2018/09/17/nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5/" target="_blank" rel="noopener">负载均衡策略</a></p><h2 id="番外篇：docker的使用和配置"><a href="#番外篇：docker的使用和配置" class="headerlink" title="番外篇：docker的使用和配置"></a>番外篇：docker的使用和配置</h2><p>请参考:<a href="/2020/01/31/linux-ri-chang-yun-wei-cao-zuo/" title="Linux日常运维操作">Linux日常运维操作</a></p>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux DevOps nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>移动端混合开发的研究</title>
      <link href="/2019/07/06/yi-dong-duan-hun-he-kai-fa-de-yan-jiu/"/>
      <url>/2019/07/06/yi-dong-duan-hun-he-kai-fa-de-yan-jiu/</url>
      
        <content type="html"><![CDATA[<h1 id="移动端混合开发的接入"><a href="#移动端混合开发的接入" class="headerlink" title="移动端混合开发的接入"></a>移动端混合开发的接入</h1><h2 id="平台"><a href="#平台" class="headerlink" title="平台"></a>平台</h2><p>目前针对移动平台的混合开发接入，主要分为Android和iOS两大阵营，相比Android平台来说，iOS端的WebView相对稳定，且前后变动不大，只需要关注业务逻辑即可。但是Android平台中WebView存在严重的碎片化问题，而且对于js的调用在Android 4.2以前存在远程执行js的漏洞，相对而言，在Android平台中要更多的考虑安全问题。</p><h2 id="技术路线"><a href="#技术路线" class="headerlink" title="技术路线"></a>技术路线</h2><ul><li><p>纯Web层面</p><ul><li>React Native<br>React框架为基础，兼容Android、iOS，官方号称70%的代码可复用，相对稳定，插件丰富。</li><li>Weex<br>Vue框架为基础，兼容Android、iOS，经过大厂验证。</li><li>ionic+cordova<br>Angular框架为基础，ionic专注前端渲染，cordova封装系统层调用信息，可跨平台多，学习成本高。</li><li>mui<br>偏前端展示，效果类同原生控件，配合HBuilder进行开发，快速高效。</li><li>apicloud</li><li>AppCan</li></ul></li><li><p>非Web层面</p><ul><li>Flutter<br>独立渲染，接近原生，新出，目前生态不是很完善，可能需要自己编写插件，但是大厂实践多，可参考内容多。</li></ul></li></ul><p>开发层面来讲，我更倾向于mui+Weex/Flutter的方式。也就是mui做一些更新比较频繁的页面，比如抽奖、秒杀这样的活动页面前端，Flutter/Weex做业务性的内容，同原生交互比较多的内容。</p><h2 id="WebView依赖"><a href="#WebView依赖" class="headerlink" title="WebView依赖"></a>WebView依赖</h2><ul><li><p>原生WebView<br>  iOS中，原生WebView相对稳定，不需要考虑太多情况。<br>  Android中，存在WebView碎片化的问题，而且需要做的兼容性工作比较多。再考虑到Android碎片化的问题，在不同的手机上，显示差别大，性能差别大！<br>  所以如果在Android端采用原生WebView，推荐使用<a href="https://github.com/Justson/AgentWeb" target="_blank" rel="noopener">AgentWeb</a>框架。</p></li><li><p>腾讯X5浏览内核<br>  腾讯X5浏览内核，针对Android WebView的问题产出，兼容性良好，但是开发成本高，存在暗坑，并非开源项目，容易造成开发成本过高的问题。不支持cordova框架！</p></li><li><p>CrossWalk<br>  cordova官方出品，Android/iOS均可用，性能最佳，但是引入包高达18MB，引入后App体积爆炸。</p></li></ul><h2 id="开发方式"><a href="#开发方式" class="headerlink" title="开发方式"></a>开发方式</h2><ul><li>Web页面嵌入<ul><li>业务层面<ol><li>需要考虑URL拦截的问题，拦截规则和跳转规则。</li><li>处理下载问题</li></ol></li><li>活动页面更新<br>  直接更新URL即可</li></ul></li><li>与原生交互<ul><li>使用JsBridge调用原生api</li><li>处理下载、文件选择等问题</li></ul></li><li>Web侧更新策略<ul><li>业务层面<br>  考虑Web侧热更新问题</li><li>活动页面<br>  直接下发更新URL</li></ul></li><li>热更新/热修复<br>  待更新</li></ul><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><ul><li>Web侧安全<ul><li>Web页面劫持</li></ul></li><li>通信接口<ul><li>js和原生代码交互</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><pre><code>接入混合开发，需要同时考虑Web侧和原生侧的问题，如何编织使其发挥各自优势，是我们需要认真考虑的问题！</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> Android iOS 移动开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>插件化与组件化</title>
      <link href="/2019/07/05/cha-jian-hua-yu-zu-jian-hua/"/>
      <url>/2019/07/05/cha-jian-hua-yu-zu-jian-hua/</url>
      
        <content type="html"><![CDATA[<h1 id="插件化、组件化、模块化的思考——App架构从开发到构建"><a href="#插件化、组件化、模块化的思考——App架构从开发到构建" class="headerlink" title="插件化、组件化、模块化的思考——App架构从开发到构建"></a>插件化、组件化、模块化的思考——App架构从开发到构建</h1><h2 id="核心参考"><a href="#核心参考" class="headerlink" title="核心参考"></a>核心参考</h2><p>从代码组织层面，我们可以参考这样下面的图片去进行。</p><p><img src="http://www.androidchina.net/wp-content/uploads/2018/04/v2-7f1cfaedf7545a9674f5fd3f0e1a93a5_hd.jpg" alt="架构图片" title="架构图片"></p><p>三面立体，上面作为<strong>职责</strong>划分，前面作为<strong>代码层级</strong>划分，右面作为<strong>功能</strong>划分。也就是说，可以从三个维度去思考，然后在每个维度里面，又从三个层次去区别和划分！这个位置是指导我们进行架构内部的模块化设计。</p><p>如果从顶层的架构设计考虑的化，这里又有两个维度，一个是架构思维，一个是架构原则。思维是我们的思考方式，是我们解决问题的方法。原则是我们思考问题的方向，是我们解决问题的一些标准。</p><p>那么我们为什么要去进行架构？</p><ol><li>项目需求扩张，旧的架构不适应新的需求；</li><li>开发团队人员增加，协作要求变高；</li><li>新技术引入；</li><li>更高的软件质量要求；</li><li>更灵活的开发；</li></ol><p>其优势在于：</p><ol><li>进行了模块化的解耦，产品相对独立，应对需求变化、技术更新更加灵活，团队协作更加方便，并减少了许多是无用功，也给团队留下了一些技术积累；</li><li>进行了必要的统一规范，组织结构更加清晰，系统更加健康；</li><li>引入了新的技术框架，，产品获得更好的体验；</li><li>进行了系统的优化工作，软件的品质更高，体验更好；</li></ol><h2 id="产出依据"><a href="#产出依据" class="headerlink" title="产出依据"></a>产出依据</h2><p>架构设计之前，首先要去确立架构的产出依据，也就是我们设计或者采用这样的架构方式，是根据什么产出的。这里主要有两点：</p><ol><li><p><strong>产品开发需求</strong><br>在目前绝大部分app中，还是以业务驱动为主，首先要做到的是满足需求内容，在这里就需要去解决业务需求和开发之间的矛盾。反映在架构层级，就是做到快速出成果，所以前期可以不考虑复杂的架构设计，可以直接去编写。但是考虑业务系统的发展，当达到一定层次和规模时，当前的架构并不满足业务的变化内容，就可以考虑对当前项目进行架构变更。说这么多还有一层意思，就是不要过度优化和过度架构，也就是<strong>不要为了架构而架构</strong>！</p></li><li><p><strong>App的性能和质量</strong><br>在app上线后，会收到各种各样的反馈意见，以及终端用户对于app内部错误的上报，那么上报后就需要考虑如何去复现、去定位到最终修改，推送用户升级。那么这个过程中，就会涉及到关于app性能的问题，比如卡顿和内存泄漏这样的问题，我们在去复现问题到解决问题的过程中可能会碰到各种各样的困难，这些困难会促使我们重新思考app的架构，是否可测试、是否够清晰、边界是否考虑周全等等这样的问题。各种各样的问题也会反馈给我们，促使我们去变更当前的app架构。</p></li></ol><h2 id="架构之前"><a href="#架构之前" class="headerlink" title="架构之前"></a>架构之前</h2><ol><li><p>拥抱变化<br>拥抱变化并不是指，我们要考虑到所有的会变化的部分去做到一个非常灵活的层次，而是去确定变和不变的层次，再进行编码。要确定在一段时间内不变的东西，考虑到可预见的变化即可。树立<strong>不要过度优化</strong>的思维！</p></li><li><p>没有永远完美的架构<br>任何架构都不是完美的，任何架构都不会做到适应所有的场景或者是所有的业务逻辑，但是我们需要考虑在当前有限范围内最合理、最适用的就好。</p></li><li><p>选择当下最合适的<br>同上</p></li><li><p>进化和演变<br>随着业务系统的发展，我们需要不断调整自身的架构，适应需求！需求是不断在变化的，但是总会在一定的时间内是不变的。我们需要做的是根据这种变和不变进行架构的调整和进化！</p></li></ol><h2 id="框架-依赖库选型"><a href="#框架-依赖库选型" class="headerlink" title="框架/依赖库选型"></a>框架/依赖库选型</h2><ul><li>选取原则</li></ul><p>1.1 稳定性<br>所有依赖库在选择的时候首先要考虑稳定性的问题，比如在GitHub上找到一个网络请求框架，我们需要考量的是当前该框架是否还有人维护、是否有很多人star或者fork、是否有很多issue待解决、待解决和已解决的比例有多大等等，毕竟用起来出现了问题，还得依靠从GitHub获取的知识去解决。</p><p>1.2 学习成本<br>所有依赖库在选择的时候都要去看看它是如何去使用的，如何去编写，是否快速上手，遇到问题是否可查，源代码要能去读去看，有条件的可以对源码进行分析审查，再决定是否使用。再一个就是组内人员水平不一的情况下，如何进行培训和指导也要考虑在内！</p><p>1.3 优缺点<br>这一点主要针对多个相同或者相似功能的依赖库对比时，需要明确各个依赖库的优缺点，简单的可以根据上面学习成本中概述的几点去考虑，复杂一些的需要阅读源代码进行。</p><p>1.4 应用场景<br>比如说，各种相似功能的控件应用在界面上是否合适，各种网络请求框架是否符合现在的架构场景。这个需要具体问题具体分析！</p><p>1.5 选择标准<br>1.业界著名的（如Square, Google的）且经过大量使用验证的框架</p><ol start="2"><li>长期维护且比较活跃（如提交issue）的框架</li><li>选择合适的框架，尽量小而精，不要大而全。这可能有些矛盾，因为大部分框架都会考虑通用性，如果仅需要其中一两个功能，就需要权衡了</li><li>根据开源级别，尽量选择允许修改源码的，这样一旦框架出现问题，可自行修改</li><li>对于国内的库，千万不要只相信star数量</li></ol><ul><li>原生层面框架挑选</li></ul><p>2.1 基础框架<br>基础框架的分类如下：</p><ul><li>网络请求与解析</li><li>事件总线</li><li>依赖注入</li><li>图片加载</li><li>图片/文件选择</li><li>数据库</li><li>传感器相关（相机/蓝牙/NFC等）</li><li>多媒体（注意视频播放部分）</li><li>性能优化</li><li>常用的View以及动画效果</li></ul><p>挑选参考：</p><ul><li><a href="https://github.com/wasabeef/awesome-android-ui" target="_blank" rel="noopener">awesome-android-ui</a></li><li><a href="https://github.com/Tim9Liu9/TimLiu-Android" target="_blank" rel="noopener">自己总结的Android开源项目及库</a></li><li><a href="https://blog.csdn.net/lyabc123456/article/details/81179015" target="_blank" rel="noopener">Android第三方库收藏汇总</a></li></ul><p>2.2 第三方服务</p><ul><li><p>地图<br>高德/百度</p></li><li><p>推送<br>极光/友盟/各大厂商自有（不止集成一个，提高到达率）</p></li><li><p>数据统计<br>友盟/百度统计</p></li><li><p>分享<br>ShareSDK/友盟</p></li><li><p>第三方登录<br>QQ/微信/微博</p></li><li><p>IM</p></li><li><p>支付<br>支付宝/微信/银联/聚合支付</p></li><li><p>广告变现<br>有米/百度推广首页开屏</p></li><li><p>短信验证码</p></li><li><p>直播平台</p></li><li><p>H5/混合开发层面</p></li></ul><p>3.1 WebView</p><ul><li>腾讯X5浏览内核</li><li>CrossWalk</li><li>AgentWeb（轻量级）</li></ul><p>3.2 网页加载优化/预加载</p><ul><li>VasSonic–网页预加载秒开</li></ul><p>3.3 和原生代码交互——JsBridge</p><p>3.4 混合开发框架</p><ul><li>PhoneGap</li><li>Ionic/Cordova</li><li>AppCan/ApiCloud/mui（国产）</li><li>Weex</li><li>ReactNative （可直接嵌入原生）</li><li>Flutter（独立渲染，可直接嵌入原生）</li></ul><p>注: 混合开发这块需要开新坑来研究讲解</p><ul><li>对现有方案的权衡和改造</li></ul><p>4.1 变更成本评估<br>主要评估下面三个维度：</p><ul><li>对当前业务逻辑的影响</li><li>对当前项目运行的影响</li><li>对内部项目成员的影响</li></ul><p>4.2 替换<br>对当前不符合应用场景的内容进行替换，使其符合当下编写需要</p><p>4.3 保留<br>可用内容要延续，并且可以考虑适当重构</p><p>4.4 新增<br>增加符合业务场景的框架</p><p>4.5 是否需要自研替代<br>对于开源库/框架不满意的情况下，考虑自研内容来代替，可以去改造开源框架来实现，也可以自己造轮子！</p><h2 id="代码架构编织"><a href="#代码架构编织" class="headerlink" title="代码架构编织"></a>代码架构编织</h2><ul><li><p>前提：三统一<br>统一开发环境，统一开发配置，统一代码风格</p></li><li><p>原生代码部分<br>原生代码部分可以通过业务拆分为以下内容：</p></li><li><p>Base部分<br>基础代码，例如基本的网络请求封装和BaseActivity这样的代码</p></li><li><p>Common部分<br>通用技术，就是在各个模块之间可以通用的代码信息，例如推送、支付这样的通用性强的模块</p></li><li><p>Business部分<br>特定业务逻辑，到具体的业务场景和业务流程，需要考虑的是，在模块化场景下如何连接各个模块信息，也就是不同模块之间如何跳转的问题。</p></li><li><p>Router<br>具体模块之间的沟通桥梁</p></li><li><p>Others部分<br>非业务层面，针对app监控的内容。</p></li><li><p>bug监控和问题反馈<br>根据监控和上报的信息做出修复，关联后续的热修复部分</p></li><li><p>数据统计信息<br>统计点击路径和用户行为</p></li><li><p>内存监控<br>可借助工具进行分析，排查内存泄漏的问题</p></li><li><p>Web/混合开发部分</p></li><li><p>本地代码<br>不常更新，使用频繁的web代码，直接嵌入assets中调用的</p></li><li><p>在线访问<br>活动页面，经常进行变更的部分，URL存本地或者从后端获取</p></li><li><p>和原生混合——RN/Flutter<br>利用混合开发做部分原生代码的事情，适用场景同上</p></li><li><p>和原生交互——JsBridge<br>web层面需要调用原生层面的代码，例如读取手机内联系人这样的场景，或者是和原生逻辑进行交互</p></li><li><p>Business层开发模式</p></li><li><p>MVC<br>传统思维，存在Activity过度冗余的情况</p></li><li><p>MVP<br>分层，C层变更会引起P层变更，联系紧密</p></li><li><p>MVVM<br>ViewModel数据绑定，从事件驱动到数据驱动</p></li><li><p>Clean Architecture<br>分层更清晰，降低耦合</p></li><li><p>Jetpack<br>官方开发套件</p></li></ul><p>上述方式可根据需要进行挑选，或者团队内部分工编写每一层的代码。</p><ul><li><p>热更新/热修复</p></li><li><p>并非银弹<br>热更新和热修复并非万能，存在数据包变大、差量包管理复杂的问题，而且还需要注意推送热更代码的到达率，避免出现推送用户群体错误的问题，以防止影响用户使用。</p></li><li><p>分类/使用场景<br>可以根据替换的资源范围（类、so、res资源）、是否即时生效、修复粒度来进行选择，可以由下面的内容进行参考：<br><a href="https://www.jianshu.com/p/2eae8a69eb27" target="_blank" rel="noopener">Android 热修复 - 各框架原理学习及对比</a><br><a href="https://jaeger.itscoder.com/android/2016/08/28/android-hot-fix.html" target="_blank" rel="noopener">Android 热修复方案对比</a><br><a href="https://www.cnblogs.com/popfisher/p/8543973.html" target="_blank" rel="noopener">Android热修复技术原理详解（最新最全版本）</a></p></li><li><p><strong>组件化、插件化、模块化</strong></p></li><li><p>模块化：一个程序按照其功能做拆分，分成相互独立的模块（例如：登陆，注册）。模块化的具体实施方法分为插件化和组件化。</p></li><li><p>组件化：开发模式下面module本来就是一个独立app，只是发布模式下变成library。</p></li><li><p>插件化：就是不存在发布模式开发模式，每个组件业务就是一个独立apk开发，然后通过主工程app动态加载部署业务组件apk。</p></li></ul><p>插件化和组件化的异同：</p><p>相同点：</p><p>都必须能够实现单独调试、集成编译、数据传输、UI 跳转、生命周期和代码边界这六大功能。</p><p>不同点：</p><ul><li><p>插件化：可以动态增加和修改线上的模块。</p></li><li><p>组件化：动态能力相对较弱，只能对线上已有模块进行动态的加载和卸载，不能新增和修改。</p></li></ul><p>根据需要，建议优先进行组件化的工作，先进行代码分层和抽取，然后再进行插件化和模块化的工作。切记<strong>不可提前优化</strong>！</p><p>参考文章：<a href="https://juejin.im/post/59194428128fe1005ccfa763" target="_blank" rel="noopener">[Android] 组件化 &amp; 模块化 &amp; 插件化演进</a></p><ul><li><p>快速开发</p></li><li><p>核心：减少重复代码的编写<br>一个是现有项目的封装的工具类和功能聚合，另一个是借助模板库和插件来减少重复代码的编写，交给机器自动创建，专注编写业务逻辑</p></li><li><p>模板库——Live Template<br>通过编写Android Studio里面的模板库来消除重复代码的内容，例如自动创建相应的文件夹和类代码</p></li><li><p>辅助插件<br>例如json解析，自动生成JavaBean这样的代码，还有就是序列化代码的自动生成，将这样的工作交给插件去完成。</p></li></ul><h2 id="持续构建"><a href="#持续构建" class="headerlink" title="持续构建"></a>持续构建</h2><ul><li><p>核心：自动化打包和构建<br>通过进行持续构建，达到随时产出可用app的目标，将人力从打包的场景下解脱出来，专注业务的开发</p></li><li><p>随时/定时构建<br>确定构建的频次，确定产出的内容可以随时被测试，提供手动进行构建的选项，方便测试人员的工作</p></li><li><p>版本管理</p></li></ul><ol><li>提交文件要少，而且要写清楚代码具体做了哪些工作，禁止一大坨代码一并提交或者一天工作后一提交的行为</li><li>稳定版本打TAG，做标记</li><li>代码最好可以多备份一次，防止丢失。可以在每次稳定版本交付后进行备份。</li></ol><ul><li><p>代码质量控制——CodeReview<br>提高产出的代码质量，可以进行互相审查，不是为了挑错，而是首先保证代码可读，然后讨论优缺点以及改进方案，最后是提升代码质量和产品质量。能否真正实施，还需要视当前开发进度和团队内部情况决定，但是可以优先统一以下代码风格和Lint静态审查规则。</p></li><li><p>测试<br>测试人员根据当前提交的内容，进行自己的工作，然后反馈问题即可。<br>开发人员是否需要编写单元测试，视团队情况而定。</p></li><li><p>项目交付<br>做到每个小功能一交付，每天一个小交付，通过这样的方式去累积交付内容，达到最终交付的时候能够平稳顺利！</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>试着将我们的工作任务进行分离，减少每项任务之间的交叉，避免东一榔头西一棒子，有条理的完成各项工作。这样一来，你的工作目标是清晰的，也更加容易的完成既定目标。同样的，我们可以通过这一理论还规划自己的人生。将我们的事业、家庭、兴趣爱好分离，再对每一项分离出更细的关注点，每个关注点在不同的阶段定下自己小目标。这样，我们对自己就可以有了更清晰的认识，认识自己已经拥有什么？最终想要什么？计划怎么做？当前怎么做？当然，并不是每一步都是能够按照既定目标走的，我们需要不断的更新对自己的认识，做出更加合适的决策。</p><p>最后，需要注意的是</p><ol><li>切记不要提前/过度优化</li><li>适应当前需要</li><li>结合团队实际情况</li><li>仅总结了大体的思路和框架，细节尚需去具体讨论，需要各位的补充信息</li></ol><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://www.androidchina.net/8193.html" target="_blank" rel="noopener">Android开发架构思考及经验总结</a><br><a href="https://blog.csdn.net/crazy1235/article/details/76349554" target="_blank" rel="noopener">模块化？组件化？插件化？热更新？热修复？</a></p>]]></content>
      
      
      <categories>
          
          <category> 移动端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Android iOS 移动开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《架构真经》读书笔记</title>
      <link href="/2019/06/02/jia-gou-zhen-jing-du-shu-bi-ji/"/>
      <url>/2019/06/02/jia-gou-zhen-jing-du-shu-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="《架构真经》读书心得"><a href="#《架构真经》读书心得" class="headerlink" title="《架构真经》读书心得"></a>《架构真经》读书心得</h1><h2 id="50条可扩展性规则"><a href="#50条可扩展性规则" class="headerlink" title="50条可扩展性规则"></a>50条可扩展性规则</h2><h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><ol><li>拒绝教条，灵活运用</li><li>结合实际，适用于当前环境，并且可稍稍向前走一点</li><li>有所为有所不为，把握实施的关键节点</li><li>批判继承，结合当前国内的互联网环境，结合公司目前所处位置以及所能调动的资源</li><li>符合自己当前的需求，符合当前公司、项目的需求</li></ol><h3 id="大道至简"><a href="#大道至简" class="headerlink" title="大道至简"></a>大道至简</h3><h4 id="避免过度设计"><a href="#避免过度设计" class="headerlink" title="避免过度设计"></a>避免过度设计</h4><p>警惕复杂的解决方案</p><h4 id="2-DID方法"><a href="#2-DID方法" class="headerlink" title="2. DID方法"></a>2. DID方法</h4><ul><li>D - Design 设计20倍容量</li><li>I - Implement 实施3倍容量</li><li>D - Deploy 部署1.5倍容量</li></ul><p>要注意的是，避免过度设计！我认为在这个位置，如果一个公司是初期技术形态，亟待创立自己的技术体系，可以先按照<strong>3-2-1</strong>模式走。如果是相对成熟的公司，可以按照DID方法考虑实施。</p><h4 id="3-三次简化方案"><a href="#3-三次简化方案" class="headerlink" title="3. 三次简化方案"></a>3. 三次简化方案</h4><ul><li>项目范围： 帕累托原则（2-8原则）简化范围</li><li>设计：考虑成本优化和可扩展性来简化设计</li><li>实施：依靠其他人经验来简化部署</li></ul><p>80%的收益来自20%的工作，确定核心业务所在边界，是非常重要的。例如，在之前的某养车项目中，其核心业务应该是车辆保养，那么应该强化车辆保养业务在项目中的地位，削弱或者降低洗车、维修、保险这样的非核心业务的优先级。并且根据保养业务需求，简化保养流程设计，并快速实施！</p><h4 id="4-减少域名解析"><a href="#4-减少域名解析" class="headerlink" title="4. 减少域名解析"></a>4. 减少域名解析</h4><p>尽量将所有对象加载放在同一个域名内。这点要批判性的看，第一就是CDN的作用，第二个就是如果这样操作是否会造成负载过大的情况。需要集体评审决定。</p><h4 id="5-减少页面目标"><a href="#5-减少页面目标" class="headerlink" title="5. 减少页面目标"></a>5. 减少页面目标</h4><ul><li>减少或合并对象，但要平衡最大并发连接数。</li><li>寻找机会减轻对象的重量</li><li>不断测试确保性能提升</li></ul><p>无法一蹴而就的行动，需要反复测试，在易用性、可用性、性能之间做平衡。</p><h4 id="6-采用同构网络"><a href="#6-采用同构网络" class="headerlink" title="6. 采用同构网络"></a>6. 采用同构网络</h4><p>这点的化，基本上在国内云主机厂商中，用一家不混用一般是可以的。</p><h3 id="分而治之–AKF扩展立方体-三轴扩展"><a href="#分而治之–AKF扩展立方体-三轴扩展" class="headerlink" title="分而治之–AKF扩展立方体+三轴扩展"></a>分而治之–AKF扩展立方体+三轴扩展</h3><p><img src="AKF%E7%AB%8B%E6%96%B9%E4%BD%93.png" alt></p><p><img src="%E4%B8%89%E8%BD%B4%E6%89%A9%E5%B1%95.png" alt></p><h4 id="7-X轴扩展"><a href="#7-X轴扩展" class="headerlink" title="7. X轴扩展"></a>7. X轴扩展</h4><p>其核心是<strong>水平复制</strong>，例如数据库架构中的一写多读，以复制数据和功能为代价获得事务的快速扩展。</p><h4 id="8-Y轴拆分"><a href="#8-Y轴拆分" class="headerlink" title="8. Y轴拆分"></a>8. Y轴拆分</h4><p>其核心是<strong>服务或者资源的拆分</strong>，沿动词（服务）拆分，沿名词（资源或者数据）拆分，或者是沿着动词/名词的边界拆分服务和数据。</p><h4 id="9-Z轴拆分"><a href="#9-Z轴拆分" class="headerlink" title="9. Z轴拆分"></a>9. Z轴拆分</h4><p>其核心是<strong>分片、分组、分块</strong>，例如基于用户群属性，将数据库拆分为学生、工人、白领、农民四个层次，在此基础上，将服务进行这四个维度的拆分，调整数据库大小和读写策略。<br>总结就是通过克隆扩展（X轴），通过拆分不同的东西扩展（Y轴），通过拆分类似的东西来扩展（Z轴）。</p><h3 id="水平扩展"><a href="#水平扩展" class="headerlink" title="水平扩展"></a>水平扩展</h3><h4 id="10-向外扩展"><a href="#10-向外扩展" class="headerlink" title="10. 向外扩展"></a>10. 向外扩展</h4><p>横向拆分数据库和事务。对应于<em>向上扩展</em>，购买更强的硬件配置实现。我认为，向上扩展可以结合当前云服务商的弹性伸缩功能去用，不要一棒子打死！</p><h4 id="11-商品化系统"><a href="#11-商品化系统" class="headerlink" title="11. 商品化系统"></a>11. 商品化系统</h4><p>阿里的去IOE行为，抛弃大型、特定的硬件，转而使用小型廉价灵活的系统，这样适合于快速调整，也有利于人员成本降低。</p><h4 id="12-托管方案扩展"><a href="#12-托管方案扩展" class="headerlink" title="12. 托管方案扩展"></a>12. 托管方案扩展</h4><p>核心是三数据中心配置：</p><p><img src="%E4%B8%89%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E9%85%8D%E7%BD%AE.png" alt></p><p><img src="%E5%A4%9A%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E6%88%90%E6%9C%AC%E6%AF%94%E8%BE%831.png" alt></p><p><img src="%E5%A4%9A%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E6%88%90%E6%9C%AC%E6%AF%94%E8%BE%832.png" alt></p><p>结合我们的AKF立方体进行拆分，在不同数据中心做不同的配置。对比冷-热双活数据库配置，这样是极好的。</p><h4 id="13-利用云"><a href="#13-利用云" class="headerlink" title="13. 利用云"></a>13. 利用云</h4><p>可以将三数据中心的一个数据中心，上云。利用云服务的弹性伸缩功能，灵活扩展，应对突发需求。但是上云也要注意数据安全！</p><h3 id="工欲善其事，必先利其器"><a href="#工欲善其事，必先利其器" class="headerlink" title="工欲善其事，必先利其器"></a>工欲善其事，必先利其器</h3><p>马斯洛的锤子理论：如果你手里拿着一把锤子，看什么就都会像钉子。不要妄图“一招鲜，吃遍天”。</p><h4 id="14-适当使用数据库"><a href="#14-适当使用数据库" class="headerlink" title="14. 适当使用数据库"></a>14. 适当使用数据库</h4><p>关系型和非关系型数据库结合使用。NoSQL划分为Key-Value和File两种，强调利用各自优势！</p><p><img src="%E6%95%B0%E6%8D%AE%E5%BA%93%E6%96%B9%E6%A1%88%E5%86%B3%E7%AD%96%E6%A8%A1%E5%9E%8B.png" alt></p><h4 id="15-慎重使用防火墙"><a href="#15-慎重使用防火墙" class="headerlink" title="15. 慎重使用防火墙"></a>15. 慎重使用防火墙</h4><p>只对核心业务以及安全性要求高的业务使用，例如：支付流程、用户信息等。</p><h4 id="16-积极使用日志文件"><a href="#16-积极使用日志文件" class="headerlink" title="16. 积极使用日志文件"></a>16. 积极使用日志文件</h4><p>通过日志信息诊断和预防问题，也可以进行事故现场复原分析。利用InfluxDB+Grafana进行监控也是极好的。<br>但是日志是有代价的，比如：运行状态变慢、过多日志带来的额外数据成本等。</p><ul><li>总结<br>引入每项新技术，都需要另外一种技能来支持。尽管工作中使用合适的工具很重要，但是不要过度强调专业化，以至于没有足够深度的技能来支持。先用，有了问题再去解决和攻关，用的时候胆大心细即可。</li></ul><h3 id="画龙点睛"><a href="#画龙点睛" class="headerlink" title="画龙点睛"></a>画龙点睛</h3><h4 id="17-避免画蛇添足"><a href="#17-避免画蛇添足" class="headerlink" title="17. 避免画蛇添足"></a>17. 避免画蛇添足</h4><p>永远不要为确认操作是否有效而读取刚刚写入的数据。同样的，扩展一下，减少在创建数据的时候重复去查询已有的数据。</p><h4 id="18-停止重定向"><a href="#18-停止重定向" class="headerlink" title="18. 停止重定向"></a>18. 停止重定向</h4><h4 id="19-放宽时间约束–针对数据库事务"><a href="#19-放宽时间约束–针对数据库事务" class="headerlink" title="19. 放宽时间约束–针对数据库事务"></a>19. 放宽时间约束–针对数据库事务</h4><p>CAP原则：</p><p>Consistency- 一致性<br>Availability- 可用性<br>Partition tolerance- 分区容错性</p><p>解决方案-BASE：</p><p>Basically Available - 基本可用<br>Soft state - 软状态<br>Eventually consistent - 最终一致性</p><h3 id="缓存为王"><a href="#缓存为王" class="headerlink" title="缓存为王"></a>缓存为王</h3><h4 id="20-利用CDN缓存"><a href="#20-利用CDN缓存" class="headerlink" title="20. 利用CDN缓存"></a>20. 利用CDN缓存</h4><p>针对静态资源获取，减少动态生成js文件，减轻服务器压力。要结合前后端分离做！<br>向上添加边缘加速服务，提升访问速度。</p><h4 id="21-灵活管理缓存"><a href="#21-灵活管理缓存" class="headerlink" title="21. 灵活管理缓存"></a>21. 灵活管理缓存</h4><p>使用Expires头和Cache-Control头来减少请求量。</p><h4 id="22-利用Ajax缓存"><a href="#22-利用Ajax缓存" class="headerlink" title="22. 利用Ajax缓存"></a>22. 利用Ajax缓存</h4><p>适当调整实时性要求不高的请求接口进行缓存，适当调整Last-Modified、Cache-Control、Expires头。核心目标是减少在网络上来回传输数据，减少用户感知的响应时间和降低服务器负载！</p><h4 id="23-利用页面缓存"><a href="#23-利用页面缓存" class="headerlink" title="23. 利用页面缓存"></a>23. 利用页面缓存</h4><p>例如Nginx中针对静态文件的缓存。要实施页面缓存，使用适当的http头，尽可能包括RFC2616中另外的HTTP头。</p><h4 id="24-利用应用缓存"><a href="#24-利用应用缓存" class="headerlink" title="24. 利用应用缓存"></a>24. 利用应用缓存</h4><p>确定可能使用的情况，根据AFK立方体进行拆分。</p><h4 id="25-利用对象缓存"><a href="#25-利用对象缓存" class="headerlink" title="25. 利用对象缓存"></a>25. 利用对象缓存</h4><p>Redis、Memcached，确定使用哪种类型的对象缓存，在数据库和应用层之间实施。最后需要监控缓存命中率，及时调整负载。</p><h4 id="26-独立对象缓存"><a href="#26-独立对象缓存" class="headerlink" title="26. 独立对象缓存"></a>26. 独立对象缓存</h4><p>缓存层的建设和划分，单独作为一层进行建设。</p><h3 id="前车之鉴"><a href="#前车之鉴" class="headerlink" title="前车之鉴"></a>前车之鉴</h3><h4 id="27-失败乃成功之母"><a href="#27-失败乃成功之母" class="headerlink" title="27. 失败乃成功之母"></a>27. 失败乃成功之母</h4><p>抓住每个机会，尤其是失败的机会，学习经验并吸取教训。犯错不可怕，从错误中学习和进步才是重要的。创建学习型团队，应对业务的不断增长！例如，针对华为项目的特殊要求，我们对当前单体应用支持逐步扩展，满足其业务需求，也对于我们自身下一步的成长打下基础！</p><ul><li>关注失败</li><li>拒绝简化解释</li><li>对操作的敏感性</li><li>坚守弹性承诺</li><li>尊重专业经验</li></ul><h4 id="28-不靠QA发现错误"><a href="#28-不靠QA发现错误" class="headerlink" title="28. 不靠QA发现错误"></a>28. 不靠QA发现错误</h4><p>交付QA时，尽量保证自己的代码测试到位！通过QA去认识问题缺陷，更好的学习！</p><h4 id="29-不能回滚注定失败"><a href="#29-不能回滚注定失败" class="headerlink" title="29. 不能回滚注定失败"></a>29. 不能回滚注定失败</h4><p>代码的版本控制。<br>数据库文件的版本控制，针对数据修改时：</p><ul><li>数据库结构的变更仅可以增加</li><li>数据库变更脚本化并经过测试</li><li>限制应用中SQL的查询的使用，尤其是：<strong>select *</strong>这样的调用</li><li>数据的语义变化</li><li>上线、下线可控</li></ul><p>产出过程可控，产物可控！例如，一个比较极端的例子，在之前的养车项目中，后端源代码丢失，导致修改时只能进行jar包反编译，添加代码后再编译打包放入服务器，由此带来的生产事故是：某次打包放入服务器的jar包有错误，修改之前未备份，导致整个服务不能工作。</p><h3 id="重中之重"><a href="#重中之重" class="headerlink" title="重中之重"></a>重中之重</h3><h4 id="30-从事务中清除商务智能"><a href="#30-从事务中清除商务智能" class="headerlink" title="30. 从事务中清除商务智能"></a>30. 从事务中清除商务智能</h4><p>减少数据库里面所使用的存储过程和商业逻辑，对不同系统进行拆分和隔离。专注于重要系统的工作，拆分出占比不高或者优先级较低的业务！</p><h4 id="31-注意昂贵的关系"><a href="#31-注意昂贵的关系" class="headerlink" title="31. 注意昂贵的关系"></a>31. 注意昂贵的关系</h4><p>设计数据库模型时，考虑数据库分离和未来可能的数据需求，谨慎的考虑数据库变更带来的成本问题。这一点可能需要提前设计！数据库三范式以及表查询连接。</p><h4 id="32-正确使用数据库锁"><a href="#32-正确使用数据库锁" class="headerlink" title="32. 正确使用数据库锁"></a>32. 正确使用数据库锁</h4><p>选择锁类型以及如何使用，重中之重！确保最佳的解决方案！</p><h4 id="33-禁用分阶段提交"><a href="#33-禁用分阶段提交" class="headerlink" title="33. 禁用分阶段提交"></a>33. 禁用分阶段提交</h4><p>两阶段和三阶段提交，锁定应用服务器太久的问题，引发超时或者无响应的反馈！</p><h4 id="34-慎用Select-for-Update"><a href="#34-慎用Select-for-Update" class="headerlink" title="34. 慎用Select for Update"></a>34. 慎用Select for Update</h4><p>For update语句导致游标长期占用锁的问题，减缓事务完成时间。</p><p>结合我们之前华为项目中大BOM列表加载时间问题，可以由此排查下？</p><h4 id="35-避免选择所有列"><a href="#35-避免选择所有列" class="headerlink" title="35. 避免选择所有列"></a>35. 避免选择所有列</h4><p>不要在查询中使用Select *。</p><h3 id="有备无患"><a href="#有备无患" class="headerlink" title="有备无患"></a>有备无患</h3><h4 id="36-用泳道隔离故障"><a href="#36-用泳道隔离故障" class="headerlink" title="36. 用泳道隔离故障"></a>36. 用泳道隔离故障</h4><p>禁止故障隔离的服务和数据间同步通信或访问。</p><p>原则：</p><ul><li>泳道之间什么都不共享</li><li>泳道之间不进行同步调用</li><li>限制泳道之间的同步调用</li><li>绝对必要时，如何实现跨越泳道边界的异步传输</li></ul><p><img src="%E6%95%85%E9%9A%9C%E9%9A%94%E7%A6%BB%E4%BC%98%E5%8A%BF.PNG" alt></p><p><img src="%E6%95%85%E9%9A%9C%E9%9A%94%E7%A6%BB%E5%8E%9F%E5%88%99.PNG" alt></p><h4 id="37-拒绝单点故障"><a href="#37-拒绝单点故障" class="headerlink" title="37. 拒绝单点故障"></a>37. 拒绝单点故障</h4><p>消除系统内的单点故障，努力实施主动/主动模式，负载均衡。单例情形下，可在主动/被动模式的实例中采用控制服务。</p><h4 id="38-避免系统串联"><a href="#38-避免系统串联" class="headerlink" title="38. 避免系统串联"></a>38. 避免系统串联</h4><p>层级之间串联，层级内并联多个相同组件，可选择性。通过减少串联组件或者增加并联组件来降低风险。</p><h4 id="39-启用与禁用功能"><a href="#39-启用与禁用功能" class="headerlink" title="39. 启用与禁用功能"></a>39. 启用与禁用功能</h4><p>可随时控制服务的上线下线。</p><h3 id="超然物外"><a href="#超然物外" class="headerlink" title="超然物外"></a>超然物外</h3><h4 id="40-力求无状态"><a href="#40-力求无状态" class="headerlink" title="40. 力求无状态"></a>40. 力求无状态</h4><p>无状态系统构建，减少相互之间依赖！简单和容易更加重要！</p><h4 id="41-在浏览器保存会话数据"><a href="#41-在浏览器保存会话数据" class="headerlink" title="41. 在浏览器保存会话数据"></a>41. 在浏览器保存会话数据</h4><p>如果一定是有状态的系统，尽量使用cookie来保存数据。使用SSL协议进行通信加解密，然后建议使用至少两个cookie的模式，一个是授权cookie，针对ajax请求；另一个，是普通cookie，请求不重要的数据时，通过不安全的HTTP传输。</p><h4 id="42-用分布式缓存处理状态"><a href="#42-用分布式缓存处理状态" class="headerlink" title="42. 用分布式缓存处理状态"></a>42. 用分布式缓存处理状态</h4><p><img src="%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%E7%A6%81%E5%BF%8C.png" alt></p><p><img src="%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%E8%80%83%E8%99%91.png" alt></p><p>改造趋势从42，41，40的路线去走，切记不要一蹴而就！</p><h3 id="异步通信"><a href="#异步通信" class="headerlink" title="异步通信"></a>异步通信</h3><h4 id="43-尽可能异步通信"><a href="#43-尽可能异步通信" class="headerlink" title="43. 尽可能异步通信"></a>43. 尽可能异步通信</h4><h4 id="44-扩展消息总线"><a href="#44-扩展消息总线" class="headerlink" title="44. 扩展消息总线"></a>44. 扩展消息总线</h4><p><img src="%E6%B6%88%E6%81%AF%E6%80%BB%E7%BA%BFAKF%E7%AB%8B%E6%96%B9%E4%BD%93.png" alt></p><p><img src="%E6%B6%88%E6%81%AF%E6%80%BB%E7%BA%BFAKF%E7%AB%8B%E6%96%B9%E4%BD%932.png" alt></p><h4 id="45-避免消息总线过度拥挤"><a href="#45-避免消息总线过度拥挤" class="headerlink" title="45. 避免消息总线过度拥挤"></a>45. 避免消息总线过度拥挤</h4><p>不要发布一切消息到消息总线中，要判断消息价值几何！</p><p>选择消息队列框架，需要根据需求和使用场景筛选。</p><h3 id="意犹未尽"><a href="#意犹未尽" class="headerlink" title="意犹未尽"></a>意犹未尽</h3><h4 id="46-警惕第三方方案"><a href="#46-警惕第三方方案" class="headerlink" title="46. 警惕第三方方案"></a>46. 警惕第三方方案</h4><p>第三方服务慎重选择！不要过度依赖供应商的产品，要进行多地灾备！</p><h4 id="47-梯形存储策略"><a href="#47-梯形存储策略" class="headerlink" title="47. 梯形存储策略"></a>47. 梯形存储策略</h4><p>将存储成本和数据价值匹配，应用RFM营销概念，即近因、频率、货币化分析！</p><p><img src="RFM%E7%9A%84AKF%E7%AB%8B%E6%96%B9%E4%BD%93.png" alt></p><p><img src="RFM%E7%9A%84%E4%BB%B7%E5%80%BC%E6%88%90%E6%9C%AC%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png" alt></p><h4 id="48-分类处理不同负载"><a href="#48-分类处理不同负载" class="headerlink" title="48 分类处理不同负载"></a>48 分类处理不同负载</h4><p>归纳、演绎、批处理、用户产品交互(OLTP)</p><h4 id="49-完善监控"><a href="#49-完善监控" class="headerlink" title="49. 完善监控"></a>49. 完善监控</h4><p>设计监控系统，结合日志进行考虑。</p><p><img src="%E7%9B%91%E6%8E%A7%E8%8C%83%E5%9B%B4%E4%B8%89%E8%A7%92%E5%BD%A2.png" alt></p><h4 id="50-保持竞争力"><a href="#50-保持竞争力" class="headerlink" title="50. 保持竞争力"></a>50. 保持竞争力</h4><p>架构的每个部分都有竞争力，而且对于团队而言，补齐短板，均衡发展！</p><h3 id="谋定而动"><a href="#谋定而动" class="headerlink" title="谋定而动"></a>谋定而动</h3><p><img src="%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E9%A3%8E%E9%99%A9%E5%88%86%E6%9E%90.png" alt></p><h2 id="最终思考路线图"><a href="#最终思考路线图" class="headerlink" title="最终思考路线图"></a>最终思考路线图</h2><p>整体套用AKF三角形！</p><p>业务拆分-&gt;数据库优化-&gt;故障隔离-&gt;异地多活-&gt;缓存应用-&gt;存储系统-&gt;负载均衡-&gt;消息队列-&gt;前端优化-&gt;监控系统</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="%E3%80%8A%E6%9E%B6%E6%9E%84%E7%9C%9F%E7%BB%8F%E3%80%8B%E6%9C%80%E7%BB%88%E8%84%91%E5%9B%BE.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> READ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书心得 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
